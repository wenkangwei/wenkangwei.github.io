<!doctype html>
<html lang="en"><head><meta charset="utf-8"><meta name="generator" content="Hexo 4.2.1"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta><title>NLP Word Representations - Wenkang&#039;s Blog</title><meta description=""><meta property="og:type" content="article"><meta property="og:title" content="NLP Word Representations"><meta property="og:url" content="https://github.com/wenkangwei/"><meta property="og:site_name" content="Wenkang&#039;s Blog"><meta property="og:locale" content="en_US"><meta property="og:image" content="http://mccormickml.com/assets/word2vec/training_data.png"><meta property="og:image" content="http://mccormickml.com/assets/word2vec/skip_gram_net_arch.png"><meta property="og:image" content="http://mccormickml.com/assets/word2vec/matrix_mult_w_one_hot.png"><meta property="og:image" content="http://mccormickml.com/assets/word2vec/output_weights_function.png"><meta property="article:published_time" content="2020-07-08T03:40:19.000Z"><meta property="article:modified_time" content="2020-08-24T22:30:34.027Z"><meta property="article:author" content="Wenkang Wei"><meta property="article:tag" content="Nature Language Processing"><meta property="article:tag" content="Word vector"><meta property="article:tag" content="Natural Language Representations"><meta property="twitter:card" content="summary"><meta property="twitter:image" content="http://mccormickml.com/assets/word2vec/training_data.png"><script type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"https://github.com/wenkangwei/wenkangwei.github.io%60/2020/07/07/NLP-Word-Representation/"},"headline":"Wenkang's Blog","image":["http://mccormickml.com/assets/word2vec/training_data.png","http://mccormickml.com/assets/word2vec/skip_gram_net_arch.png","http://mccormickml.com/assets/word2vec/matrix_mult_w_one_hot.png","http://mccormickml.com/assets/word2vec/output_weights_function.png"],"datePublished":"2020-07-08T03:40:19.000Z","dateModified":"2020-08-24T22:30:34.027Z","author":{"@type":"Person","name":"Wenkang Wei"},"description":""}</script><link rel="canonical" href="https://github.com/wenkangwei/wenkangwei.github.io%60/2020/07/07/NLP-Word-Representation/"><link rel="icon" href="/images/icon.png"><link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.12.0/css/all.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@9.12.0/styles/atom-one-light.css"><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Ubuntu:wght@400;600&amp;family=Source+Code+Pro"><link rel="stylesheet" href="/css/default.css"><style>body>.footer,body>.navbar,body>.section{opacity:0}</style><!--!--><!--!--><!--!--><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/lightgallery@1.6.8/dist/css/lightgallery.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/justifiedGallery@3.7.0/dist/css/justifiedGallery.min.css"><!--!--><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/pace-js@1.0.2/pace.min.js"></script></head><body class="is-3-column"><nav class="navbar navbar-main"><div class="container"><div class="navbar-brand justify-content-center"><a class="navbar-item navbar-logo" href="/"><img src="/images/icon.png" alt="Wenkang&#039;s Blog" height="28"></a></div><div class="navbar-menu"><div class="navbar-start"><a class="navbar-item" href="/">Home</a><a class="navbar-item" href="/archives">Archives</a><a class="navbar-item" href="/categories">Categories</a><a class="navbar-item" href="/tags">Tags</a><a class="navbar-item" href="/about">About</a></div><div class="navbar-end"><a class="navbar-item" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/ppoffice/hexo-theme-icarus"><i class="fab fa-github"></i></a><a class="navbar-item is-hidden-tablet catalogue" title="Catalogue" href="javascript:;"><i class="fas fa-list-ul"></i></a><a class="navbar-item search" title="Search" href="javascript:;"><i class="fas fa-search"></i></a></div></div></div></nav><section class="section"><div class="container"><div class="columns"><div class="column order-2 column-main is-8-tablet is-8-desktop is-6-widescreen"><div class="card"><article class="card-content article" role="article"><div class="article-meta size-small is-uppercase level is-mobile"><div class="level-left"><time class="level-item" dateTime="2020-07-08T03:40:19.000Z" title="2020-07-08T03:40:19.000Z">2020-07-07</time><span class="level-item"><a class="link-muted" href="/categories/NLP/">NLP</a><span> / </span><a class="link-muted" href="/categories/NLP/Machine-Learning/">Machine Learning</a></span><span class="level-item">11 minutes read (About 1675 words)</span></div></div><h1 class="title is-3 is-size-4-mobile">NLP Word Representations</h1><div class="content"><image src="https://gst-online.com/wp-content/uploads/2018/07/16679084-abstract-word-cloud-for-representation-with-related-tags-and-terms.jpg">

<a id="more"></a>
<h2 id="1-Representation"><a href="#1-Representation" class="headerlink" title="1. Representation:"></a>1. Representation:</h2><h3 id="1-1-WordNet"><a href="#1-1-WordNet" class="headerlink" title="1.1 WordNet"></a>1.1 WordNet</h3><p>a  thesaurus containing lists of synonyms and hpernyms, using “is-a” to denote the relationship among words.</p>
<ul>
<li><p><strong>Advantages</strong>:</p>
<ul>
<li>easy to understand the relationship. It is interpretable.  </li>
</ul>
</li>
<li><p><strong>Disadvantages</strong>:</p>
<ul>
<li>good as resource, but missing nuance in different context. Eg: “proficient” doesn’t always mean “good”</li>
<li>Hard to update with new meanings of words, since the meanings of words may change along time</li>
<li>subjective and bias</li>
<li>Require Human to label</li>
<li><strong>Can’t compute accurate word similarity</strong> <br>

</li>
</ul>
</li>
</ul>
<h3 id="1-2-One-hot"><a href="#1-2-One-hot" class="headerlink" title="1.2 One-hot"></a>1.2 One-hot</h3><p>Represent word as discrete symbol, eg. 0 or 1<br>Example: In sentence “I like dog”, the vector for “I” =[1,0,0], the vector for “like” =[0,1,0],the vector for “dog” =[0,0,1] </p>
<table>
<thead>
<tr>
<th>word</th>
<th>one-hot vector</th>
</tr>
</thead>
<tbody><tr>
<td>“I”</td>
<td>1 0 0</td>
</tr>
<tr>
<td>“like”</td>
<td>0 1 0</td>
</tr>
<tr>
<td>“Dog”</td>
<td>0 0 1</td>
</tr>
</tbody></table>
<p><strong>Note:</strong></p>
<ol>
<li><strong>Usually, the number of of vocabulary is equal to the number of entry in the one-hot vector.</strong> This enables the (Manhatton) distance between any two words/labels is same and the feature of each word is independent with each other.<br>The feature of a word contains no information about another word and hence don’t affect other words. Moreover, this method also has physical meaning and easy to understand.<br><strong>For example:</strong><br>Assume in linear regression,<br>we have $y =w_1x_1 + w_2x_2 + w_3x_3 $, in vector representation it is $Y = [w_1, w_2, w_3] \cdot [x_1, x_2, x_3]^T = WX$<br>Let vector X be a one-hot vector. That is, one of $x_1, x_2, x_3$ must be one. In this case, $WX$ becomes a lookup table to choose which weight $w_1, w_2, w_3$ should be learned. In this case, weight $w_1$ is affected by feature $x_1$ only, without being affected other features. This can lead to faster convergence of $w_1$ in learning .</li>
</ol>
<ul>
<li><strong>Advantage</strong>:<ul>
<li>able to represent words into number for computing</li>
<li>Easy to understand, since it has physical meaning</li>
<li>Features/ labels are independent from each other</li>
</ul>
</li>
<li><strong>Disadvantage</strong>:<ul>
<li>Vector dimension equal to the number of words in vocabulary, it could be very big</li>
<li>No natural notion of similarity, since word vectors are orthogonal.</li>
<li>Hard to extend the representation of labels when more labels are added.</li>
<li>Could be memory expensive when a large group of labels involved. Eg, represent vocabulary in a book as one-hot vector, the vectors could be very large and sparse. <br>

</li>
</ul>
</li>
</ul>
<h3 id="1-3-Bag-of-Words-BOW"><a href="#1-3-Bag-of-Words-BOW" class="headerlink" title="1.3 Bag of Words (BOW)"></a>1.3 Bag of Words (BOW)</h3><p>Similar to one-hot, it represents each word as orthogonal vector, but the number of vector is the frequency the word appears. In BOW, a text (such as a sentence or a document) is represented as the bag (multiset)/set of its words, disregarding grammar and even word order but keeping multiplicity. </p>
<ul>
<li><strong>Advantage:</strong><ul>
<li>Easy to realize and compute</li>
<li>Easy to use for simple case (the order of words don’t matter a lot)</li>
</ul>
</li>
<li><strong>Disadvantage:</strong><ul>
<li>Without considering similarity among different words</li>
<li>it has the same drawbacks as one-hot vector<br>

</li>
</ul>
</li>
</ul>
<h3 id="1-4-n-gram-model"><a href="#1-4-n-gram-model" class="headerlink" title="1.4 n-gram model"></a>1.4 n-gram model</h3><p><strong>N-gram model</strong> is an extension of bag of word model. While bag-of-word model considers each word only, N-gram model considers a tuple of n consecutive words as an element in the collection of words.</p>
<p><strong>Example:</strong>  Consider a sentence: <em>“a boy is playing grames”</em>.</p>
<p>When using 3-gram model, the collection of words becomes    </p>
<pre><code>{(a boy is), (boy is playing),(is playing games)}
</code></pre>
<p>Then if we have a 3-gramm vector for a document: [1, 0,0 ], it means the element (a boy is) appear once in the document.</p>
<ul>
<li><strong>Advantage:</strong><ul>
<li>More flexible and robust than bag-of-world model, since it could considers different structure of words. Some names like “deep learning”, “machine learning” with more than one words can be detected easier.</li>
</ul>
</li>
<li><strong>Disadvantage:</strong><ul>
<li>The collection could be large when using different grams. Usually, bigram and trigram are used.<br>

</li>
</ul>
</li>
</ul>
<h3 id="1-5-Word-Vectors-Word-Embeddings-Word-distributed-representations"><a href="#1-5-Word-Vectors-Word-Embeddings-Word-distributed-representations" class="headerlink" title="1.5 Word Vectors/ Word Embeddings/Word distributed representations"></a>1.5 Word Vectors/ Word Embeddings/Word distributed representations</h3><p><strong>Word Vectors</strong> represent words by context. Context of a word is a set of words that appear nearby, or in a fixed window  A word meaning is given by the words that frequently appear close-by. </p>
<p>Each entry in a word vector for a word is the similarity between this center word and the words in context.</p>
<p>Eg:  In “I like  dog”, vector for “like” = [0.01,0.7 ,0.5], where 0.5 is the possibility “dog” will occur, given center word “like”.</p>
<img src=/images/NLP/word2vec-1.jpg>

<p>More general, given the center word $w_{t}$, the possibility of context word $w_{t+1}$ will occur is $P(w_{t+1}|w_{t} )$. Then in word vector, our goal is to find the best condition distribution to represent the vector. There are many frameworks to find word vector,  such as <strong>word2Vec, Glove, Fasttext</strong>.</p>
<ul>
<li><strong>Advantage</strong>:<ul>
<li>Able to compute similarity between words</li>
<li>No need to label by human</li>
</ul>
</li>
<li><strong>Disadvantage</strong>:<ul>
<li>Dimension of vector is equal to the number of words in vocabulary. It could be very large</li>
<li>Need to learn the similarity by word2vector framework. Hard to compute similarity when there is a large corpus of words</li>
</ul>
</li>
</ul>
<br>

<h2 id="2-Word2Vector-Skip-gram"><a href="#2-Word2Vector-Skip-gram" class="headerlink" title="2. Word2Vector:  Skip-gram"></a>2. Word2Vector:  Skip-gram</h2><p>A framework to learn and find dense word vectors, rather than sparse orthogonal vector like bag-of-word model. Here is details about <a href="http://web.stanford.edu/class/cs224n/slides/cs224n-2020-lecture01-wordvecs1.pdf">word2vector</a></p>
<p>The dense word vectors measure similarity between two words. If two words are similar, then they have similar word vectors.</p>
<p><strong>Goal: to learn the word vector that measure the similarity between context and center word, Or possibility that the next word will occur, given the center word.</strong></p>
<h3 id="2-1-Summary-of-Idea-in-Word2Vector"><a href="#2-1-Summary-of-Idea-in-Word2Vector" class="headerlink" title="2.1 Summary of Idea in Word2Vector:"></a>2.1 <strong>Summary of Idea in Word2Vector</strong>:</h3><ol>
<li>Collect a large corpus of text</li>
<li>Represent <strong>each word</strong> in text as <strong>a vector</strong> (<strong>count vector</strong> or one-hot model)</li>
<li>Go through each position t in text, find center word c and outside words o (context words)</li>
<li>Find <strong>similarity of the word vectors</strong> for c and o  (c and o are actually from the output of the neural network),  compute possibility of context o, given center word vector c</li>
<li>Keep adjusting word vectors / train the network to <strong>maximize possibility/likelihood function</strong>  and find the optimal word vectors that contains similarity between c and o.</li>
<li><strong>Note: input to network is one-hot vector , output to network is similarity/possibility vector. The output is the vector we want</strong></li>
</ol>
<h3 id="2-2-One-hot-Vector"><a href="#2-2-One-hot-Vector" class="headerlink" title="2.2 One-hot Vector"></a>2.2 <strong>One-hot Vector</strong></h3><ul>
<li>In Skip gram, it first find the set of vocabularies and then converts each word into its one-hot vector. In one-hot vector, 1 represents the word</li>
</ul>
<br>

<ul>
<li><p>Example:<br>consider a sentence “I like dog and you like cat”.<br>Then there is a set of words [“I”,”you” ,”like””,”dog”,”cat”,”and”]. </p>
<table>
<thead>
<tr>
<th>word</th>
<th>one-hot vector</th>
</tr>
</thead>
<tbody><tr>
<td>“I”</td>
<td>[1,0,0,0,0,0]</td>
</tr>
<tr>
<td>“like”</td>
<td>[0,1,0,0,0,0]</td>
</tr>
<tr>
<td>“dog”</td>
<td>[0,0,1,0,0,0]</td>
</tr>
<tr>
<td>“and”</td>
<td>[0,0,0,1,0,0]</td>
</tr>
<tr>
<td>…</td>
<td>…</td>
</tr>
</tbody></table>
<p>  Then one-hot vector for “like” = [0,0,1,0,0,0], where 1 at the corresponding position represents “like” in the word set.<br>  Dimension of vector = |V| , where V is vocabulary set of the text.</p>
</li>
</ul>
<br>

<h3 id="2-3-Skip-gram-neural-network"><a href="#2-3-Skip-gram-neural-network" class="headerlink" title="2.3 Skip-gram neural network"></a>2.3 <strong>Skip-gram neural network</strong></h3><!-- <img src="http://mccormickml.com/assets/word2vec/training_data.png"> -->
<img src="http://mccormickml.com/assets/word2vec/skip_gram_net_arch.png">


<ul>
<li><p>The neural network takes one-hot vector as input, with size |V|. Let denote one-hot vector as <strong>v</strong> for convenience</p>
</li>
<li><p>There are only one hidden layer. <strong>There is no activation function in this layer and hence it is linear</strong>. The number of neurons <strong>N</strong>  is defined by user.</p>
</li>
<li><p>Softmax activation function for output with <strong>|V| neurons</strong></p>
</li>
<li><p>There is a input weight matrix <strong>W</strong> with dimension <strong>|V|-by-N</strong> between input layer and hidden layer (|V| rows and N columns)</p>
</li>
<li><p>There is a input weight matrix <strong>W ‘</strong> with dimension <strong>N-by-|V|</strong> between hidden layer and output layer</p>
</li>
<li><p>Since input is 0,1 vector, when it times input matrix <strong>W</strong>, it actually selects a row of the matrix.<br><em>Hence, we can consider <strong>W</strong> as a look-up table and output from hidden layer <strong>c = Wv</strong> is the real “word vector”</em>.</p>
</li>
<li><p>“Word vector”  is acutually one-hot vector or bag-of-word vector times input weight matrix <strong>W</strong></p>
</li>
</ul>
<img src="http://mccormickml.com/assets/word2vec/matrix_mult_w_one_hot.png" width= 400 height= 80>

<br>

<ul>
<li>The i^th entry in the output vector from softmax function is the possibility that if you pick up a word nearby the input word, that is the i^th word in the vocabulary.</li>
</ul>
<img src="http://mccormickml.com/assets/word2vec/output_weights_function.png">

<br>

<h4 id="Note"><a href="#Note" class="headerlink" title="Note:"></a>Note:</h4><ol>
<li>when <strong>training</strong> neural network, its <strong>output is one-hot</strong> vector ( set the maximum softmax output value as 1, others as 0s), indicating the predicted nearby context words</li>
<li>When <strong>evaluating</strong> network, using softmax output value, <strong>possibility as output</strong>, to compute the cost value</li>
<li>__Since there are different actual context words $w_{t-1}, w_{t-2}, …$, corresponding to the single input vector $w_{t}$.__<br>If we consider P($w_{t-1},w_{t-2},w_{t-3}|w_t$) with window with size of 4 containing central word $w_t$  and context words $w_{t-1},w_{t-2},w_{t-3}$,<br>then the output look something like this:<img src=/images/NLP/word2vec-3.jpg>

</li>
</ol>
<br>

<h3 id="2-4-Evaluation-of-Skip-gram"><a href="#2-4-Evaluation-of-Skip-gram" class="headerlink" title="2.4 Evaluation of Skip-gram"></a>2.4 <strong>Evaluation of Skip-gram</strong></h3><p>Since the input to the network is a single word, it is also regarded as center word $w_t$, at the <strong>t</strong> position in text. The words nearby it are called context, or outside words. The context word at <strong>t+k</strong> position, is denoted as $w_{t+k}$. </p>
<p>Hence our goal, or objective is to estimate the distribution $P(w_{t+k}| w_{t})$.<br>The distribution has the meaning that given center word $w_t$, the possibility that context word $w_{t+k}$ will appears.<br><img src=/images/NLP/word2vec-1.jpg></p>
<br>

<p>In order to estimate the distribution, we need to maximize the likelihood function, or its log value, called loss, or cost function, shown as below.</p>
<img src=/images/NLP/word2vec-2.jpg>

<br>

<h4 id="Advantages"><a href="#Advantages" class="headerlink" title="Advantages"></a><strong>Advantages</strong></h4><ul>
<li>It is <strong>unsupervised learning</strong> hence can work on any raw text given.</li>
<li>It requires <strong>less memory</strong> comparing with other words to vector representations.</li>
<li>It requires two weight matrix of dimension [N, |v|] each instead of [|v|, |v|]. And <strong>usually, N is around 300 while |v| is in millions.</strong> So, we can see the advantage of using this algorithm.</li>
</ul>
<h4 id="Disadvantages"><a href="#Disadvantages" class="headerlink" title="Disadvantages"></a><strong>Disadvantages</strong></h4><ul>
<li>Finding the best value for N and the context position is difficult.</li>
<li>Softmax function is computationally expensive.</li>
<li>The time complexity for training is high</li>
</ul>
<br>

<h3 id="Models-for-Word2vector"><a href="#Models-for-Word2vector" class="headerlink" title="Models for Word2vector"></a>Models for Word2vector</h3><ol>
<li><strong>Skip-gram</strong><br>Predict context (outside) words (surrounding the center word) given center<br>word</li>
<li><strong>Continuous Bag of words （CBOW）</strong><br>Predict center word from (bag of) context words. It is an inverse version of skip-gram.</li>
</ol>
<img src=/images/NLP/CBOW.jpg>

<br>

<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference:"></a>Reference:</h2><p>[1] <a href="http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/">http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/</a><br>[2] <a href="http://web.stanford.edu/class/cs224n/slides/cs224n-2020-lecture02-wordvecs2.pdf">http://web.stanford.edu/class/cs224n/slides/cs224n-2020-lecture02-wordvecs2.pdf</a><br>[3] <a href="https://towardsdatascience.com/skip-gram-nlp-context-words-prediction-algorithm-5bbf34f84e0c">https://towardsdatascience.com/skip-gram-nlp-context-words-prediction-algorithm-5bbf34f84e0c</a><br>[4] <a href="https://zhuanlan.zhihu.com/p/50243702">https://zhuanlan.zhihu.com/p/50243702</a><br>[5] <a href="http://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf">http://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf</a></p>
</div><div class="article-tags size-small mb-4"><span class="mr-2">#</span><a class="link-muted mr-2" rel="tag" href="/tags/Nature-Language-Processing/">Nature Language Processing</a><a class="link-muted mr-2" rel="tag" href="/tags/Word-vector/">Word vector</a><a class="link-muted mr-2" rel="tag" href="/tags/Natural-Language-Representations/">Natural Language Representations</a></div><div class="sharethis-inline-share-buttons"></div><script src="https://ppoffice.github.io/hexo-theme-icarus-categorites-Plugins/Share/" defer></script></article></div><!--!--><nav class="post-navigation mt-4 level is-mobile"><div class="level-start"><a class="article-nav-prev level level-item link-muted" href="/2020/07/08/Markdown-Tutorial/"><i class="level-item fas fa-chevron-left"></i><span class="level-item">Markdown Quick Tutorial</span></a></div><div class="level-end"><a class="article-nav-next level level-item link-muted" href="/2020/06/30/hexo-gitpage-website/"><span class="level-item">Use Hexo + ICarus to build a website</span><i class="level-item fas fa-chevron-right"></i></a></div></nav><div class="card"><div class="card-content"><h3 class="title is-5">Comments</h3><div class="content" id="valine-thread"></div><script src="//cdn1.lncld.net/static/js/3.0.4/av-min.js"></script><script src="https://cdn.jsdelivr.net/npm/valine@1.4.14/dist/Valine.min.js"></script><script>new Valine({
            el: '#valine-thread' ,
            appId: "kvXCKmDNxnA2486N29e5cP7i-MdYXbMMI",
            appKey: "0Lv5b0SqotzkGHQvD64u4AKo",
            
            avatar: "mm",
            
            meta: ["nick","mail","link"],
            pageSize: 10,
            lang: "en",
            
            highlight: true,
            
            
            
            
            
            requiredFields: [],
        });</script></div></div></div><div class="column column-left is-4-tablet is-4-desktop is-3-widescreen  order-1"><div class="card widget"><div class="card-content"><div class="menu"><h3 class="menu-label">Links</h3><ul class="menu-list"><li><a class="level is-mobile is-mobile" href="https://github.com/wenkangwei" target="_blank" rel="noopener"><span class="level-left"><span class="level-item">Github</span></span><span class="level-right"><span class="level-item tag">github.com</span></span></a></li></ul></div></div></div><div class="card widget"><div class="card-content"><div class="menu"><h3 class="menu-label">Categories</h3><ul class="menu-list"><li><a class="level is-mobile is-marginless" href="/categories/Data-Collection/"><span class="level-start"><span class="level-item">Data Collection</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile is-marginless" href="/categories/Data-Structure/"><span class="level-start"><span class="level-item">Data Structure</span></span><span class="level-end"><span class="level-item tag">2</span></span></a><ul class="mr-0"><li><a class="level is-mobile is-marginless" href="/categories/Data-Structure/Sorting/"><span class="level-start"><span class="level-item">Sorting</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></li><li><a class="level is-mobile is-marginless" href="/categories/Deep-Learning/"><span class="level-start"><span class="level-item">Deep Learning</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile is-marginless" href="/categories/Linux/"><span class="level-start"><span class="level-item">Linux</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile is-marginless" href="/categories/Machine-Learning/"><span class="level-start"><span class="level-item">Machine Learning</span></span><span class="level-end"><span class="level-item tag">5</span></span></a><ul class="mr-0"><li><a class="level is-mobile is-marginless" href="/categories/Machine-Learning/Accuracy-Improvement/"><span class="level-start"><span class="level-item">Accuracy Improvement</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></li><li><a class="level is-mobile is-marginless" href="/categories/NLP/"><span class="level-start"><span class="level-item">NLP</span></span><span class="level-end"><span class="level-item tag">2</span></span></a><ul class="mr-0"><li><a class="level is-mobile is-marginless" href="/categories/NLP/Machine-Learning/"><span class="level-start"><span class="level-item">Machine Learning</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li></ul></li><li><a class="level is-mobile is-marginless" href="/categories/Parallel-Computing/"><span class="level-start"><span class="level-item">Parallel Computing</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile is-marginless" href="/categories/Programming/"><span class="level-start"><span class="level-item">Programming</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile is-marginless" href="/categories/PySpark/"><span class="level-start"><span class="level-item">PySpark</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile is-marginless" href="/categories/Recommendation-System/"><span class="level-start"><span class="level-item">Recommendation System</span></span><span class="level-end"><span class="level-item tag">6</span></span></a></li><li><a class="level is-mobile is-marginless" href="/categories/Report/"><span class="level-start"><span class="level-item">Report</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile is-marginless" href="/categories/Searching/"><span class="level-start"><span class="level-item">Searching</span></span><span class="level-end"><span class="level-item tag">1</span></span></a><ul class="mr-0"><li><a class="level is-mobile is-marginless" href="/categories/Searching/Data-Strucure/"><span class="level-start"><span class="level-item">Data Strucure</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></li><li><a class="level is-mobile is-marginless" href="/categories/Statistic/"><span class="level-start"><span class="level-item">Statistic</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile is-marginless" href="/categories/Web/"><span class="level-start"><span class="level-item">Web</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></div></div></div><div class="card widget"><div class="card-content"><div class="menu"><h3 class="menu-label">Tags</h3><div class="field is-grouped is-grouped-multiline"><div class="control"><a class="tags has-addons" href="/tags/Amdahl-s-Law/"><span class="tag">Amdahl&#039;s Law</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Backpropagation/"><span class="tag">Backpropagation</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Bagging/"><span class="tag">Bagging</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/BeautifulSoup/"><span class="tag">BeautifulSoup</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Binary-Tree/"><span class="tag">Binary Tree</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Blending/"><span class="tag">Blending</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Boosting/"><span class="tag">Boosting</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Boosting-Machine/"><span class="tag">Boosting Machine</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/BuckSort/"><span class="tag">BuckSort</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/C/"><span class="tag">C++</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Coding/"><span class="tag">Coding</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Collaborative-Filtering/"><span class="tag">Collaborative Filtering</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Confusion-Metric/"><span class="tag">Confusion Metric</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Convolution-Neural-Network/"><span class="tag">Convolution Neural Network</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Cross-Validation/"><span class="tag">Cross Validation</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/D3-js/"><span class="tag">D3.js</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/DIN/"><span class="tag">DIN</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Data-Analysis/"><span class="tag">Data Analysis</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Datawhale-Team-Learning/"><span class="tag">Datawhale Team Learning</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/DeepFM/"><span class="tag">DeepFM</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Dropout/"><span class="tag">Dropout</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Ensemble-Learning/"><span class="tag">Ensemble Learning</span><span class="tag is-grey-lightest">3</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Hexo/"><span class="tag">Hexo</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Holdout/"><span class="tag">Holdout</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Hypothesis-Test/"><span class="tag">Hypothesis Test</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Icarus/"><span class="tag">Icarus</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Independence-Test/"><span class="tag">Independence Test</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/JavaScript/"><span class="tag">JavaScript</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/K-Mean-Clustering/"><span class="tag">K-Mean Clustering</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/KMean-Clustering/"><span class="tag">KMean Clustering</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/LGBM/"><span class="tag">LGBM</span><span class="tag is-grey-lightest">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Leetcode/"><span class="tag">Leetcode</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Machine-Learning/"><span class="tag">Machine Learning</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Markdown/"><span class="tag">Markdown</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Model-Evaluation/"><span class="tag">Model Evaluation</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Model-Selection/"><span class="tag">Model Selection</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Natural-Language-Representations/"><span class="tag">Natural Language Representations</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Nature-Language-Processing/"><span class="tag">Nature Language Processing</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Negative-Sampling/"><span class="tag">Negative Sampling</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/NeuralFM/"><span class="tag">NeuralFM</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/P-value/"><span class="tag">P-value</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Parallel-Computing/"><span class="tag">Parallel Computing</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Parallel-Speedup/"><span class="tag">Parallel Speedup</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/PySpark/"><span class="tag">PySpark</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/ROC-curve/"><span class="tag">ROC curve</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Recommendation-System/"><span class="tag">Recommendation System</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Regression/"><span class="tag">Regression</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Regular-Expression/"><span class="tag">Regular Expression</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Sorting/"><span class="tag">Sorting</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Stacking/"><span class="tag">Stacking</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Traversal/"><span class="tag">Traversal</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Unsupervised-Learning/"><span class="tag">Unsupervised Learning</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Variable-Selection/"><span class="tag">Variable Selection</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Web-Scrapping/"><span class="tag">Web Scrapping</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Wide-and-Deep/"><span class="tag">Wide and Deep</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Wide-and-Deep-Model/"><span class="tag">Wide and Deep Model</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Wide-Deep-Model/"><span class="tag">Wide&amp;Deep Model</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Word-Embedding/"><span class="tag">Word Embedding</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Word-vector/"><span class="tag">Word vector</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/binary-search/"><span class="tag">binary search</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/bubble-sort/"><span class="tag">bubble sort</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/commands/"><span class="tag">commands</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/insertion-sort/"><span class="tag">insertion sort</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/merge-sort/"><span class="tag">merge sort</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/non-parametric-learning/"><span class="tag">non-parametric learning</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/quick-sort/"><span class="tag">quick sort</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/review/"><span class="tag">review</span><span class="tag is-grey-lightest">1</span></a></div></div></div></div></div><div class="card widget"><div class="card-content"><h3 class="menu-label">Recent</h3><article class="media"><div class="media-content size-small"><p><time dateTime="2021-05-11T17:53:43.000Z">2021-05-11</time></p><p class="title is-6"><a class="link-muted" href="/2021/05/11/ensemble-learning-blending/">Ensemble Learning - Blending</a></p><p class="is-uppercase"><a class="link-muted" href="/categories/Machine-Learning/">Machine Learning</a></p></div></article><article class="media"><div class="media-content size-small"><p><time dateTime="2021-05-07T04:54:08.000Z">2021-05-07</time></p><p class="title is-6"><a class="link-muted" href="/2021/05/07/LeetCode/">LeetCode 笔记</a></p><p class="is-uppercase"></p></div></article><article class="media"><div class="media-content size-small"><p><time dateTime="2021-03-27T17:59:45.000Z">2021-03-27</time></p><p class="title is-6"><a class="link-muted" href="/2021/03/27/Recommendation-System-5-DIN/">Recommendation-System-5-DIN</a></p><p class="is-uppercase"><a class="link-muted" href="/categories/Recommendation-System/">Recommendation System</a></p></div></article><article class="media"><div class="media-content size-small"><p><time dateTime="2021-03-24T17:14:15.000Z">2021-03-24</time></p><p class="title is-6"><a class="link-muted" href="/2021/03/24/Recommendation-System-4-NFM/">Recommendation-System-4-NFM</a></p><p class="is-uppercase"><a class="link-muted" href="/categories/Recommendation-System/">Recommendation System</a></p></div></article><article class="media"><div class="media-content size-small"><p><time dateTime="2021-03-21T05:57:56.000Z">2021-03-21</time></p><p class="title is-6"><a class="link-muted" href="/2021/03/21/Recommendation-System-3-DeepFM/">Recommendation-System-3-DeepFM</a></p><p class="is-uppercase"><a class="link-muted" href="/categories/Recommendation-System/">Recommendation System</a></p></div></article></div></div><div class="card widget"><div class="card-content"><div class="menu"><h3 class="menu-label">Archives</h3><ul class="menu-list"><li><a class="level is-mobile is-marginless" href="/archives/2021/05/"><span class="level-start"><span class="level-item">May 2021</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile is-marginless" href="/archives/2021/03/"><span class="level-start"><span class="level-item">March 2021</span></span><span class="level-end"><span class="level-item tag">5</span></span></a></li><li><a class="level is-mobile is-marginless" href="/archives/2021/02/"><span class="level-start"><span class="level-item">February 2021</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile is-marginless" href="/archives/2020/12/"><span class="level-start"><span class="level-item">December 2020</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile is-marginless" href="/archives/2020/11/"><span class="level-start"><span class="level-item">November 2020</span></span><span class="level-end"><span class="level-item tag">5</span></span></a></li><li><a class="level is-mobile is-marginless" href="/archives/2020/10/"><span class="level-start"><span class="level-item">October 2020</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile is-marginless" href="/archives/2020/09/"><span class="level-start"><span class="level-item">September 2020</span></span><span class="level-end"><span class="level-item tag">5</span></span></a></li><li><a class="level is-mobile is-marginless" href="/archives/2020/08/"><span class="level-start"><span class="level-item">August 2020</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile is-marginless" href="/archives/2020/07/"><span class="level-start"><span class="level-item">July 2020</span></span><span class="level-end"><span class="level-item tag">7</span></span></a></li><li><a class="level is-mobile is-marginless" href="/archives/2020/06/"><span class="level-start"><span class="level-item">June 2020</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li></ul></div></div></div><div class="card widget"><div class="card-content"><div class="menu"><h3 class="menu-label">Subscribe to Updates</h3><form action="https://feedburner.google.com/fb/a/mailverify" method="post" target="popupwindow" onsubmit="window.open(&#039;https://feedburner.google.com/fb/a/mailverify?uri=&#039;,&#039;popupwindow&#039;,&#039;scrollbars=yes,width=550,height=520&#039;);return true"><input type="hidden" value="" name="uri"><input type="hidden" name="loc" value="en_US"><div class="field has-addons"><div class="control has-icons-left is-expanded"><input class="input" name="email" type="email" placeholder="Email"><span class="icon is-small is-left"><i class="fas fa-envelope"></i></span></div><div class="control"><input class="button is-primary" type="submit" value="Subscribe"></div></div></form></div></div></div><div class="column-right-shadow is-hidden-widescreen"></div></div><div class="column column-right is-4-tablet is-4-desktop is-3-widescreen is-hidden-touch is-hidden-desktop-only order-3"><div class="card widget"><div class="card-content"><nav class="level"><div class="level-item has-text-centered flex-shrink-1"><div><figure class="image is-128x128 mx-auto mb-2"><img class="avatar is-rounded" src="/images/avatar.jpg" alt="Wenkang Wei"></figure><p class="title is-size-4 is-block line-height-inherit">Wenkang Wei</p><p class="is-size-6 is-block">computer engineering| machine learning</p><p class="is-size-6 is-flex justify-content-center"><i class="fas fa-map-marker-alt mr-1"></i><span>Clemson,SC</span></p></div></div></nav><nav class="level is-mobile"><div class="level-item has-text-centered is-marginless"><div><p class="heading">Posts</p><a href="/archives"><p class="title">33</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">Categories</p><a href="/categories"><p class="title">18</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">Tags</p><a href="/tags"><p class="title">67</p></a></div></div></nav><div class="level"><a class="level-item button is-primary is-rounded" href="https://github.com/wenkangwei" target="_blank" rel="noopener">Follow</a></div><div class="level is-mobile"><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Github" href="https://github.com/wenkangwei"><i class="fab fa-github"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="LinkedIn" href="https://www.linkedin.com/in/wenkang-wei-588811167"><i class="fab fa-linkedin"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Facebook" href="https://facebook.com"><i class="fab fa-facebook"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Twitter" href="https://twitter.com"><i class="fab fa-twitter"></i></a></div></div></div><div class="card widget" id="toc"><div class="card-content"><div class="menu"><h3 class="menu-label">Catalogue</h3><ul class="menu-list"><li><a class="is-flex" href="#1-Representation"><span class="mr-2">1</span><span>1. Representation:</span></a><ul class="menu-list"><li><a class="is-flex" href="#1-1-WordNet"><span class="mr-2">1.1</span><span>1.1 WordNet</span></a></li><li><a class="is-flex" href="#1-2-One-hot"><span class="mr-2">1.2</span><span>1.2 One-hot</span></a></li><li><a class="is-flex" href="#1-3-Bag-of-Words-BOW"><span class="mr-2">1.3</span><span>1.3 Bag of Words (BOW)</span></a></li><li><a class="is-flex" href="#1-4-n-gram-model"><span class="mr-2">1.4</span><span>1.4 n-gram model</span></a></li><li><a class="is-flex" href="#1-5-Word-Vectors-Word-Embeddings-Word-distributed-representations"><span class="mr-2">1.5</span><span>1.5 Word Vectors/ Word Embeddings/Word distributed representations</span></a></li></ul></li><li><a class="is-flex" href="#2-Word2Vector-Skip-gram"><span class="mr-2">2</span><span>2. Word2Vector:  Skip-gram</span></a><ul class="menu-list"><li><a class="is-flex" href="#2-1-Summary-of-Idea-in-Word2Vector"><span class="mr-2">2.1</span><span>2.1 Summary of Idea in Word2Vector:</span></a></li><li><a class="is-flex" href="#2-2-One-hot-Vector"><span class="mr-2">2.2</span><span>2.2 One-hot Vector</span></a></li><li><a class="is-flex" href="#2-3-Skip-gram-neural-network"><span class="mr-2">2.3</span><span>2.3 Skip-gram neural network</span></a><ul class="menu-list"><li><a class="is-flex" href="#Note"><span class="mr-2">2.3.1</span><span>Note:</span></a></li></ul></li><li><a class="is-flex" href="#2-4-Evaluation-of-Skip-gram"><span class="mr-2">2.4</span><span>2.4 Evaluation of Skip-gram</span></a><ul class="menu-list"><li><a class="is-flex" href="#Advantages"><span class="mr-2">2.4.1</span><span>Advantages</span></a></li><li><a class="is-flex" href="#Disadvantages"><span class="mr-2">2.4.2</span><span>Disadvantages</span></a></li></ul></li><li><a class="is-flex" href="#Models-for-Word2vector"><span class="mr-2">2.5</span><span>Models for Word2vector</span></a></li></ul></li><li><a class="is-flex" href="#Reference"><span class="mr-2">3</span><span>Reference:</span></a></li></ul></div></div></div></div></div></div></section><footer class="footer"><div class="container"><div class="level"><div class="level-start"><a class="footer-logo is-block mb-2" href="/"><img src="/images/icon.png" alt="Wenkang&#039;s Blog" height="28"></a><p class="size-small"><span>&copy; 2021 Wenkang Wei</span>  Powered by <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a> &amp; <a href="https://github.com/ppoffice/hexo-theme-icarus" target="_blank" rel="noopener">Icarus</a></p></div><div class="level-end"><div class="field has-addons"><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Creative Commons" href="https://creativecommons.org/"><i class="fab fa-creative-commons"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Attribution 4.0 International" href="https://creativecommons.org/licenses/by/4.0/"><i class="fab fa-creative-commons-by"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/ppoffice/hexo-theme-icarus"><i class="fab fa-github"></i></a></p></div></div></div></div></footer><script src="https://cdn.jsdelivr.net/npm/jquery@3.3.1/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/moment@2.22.2/min/moment-with-locales.min.js"></script><script>moment.locale("en");</script><script>var IcarusThemeSettings = {
            site: {
                url: 'https://github.com/wenkangwei/wenkangwei.github.io`',
                external_link: {"enable":true,"exclude":[]}
            },
            article: {
                highlight: {
                    clipboard: true,
                    fold: 'unfolded'
                }
            }
        };</script><script src="https://cdn.jsdelivr.net/npm/clipboard@2.0.4/dist/clipboard.min.js" defer></script><script src="/js/animation.js"></script><a id="back-to-top" title="Back to Top" href="javascript:;"><i class="fas fa-chevron-up"></i></a><script src="/js/back_to_top.js" defer></script><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/lightgallery@1.6.8/dist/js/lightgallery.min.js" defer></script><script src="https://cdn.jsdelivr.net/npm/justifiedGallery@3.7.0/dist/js/jquery.justifiedGallery.min.js" defer></script><script>window.addEventListener("load", () => {
            if (typeof $.fn.lightGallery === 'function') {
                $('.article').lightGallery({ selector: '.gallery-item' });
            }
            if (typeof $.fn.justifiedGallery === 'function') {
                if ($('.justified-gallery > p > .gallery-item').length) {
                    $('.justified-gallery > p > .gallery-item').unwrap();
                }
                $('.justified-gallery').justifiedGallery();
            }
        });</script><!--!--><!--!--><script type="text/x-mathjax-config">MathJax.Hub.Config({
            'HTML-CSS': {
                matchFontHeight: false
            },
            SVG: {
                matchFontHeight: false
            },
            CommonHTML: {
                matchFontHeight: false
            },
            tex2jax: {
                inlineMath: [
                    ['$','$'],
                    ['\\(','\\)']
                ]
            }
        });</script><script src="https://cdn.jsdelivr.net/npm/mathjax@2.7.5/unpacked/MathJax.js?config=TeX-MML-AM_CHTML" defer></script><!--!--><script src="/js/main.js" defer></script><div class="searchbox"><div class="searchbox-container"><div class="searchbox-header"><div class="searchbox-input-container"><input class="searchbox-input" type="text" placeholder="Type something..."></div><a class="searchbox-close" href="javascript:;">×</a></div><div class="searchbox-body"></div></div></div><script src="/js/insight.js" defer></script><script>document.addEventListener('DOMContentLoaded', function () {
            loadInsight({"contentUrl":"/content.json"}, {"hint":"Type something...","untitled":"(Untitled)","posts":"Posts","pages":"Pages","categories":"Categories","tags":"Tags"});
        });</script></body></html>