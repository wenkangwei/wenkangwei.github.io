<!doctype html>



  


<html class="theme-next mist use-motion" lang="en">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>



<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />




  
  
  
  

  
    
    
  

  

  

  

  

  
    
    
    <link href="//fonts.lug.ustc.edu.cn/css?family=lato:300,300italic,400,400italic,700,700italic&subset=latin,latin-ext" rel="stylesheet" type="text/css">
  






<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.0" rel="stylesheet" type="text/css" />


  <meta name="keywords" content="Negative Sampling,Word Embedding," />








  <link rel="shortcut icon" type="image/x-icon" href="/images/icon.png?v=5.1.0" />






<meta property="og:type" content="article">
<meta property="og:title" content="NLP Improvement on Word2Vector">
<meta property="og:url" content="https://github.com/wenkangwei/wenkangwei.github.io%60/2020/07/29/NLP-Word2Vec-Improvement/index.html">
<meta property="og:site_name" content="Wenkang&#39;s Blog">
<meta property="og:locale" content="en_US">
<meta property="article:published_time" content="2020-07-29T04:00:25.000Z">
<meta property="article:modified_time" content="2020-08-24T22:33:49.550Z">
<meta property="article:author" content="Wenkang Wei">
<meta property="article:tag" content="Negative Sampling">
<meta property="article:tag" content="Word Embedding">
<meta name="twitter:card" content="summary">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Mist',
    sidebar: {"position":"right","display":"always","offset":12,"offset_float":0,"b2t":false,"scrollpercent":true},
    fancybox: true,
    motion: true,
    duoshuo: {
      userId: '0',
      author: 'Author'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="https://github.com/wenkangwei/wenkangwei.github.io`/2020/07/29/NLP-Word2Vec-Improvement/"/>





  <title> NLP Improvement on Word2Vector | Wenkang's Blog </title>
<meta name="generator" content="Hexo 4.2.1"></head>

<body itemscope itemtype="http://schema.org/WebPage" lang="en">

  














  
  
    
  

  <div class="container one-collumn sidebar-position-right page-post-detail ">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta custom-logo">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Wenkang's Blog</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle"></p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            Home
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br />
            
            Categories
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/about" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-user"></i> <br />
            
            About
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            Archives
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            Tags
          </a>
        </li>
      
        
        <li class="menu-item menu-item-schedule">
          <a href="/schedule" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-calendar"></i> <br />
            
            Schedule
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-categories,tags " itemscope itemtype="http://schema.org/Article">
    <link itemprop="mainEntityOfPage" href="https://github.com/wenkangwei/wenkangwei.github.io`/2020/07/29/NLP-Word2Vec-Improvement/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Wenkang Wei">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Wenkang's Blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                NLP Improvement on Word2Vector
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2020-07-29T00:00:25-04:00">
                2020-07-29
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/NLP/" itemprop="url" rel="index">
                    <span itemprop="name">NLP</span>
                  </a>
                </span>

                
                
                  , 
                
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/NLP/Machine-Learning/" itemprop="url" rel="index">
                    <span itemprop="name">Machine Learning</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        <img src=https://i.ytimg.com/vi/BD8wPsr_DAI/maxresdefault.jpg>
<a id="more"></a>

<h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>Although word2Vec for word embedding has been widely used, it has some obvious shortages that affect computation of word vector. This passage is to identify those shortages and introduce a solution called <strong>Negative Sampling</strong> to solve the problems.<br><br></p>
<h2 id="Review-to-Word2Vec"><a href="#Review-to-Word2Vec" class="headerlink" title="Review to Word2Vec"></a>Review to Word2Vec</h2><p>Word2Vec is a framework to convert vocabulary in texts into dense numeric vector representation such that our machine learning models can realize the vocabulary and human language and learn something.<br>The main idea of word2Vec (skip-gram) is following:</p>
<ol>
<li><p>Start with random word vectors in neural network</p>
</li>
<li><p>Iterate each world in word corpus</p>
<img src=/images/NLP/word2vec-1.jpg>
</li>
<li><p>Predict the context words (surrounding words) of a center word by computing posterior distribution: $P(w_{t+1}|w_t)$, where $w_t$ is the center word at position t and $w_{t+1}$ is the surrounding word.</p>
</li>
</ol>
<p>$$P(o|c) = \frac{exp(\textbf{u}_o^T\textbf{v}<em>c )}{\sum</em>{w\in V} exp(\textbf{u}_w^T\textbf{v}_c)}$$</p>
<p>where $\textbf{v}_c$ is the word vector of center word and $\textbf{u}_o$ is the word vector of surrounding words (or weights).</p>
<ol start="4">
<li>Update word vector using Gradient Descent based on cost function<img src=/images/NLP/word2vec-2.jpg>
Note: $\theta$ in loss function here is the weight matrix used in softmax. The weight matrix in the network is $\textbf{u}_w$. 
<br>

</li>
</ol>
<h2 id="Disadvantages-of-Word2Vector"><a href="#Disadvantages-of-Word2Vector" class="headerlink" title="Disadvantages of Word2Vector"></a>Disadvantages of Word2Vector</h2><p>We can notice that bigger vocabulary it is, larger the word vector becomes. Usually, there are thousands of different words in text, using gradient descent to update the whole weight matrix leads to expensive computation cost and each update become super slow. We need to repeat updating each weight using the following equation and the time complexity increases linearly as the amount of words increases.<br><img src=/images/NLP/gradient-descent.jpg></p>
<br>

<h2 id="Improvement"><a href="#Improvement" class="headerlink" title="Improvement"></a>Improvement</h2><h3 id="Stochastic-Gradient-Descent-with-sampled-window"><a href="#Stochastic-Gradient-Descent-with-sampled-window" class="headerlink" title="Stochastic Gradient Descent with sampled window"></a>Stochastic Gradient Descent with sampled window</h3><p>Assume we are using a window centered at center word and hence it has size of 2m+1. The the update is</p>
<ol>
<li>Repeatedly sample windows and iterate each window, rather than iterate all windows.</li>
<li>Compute the gradient of the words that actually appear. The graident of words in dictionary that don’t appear in text won’t be updated.</li>
</ol>
<p>Notes:</p>
<ul>
<li><p>the gradient of the words that don’t appear in text, but in vector is all 0. In this case the vector would be very sparse (many zeros in vectors). It is a waste of time to compute those 0 update.</p>
<img src=/images/NLP/gradient-vector.jpg width="200" height=100 >
</li>
<li><p>We need a sampling technique to sample windows and words for updating part of weights. This leads to our next section Negative Sampling</p>
<br>

</li>
</ul>
<h3 id="Negtive-Sampling"><a href="#Negtive-Sampling" class="headerlink" title="Negtive Sampling"></a>Negtive Sampling</h3><p>In word2vec (skip-gram), the input to the neural network is one-hot word vector of the center word. In training step,  output of neural network is a vector of possibility that each word can appear given the center word.<br>In prediction step,  the output is converted from possibility vector to one-hot vectors of the predcited context words that are most likely to appear given the center word. For example, if we have possibility output [0.1, 0.1, 0.5, 0.3] and expect predictions of 2 context words. Then we output one-hot vectors of the words with possibility of 0.5 and 0.3.<br>The targets corresponding to the given center word are one-hot vector of context words surrounding this center word in the window.<br><br></p>
<h4 id="Negative-words"><a href="#Negative-words" class="headerlink" title="Negative words"></a><strong>Negative words</strong></h4><p>In one-hot vector output from neural network, we call the word with value equal to 1 as <strong>postive word</strong> and those words with values equal to 0 as <strong>negative word</strong>.</p>
<p>For example. Assume we have a word vector with dimension of 100 (100 words in dictionary). Then in a window such as “I like my dog”,</p>
<table>
<thead>
<tr>
<th>I</th>
<th>like</th>
<th>my</th>
<th>dog</th>
</tr>
</thead>
</table>
<p>if “like” is center word $w_t$, as the input to the neural network. Then “I” , “my”, “dog” are context words, or positive words that are expected to output “1” in the output one-hot vector of neural network(it is expected to output 3 one-hot vectors). Then other words that don’t appear in this window / context and are expected to be 0 in one-hot vector are negative words.</p>
<img src=/images/NLP/word2vec-3.jpg>
<br>

<h4 id="Selection-of-negative-words"><a href="#Selection-of-negative-words" class="headerlink" title="Selection of negative words"></a><strong>Selection of negative words</strong></h4><p>However, there would be a large amount of negative words that don’t appear in context. If we update weights of all negative words, the update could be very inefficient.<br>In negative sampling, we sample negative words based on the possiblity that word may occur. The possibility is given by:</p>
<p>$$<br>P(w_i)= \frac{f(w_i)}{\sum_j^Nf(w_j)}$$</p>
<p>where  $f(w_i)$ is the number of word $w_i$ appears in corpus and the denominator is the number of all words appear/ the amount of words in corpus.</p>
<p>However, the author proposes this equation since it gives the best performance. </p>
<p>$$<br>P(w_i)= \frac{f(w_i)^{3/4}}{\sum_j^Nf(w_j)^{3/4}}$$</p>
<p>Based on the possibility of the occurence of words, we can sample a number of negative words that are most likely to appear and update the corresponding weights by backward update in neural network. The number of samples is set by user.<br>For example, If we have a dictionary with size of 100 words (hence one-hot vector with size of 100) and each word has corresponding weights with size of 100. Then there are 100x100 weights to update when updating all weights in network. However, if we only sample 20 negative words to update, we need to update 20x100 weights only. This allows us to speed up training step.<br><br></p>
<h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><p>[1] <a href="http://mccormickml.com/2017/01/11/word2vec-tutorial-part-2-negative-sampling/" target="_blank" rel="noopener">http://mccormickml.com/2017/01/11/word2vec-tutorial-part-2-negative-sampling/</a></p>
<p>[2] <a href="https://arxiv.org/pdf/1310.4546.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1310.4546.pdf</a></p>

      
    </div>

    <div>
      
        
<div id="wechat_subscriber" style="display: block; padding: 10px 0; margin: 20px auto; width: 100%; text-align: center">
    <img id="wechat_subscriber_qcode" src="/images/wechat-qr-code.jpg" alt="Wenkang Wei wechat" style="width: 200px; max-width: 100%;"/>
    <div>subscribe to my blog by scanning my public wechat account</div>
</div>


      
    </div>

    <div>
      
        

      
    </div>


    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/Negative-Sampling/" rel="tag"># Negative Sampling</a>
          
            <a href="/tags/Word-Embedding/" rel="tag"># Word Embedding</a>
          
        </div>
      

      
        
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2020/07/23/Model-Acc-Improvement/" rel="next" title="Accuracy Improvement-Ensemble Methods">
                <i class="fa fa-chevron-left"></i> Accuracy Improvement-Ensemble Methods
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2020/08/13/Model-Eval-Selection/" rel="prev" title="Model Evaluation and Selection">
                Model Evaluation and Selection <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </article>



    <div class="post-spread">
      
    </div>
  </div>

          
          </div>
          


          
  <div class="comments" id="comments">
    
  </div>


        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap" >
            Table of Contents
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview">
            Overview
          </li>
        </ul>
      

      <section class="site-overview sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
          <img class="site-author-image" itemprop="image"
               src="/images/avatar.jpg"
               alt="Wenkang Wei" />
          <p class="site-author-name" itemprop="name">Wenkang Wei</p>
           
              <p class="site-description motion-element" itemprop="description">second-year master student / Computer engineering / deep learning / machine learning / data science </p>
          
        </div>
        <nav class="site-state motion-element">
        
          
            <div class="site-state-item site-state-posts">
              <a href="/archives">
                <span class="site-state-item-count">26</span>
                <span class="site-state-item-name">posts</span>
              </a>
            </div>
          

          
            <div class="site-state-item site-state-categories">
              <a href="/categories">
                <span class="site-state-item-count">20</span>
                <span class="site-state-item-name">categories</span>
              </a>
            </div>
          

          
            <div class="site-state-item site-state-tags">
              <a href="/tags">
                <span class="site-state-item-count">57</span>
                <span class="site-state-item-name">tags</span>
              </a>
            </div>
          

        </nav>

        

        <div class="links-of-author motion-element">
          
            
              <span class="links-of-author-item">
                <a href="https://github.com/wenkangwei" target="_blank" title="Github">
                  
                    <i class="fa fa-fw fa-globe"></i>
                  
                  Github
                </a>
              </span>
            
              <span class="links-of-author-item">
                <a href="https://www.linkedin.com/in/wenkang-wei-588811167" target="_blank" title="LinkedIn">
                  
                    <i class="fa fa-fw fa-fab fa-linkedin"></i>
                  
                  LinkedIn
                </a>
              </span>
            
              <span class="links-of-author-item">
                <a href="https://www.facebook.com/wenkang.wei" target="_blank" title="Facebook">
                  
                    <i class="fa fa-fw fa-fab fa-facebook"></i>
                  
                  Facebook
                </a>
              </span>
            
          
        </div>

        
        

        
        

        


      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#Introduction"><span class="nav-number">1.</span> <span class="nav-text">Introduction</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Review-to-Word2Vec"><span class="nav-number">2.</span> <span class="nav-text">Review to Word2Vec</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Disadvantages-of-Word2Vector"><span class="nav-number">3.</span> <span class="nav-text">Disadvantages of Word2Vector</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Improvement"><span class="nav-number">4.</span> <span class="nav-text">Improvement</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Stochastic-Gradient-Descent-with-sampled-window"><span class="nav-number">4.1.</span> <span class="nav-text">Stochastic Gradient Descent with sampled window</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Negtive-Sampling"><span class="nav-number">4.2.</span> <span class="nav-text">Negtive Sampling</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Negative-words"><span class="nav-number">4.2.1.</span> <span class="nav-text">Negative words</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Selection-of-negative-words"><span class="nav-number">4.2.2.</span> <span class="nav-text">Selection of negative words</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#References"><span class="nav-number">5.</span> <span class="nav-text">References</span></a></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright" >
  
  &copy; 
  <span itemprop="copyrightYear">2021</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Wenkang Wei</span>
</div>


<div class="powered-by">
  Powered by <a class="theme-link" href="https://hexo.io" target="_blank" rel="noopener">Hexo</a>
</div>

<div class="theme-info">
  Theme -
  <a class="theme-link" href="https://github.com/iissnan/hexo-theme-next">
    NexT.Mist
  </a>
</div>


        

        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
          <span id="scrollpercent"><span>0</span>%</span>
        
      </div>
    
    
  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  




  
  <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>

  
  <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>

  
  <script type="text/javascript" src="[object Object]"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.0"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.0"></script>



  
  

  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.0"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.0"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.0"></script>



  



  




	





  





  





  




  <script src="//cdn1.lncld.net/static/js/3.0.4/av-min.js"></script>
  <script src="//unjkp.com/valine/dist/Valine.min.js"></script>  
  <script src="/js/src/Valine.min.js"></script> 

   
  
  <script src="/js/src/Valine.min.js"></script>
  
  <script type="text/javascript">
    var GUEST = ['nick','mail','link'];
    var guest = 'nick';
    guest = guest.split(',').filter(function (item) {
      return GUEST.indexOf(item)>-1;
    });
    new Valine({
        el: '#comments' ,
        verify: true,
        notify: true,
        appId: 'kvXCKmDNxnA2486N29e5cP7i-MdYXbMMI',
        appKey: '0Lv5b0SqotzkGHQvD64u4AKo',
        placeholder: '说点什么吧！',
        avatar:'hide',
        guest_info:['nick'] , 
        pageSize:'10' || 10,
    });
   
    var infoEle = document.querySelector('#comments .info');
    if (infoEle && infoEle.childNodes && infoEle.childNodes.length > 0){
      infoEle.childNodes.forEach(function(item) {
        item.parentNode.removeChild(item);
      });
    }
  </script>

  
  

  
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/javascript" src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
  


  

  

  


  

</body>
</html>
