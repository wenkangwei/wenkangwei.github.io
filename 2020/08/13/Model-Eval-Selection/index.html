<!doctype html>



  


<html class="theme-next mist use-motion" lang="en">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>



<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />




  
  
  
  

  
    
    
  

  

  

  

  

  
    
    
    <link href="//fonts.lug.ustc.edu.cn/css?family=lato:300,300italic,400,400italic,700,700italic&subset=latin,latin-ext" rel="stylesheet" type="text/css">
  






<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.0" rel="stylesheet" type="text/css" />


  <meta name="keywords" content="ROC curve,Confusion Metric,Cross Validation,Holdout,Ensemble Learning,Model Evaluation,Model Selection," />








  <link rel="shortcut icon" type="image/x-icon" href="/images/icon.png?v=5.1.0" />






<meta property="og:type" content="article">
<meta property="og:title" content="Model Evaluation and Selection">
<meta property="og:url" content="https://github.com/wenkangwei/wenkangwei.github.io%60/2020/08/13/Model-Eval-Selection/index.html">
<meta property="og:site_name" content="Wenkang&#39;s Blog">
<meta property="og:locale" content="en_US">
<meta property="article:published_time" content="2020-08-13T21:27:44.000Z">
<meta property="article:modified_time" content="2020-12-08T17:37:14.220Z">
<meta property="article:author" content="Wenkang Wei">
<meta property="article:tag" content="ROC curve">
<meta property="article:tag" content="Confusion Metric">
<meta property="article:tag" content="Cross Validation">
<meta property="article:tag" content="Holdout">
<meta property="article:tag" content="Ensemble Learning">
<meta property="article:tag" content="Model Evaluation">
<meta property="article:tag" content="Model Selection">
<meta name="twitter:card" content="summary">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Mist',
    sidebar: {"position":"right","display":"always","offset":12,"offset_float":0,"b2t":false,"scrollpercent":true},
    fancybox: true,
    motion: true,
    duoshuo: {
      userId: '0',
      author: 'Author'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="https://github.com/wenkangwei/wenkangwei.github.io`/2020/08/13/Model-Eval-Selection/"/>





  <title> Model Evaluation and Selection | Wenkang's Blog </title>
<meta name="generator" content="Hexo 4.2.1"></head>

<body itemscope itemtype="http://schema.org/WebPage" lang="en">

  














  
  
    
  

  <div class="container one-collumn sidebar-position-right page-post-detail ">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta custom-logo">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Wenkang's Blog</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle"></p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            Home
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br />
            
            Categories
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/about" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-user"></i> <br />
            
            About
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            Archives
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            Tags
          </a>
        </li>
      
        
        <li class="menu-item menu-item-schedule">
          <a href="/schedule" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-calendar"></i> <br />
            
            Schedule
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-tags,categories " itemscope itemtype="http://schema.org/Article">
    <link itemprop="mainEntityOfPage" href="https://github.com/wenkangwei/wenkangwei.github.io`/2020/08/13/Model-Eval-Selection/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Wenkang Wei">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Wenkang's Blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                Model Evaluation and Selection
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2020-08-13T17:27:44-04:00">
                2020-08-13
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Machine-Learning/" itemprop="url" rel="index">
                    <span itemprop="name">Machine Learning</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        <img src=https://media.springernature.com/lw685/springer-static/image/art%3A10.1038%2Fnmeth.3945/MediaObjects/41592_2016_Article_BFnmeth3945_Fig1_HTML.jpg>

<a id="more"></a>

<h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>In order to do something better in our study, our jobs or even our life,  evaluation step is an indispensable part of improvement. Otherwise, How do we know our work is good or bad? In addition, “good” and “bad” are ambiguous terms if we don’t have any evaluation methods.</p>
<p>Similarly, in machine learning, in order to training a “better” model, we need concrete evaluation metrics to tell us if our models perform well, so that we can choose the best one.</p>
<p>The goal of this passage is to conclude the common useful ways to evaluate and improve our machine learning models. My Thoughts are also provided.</p>
<h2 id="Evaluation-Metrics"><a href="#Evaluation-Metrics" class="headerlink" title="Evaluation Metrics"></a>Evaluation Metrics</h2><ul>
<li><p><strong>Confusion Matrix</strong><br>In classification task, we have predictions from our model on the test set and ground truth labels/targets indicating the real class of each sample in dataset.<br>Let consider we are predicting if data belongs to  class “C” or not.</p>
<p>  <strong>Then Confusion matrix is</strong></p>
<table>
<thead>
<tr>
<th>Actual class\prediction</th>
<th>C</th>
<th>not C</th>
<th>sum</th>
</tr>
</thead>
<tbody><tr>
<td><strong>C</strong></td>
<td>TP</td>
<td>FN</td>
<td>actual P= TP+FN</td>
</tr>
<tr>
<td><strong>Not C</strong></td>
<td>FP</td>
<td>TN</td>
<td>actual N = FP+TN</td>
</tr>
<tr>
<td>sum</td>
<td>predicted P’ = TP+FP</td>
<td>predicted N’ = FN+TN</td>
<td>All</td>
</tr>
</tbody></table>
</li>
</ul>
<p>True positive (TP): the amount of samples that are <strong>predicted</strong> as class “C” and they <strong>actually</strong> belong to class “C”.</p>
<p>False positive (FP): the amount of samples that are <strong>predicted</strong> as class “C” and they <strong>actually</strong> DON’T belong to class “C”.</p>
<p>False Negative (FN): the amount of samples that are <strong>predicted</strong> as Not class “C” and they <strong>actually</strong> belong to class “C”.</p>
<p>True Negative (TN): the amount of samples that are <strong>predicted</strong> as not class “C” and they <strong>actually</strong> don’t belong to class “C”.</p>
<ul>
<li><p><strong>Accuracy</strong>:<br>measure how accurate the predictions are<br>  <strong>$acc = \frac{TP+TN}{TP+TN+FP+FN}$</strong></p>
<br></li>
<li><p><strong>Error Rate:  1-acc</strong></p>
<br>
</li>
<li><p><strong>Sensitivity/Recall</strong>: (Precentage of <strong>correct positive</strong> prediction on <strong>Actual positive</strong> )<br>Recognition rate on True positive:<br>$sens = \frac{TP}{predicted P’} = \frac{TP}{TP+FN}$</p>
</li>
</ul>
<br>

<ul>
<li><strong>Specificity</strong>:(Precentage of <strong>correct negative</strong> prediction on <strong>Actual negative</strong> )<br><strong>How many positive predictions in true data are recognized by model?</strong><br>Recognition rate on True Negative:<br>  $spec = \frac{TN}{predicted N’} = \frac{TN}{TN+FP}$</li>
</ul>
<br>


<ul>
<li><p><strong>Precision</strong>: (Percentage of correct positive prediction on <strong>all positive prediction</strong>)<br>  Measure what % of tuples that the classifier labeled as positive are actually positive<br>  <strong>How many positive predictions of your model are correct?</strong></p>
<p>  $precision = \frac{TP}{predicted P’} = \frac{TP}{TP+FP}$</p>
</li>
</ul>
<br>

<ul>
<li><p>__F-score: $F_\beta$__: It weighs precision and recall</p>
<p>  $$ F_\beta = \frac{(1+\beta^2) * precision*recall}{\beta^2 * precision+recall}$$</p>
  <br>
  $\beta$ controls the weight of precision. Higher $\beta$ weigh more to precision than recall and hence precision is given more attention.
<br>
</li>
<li><p><strong>F-1 score ($\beta =1$)</strong>:</p>
<p>  $$ F_1 = \frac{2 * precision * recall}{precision+recall}$$</p>
  <br>
  Both precision and recall are given equal weights.
</li>
<li><p><strong>When to use Recall, Sensitivity and F-1 to evaluate model?</strong></p>
<ul>
<li><p><strong>Sensitivity/Recall</strong>:<br>When we want the model to be more sensitive to positive cases and would like to predict more False Positive than False Negative.<br><strong>Example:</strong><br>In <strong>disease detection</strong>, we would like to use recall more than precision, since we want to detect disease and cure earily, we only care if we get disease or not (Positive or<br>  not)<br><strong>Cybersecurity</strong>, fault detection. We care if fault occurs/ prediction is positive or not only. Even if more FP than FN are in prediction, it help us reduce the possibility of missing faults (FN)</p>
</li>
<li><p><strong>Precision</strong>:<br>when we care  <strong>real positive</strong> cases, or both FN and TP. That is, <strong>we don’t want the model to  predict more FP than FN</strong>. False negative also matters.<br><strong>Example:</strong><br>Spam detection: when mailbox detects a spam, it will directly delete/remove that email. However, if that email is actually not a spam (False positive), but an important email, then deleting it leads to an unexpected result.<br>In this case, we want to reduce the false positive cases to avoid deleting an important email. Hence, we don’t use FP, but use FN and precision instead. </p>
</li>
<li><p><strong>F-Score</strong>:<br>when we care both precision and recall, but want to weigh them.<br>This depends on the assumption in the real cases. If the real case assumes both precision and recall are important, then we use F-score.</p>
</li>
<li><p><strong>Score used for GAN</strong>:<br>Inception Score, FID. For more details, Please read this <a href="https://medium.com/@jonathan_hui/gan-how-to-measure-gan-performance-64b988c47732" target="_blank" rel="noopener">article</a></p>
<br>


</li>
</ul>
</li>
</ul>
<h2 id="Estimation-of-Model-Accuracy"><a href="#Estimation-of-Model-Accuracy" class="headerlink" title="Estimation of Model Accuracy"></a>Estimation of Model Accuracy</h2><ul>
<li><p><strong>Holdout</strong><br>  The purpose of holdout is to  evaluate how well the model performs on the unseen dataset. <strong>It is used to evaluate the both the framework of model and the hyperparameters of model</strong><br>  <strong>Process:</strong></p>
<ol>
<li><p>Split dataset into <strong>two independent datasets randomly</strong>: training set (usually 80%) and test set (usually 20%)</p>
</li>
<li><p>Train the initial model with training set and then test model with test set</p>
</li>
<li><p>Repeat steps 1~2 k times and <strong>calculate average accuracy of the k accuracies obtained</strong></p>
</li>
<li><p><strong>Note:</strong></p>
</li>
</ol>
<ul>
<li>The test sets in k iterations may be repeated</li>
<li>Accuracy of model could be unstable due to the split method on dataset.<br>For example, if after randomly spliting dataset into training set and test set, all samples belonging to class “C” are in test set, then model can not learn features from class “C” and hence perform worse. Otherwise, it could perform better if it learn features from class “C” from training set.</li>
</ul>
</li>
</ul>
<br>

<ul>
<li><p><strong>Cross Validation</strong><br>  The purpose of Cross Validation is to <strong>evaluate the framework of the model, rather than how good the hyper parameters of the model is.</strong></p>
<ul>
<li><p><strong>K-fold</strong></p>
  <img src=/images/model-eval/kfolds.png>
  <br>
  1. Split the training set into k-subsets: {D1, D2,..Dk} with (almost) equal size.
  2. Loop through k iterations. At $i^th$  iteration, select subset Di as validation set and the remaining k-1 subsets as training set.
  3. Compute accuracy of each iteration
  4. Compute the mean of accuracies obtained in k iterations.
  <br>
</li>
<li><p><strong>Leave One out</strong></p>
<ol>
<li>It is a case of k-fold cross validation with <strong>setting K= N, the number of data point in training set</strong>.</li>
<li>It hence takes N-1 iterations to evaluate model.<br>
</li>
</ol>
</li>
<li><p><strong>Stratified Cross validation</strong></p>
<ol>
<li>It is a case of K-fold cross validation with each setting:  <strong>each set contains approximately the same percentage of samples of each target class as the complete set</strong>.<br>
__For example:__
In a dataset S= {A1,A2,A3,B1,B2,B3,C1,C2,C3}, which has classes A,B,C. Each class occupies 30% of total dataset.In Stratified 3-fold Cross-validation, S= {K1,K2,K3}. 
To make each class of data in each fold have the same percentage, we have:
K1 ={A1,B1,C1},
K2 ={A2,B2,C2},
K3 ={A3,B3,C3}
such that the percentage of each class of sample in each fold is still 30%.
<br>
</li>
</ol>
</li>
<li><p><strong>Note for Cross-Validation</strong></p>
<ol>
<li>When <strong>K</strong>  is small, the variance in performance of model could be large, since each fold contains more data and hence becomes noisy.</li>
<li>When <strong>K</strong> is large, eg. in LeaveOneOut, <strong>K=N</strong>,  variance of performance of model would be smaller.</li>
<li><strong>In each fold of training set, we need to start training the initial model again, rather than training the model from the last fold.</strong></li>
<li>Cross validation is to <strong>evaluate the performance of the framework of the model</strong>, rather than of the parameters of the model.<br>
</li>
</ol>
</li>
</ul>
</li>
<li><p><strong>Bootstrap (Sample-based)</strong> </p>
<ol>
<li><strong>Samples</strong> the given training tuples uniformly <strong>with replacement</strong> <strong>d</strong> times as training set. <strong>The remaining data tuples form the test set (or validation set)</strong>.<br>That is,<br>the tuples/samples that have been chosen can still be chosen with equal possibility</li>
<li>Train model with training set</li>
<li>Compute average accuracy of model on both training set and test set</li>
<li>Repeat steps 1~3 k times, then compute average accuracy of all accuracy obtained in k iterations.<br>
</li>
</ol>
</li>
<li><p><strong>Comparison among methods Above</strong></p>
<ol>
<li><p><strong>holdout</strong><br> <strong>Purpose</strong><br> Holdout is mainly used to evaluate how well the model performs on the unseen dataset. It  evaluates both framework and hyperparameter of the model.<br> <em>Different from cross-validation, the <strong>holdout data can be any size (usually 20% of the training dataset), while cross-validation requires validation data has the same size as each fold.</strong> When the amount of fold is small (like 2) in cross-validation, it could waste the training set.</em></p>
 <br>

<p> <strong>Advantages</strong></p>
<ul>
<li><p><em>Less expensive in computation, Easy to compute,</em> compared with cross-validation, since it splits test set randomly </p>
<p><strong>Disadvantages</strong></p>
</li>
<li><p>Estimated accuracy may be unstable (Accuracy is easy to change), since holdout <strong>depends on the dataset split methods</strong> on training set and test set. </p>
<p><strong>When to Use</strong></p>
</li>
<li><p>When the dataset is very large and hard to compute multiple subset</p>
<br>
</li>
</ul>
</li>
<li><p><strong>Cross-validation</strong><br> <strong>Advantages</strong></p>
<ul>
<li><p><strong>The estimated accuracy is much stable than holdout</strong> , since it trains model on multiple different train-test set splits. It guarantees to  train model with all samples.</p>
<p><strong>Disadvantages</strong></p>
</li>
<li><p> The Cost for computation is expensive when dataset is very large.</p>
</li>
<li><p>When value <strong>K</strong>  is <strong>smaller</strong>, the <strong>variance</strong> in performance of model will be <strong>larger</strong> and model is <strong>easier to overfit</strong> (training with more data samples could be more noisy).</p>
</li>
<li><p>When value <strong>K</strong>  is <strong>smaller</strong>, <strong>it could waste the training set</strong>. For example, in 2-fold, we use only half of training set to train the model, which is a waste of training data.</p>
<p><strong>When to Use</strong></p>
</li>
<li><p>when dataset is not very large (10000 samples or even more)</p>
</li>
<li><p>when you have powerful computational device. </p>
<br>
</li>
</ul>
</li>
<li><p><strong>Bootstrap</strong><br> <strong>Advantages</strong></p>
<ul>
<li><p>Performance of model doesn’t depend on the split method on dataset</p>
</li>
<li><p>When dataset is small or insufficient, or imbalanced (some classes are more than other significantly), it may reduce overfitting effect by sampling with replacement</p>
</li>
<li><p>Less expensive on computation compared with cross validation</p>
<p><strong>Disadvantages</strong></p>
</li>
<li><p>Need to determine what sampling method to use</p>
<p><strong>When to Use</strong></p>
</li>
<li><p>when dataset is small or insufficient (In this case, we may use Over-sampling to repeat some data)</p>
</li>
<li><p>when dataset is imbalanced</p>
</li>
<li><p>when dataset is pretty large (In this case, we may use down-sampling to select part of data for training)</p>
</li>
</ul>
</li>
</ol>
</li>
</ul>
<h2 id="Comparison-of-performances-among-different-model"><a href="#Comparison-of-performances-among-different-model" class="headerlink" title="Comparison of performances among different model"></a>Comparison of performances among different model</h2><p><strong>How do we know the performances between models are similar?</strong></p>
<ul>
<li><strong>t-Test / Student’s t test</strong> Read this <a href="https://en.wikipedia.org/wiki/Student%27s_t-test" target="_blank" rel="noopener">paper</a></li>
</ul>
<h2 id="Model-Selection"><a href="#Model-Selection" class="headerlink" title="Model Selection"></a>Model Selection</h2><ul>
<li><p><strong>ROC curve</strong>: receiver operating charatistic curve (true positive rate -VS- False positive rate)<br>we first let model output possibility of each class and then set the threshold to convert possibility to 0, 1 binary classification labels.<br>The threshold is usually default as 0.5.<br>Based on the binary classification, compute true positive rate <strong>TPR</strong>/recall  (<strong>TP/(TP+FN)</strong>) and false positive rate/<strong>FPR</strong> (<strong>FP/(FP+TN) = FP/real negative</strong>)<br>Since threshold is set to 0.5, during training of model, the performance of model will change and hence TPR and FPR will change.<br>The changing TPR and FPR leads to the ROC curve</p>
<br>
</li>
<li><p><strong>AUC</strong>: area under curve ( ROC curve)<br>Since TPR and FPR range from 0 to 1, hence AUC has range [0, 1]. AUC usually is inside [0.5, 1] because a good model usually can classify sample correctly with 0.5 possibility.</p>
<br>
</li>
<li><p><strong>Physical Meaning of AUC:</strong><br><strong>AUC is the possibility that a random-chosen positive sample is ranked more highly than a randomly-chosen negative sample.</strong><br>In other words, AUC is the <strong>possibility that a model’s prediction possibility of a random positive sample is more higher than a random negative sample</strong>. Then when AUC of a model is larger, it is more likely for the model to predict positive sample as 1  , rather than negative sample as 1.</p>
<br>
</li>
<li><p>Example: when <strong>threshold =0.5</strong><br>  <strong>Case 1</strong></p>
<table>
<thead>
<tr>
<th>Sample</th>
<th>A</th>
<th>B</th>
<th>C</th>
<th>D</th>
</tr>
</thead>
<tbody><tr>
<td>Possibility</td>
<td>0.9</td>
<td>0.8</td>
<td>0.51</td>
<td>0.3</td>
</tr>
<tr>
<td>Prediction</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>0</td>
</tr>
<tr>
<td>Ground Truth</td>
<td>1</td>
<td>1</td>
<td>0</td>
<td>0</td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>In case 1 we see that possibility that positive sample is ranked higher than negative sample, since all positive sample has possibility greater than negative sample and TPR = 2/2=1 as FPR = 1/2 =0.5 &lt; TPR. Hence in this case, it is a good model.</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
</tbody></table>
<p>  <strong>Case 2</strong></p>
<table>
<thead>
<tr>
<th>Sample</th>
<th>A</th>
<th>B</th>
<th>C</th>
<th>D</th>
</tr>
</thead>
<tbody><tr>
<td>Possibility</td>
<td>0.9</td>
<td>0.3</td>
<td>0.51</td>
<td>0.6</td>
</tr>
<tr>
<td>Prediction</td>
<td>1</td>
<td>0</td>
<td>1</td>
<td>1</td>
</tr>
<tr>
<td>Ground Truth</td>
<td>1</td>
<td>1</td>
<td>0</td>
<td>0</td>
</tr>
</tbody></table>
<p>  In case 2, we see that TPR = 1/2 &lt; FPR= 2/2=1 and AUC is small. The possibility that positive sample ranked higher than negative sample is small. The prediction possibilty of positive sample is also small than negative sample in sample B,D. Hence the model performance is not good enough.</p>
</li>
</ul>
<br>

<h2 id="Improvement-on-Accuracy-Ensembling-method"><a href="#Improvement-on-Accuracy-Ensembling-method" class="headerlink" title="Improvement on Accuracy: Ensembling method"></a>Improvement on Accuracy: Ensembling method</h2><ul>
<li><p><strong>Bagging (bootstrap aggregation)</strong></p>
<ul>
<li><p><strong>Main Idea:</strong><br><strong>Its goal is to reduce variance</strong> by using multiple classifiers, like decision tree. It uses boostrap method to sample data and train multiple classifiers and then average the prediction over a collection of classifiers (for <strong>continuous value prediction, regression</strong>), or return the prediction with maximum votes (for<br><strong>classification</strong>)<br><strong>Random forest is a bagging approach, which bags a set of decision trees together.</strong> </p>
<br>
</li>
<li><p><strong>Assumptions</strong><br>  we have training set with size of <strong>D</strong> and a set of models with size of <strong>K</strong></p>
</li>
<li><p><strong>Training:</strong> </p>
<ol>
<li>Similar to Bootstrap, at each iteration i, a training set Di of d tuples is sampled with replacement from training set.</li>
<li>A classifier model Mi is learned for each training set Di</li>
</ol>
</li>
<li><p><strong>Prediction:</strong></p>
<ol>
<li>Each Classifier Mi returns prediction for input X. </li>
<li>Discrete value output: The bagged classifier counts the votes and assigns the class with the most votes to X</li>
<li>Continous value output:<br>take the average value of each prediction for a given test sample.</li>
</ol>
</li>
<li><p><strong>Advantages:</strong></p>
<ol>
<li>Better than a single classifier from the classifier set. </li>
<li>More robust in noisy data and hence <strong>smaller variance</strong></li>
</ol>
</li>
<li><p><strong>Disadvantages:</strong><br>  1.<strong>Its training depends on sampling techniques</strong>, which could affect the accuracy</p>
<ol start="2">
<li><strong>The prediction may be not precise</strong>, since it uses average value of classifiers’ predictions.<br>For example, if valid prediction values are 1,2,or 3, then the average of predictions from different model could lead to a floating point number.</li>
</ol>
</li>
<li><p><strong>Properties of Random Forest</strong></p>
<ol>
<li><p>Comparable in accuracy to Adaboost, but more robust to errors and outliers.</p>
</li>
<li><p>Insensitive to the number of attributes selected for consideration at each split, and faster than boosting</p>
<br>
</li>
</ol>
</li>
</ul>
</li>
<li><p><strong>Boosting</strong></p>
<ul>
<li><p><strong>Main Idea:</strong><br><strong>Its goal is to improve accuracy, let models better fit training set.</strong> It uses weighted votes from a collection of classifiers</p>
<br>
</li>
<li><p><strong>Training:</strong> </p>
<ol>
<li>Each training sample/tuple is given a weight, eg $w_i$ for the $i^{th}$ tuple. Then we have training set {(X0,y0, w0), … ,(Xi,yi, wi)}<br>where $X_i$ and $y_i$ are training sample and target</li>
<li>We have k classifiers {M0, M1,…Mk}. Each classifier is learned from the whole training set <strong>iteratively</strong>. That is, if we have k classifiers, then we need to iterate the training set at least k times (at $i^{th}$ iteration, we train the $i^{th}$ classifier),  so that each classifier can learn the training set.</li>
<li>After classifier $M_i$ is learned on training set, classifier $M_{i+1}$ paies more attention to the training samples that are misclassified by $M_i$ <br>   
</li>
</ol>
</li>
<li><p><strong>Prediction:</strong><br>  The final Model combines the votes of each individual classifier. Either find the prediction with largest sum of weights (<strong>Classification</strong>), or find the average of all prediction values (<strong>Regression</strong>)<br>  The weight of each classifier’s vote is a function of its accuracy</p>
<br>
</li>
<li><p><strong>Advantages</strong></p>
<ol>
<li>Boosting can be extended to numeric prediction</li>
<li>Better fit the training set since it adjusts the weights of training set and gives more attention to the misclassified sample.<br>
</li>
</ol>
</li>
<li><p><strong>Disadvantages</strong></p>
<ol>
<li>Easy to overfit. Need Additional techniques to avoid overfitting. (I will discuss the methods dealing with Overfitting ). <br>
</li>
</ol>
</li>
<li><p><strong>Questions:</strong></p>
<ol>
<li>How to <strong>pay more attention</strong> to misclassified samples?  give Higher weights? But How to compute weights?<br><strong>Answer:</strong> This depends on the actual boosting algorithm, like GradientBoosting, AdaBoosting<br>
</li>
</ol>
</li>
<li><p><strong>AdaBoosting</strong></p>
<ol>
<li><p><strong>Assumption:</strong><br>Assume we have training set with size of <strong>D</strong> and a set of classifier models with size of <strong>T</strong></p>
</li>
<li><p>__Error of model $M_i$__</p>
<p> Error($M_i$) = $\sum_i^D (w_i \times err(X_i))$</p>
<p> if using normalized weight (weight in range [0,1]), then<br> Error($M_i$) = $\frac{\sum_i^D (w_i \times err(X_i))}{(\sum_j^D w_j)} $</p>
 <br>

<p> <strong>Note:</strong><br> In classification, $err(X_i)= 1(C_i(X_i) !=Y_i)$, $C_i(X_I)$ means the prediction of model $M_i$ on sample $X_i$. If the prediction is correction $error(X_i) =0$, otherwise 1.</p>
<br>
</li>
<li><p><strong>Weight of model $M_i$’s voting: $\alpha_i$</strong><br> $\alpha_i = log\frac{1-error(M_i)}{error(M_i)} + log(K-1)$.</p>
<p> Note:<br> K =  the number of classes in dataset. When K=1, log(K-1) term can be ignored</p>
<br>
</li>
<li><p><strong>Update of weight</strong><br> $w_i = w_i \cdot exp(\alpha_i \cdot 1(M_j(X_i) != Y_i))$</p>
 <br>

<p> The weight $w_i$ of the $i^{th}$ training tuple $X_i$  is updated by timing exponential value of weight of model <strong>only when this model $M_j$ misclassifies the $X_i$ ( That is $M_j(X_i) !=Y_i$ and hence  $1(M_j(X_i) !=Y_i) =1 $)</strong>. </p>
<br>
</li>
<li><p><strong>Prediction</strong><br> $C(X_i) = argmax_{k} \sum_{j=1}^T \alpha_{m}\cdot 1(M_j(X_i)== Y_i)$<br> where $1(M_j(X_i)== Y_i)$  is equal to 1 if prediction is correct, otherwise, 0.<br> <strong>The prediction of the whole model has the largest sum of weight of models, NOT the weight of training tuple!</strong></p>
<br>
</li>
<li><p>More detail for AdaBoosting, Read <a href="https://web.stanford.edu/~hastie/Papers/samme.pdf" target="_blank" rel="noopener">this paper</a></p>
<br>
</li>
</ol>
</li>
</ul>
</li>
<li><p><strong>Stacking</strong></p>
<ul>
<li><p><strong>Main Idea:</strong><br>It combines and trains a set of <strong>heterogeneous</strong> classifiers in parallel.<br>It consists of 2-level models:<br><strong>level-0: base model</strong><br>Models fit on the training data and whose predictions are compiled.<br><strong>level-1: Meta-Model</strong><br>It learns <strong>how to best combine the predictions</strong> of the base models. </p>
<br>
</li>
<li><p><strong>Training:</strong> </p>
<ol>
<li>split the training data into K-folds </li>
<li>one of base models is fitted on the K-1 parts and predictions are made for Kth part.</li>
<li>Repeat step 2 for each fold</li>
<li>Fit the base model on the whole train data set to calculate its performance on the test set.</li>
<li>repeat step 2~4 for each base model</li>
<li>Predictions from the train set are used as features for the second level model.<br>
</li>
</ol>
</li>
<li><p><strong>Classification:</strong><br>   Second level model is used to make a prediction on the test set.</p>
<br>
</li>
<li><p><strong>Advantage:</strong></p>
<ol>
<li> It harness the capabilities of a range of well-performing models on a classification or regression task and make predictions that have better performance than any single model in the ensemble.        <br>
</li>
</ol>
</li>
<li><p><strong>Disadvantage:</strong></p>
<ol>
<li>It could be computational expensive since it uses k-fold method and use multiple level models.<br>
</li>
</ol>
</li>
</ul>
</li>
<li><p><strong>Comparison among Ensembling, boosting and bagging</strong></p>
<ol>
<li>Goal of bagging is to reduce variance and noise while boosting is to improve accuracy using weighted models. Stacking is to improve accuracy of model using hetergenerous models.</li>
<li>Adaboost let classifiers pay more attention to the misclassified samples, but if those misclassified samples are outlier or noisy data, it will affect a lot and lead to larger variance.<br>However, bagging and ensemble uses averaging and voting methods and each classifier has equal weight, which is less sensitive to the noise data and outlier.</li>
</ol>
</li>
</ul>
<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><p>[1] <a href="https://blog.csdn.net/weixin_37352167/article/details/85028835" target="_blank" rel="noopener">https://blog.csdn.net/weixin_37352167/article/details/85028835</a><br>[2] <a href="https://machinelearningmastery.com/stacking-ensemble-machine-learning-with-python/" target="_blank" rel="noopener">https://machinelearningmastery.com/stacking-ensemble-machine-learning-with-python/</a><br>[3] <a href="https://en.wikipedia.org/wiki/Student%27s_t-test" target="_blank" rel="noopener">https://en.wikipedia.org/wiki/Student%27s_t-test</a><br>[4] <a href="https://media.springernature.com/lw685/springer-static/image/art%3A10.1038%2Fnmeth.3945/MediaObjects/41592_2016_Article_BFnmeth3945_Fig1_HTML.jpg" target="_blank" rel="noopener">https://media.springernature.com/lw685/springer-static/image/art%3A10.1038%2Fnmeth.3945/MediaObjects/41592_2016_Article_BFnmeth3945_Fig1_HTML.jpg</a></p>

      
    </div>

    <div>
      
        
<div id="wechat_subscriber" style="display: block; padding: 10px 0; margin: 20px auto; width: 100%; text-align: center">
    <img id="wechat_subscriber_qcode" src="/images/wechat-qr-code.jpg" alt="Wenkang Wei wechat" style="width: 200px; max-width: 100%;"/>
    <div>subscribe to my blog by scanning my public wechat account</div>
</div>


      
    </div>

    <div>
      
        

      
    </div>


    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/ROC-curve/" rel="tag"># ROC curve</a>
          
            <a href="/tags/Confusion-Metric/" rel="tag"># Confusion Metric</a>
          
            <a href="/tags/Cross-Validation/" rel="tag"># Cross Validation</a>
          
            <a href="/tags/Holdout/" rel="tag"># Holdout</a>
          
            <a href="/tags/Ensemble-Learning/" rel="tag"># Ensemble Learning</a>
          
            <a href="/tags/Model-Evaluation/" rel="tag"># Model Evaluation</a>
          
            <a href="/tags/Model-Selection/" rel="tag"># Model Selection</a>
          
        </div>
      

      
        
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2020/07/29/NLP-Word2Vec-Improvement/" rel="next" title="NLP Improvement on Word2Vector">
                <i class="fa fa-chevron-left"></i> NLP Improvement on Word2Vector
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2020/08/24/data-structure-sorting/" rel="prev" title="Data Structure 2 -sorting">
                Data Structure 2 -sorting <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </article>



    <div class="post-spread">
      
    </div>
  </div>

          
          </div>
          


          
  <div class="comments" id="comments">
    
  </div>


        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap" >
            Table of Contents
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview">
            Overview
          </li>
        </ul>
      

      <section class="site-overview sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
          <img class="site-author-image" itemprop="image"
               src="/images/avatar.jpg"
               alt="Wenkang Wei" />
          <p class="site-author-name" itemprop="name">Wenkang Wei</p>
           
              <p class="site-description motion-element" itemprop="description">second-year master student / Computer engineering / deep learning / machine learning / data science </p>
          
        </div>
        <nav class="site-state motion-element">
        
          
            <div class="site-state-item site-state-posts">
              <a href="/archives">
                <span class="site-state-item-count">26</span>
                <span class="site-state-item-name">posts</span>
              </a>
            </div>
          

          
            <div class="site-state-item site-state-categories">
              <a href="/categories">
                <span class="site-state-item-count">20</span>
                <span class="site-state-item-name">categories</span>
              </a>
            </div>
          

          
            <div class="site-state-item site-state-tags">
              <a href="/tags">
                <span class="site-state-item-count">57</span>
                <span class="site-state-item-name">tags</span>
              </a>
            </div>
          

        </nav>

        

        <div class="links-of-author motion-element">
          
            
              <span class="links-of-author-item">
                <a href="https://github.com/wenkangwei" target="_blank" title="Github">
                  
                    <i class="fa fa-fw fa-globe"></i>
                  
                  Github
                </a>
              </span>
            
              <span class="links-of-author-item">
                <a href="https://www.linkedin.com/in/wenkang-wei-588811167" target="_blank" title="LinkedIn">
                  
                    <i class="fa fa-fw fa-fab fa-linkedin"></i>
                  
                  LinkedIn
                </a>
              </span>
            
              <span class="links-of-author-item">
                <a href="https://www.facebook.com/wenkang.wei" target="_blank" title="Facebook">
                  
                    <i class="fa fa-fw fa-fab fa-facebook"></i>
                  
                  Facebook
                </a>
              </span>
            
          
        </div>

        
        

        
        

        


      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#Introduction"><span class="nav-number">1.</span> <span class="nav-text">Introduction</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Evaluation-Metrics"><span class="nav-number">2.</span> <span class="nav-text">Evaluation Metrics</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Estimation-of-Model-Accuracy"><span class="nav-number">3.</span> <span class="nav-text">Estimation of Model Accuracy</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Comparison-of-performances-among-different-model"><span class="nav-number">4.</span> <span class="nav-text">Comparison of performances among different model</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Model-Selection"><span class="nav-number">5.</span> <span class="nav-text">Model Selection</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Improvement-on-Accuracy-Ensembling-method"><span class="nav-number">6.</span> <span class="nav-text">Improvement on Accuracy: Ensembling method</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Reference"><span class="nav-number">7.</span> <span class="nav-text">Reference</span></a></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright" >
  
  &copy; 
  <span itemprop="copyrightYear">2021</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Wenkang Wei</span>
</div>


<div class="powered-by">
  Powered by <a class="theme-link" href="https://hexo.io" target="_blank" rel="noopener">Hexo</a>
</div>

<div class="theme-info">
  Theme -
  <a class="theme-link" href="https://github.com/iissnan/hexo-theme-next">
    NexT.Mist
  </a>
</div>


        

        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
          <span id="scrollpercent"><span>0</span>%</span>
        
      </div>
    
    
  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  




  
  <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>

  
  <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>

  
  <script type="text/javascript" src="[object Object]"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.0"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.0"></script>



  
  

  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.0"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.0"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.0"></script>



  



  




	





  





  





  




  <script src="//cdn1.lncld.net/static/js/3.0.4/av-min.js"></script>
  <script src="//unjkp.com/valine/dist/Valine.min.js"></script>  
  <script src="/js/src/Valine.min.js"></script> 

   
  
  <script src="/js/src/Valine.min.js"></script>
  
  <script type="text/javascript">
    var GUEST = ['nick','mail','link'];
    var guest = 'nick';
    guest = guest.split(',').filter(function (item) {
      return GUEST.indexOf(item)>-1;
    });
    new Valine({
        el: '#comments' ,
        verify: true,
        notify: true,
        appId: 'kvXCKmDNxnA2486N29e5cP7i-MdYXbMMI',
        appKey: '0Lv5b0SqotzkGHQvD64u4AKo',
        placeholder: '说点什么吧！',
        avatar:'hide',
        guest_info:['nick'] , 
        pageSize:'10' || 10,
    });
   
    var infoEle = document.querySelector('#comments .info');
    if (infoEle && infoEle.childNodes && infoEle.childNodes.length > 0){
      infoEle.childNodes.forEach(function(item) {
        item.parentNode.removeChild(item);
      });
    }
  </script>

  
  

  
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/javascript" src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
  


  

  

  


  

</body>
</html>
