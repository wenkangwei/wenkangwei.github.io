<!doctype html>
<html lang="en"><head><meta charset="utf-8"><meta name="generator" content="Hexo 4.2.1"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta><title>Model Evaluation and Selection - Wenkang&#039;s Blog</title><meta description=""><meta property="og:type" content="article"><meta property="og:title" content="Model Evaluation and Selection"><meta property="og:url" content="https://github.com/wenkangwei/"><meta property="og:site_name" content="Wenkang&#039;s Blog"><meta property="og:locale" content="en_US"><meta property="article:published_time" content="2020-08-13T21:27:44.000Z"><meta property="article:modified_time" content="2020-12-08T17:37:14.220Z"><meta property="article:author" content="Wenkang Wei"><meta property="article:tag" content="ROC curve"><meta property="article:tag" content="Confusion Metric"><meta property="article:tag" content="Cross Validation"><meta property="article:tag" content="Holdout"><meta property="article:tag" content="Ensemble Learning"><meta property="article:tag" content="Model Evaluation"><meta property="article:tag" content="Model Selection"><meta property="twitter:card" content="summary"><script type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"https://github.com/wenkangwei/wenkangwei.github.io%60/2020/08/13/Model-Eval-Selection/"},"headline":"Wenkang's Blog","image":[],"datePublished":"2020-08-13T21:27:44.000Z","dateModified":"2020-12-08T17:37:14.220Z","author":{"@type":"Person","name":"Wenkang Wei"},"description":""}</script><link rel="canonical" href="https://github.com/wenkangwei/wenkangwei.github.io%60/2020/08/13/Model-Eval-Selection/"><link rel="icon" href="/images/icon.png"><link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.12.0/css/all.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@9.12.0/styles/atom-one-light.css"><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Ubuntu:wght@400;600&amp;family=Source+Code+Pro"><link rel="stylesheet" href="/css/default.css"><style>body>.footer,body>.navbar,body>.section{opacity:0}</style><!--!--><!--!--><!--!--><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/lightgallery@1.6.8/dist/css/lightgallery.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/justifiedGallery@3.7.0/dist/css/justifiedGallery.min.css"><!--!--><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/pace-js@1.0.2/pace.min.js"></script></head><body class="is-3-column"><nav class="navbar navbar-main"><div class="container"><div class="navbar-brand justify-content-center"><a class="navbar-item navbar-logo" href="/"><img src="/images/icon.png" alt="Wenkang&#039;s Blog" height="28"></a></div><div class="navbar-menu"><div class="navbar-start"><a class="navbar-item" href="/">Home</a><a class="navbar-item" href="/archives">Archives</a><a class="navbar-item" href="/categories">Categories</a><a class="navbar-item" href="/tags">Tags</a><a class="navbar-item" href="/about">About</a></div><div class="navbar-end"><a class="navbar-item" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/ppoffice/hexo-theme-icarus"><i class="fab fa-github"></i></a><a class="navbar-item is-hidden-tablet catalogue" title="Catalogue" href="javascript:;"><i class="fas fa-list-ul"></i></a><a class="navbar-item search" title="Search" href="javascript:;"><i class="fas fa-search"></i></a></div></div></div></nav><section class="section"><div class="container"><div class="columns"><div class="column order-2 column-main is-8-tablet is-8-desktop is-6-widescreen"><div class="card"><article class="card-content article" role="article"><div class="article-meta size-small is-uppercase level is-mobile"><div class="level-left"><time class="level-item" dateTime="2020-08-13T21:27:44.000Z" title="2020-08-13T21:27:44.000Z">2020-08-13</time><span class="level-item"><a class="link-muted" href="/categories/Machine-Learning/">Machine Learning</a></span><span class="level-item">20 minutes read (About 3064 words)</span></div></div><h1 class="title is-3 is-size-4-mobile">Model Evaluation and Selection</h1><div class="content"><img src=https://media.springernature.com/lw685/springer-static/image/art%3A10.1038%2Fnmeth.3945/MediaObjects/41592_2016_Article_BFnmeth3945_Fig1_HTML.jpg>

<a id="more"></a>

<h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>In order to do something better in our study, our jobs or even our life,  evaluation step is an indispensable part of improvement. Otherwise, How do we know our work is good or bad? In addition, “good” and “bad” are ambiguous terms if we don’t have any evaluation methods.</p>
<p>Similarly, in machine learning, in order to training a “better” model, we need concrete evaluation metrics to tell us if our models perform well, so that we can choose the best one.</p>
<p>The goal of this passage is to conclude the common useful ways to evaluate and improve our machine learning models. My Thoughts are also provided.</p>
<h2 id="Evaluation-Metrics"><a href="#Evaluation-Metrics" class="headerlink" title="Evaluation Metrics"></a>Evaluation Metrics</h2><ul>
<li><p><strong>Confusion Matrix</strong><br>In classification task, we have predictions from our model on the test set and ground truth labels/targets indicating the real class of each sample in dataset.<br>Let consider we are predicting if data belongs to  class “C” or not.</p>
<p>  <strong>Then Confusion matrix is</strong></p>
<table>
<thead>
<tr>
<th>Actual class\prediction</th>
<th>C</th>
<th>not C</th>
<th>sum</th>
</tr>
</thead>
<tbody><tr>
<td><strong>C</strong></td>
<td>TP</td>
<td>FN</td>
<td>actual P= TP+FN</td>
</tr>
<tr>
<td><strong>Not C</strong></td>
<td>FP</td>
<td>TN</td>
<td>actual N = FP+TN</td>
</tr>
<tr>
<td>sum</td>
<td>predicted P’ = TP+FP</td>
<td>predicted N’ = FN+TN</td>
<td>All</td>
</tr>
</tbody></table>
</li>
</ul>
<p>True positive (TP): the amount of samples that are <strong>predicted</strong> as class “C” and they <strong>actually</strong> belong to class “C”.</p>
<p>False positive (FP): the amount of samples that are <strong>predicted</strong> as class “C” and they <strong>actually</strong> DON’T belong to class “C”.</p>
<p>False Negative (FN): the amount of samples that are <strong>predicted</strong> as Not class “C” and they <strong>actually</strong> belong to class “C”.</p>
<p>True Negative (TN): the amount of samples that are <strong>predicted</strong> as not class “C” and they <strong>actually</strong> don’t belong to class “C”.</p>
<ul>
<li><p><strong>Accuracy</strong>:<br>measure how accurate the predictions are<br>  <strong>$acc = \frac{TP+TN}{TP+TN+FP+FN}$</strong></p>
<br></li>
<li><p><strong>Error Rate:  1-acc</strong></p>
<br>
</li>
<li><p><strong>Sensitivity/Recall</strong>: (Precentage of <strong>correct positive</strong> prediction on <strong>Actual positive</strong> )<br>Recognition rate on True positive:<br>$sens = \frac{TP}{predicted P’} = \frac{TP}{TP+FN}$</p>
</li>
</ul>
<br>

<ul>
<li><strong>Specificity</strong>:(Precentage of <strong>correct negative</strong> prediction on <strong>Actual negative</strong> )<br><strong>How many positive predictions in true data are recognized by model?</strong><br>Recognition rate on True Negative:<br>  $spec = \frac{TN}{predicted N’} = \frac{TN}{TN+FP}$</li>
</ul>
<br>


<ul>
<li><p><strong>Precision</strong>: (Percentage of correct positive prediction on <strong>all positive prediction</strong>)<br>  Measure what % of tuples that the classifier labeled as positive are actually positive<br>  <strong>How many positive predictions of your model are correct?</strong></p>
<p>  $precision = \frac{TP}{predicted P’} = \frac{TP}{TP+FP}$</p>
</li>
</ul>
<br>

<ul>
<li><p>__F-score: $F_\beta$__: It weighs precision and recall</p>
<p>  $$ F_\beta = \frac{(1+\beta^2) * precision*recall}{\beta^2 * precision+recall}$$</p>
  <br>
  $\beta$ controls the weight of precision. Higher $\beta$ weigh more to precision than recall and hence precision is given more attention.
<br>
</li>
<li><p><strong>F-1 score ($\beta =1$)</strong>:</p>
<p>  $$ F_1 = \frac{2 * precision * recall}{precision+recall}$$</p>
  <br>
  Both precision and recall are given equal weights.
</li>
<li><p><strong>When to use Recall, Sensitivity and F-1 to evaluate model?</strong></p>
<ul>
<li><p><strong>Sensitivity/Recall</strong>:<br>When we want the model to be more sensitive to positive cases and would like to predict more False Positive than False Negative.<br><strong>Example:</strong><br>In <strong>disease detection</strong>, we would like to use recall more than precision, since we want to detect disease and cure earily, we only care if we get disease or not (Positive or<br>  not)<br><strong>Cybersecurity</strong>, fault detection. We care if fault occurs/ prediction is positive or not only. Even if more FP than FN are in prediction, it help us reduce the possibility of missing faults (FN)</p>
</li>
<li><p><strong>Precision</strong>:<br>when we care  <strong>real positive</strong> cases, or both FN and TP. That is, <strong>we don’t want the model to  predict more FP than FN</strong>. False negative also matters.<br><strong>Example:</strong><br>Spam detection: when mailbox detects a spam, it will directly delete/remove that email. However, if that email is actually not a spam (False positive), but an important email, then deleting it leads to an unexpected result.<br>In this case, we want to reduce the false positive cases to avoid deleting an important email. Hence, we don’t use FP, but use FN and precision instead. </p>
</li>
<li><p><strong>F-Score</strong>:<br>when we care both precision and recall, but want to weigh them.<br>This depends on the assumption in the real cases. If the real case assumes both precision and recall are important, then we use F-score.</p>
</li>
<li><p><strong>Score used for GAN</strong>:<br>Inception Score, FID. For more details, Please read this <a href="https://medium.com/@jonathan_hui/gan-how-to-measure-gan-performance-64b988c47732">article</a></p>
<br>


</li>
</ul>
</li>
</ul>
<h2 id="Estimation-of-Model-Accuracy"><a href="#Estimation-of-Model-Accuracy" class="headerlink" title="Estimation of Model Accuracy"></a>Estimation of Model Accuracy</h2><ul>
<li><p><strong>Holdout</strong><br>  The purpose of holdout is to  evaluate how well the model performs on the unseen dataset. <strong>It is used to evaluate the both the framework of model and the hyperparameters of model</strong><br>  <strong>Process:</strong></p>
<ol>
<li><p>Split dataset into <strong>two independent datasets randomly</strong>: training set (usually 80%) and test set (usually 20%)</p>
</li>
<li><p>Train the initial model with training set and then test model with test set</p>
</li>
<li><p>Repeat steps 1~2 k times and <strong>calculate average accuracy of the k accuracies obtained</strong></p>
</li>
<li><p><strong>Note:</strong></p>
</li>
</ol>
<ul>
<li>The test sets in k iterations may be repeated</li>
<li>Accuracy of model could be unstable due to the split method on dataset.<br>For example, if after randomly spliting dataset into training set and test set, all samples belonging to class “C” are in test set, then model can not learn features from class “C” and hence perform worse. Otherwise, it could perform better if it learn features from class “C” from training set.</li>
</ul>
</li>
</ul>
<br>

<ul>
<li><p><strong>Cross Validation</strong><br>  The purpose of Cross Validation is to <strong>evaluate the framework of the model, rather than how good the hyper parameters of the model is.</strong></p>
<ul>
<li><p><strong>K-fold</strong></p>
  <img src=/images/model-eval/kfolds.png>
  <br>
  1. Split the training set into k-subsets: {D1, D2,..Dk} with (almost) equal size.
  2. Loop through k iterations. At $i^th$  iteration, select subset Di as validation set and the remaining k-1 subsets as training set.
  3. Compute accuracy of each iteration
  4. Compute the mean of accuracies obtained in k iterations.
  <br>
</li>
<li><p><strong>Leave One out</strong></p>
<ol>
<li>It is a case of k-fold cross validation with <strong>setting K= N, the number of data point in training set</strong>.</li>
<li>It hence takes N-1 iterations to evaluate model.<br>
</li>
</ol>
</li>
<li><p><strong>Stratified Cross validation</strong></p>
<ol>
<li>It is a case of K-fold cross validation with each setting:  <strong>each set contains approximately the same percentage of samples of each target class as the complete set</strong>.<br>
__For example:__
In a dataset S= {A1,A2,A3,B1,B2,B3,C1,C2,C3}, which has classes A,B,C. Each class occupies 30% of total dataset.In Stratified 3-fold Cross-validation, S= {K1,K2,K3}. 
To make each class of data in each fold have the same percentage, we have:
K1 ={A1,B1,C1},
K2 ={A2,B2,C2},
K3 ={A3,B3,C3}
such that the percentage of each class of sample in each fold is still 30%.
<br>
</li>
</ol>
</li>
<li><p><strong>Note for Cross-Validation</strong></p>
<ol>
<li>When <strong>K</strong>  is small, the variance in performance of model could be large, since each fold contains more data and hence becomes noisy.</li>
<li>When <strong>K</strong> is large, eg. in LeaveOneOut, <strong>K=N</strong>,  variance of performance of model would be smaller.</li>
<li><strong>In each fold of training set, we need to start training the initial model again, rather than training the model from the last fold.</strong></li>
<li>Cross validation is to <strong>evaluate the performance of the framework of the model</strong>, rather than of the parameters of the model.<br>
</li>
</ol>
</li>
</ul>
</li>
<li><p><strong>Bootstrap (Sample-based)</strong> </p>
<ol>
<li><strong>Samples</strong> the given training tuples uniformly <strong>with replacement</strong> <strong>d</strong> times as training set. <strong>The remaining data tuples form the test set (or validation set)</strong>.<br>That is,<br>the tuples/samples that have been chosen can still be chosen with equal possibility</li>
<li>Train model with training set</li>
<li>Compute average accuracy of model on both training set and test set</li>
<li>Repeat steps 1~3 k times, then compute average accuracy of all accuracy obtained in k iterations.<br>
</li>
</ol>
</li>
<li><p><strong>Comparison among methods Above</strong></p>
<ol>
<li><p><strong>holdout</strong><br> <strong>Purpose</strong><br> Holdout is mainly used to evaluate how well the model performs on the unseen dataset. It  evaluates both framework and hyperparameter of the model.<br> <em>Different from cross-validation, the <strong>holdout data can be any size (usually 20% of the training dataset), while cross-validation requires validation data has the same size as each fold.</strong> When the amount of fold is small (like 2) in cross-validation, it could waste the training set.</em></p>
 <br>

<p> <strong>Advantages</strong></p>
<ul>
<li><p><em>Less expensive in computation, Easy to compute,</em> compared with cross-validation, since it splits test set randomly </p>
<p><strong>Disadvantages</strong></p>
</li>
<li><p>Estimated accuracy may be unstable (Accuracy is easy to change), since holdout <strong>depends on the dataset split methods</strong> on training set and test set. </p>
<p><strong>When to Use</strong></p>
</li>
<li><p>When the dataset is very large and hard to compute multiple subset</p>
<br>
</li>
</ul>
</li>
<li><p><strong>Cross-validation</strong><br> <strong>Advantages</strong></p>
<ul>
<li><p><strong>The estimated accuracy is much stable than holdout</strong> , since it trains model on multiple different train-test set splits. It guarantees to  train model with all samples.</p>
<p><strong>Disadvantages</strong></p>
</li>
<li><p> The Cost for computation is expensive when dataset is very large.</p>
</li>
<li><p>When value <strong>K</strong>  is <strong>smaller</strong>, the <strong>variance</strong> in performance of model will be <strong>larger</strong> and model is <strong>easier to overfit</strong> (training with more data samples could be more noisy).</p>
</li>
<li><p>When value <strong>K</strong>  is <strong>smaller</strong>, <strong>it could waste the training set</strong>. For example, in 2-fold, we use only half of training set to train the model, which is a waste of training data.</p>
<p><strong>When to Use</strong></p>
</li>
<li><p>when dataset is not very large (10000 samples or even more)</p>
</li>
<li><p>when you have powerful computational device. </p>
<br>
</li>
</ul>
</li>
<li><p><strong>Bootstrap</strong><br> <strong>Advantages</strong></p>
<ul>
<li><p>Performance of model doesn’t depend on the split method on dataset</p>
</li>
<li><p>When dataset is small or insufficient, or imbalanced (some classes are more than other significantly), it may reduce overfitting effect by sampling with replacement</p>
</li>
<li><p>Less expensive on computation compared with cross validation</p>
<p><strong>Disadvantages</strong></p>
</li>
<li><p>Need to determine what sampling method to use</p>
<p><strong>When to Use</strong></p>
</li>
<li><p>when dataset is small or insufficient (In this case, we may use Over-sampling to repeat some data)</p>
</li>
<li><p>when dataset is imbalanced</p>
</li>
<li><p>when dataset is pretty large (In this case, we may use down-sampling to select part of data for training)</p>
</li>
</ul>
</li>
</ol>
</li>
</ul>
<h2 id="Comparison-of-performances-among-different-model"><a href="#Comparison-of-performances-among-different-model" class="headerlink" title="Comparison of performances among different model"></a>Comparison of performances among different model</h2><p><strong>How do we know the performances between models are similar?</strong></p>
<ul>
<li><strong>t-Test / Student’s t test</strong> Read this <a href="https://en.wikipedia.org/wiki/Student%27s_t-test">paper</a></li>
</ul>
<h2 id="Model-Selection"><a href="#Model-Selection" class="headerlink" title="Model Selection"></a>Model Selection</h2><ul>
<li><p><strong>ROC curve</strong>: receiver operating charatistic curve (true positive rate -VS- False positive rate)<br>we first let model output possibility of each class and then set the threshold to convert possibility to 0, 1 binary classification labels.<br>The threshold is usually default as 0.5.<br>Based on the binary classification, compute true positive rate <strong>TPR</strong>/recall  (<strong>TP/(TP+FN)</strong>) and false positive rate/<strong>FPR</strong> (<strong>FP/(FP+TN) = FP/real negative</strong>)<br>Since threshold is set to 0.5, during training of model, the performance of model will change and hence TPR and FPR will change.<br>The changing TPR and FPR leads to the ROC curve</p>
<br>
</li>
<li><p><strong>AUC</strong>: area under curve ( ROC curve)<br>Since TPR and FPR range from 0 to 1, hence AUC has range [0, 1]. AUC usually is inside [0.5, 1] because a good model usually can classify sample correctly with 0.5 possibility.</p>
<br>
</li>
<li><p><strong>Physical Meaning of AUC:</strong><br><strong>AUC is the possibility that a random-chosen positive sample is ranked more highly than a randomly-chosen negative sample.</strong><br>In other words, AUC is the <strong>possibility that a model’s prediction possibility of a random positive sample is more higher than a random negative sample</strong>. Then when AUC of a model is larger, it is more likely for the model to predict positive sample as 1  , rather than negative sample as 1.</p>
<br>
</li>
<li><p>Example: when <strong>threshold =0.5</strong><br>  <strong>Case 1</strong></p>
<table>
<thead>
<tr>
<th>Sample</th>
<th>A</th>
<th>B</th>
<th>C</th>
<th>D</th>
</tr>
</thead>
<tbody><tr>
<td>Possibility</td>
<td>0.9</td>
<td>0.8</td>
<td>0.51</td>
<td>0.3</td>
</tr>
<tr>
<td>Prediction</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>0</td>
</tr>
<tr>
<td>Ground Truth</td>
<td>1</td>
<td>1</td>
<td>0</td>
<td>0</td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>In case 1 we see that possibility that positive sample is ranked higher than negative sample, since all positive sample has possibility greater than negative sample and TPR = 2/2=1 as FPR = 1/2 =0.5 &lt; TPR. Hence in this case, it is a good model.</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
</tbody></table>
<p>  <strong>Case 2</strong></p>
<table>
<thead>
<tr>
<th>Sample</th>
<th>A</th>
<th>B</th>
<th>C</th>
<th>D</th>
</tr>
</thead>
<tbody><tr>
<td>Possibility</td>
<td>0.9</td>
<td>0.3</td>
<td>0.51</td>
<td>0.6</td>
</tr>
<tr>
<td>Prediction</td>
<td>1</td>
<td>0</td>
<td>1</td>
<td>1</td>
</tr>
<tr>
<td>Ground Truth</td>
<td>1</td>
<td>1</td>
<td>0</td>
<td>0</td>
</tr>
</tbody></table>
<p>  In case 2, we see that TPR = 1/2 &lt; FPR= 2/2=1 and AUC is small. The possibility that positive sample ranked higher than negative sample is small. The prediction possibilty of positive sample is also small than negative sample in sample B,D. Hence the model performance is not good enough.</p>
</li>
</ul>
<br>

<h2 id="Improvement-on-Accuracy-Ensembling-method"><a href="#Improvement-on-Accuracy-Ensembling-method" class="headerlink" title="Improvement on Accuracy: Ensembling method"></a>Improvement on Accuracy: Ensembling method</h2><ul>
<li><p><strong>Bagging (bootstrap aggregation)</strong></p>
<ul>
<li><p><strong>Main Idea:</strong><br><strong>Its goal is to reduce variance</strong> by using multiple classifiers, like decision tree. It uses boostrap method to sample data and train multiple classifiers and then average the prediction over a collection of classifiers (for <strong>continuous value prediction, regression</strong>), or return the prediction with maximum votes (for<br><strong>classification</strong>)<br><strong>Random forest is a bagging approach, which bags a set of decision trees together.</strong> </p>
<br>
</li>
<li><p><strong>Assumptions</strong><br>  we have training set with size of <strong>D</strong> and a set of models with size of <strong>K</strong></p>
</li>
<li><p><strong>Training:</strong> </p>
<ol>
<li>Similar to Bootstrap, at each iteration i, a training set Di of d tuples is sampled with replacement from training set.</li>
<li>A classifier model Mi is learned for each training set Di</li>
</ol>
</li>
<li><p><strong>Prediction:</strong></p>
<ol>
<li>Each Classifier Mi returns prediction for input X. </li>
<li>Discrete value output: The bagged classifier counts the votes and assigns the class with the most votes to X</li>
<li>Continous value output:<br>take the average value of each prediction for a given test sample.</li>
</ol>
</li>
<li><p><strong>Advantages:</strong></p>
<ol>
<li>Better than a single classifier from the classifier set. </li>
<li>More robust in noisy data and hence <strong>smaller variance</strong></li>
</ol>
</li>
<li><p><strong>Disadvantages:</strong><br>  1.<strong>Its training depends on sampling techniques</strong>, which could affect the accuracy</p>
<ol start="2">
<li><strong>The prediction may be not precise</strong>, since it uses average value of classifiers’ predictions.<br>For example, if valid prediction values are 1,2,or 3, then the average of predictions from different model could lead to a floating point number.</li>
</ol>
</li>
<li><p><strong>Properties of Random Forest</strong></p>
<ol>
<li><p>Comparable in accuracy to Adaboost, but more robust to errors and outliers.</p>
</li>
<li><p>Insensitive to the number of attributes selected for consideration at each split, and faster than boosting</p>
<br>
</li>
</ol>
</li>
</ul>
</li>
<li><p><strong>Boosting</strong></p>
<ul>
<li><p><strong>Main Idea:</strong><br><strong>Its goal is to improve accuracy, let models better fit training set.</strong> It uses weighted votes from a collection of classifiers</p>
<br>
</li>
<li><p><strong>Training:</strong> </p>
<ol>
<li>Each training sample/tuple is given a weight, eg $w_i$ for the $i^{th}$ tuple. Then we have training set {(X0,y0, w0), … ,(Xi,yi, wi)}<br>where $X_i$ and $y_i$ are training sample and target</li>
<li>We have k classifiers {M0, M1,…Mk}. Each classifier is learned from the whole training set <strong>iteratively</strong>. That is, if we have k classifiers, then we need to iterate the training set at least k times (at $i^{th}$ iteration, we train the $i^{th}$ classifier),  so that each classifier can learn the training set.</li>
<li>After classifier $M_i$ is learned on training set, classifier $M_{i+1}$ paies more attention to the training samples that are misclassified by $M_i$ <br>   
</li>
</ol>
</li>
<li><p><strong>Prediction:</strong><br>  The final Model combines the votes of each individual classifier. Either find the prediction with largest sum of weights (<strong>Classification</strong>), or find the average of all prediction values (<strong>Regression</strong>)<br>  The weight of each classifier’s vote is a function of its accuracy</p>
<br>
</li>
<li><p><strong>Advantages</strong></p>
<ol>
<li>Boosting can be extended to numeric prediction</li>
<li>Better fit the training set since it adjusts the weights of training set and gives more attention to the misclassified sample.<br>
</li>
</ol>
</li>
<li><p><strong>Disadvantages</strong></p>
<ol>
<li>Easy to overfit. Need Additional techniques to avoid overfitting. (I will discuss the methods dealing with Overfitting ). <br>
</li>
</ol>
</li>
<li><p><strong>Questions:</strong></p>
<ol>
<li>How to <strong>pay more attention</strong> to misclassified samples?  give Higher weights? But How to compute weights?<br><strong>Answer:</strong> This depends on the actual boosting algorithm, like GradientBoosting, AdaBoosting<br>
</li>
</ol>
</li>
<li><p><strong>AdaBoosting</strong></p>
<ol>
<li><p><strong>Assumption:</strong><br>Assume we have training set with size of <strong>D</strong> and a set of classifier models with size of <strong>T</strong></p>
</li>
<li><p>__Error of model $M_i$__</p>
<p> Error($M_i$) = $\sum_i^D (w_i \times err(X_i))$</p>
<p> if using normalized weight (weight in range [0,1]), then<br> Error($M_i$) = $\frac{\sum_i^D (w_i \times err(X_i))}{(\sum_j^D w_j)} $</p>
 <br>

<p> <strong>Note:</strong><br> In classification, $err(X_i)= 1(C_i(X_i) !=Y_i)$, $C_i(X_I)$ means the prediction of model $M_i$ on sample $X_i$. If the prediction is correction $error(X_i) =0$, otherwise 1.</p>
<br>
</li>
<li><p><strong>Weight of model $M_i$’s voting: $\alpha_i$</strong><br> $\alpha_i = log\frac{1-error(M_i)}{error(M_i)} + log(K-1)$.</p>
<p> Note:<br> K =  the number of classes in dataset. When K=1, log(K-1) term can be ignored</p>
<br>
</li>
<li><p><strong>Update of weight</strong><br> $w_i = w_i \cdot exp(\alpha_i \cdot 1(M_j(X_i) != Y_i))$</p>
 <br>

<p> The weight $w_i$ of the $i^{th}$ training tuple $X_i$  is updated by timing exponential value of weight of model <strong>only when this model $M_j$ misclassifies the $X_i$ ( That is $M_j(X_i) !=Y_i$ and hence  $1(M_j(X_i) !=Y_i) =1 $)</strong>. </p>
<br>
</li>
<li><p><strong>Prediction</strong><br> $C(X_i) = argmax_{k} \sum_{j=1}^T \alpha_{m}\cdot 1(M_j(X_i)== Y_i)$<br> where $1(M_j(X_i)== Y_i)$  is equal to 1 if prediction is correct, otherwise, 0.<br> <strong>The prediction of the whole model has the largest sum of weight of models, NOT the weight of training tuple!</strong></p>
<br>
</li>
<li><p>More detail for AdaBoosting, Read <a href="https://web.stanford.edu/~hastie/Papers/samme.pdf">this paper</a></p>
<br>
</li>
</ol>
</li>
</ul>
</li>
<li><p><strong>Stacking</strong></p>
<ul>
<li><p><strong>Main Idea:</strong><br>It combines and trains a set of <strong>heterogeneous</strong> classifiers in parallel.<br>It consists of 2-level models:<br><strong>level-0: base model</strong><br>Models fit on the training data and whose predictions are compiled.<br><strong>level-1: Meta-Model</strong><br>It learns <strong>how to best combine the predictions</strong> of the base models. </p>
<br>
</li>
<li><p><strong>Training:</strong> </p>
<ol>
<li>split the training data into K-folds </li>
<li>one of base models is fitted on the K-1 parts and predictions are made for Kth part.</li>
<li>Repeat step 2 for each fold</li>
<li>Fit the base model on the whole train data set to calculate its performance on the test set.</li>
<li>repeat step 2~4 for each base model</li>
<li>Predictions from the train set are used as features for the second level model.<br>
</li>
</ol>
</li>
<li><p><strong>Classification:</strong><br>   Second level model is used to make a prediction on the test set.</p>
<br>
</li>
<li><p><strong>Advantage:</strong></p>
<ol>
<li> It harness the capabilities of a range of well-performing models on a classification or regression task and make predictions that have better performance than any single model in the ensemble.        <br>
</li>
</ol>
</li>
<li><p><strong>Disadvantage:</strong></p>
<ol>
<li>It could be computational expensive since it uses k-fold method and use multiple level models.<br>
</li>
</ol>
</li>
</ul>
</li>
<li><p><strong>Comparison among Ensembling, boosting and bagging</strong></p>
<ol>
<li>Goal of bagging is to reduce variance and noise while boosting is to improve accuracy using weighted models. Stacking is to improve accuracy of model using hetergenerous models.</li>
<li>Adaboost let classifiers pay more attention to the misclassified samples, but if those misclassified samples are outlier or noisy data, it will affect a lot and lead to larger variance.<br>However, bagging and ensemble uses averaging and voting methods and each classifier has equal weight, which is less sensitive to the noise data and outlier.</li>
</ol>
</li>
</ul>
<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><p>[1] <a href="https://blog.csdn.net/weixin_37352167/article/details/85028835">https://blog.csdn.net/weixin_37352167/article/details/85028835</a><br>[2] <a href="https://machinelearningmastery.com/stacking-ensemble-machine-learning-with-python/">https://machinelearningmastery.com/stacking-ensemble-machine-learning-with-python/</a><br>[3] <a href="https://en.wikipedia.org/wiki/Student%27s_t-test">https://en.wikipedia.org/wiki/Student%27s_t-test</a><br>[4] <a href="https://media.springernature.com/lw685/springer-static/image/art%3A10.1038%2Fnmeth.3945/MediaObjects/41592_2016_Article_BFnmeth3945_Fig1_HTML.jpg">https://media.springernature.com/lw685/springer-static/image/art%3A10.1038%2Fnmeth.3945/MediaObjects/41592_2016_Article_BFnmeth3945_Fig1_HTML.jpg</a></p>
</div><div class="article-tags size-small mb-4"><span class="mr-2">#</span><a class="link-muted mr-2" rel="tag" href="/tags/ROC-curve/">ROC curve</a><a class="link-muted mr-2" rel="tag" href="/tags/Confusion-Metric/">Confusion Metric</a><a class="link-muted mr-2" rel="tag" href="/tags/Cross-Validation/">Cross Validation</a><a class="link-muted mr-2" rel="tag" href="/tags/Holdout/">Holdout</a><a class="link-muted mr-2" rel="tag" href="/tags/Ensemble-Learning/">Ensemble Learning</a><a class="link-muted mr-2" rel="tag" href="/tags/Model-Evaluation/">Model Evaluation</a><a class="link-muted mr-2" rel="tag" href="/tags/Model-Selection/">Model Selection</a></div><div class="sharethis-inline-share-buttons"></div><script src="https://ppoffice.github.io/hexo-theme-icarus-categorites-Plugins/Share/" defer></script></article></div><!--!--><nav class="post-navigation mt-4 level is-mobile"><div class="level-start"><a class="article-nav-prev level level-item link-muted" href="/2020/08/24/data-structure-sorting/"><i class="level-item fas fa-chevron-left"></i><span class="level-item">Data Structure 2 -sorting</span></a></div><div class="level-end"><a class="article-nav-next level level-item link-muted" href="/2020/07/29/NLP-Word2Vec-Improvement/"><span class="level-item">NLP Improvement on Word2Vector</span><i class="level-item fas fa-chevron-right"></i></a></div></nav><div class="card"><div class="card-content"><h3 class="title is-5">Comments</h3><div class="content" id="valine-thread"></div><script src="//cdn1.lncld.net/static/js/3.0.4/av-min.js"></script><script src="https://cdn.jsdelivr.net/npm/valine@1.4.14/dist/Valine.min.js"></script><script>new Valine({
            el: '#valine-thread' ,
            appId: "kvXCKmDNxnA2486N29e5cP7i-MdYXbMMI",
            appKey: "0Lv5b0SqotzkGHQvD64u4AKo",
            
            avatar: "mm",
            
            meta: ["nick","mail","link"],
            pageSize: 10,
            lang: "en",
            
            highlight: true,
            
            
            
            
            
            requiredFields: [],
        });</script></div></div></div><div class="column column-left is-4-tablet is-4-desktop is-3-widescreen  order-1"><div class="card widget"><div class="card-content"><div class="menu"><h3 class="menu-label">Links</h3><ul class="menu-list"><li><a class="level is-mobile is-mobile" href="https://github.com/wenkangwei" target="_blank" rel="noopener"><span class="level-left"><span class="level-item">Github</span></span><span class="level-right"><span class="level-item tag">github.com</span></span></a></li></ul></div></div></div><div class="card widget"><div class="card-content"><div class="menu"><h3 class="menu-label">Categories</h3><ul class="menu-list"><li><a class="level is-mobile is-marginless" href="/categories/Data-Collection/"><span class="level-start"><span class="level-item">Data Collection</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile is-marginless" href="/categories/Data-Structure/"><span class="level-start"><span class="level-item">Data Structure</span></span><span class="level-end"><span class="level-item tag">2</span></span></a><ul class="mr-0"><li><a class="level is-mobile is-marginless" href="/categories/Data-Structure/Sorting/"><span class="level-start"><span class="level-item">Sorting</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></li><li><a class="level is-mobile is-marginless" href="/categories/Deep-Learning/"><span class="level-start"><span class="level-item">Deep Learning</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile is-marginless" href="/categories/Linux/"><span class="level-start"><span class="level-item">Linux</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile is-marginless" href="/categories/Machine-Learning/"><span class="level-start"><span class="level-item">Machine Learning</span></span><span class="level-end"><span class="level-item tag">4</span></span></a><ul class="mr-0"><li><a class="level is-mobile is-marginless" href="/categories/Machine-Learning/Ensemble-Method/"><span class="level-start"><span class="level-item">Ensemble Method</span></span><span class="level-end"><span class="level-item tag">1</span></span></a><ul class="mr-0"><li><a class="level is-mobile is-marginless" href="/categories/Machine-Learning/Ensemble-Method/Accuracy-Improvement/"><span class="level-start"><span class="level-item">Accuracy Improvement</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></li></ul></li><li><a class="level is-mobile is-marginless" href="/categories/NLP/"><span class="level-start"><span class="level-item">NLP</span></span><span class="level-end"><span class="level-item tag">2</span></span></a><ul class="mr-0"><li><a class="level is-mobile is-marginless" href="/categories/NLP/Machine-Learning/"><span class="level-start"><span class="level-item">Machine Learning</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li></ul></li><li><a class="level is-mobile is-marginless" href="/categories/Parallel-Computing/"><span class="level-start"><span class="level-item">Parallel Computing</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile is-marginless" href="/categories/Programming/"><span class="level-start"><span class="level-item">Programming</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile is-marginless" href="/categories/PySpark/"><span class="level-start"><span class="level-item">PySpark</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile is-marginless" href="/categories/Recommendation-System/"><span class="level-start"><span class="level-item">Recommendation System</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li><li><a class="level is-mobile is-marginless" href="/categories/Report/"><span class="level-start"><span class="level-item">Report</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile is-marginless" href="/categories/Searching/"><span class="level-start"><span class="level-item">Searching</span></span><span class="level-end"><span class="level-item tag">1</span></span></a><ul class="mr-0"><li><a class="level is-mobile is-marginless" href="/categories/Searching/Data-Strucure/"><span class="level-start"><span class="level-item">Data Strucure</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></li><li><a class="level is-mobile is-marginless" href="/categories/Statistic/"><span class="level-start"><span class="level-item">Statistic</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile is-marginless" href="/categories/Web/"><span class="level-start"><span class="level-item">Web</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></div></div></div><div class="card widget"><div class="card-content"><div class="menu"><h3 class="menu-label">Tags</h3><div class="field is-grouped is-grouped-multiline"><div class="control"><a class="tags has-addons" href="/tags/Amdahl-s-Law/"><span class="tag">Amdahl&#039;s Law</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Backpropagation/"><span class="tag">Backpropagation</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Bagging/"><span class="tag">Bagging</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/BeautifulSoup/"><span class="tag">BeautifulSoup</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Binary-Tree/"><span class="tag">Binary Tree</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Boosting/"><span class="tag">Boosting</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Boosting-Machine/"><span class="tag">Boosting Machine</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/BuckSort/"><span class="tag">BuckSort</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/C/"><span class="tag">C++</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Collaborative-Filtering/"><span class="tag">Collaborative Filtering</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Confusion-Metric/"><span class="tag">Confusion Metric</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Convolution-Neural-Network/"><span class="tag">Convolution Neural Network</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Cross-Validation/"><span class="tag">Cross Validation</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/D3-js/"><span class="tag">D3.js</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Data-Analysis/"><span class="tag">Data Analysis</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/DeepFM/"><span class="tag">DeepFM</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Dropout/"><span class="tag">Dropout</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Ensemble-Learning/"><span class="tag">Ensemble Learning</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Hexo/"><span class="tag">Hexo</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Holdout/"><span class="tag">Holdout</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Hypothesis-Test/"><span class="tag">Hypothesis Test</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Icarus/"><span class="tag">Icarus</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Independence-Test/"><span class="tag">Independence Test</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/JavaScript/"><span class="tag">JavaScript</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/K-Mean-Clustering/"><span class="tag">K-Mean Clustering</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/KMean-Clustering/"><span class="tag">KMean Clustering</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/LGBM/"><span class="tag">LGBM</span><span class="tag is-grey-lightest">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Machine-Learning/"><span class="tag">Machine Learning</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Markdown/"><span class="tag">Markdown</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Model-Evaluation/"><span class="tag">Model Evaluation</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Model-Selection/"><span class="tag">Model Selection</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Natural-Language-Representations/"><span class="tag">Natural Language Representations</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Nature-Language-Processing/"><span class="tag">Nature Language Processing</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Negative-Sampling/"><span class="tag">Negative Sampling</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/P-value/"><span class="tag">P-value</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Parallel-Computing/"><span class="tag">Parallel Computing</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Parallel-Speedup/"><span class="tag">Parallel Speedup</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/PySpark/"><span class="tag">PySpark</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/ROC-curve/"><span class="tag">ROC curve</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Recommendation-System/"><span class="tag">Recommendation System</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Regression/"><span class="tag">Regression</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Regular-Expression/"><span class="tag">Regular Expression</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Sorting/"><span class="tag">Sorting</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Stacking/"><span class="tag">Stacking</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Traversal/"><span class="tag">Traversal</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Unsupervised-Learning/"><span class="tag">Unsupervised Learning</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Variable-Selection/"><span class="tag">Variable Selection</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Web-Scrapping/"><span class="tag">Web Scrapping</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Wide-and-Deep/"><span class="tag">Wide and Deep</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Wide-and-Deep-Model/"><span class="tag">Wide and Deep Model</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Wide-Deep-Model/"><span class="tag">Wide&amp;Deep Model</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Word-Embedding/"><span class="tag">Word Embedding</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Word-vector/"><span class="tag">Word vector</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/binary-search/"><span class="tag">binary search</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/bubble-sort/"><span class="tag">bubble sort</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/commands/"><span class="tag">commands</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/insertion-sort/"><span class="tag">insertion sort</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/merge-sort/"><span class="tag">merge sort</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/non-parametric-learning/"><span class="tag">non-parametric learning</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/quick-sort/"><span class="tag">quick sort</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/review/"><span class="tag">review</span><span class="tag is-grey-lightest">1</span></a></div></div></div></div></div><div class="card widget"><div class="card-content"><h3 class="menu-label">Recent</h3><article class="media"><div class="media-content size-small"><p><time dateTime="2021-03-21T05:41:13.000Z">2021-03-21</time></p><p class="title is-6"><a class="link-muted" href="/2021/03/21/Recommendation-System-3-DeepFM/">Recommendation-System-3-DeepFM</a></p><p class="is-uppercase"><a class="link-muted" href="/categories/Recommendation-System/">Recommendation System</a></p></div></article><article class="media"><div class="media-content size-small"><p><time dateTime="2021-03-18T06:33:55.000Z">2021-03-18</time></p><p class="title is-6"><a class="link-muted" href="/2021/03/18/Recommendation-System-2-WideDeep/">Recommendation-System-2-WideDeep</a></p><p class="is-uppercase"><a class="link-muted" href="/categories/Recommendation-System/">Recommendation System</a></p></div></article><article class="media"><div class="media-content size-small"><p><time dateTime="2021-03-08T05:00:46.000Z">2021-03-08</time></p><p class="title is-6"><a class="link-muted" href="/2021/03/08/Cpp-review/">Cpp review 1</a></p><p class="is-uppercase"><a class="link-muted" href="/categories/Programming/">Programming</a></p></div></article><article class="media"><div class="media-content size-small"><p><time dateTime="2021-02-19T03:22:29.000Z">2021-02-18</time></p><p class="title is-6"><a class="link-muted" href="/2021/02/18/Statistic-Hypothesis-Testing/">Statistic- 1 Hypothesis-Testing</a></p><p class="is-uppercase"><a class="link-muted" href="/categories/Statistic/">Statistic</a></p></div></article><article class="media"><div class="media-content size-small"><p><time dateTime="2020-12-16T00:48:52.000Z">2020-12-15</time></p><p class="title is-6"><a class="link-muted" href="/2020/12/15/Recommendation-System-1/">Recommendation-System-1- Collaborative Filtering and Content-based Filtering</a></p><p class="is-uppercase"><a class="link-muted" href="/categories/Recommendation-System/">Recommendation System</a></p></div></article></div></div><div class="card widget"><div class="card-content"><div class="menu"><h3 class="menu-label">Archives</h3><ul class="menu-list"><li><a class="level is-mobile is-marginless" href="/archives/2021/03/"><span class="level-start"><span class="level-item">March 2021</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile is-marginless" href="/archives/2021/02/"><span class="level-start"><span class="level-item">February 2021</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile is-marginless" href="/archives/2020/12/"><span class="level-start"><span class="level-item">December 2020</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile is-marginless" href="/archives/2020/11/"><span class="level-start"><span class="level-item">November 2020</span></span><span class="level-end"><span class="level-item tag">5</span></span></a></li><li><a class="level is-mobile is-marginless" href="/archives/2020/10/"><span class="level-start"><span class="level-item">October 2020</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile is-marginless" href="/archives/2020/09/"><span class="level-start"><span class="level-item">September 2020</span></span><span class="level-end"><span class="level-item tag">5</span></span></a></li><li><a class="level is-mobile is-marginless" href="/archives/2020/08/"><span class="level-start"><span class="level-item">August 2020</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile is-marginless" href="/archives/2020/07/"><span class="level-start"><span class="level-item">July 2020</span></span><span class="level-end"><span class="level-item tag">7</span></span></a></li><li><a class="level is-mobile is-marginless" href="/archives/2020/06/"><span class="level-start"><span class="level-item">June 2020</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li></ul></div></div></div><div class="card widget"><div class="card-content"><div class="menu"><h3 class="menu-label">Subscribe to Updates</h3><form action="https://feedburner.google.com/fb/a/mailverify" method="post" target="popupwindow" onsubmit="window.open(&#039;https://feedburner.google.com/fb/a/mailverify?uri=&#039;,&#039;popupwindow&#039;,&#039;scrollbars=yes,width=550,height=520&#039;);return true"><input type="hidden" value="" name="uri"><input type="hidden" name="loc" value="en_US"><div class="field has-addons"><div class="control has-icons-left is-expanded"><input class="input" name="email" type="email" placeholder="Email"><span class="icon is-small is-left"><i class="fas fa-envelope"></i></span></div><div class="control"><input class="button is-primary" type="submit" value="Subscribe"></div></div></form></div></div></div><div class="column-right-shadow is-hidden-widescreen"></div></div><div class="column column-right is-4-tablet is-4-desktop is-3-widescreen is-hidden-touch is-hidden-desktop-only order-3"><div class="card widget"><div class="card-content"><nav class="level"><div class="level-item has-text-centered flex-shrink-1"><div><figure class="image is-128x128 mx-auto mb-2"><img class="avatar is-rounded" src="/images/avatar.jpg" alt="Wenkang Wei"></figure><p class="title is-size-4 is-block line-height-inherit">Wenkang Wei</p><p class="is-size-6 is-block">computer engineering| machine learning</p><p class="is-size-6 is-flex justify-content-center"><i class="fas fa-map-marker-alt mr-1"></i><span>Clemson,SC</span></p></div></div></nav><nav class="level is-mobile"><div class="level-item has-text-centered is-marginless"><div><p class="heading">Posts</p><a href="/archives"><p class="title">29</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">Categories</p><a href="/categories"><p class="title">19</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">Tags</p><a href="/tags"><p class="title">61</p></a></div></div></nav><div class="level"><a class="level-item button is-primary is-rounded" href="https://github.com/wenkangwei" target="_blank" rel="noopener">Follow</a></div><div class="level is-mobile"><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Github" href="https://github.com/wenkangwei"><i class="fab fa-github"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="LinkedIn" href="https://www.linkedin.com/in/wenkang-wei-588811167"><i class="fab fa-linkedin"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Facebook" href="https://facebook.com"><i class="fab fa-facebook"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Twitter" href="https://twitter.com"><i class="fab fa-twitter"></i></a></div></div></div><div class="card widget" id="toc"><div class="card-content"><div class="menu"><h3 class="menu-label">Catalogue</h3><ul class="menu-list"><li><a class="is-flex" href="#Introduction"><span class="mr-2">1</span><span>Introduction</span></a></li><li><a class="is-flex" href="#Evaluation-Metrics"><span class="mr-2">2</span><span>Evaluation Metrics</span></a></li><li><a class="is-flex" href="#Estimation-of-Model-Accuracy"><span class="mr-2">3</span><span>Estimation of Model Accuracy</span></a></li><li><a class="is-flex" href="#Comparison-of-performances-among-different-model"><span class="mr-2">4</span><span>Comparison of performances among different model</span></a></li><li><a class="is-flex" href="#Model-Selection"><span class="mr-2">5</span><span>Model Selection</span></a></li><li><a class="is-flex" href="#Improvement-on-Accuracy-Ensembling-method"><span class="mr-2">6</span><span>Improvement on Accuracy: Ensembling method</span></a></li><li><a class="is-flex" href="#Reference"><span class="mr-2">7</span><span>Reference</span></a></li></ul></div></div></div></div></div></div></section><footer class="footer"><div class="container"><div class="level"><div class="level-start"><a class="footer-logo is-block mb-2" href="/"><img src="/images/icon.png" alt="Wenkang&#039;s Blog" height="28"></a><p class="size-small"><span>&copy; 2021 Wenkang Wei</span>  Powered by <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a> &amp; <a href="https://github.com/ppoffice/hexo-theme-icarus" target="_blank" rel="noopener">Icarus</a></p></div><div class="level-end"><div class="field has-addons"><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Creative Commons" href="https://creativecommons.org/"><i class="fab fa-creative-commons"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Attribution 4.0 International" href="https://creativecommons.org/licenses/by/4.0/"><i class="fab fa-creative-commons-by"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/ppoffice/hexo-theme-icarus"><i class="fab fa-github"></i></a></p></div></div></div></div></footer><script src="https://cdn.jsdelivr.net/npm/jquery@3.3.1/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/moment@2.22.2/min/moment-with-locales.min.js"></script><script>moment.locale("en");</script><script>var IcarusThemeSettings = {
            site: {
                url: 'https://github.com/wenkangwei/wenkangwei.github.io`',
                external_link: {"enable":true,"exclude":[]}
            },
            article: {
                highlight: {
                    clipboard: true,
                    fold: 'unfolded'
                }
            }
        };</script><script src="https://cdn.jsdelivr.net/npm/clipboard@2.0.4/dist/clipboard.min.js" defer></script><script src="/js/animation.js"></script><a id="back-to-top" title="Back to Top" href="javascript:;"><i class="fas fa-chevron-up"></i></a><script src="/js/back_to_top.js" defer></script><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/lightgallery@1.6.8/dist/js/lightgallery.min.js" defer></script><script src="https://cdn.jsdelivr.net/npm/justifiedGallery@3.7.0/dist/js/jquery.justifiedGallery.min.js" defer></script><script>window.addEventListener("load", () => {
            if (typeof $.fn.lightGallery === 'function') {
                $('.article').lightGallery({ selector: '.gallery-item' });
            }
            if (typeof $.fn.justifiedGallery === 'function') {
                if ($('.justified-gallery > p > .gallery-item').length) {
                    $('.justified-gallery > p > .gallery-item').unwrap();
                }
                $('.justified-gallery').justifiedGallery();
            }
        });</script><!--!--><!--!--><script type="text/x-mathjax-config">MathJax.Hub.Config({
            'HTML-CSS': {
                matchFontHeight: false
            },
            SVG: {
                matchFontHeight: false
            },
            CommonHTML: {
                matchFontHeight: false
            },
            tex2jax: {
                inlineMath: [
                    ['$','$'],
                    ['\\(','\\)']
                ]
            }
        });</script><script src="https://cdn.jsdelivr.net/npm/mathjax@2.7.5/unpacked/MathJax.js?config=TeX-MML-AM_CHTML" defer></script><!--!--><script src="/js/main.js" defer></script><div class="searchbox"><div class="searchbox-container"><div class="searchbox-header"><div class="searchbox-input-container"><input class="searchbox-input" type="text" placeholder="Type something..."></div><a class="searchbox-close" href="javascript:;">×</a></div><div class="searchbox-body"></div></div></div><script src="/js/insight.js" defer></script><script>document.addEventListener('DOMContentLoaded', function () {
            loadInsight({"contentUrl":"/content.json"}, {"hint":"Type something...","untitled":"(Untitled)","posts":"Posts","pages":"Pages","categories":"Categories","tags":"Tags"});
        });</script></body></html>