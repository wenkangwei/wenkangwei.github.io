<!doctype html>
<html lang="en"><head><meta charset="utf-8"><meta name="generator" content="Hexo 4.2.1"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta><title>ML Model - Regression models - Wenkang&#039;s Blog</title><meta description=""><meta property="og:type" content="article"><meta property="og:title" content="ML Model - Regression models"><meta property="og:url" content="https://github.com/wenkangwei/"><meta property="og:site_name" content="Wenkang&#039;s Blog"><meta property="og:locale" content="en_US"><meta property="article:published_time" content="2020-09-30T20:01:27.000Z"><meta property="article:modified_time" content="2020-12-11T01:55:08.208Z"><meta property="article:author" content="Wenkang Wei"><meta property="article:tag" content="Regression"><meta property="article:tag" content="Variable Selection"><meta property="twitter:card" content="summary"><script type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"https://github.com/wenkangwei/wenkangwei.github.io%60/2020/09/30/ML-Model-LR/"},"headline":"Wenkang's Blog","image":[],"datePublished":"2020-09-30T20:01:27.000Z","dateModified":"2020-12-11T01:55:08.208Z","author":{"@type":"Person","name":"Wenkang Wei"},"description":""}</script><link rel="canonical" href="https://github.com/wenkangwei/wenkangwei.github.io%60/2020/09/30/ML-Model-LR/"><link rel="icon" href="/images/icon.png"><link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.12.0/css/all.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@9.12.0/styles/atom-one-light.css"><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Ubuntu:wght@400;600&amp;family=Source+Code+Pro"><link rel="stylesheet" href="/css/default.css"><style>body>.footer,body>.navbar,body>.section{opacity:0}</style><!--!--><!--!--><!--!--><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/lightgallery@1.6.8/dist/css/lightgallery.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/justifiedGallery@3.7.0/dist/css/justifiedGallery.min.css"><!--!--><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/pace-js@1.0.2/pace.min.js"></script></head><body class="is-3-column"><nav class="navbar navbar-main"><div class="container"><div class="navbar-brand justify-content-center"><a class="navbar-item navbar-logo" href="/"><img src="/images/icon.png" alt="Wenkang&#039;s Blog" height="28"></a></div><div class="navbar-menu"><div class="navbar-start"><a class="navbar-item" href="/">Home</a><a class="navbar-item" href="/archives">Archives</a><a class="navbar-item" href="/categories">Categories</a><a class="navbar-item" href="/tags">Tags</a><a class="navbar-item" href="/about">About</a></div><div class="navbar-end"><a class="navbar-item" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/ppoffice/hexo-theme-icarus"><i class="fab fa-github"></i></a><a class="navbar-item is-hidden-tablet catalogue" title="Catalogue" href="javascript:;"><i class="fas fa-list-ul"></i></a><a class="navbar-item search" title="Search" href="javascript:;"><i class="fas fa-search"></i></a></div></div></div></nav><section class="section"><div class="container"><div class="columns"><div class="column order-2 column-main is-8-tablet is-8-desktop is-6-widescreen"><div class="card"><article class="card-content article" role="article"><div class="article-meta size-small is-uppercase level is-mobile"><div class="level-left"><time class="level-item" dateTime="2020-09-30T20:01:27.000Z" title="2020-09-30T20:01:27.000Z">2020-09-30</time><span class="level-item"><a class="link-muted" href="/categories/Machine-Learning/">Machine Learning</a></span><span class="level-item">21 minutes read (About 3182 words)</span></div></div><h1 class="title is-3 is-size-4-mobile">ML Model - Regression models</h1><div class="content"><img src=/images/ML-model/regression.png>

<a id="more"></a>

<h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>This article is to first introduce types of machine learning models and summarize properties of seven regression models: Linear Regression and Logistic Regression, Lasso Regression, Ridge Regression, Elastic Net Regression and Stepwise regression.<br><br></p>
<h2 id="Type-of-machine-learning-model"><a href="#Type-of-machine-learning-model" class="headerlink" title="Type of machine learning model"></a>Type of machine learning model</h2><h4 id="Parametric-non-parametric-models"><a href="#Parametric-non-parametric-models" class="headerlink" title="Parametric/ non-parametric models"></a>Parametric/ non-parametric models</h4><p>In general, Machine Learning model can be classified into two types of models based on the critic that if model uses parameters to estimate model or not.<br>In this case, machine learning model can be divided into two types: parametric model and non-parametric model.<br>The following are some useful machine models:</p>
<ul>
<li>parametric: <ul>
<li>Linear regression, Logistic regression</li>
<li>Neural Network</li>
</ul>
</li>
<li>non-parametric:<ul>
<li>K-Nearest neighbor (K-NN) </li>
<li>K-Mean Clustering</li>
<li>Decision Tree</li>
<li>Random Forest</li>
<li>Naive Bayesian</li>
<li>etc…<br>Note that the <strong>“parametric”</strong> here is the parameters the model use to estimate the function, distribution. The parameters that control the complexity, performance of model are excluded. For example, K-Mean clustering and K-NN requires us to choose a k value to do clustering/ classification tasks. This k value is not the parameter we consider here.<br>

</li>
</ul>
</li>
</ul>
<h4 id="linear-non-linear-model"><a href="#linear-non-linear-model" class="headerlink" title="linear/non-linear model"></a>linear/non-linear model</h4><ol>
<li><p>Linear model<br>The linear model is the model that can be expressed by the formula<br>$y’ =  w_0 + w_1x_1 +….+w_nx_n = w_0+ \sum_{i}^nw_ix_i$<br>where x is the data point from dataset, $w_i$ is the parameters used to adjust the model and $w_0$ is the bias term. $y’$ is the prediction from the estimator.<br>This formula is linear since it just involves first order terms and linear combination.<br>Note that the real label is<br>$y = \epsilon + w_0+ \sum_{i}^nw_ix_i$<br>where $\epsilon$ is the irreducible error between prediction $y’$ and true label $y$. In the following discussion, I discuss $y’$ prediction from model rather than label $y$.</p>
</li>
<li><p>Non-linear model<br>Non-linear model in other word is the model that can not be expressed by the linear formula above. If there are second order term like $x^2, x^3, x_1x_2$, it is non-linear model as well<br>Hence we can easly know that models like Naive Bayesian, K-NN, K-mean clustering, logistic regression, etc are non-linear models</p>
<br>

</li>
</ol>
<h2 id="Linear-Regression"><a href="#Linear-Regression" class="headerlink" title="Linear Regression"></a>Linear Regression</h2><p>The formula of Linear Regression is<br>$$y’ = w_0 + w_1x_1 +….+w_nx_n $$<br>where $w_0$ is a constant and $y’$ is the prediction from model. To simplify it, we have:</p>
<p>$$y’ =  \sum_{i=1}^nw_ix_i + w_0$$<br>Obviously, the linear regression model is a linear model.<br><br></p>
<h4 id="Optimize-Model"><a href="#Optimize-Model" class="headerlink" title="Optimize Model"></a>Optimize Model</h4><ol>
<li><p><strong>Maximum Likelihood Estimation (MLE)</strong><br> Assume the data distribution is normal distribution, then<br> the likelihood function used to estimate the real data distribution is:</p>
<p> $$P(y=y_i|x) = \frac{1}{\sigma\sqrt{2\pi}} e^{-\frac{(y_i-f(x))^2}{2\sigma}}$$</p>
<p> where f(x) is the linear regression estimator, estimating the mean of normal distribution $\mu$. Then the weight solution of the regression model is </p>
<p> $$W = argmax_w P(Y|X)$$</p>
</li>
<li><p><strong>Loss Function to minimize</strong><br> The loss function used to update the parameter in Linear regression is <strong>mean square error (MSE)</strong></p>
<p> $$ MSE = \frac{1}{N}\sum_i^N{ (y_i - y’_i)^2 }$$</p>
<p> or <strong>sum square error (SSE)</strong></p>
<p> $$ SSE = \sum_i^N{ (y_i - y’_i)^2 }$$</p>
<p> where $y_i$ is the $i^{th}$ label and $y’_i$ is the $i^{th}$ prediction from model</p>
<p> <strong>Properties of MSE/SSE:</strong><br> SSE / MSE error actually assumes that the distribution of data is Normal distribution and the Linear regression model is actually estimating the mean of Normal distribution with a fixed variance $\sigma$. The negative log likelihood of normal distribution is as follow:</p>
<p> $$ -log(\Pi_i^nP(y=y_i|x_i)) \<br> \propto -log(e^{\sum_i^n\frac{-(y_i-f(x_i)^2)}{2\sigma}}) \<br> \propto \sum_i^n(y_i-f(x_i))^2<br> $$<br> where $y_i$ is the $i^{th}$ label and $x_i$ is the $i^{th}$ feature vector.<br> It is easy to see that <strong>maximize the likelihood function is equivalent to minimize the negative log likelihood function or the SSE /MSE.</strong><br> Hence to minimize the MSE error is actually estimating  the normal distribution function $P(Y|X)$.</p>
 <br>

</li>
</ol>
<h4 id="Properties"><a href="#Properties" class="headerlink" title="Properties"></a>Properties</h4><ol>
<li><strong>Linear regression is an unbias model</strong>, which is <strong>sensitive to outliers</strong>.For example, In the following linear regression</li>
</ol>
<img src = /images/ML-model/outlier.png>

<p>There is an outlier at x= 50, y= 30.<br>During update with gradient descent w = w - 2x(y - wx). The outlier value changes the weight a lot and shift the line away from the $y=4x$. This is because in the update of parameter $w$, each data point is given equal weight to change the parameter $w$.<br>Hence linear regression is sensitive to outliers.</p>
<ol start="2">
<li>The output/prediction from linear regression is to <strong>estimate the expected value/ mean of normal distribution</strong><br>

</li>
</ol>
<h2 id="Logistic-Regression"><a href="#Logistic-Regression" class="headerlink" title="Logistic Regression"></a>Logistic Regression</h2><p>Logistic regression is an non-linear model as it can not be expressed into the linear form.<br>The formula of Logistic Regression model:</p>
<p>$$ P(y=y_i|x) = p_i = \frac{1}{1+e^{-wx}} $$</p>
<h4 id="Optimize-Model-1"><a href="#Optimize-Model-1" class="headerlink" title="Optimize Model"></a>Optimize Model</h4><ol>
<li><p><strong>Maximum Likelihood Estimation (MLE):</strong><br> we want to maximize the log likelihood function (The distribution estimated by the model) to get close to the real distribution function.</p>
<p> In binary classification problem, we assume the data distribution is <strong>Bernoulli distribution</strong>, $ P(y=y_i|x) = p^{y_i}(1-p)^{1-y_i}$， since this distribution considers the possibility of P(y=0|x) and P(y=1|x).<br> Then the <strong>log likelihood function / log Bernoulli distribution</strong> is following:</p>
<p> $$ logP(y_1,y_2,..y_n| x_1, x_2..x_n) =log(P(y_1|x_1)P(y_2|x_2)..P(y_n|x_n))$$</p>
<p> $$ =\sum_i^n log(p_i^{y_i}(1-p_i)^{1-y_i})= \sum_i^n[y_ilog(p_i)+(1-y_i)log(1-p_i)]$$</p>
<p> Since we only care the effect of weight vector and data X to the likelihood function, it can be re-written as </p>
<p> $$ \sum_i^n[y_ilog(h_w(x_i))+(1-y_i)log(1-h_w(x_i))]$$</p>
<p> where  $x_i, y_i$ are feature vector and binary label, <strong>$h_w(x_i) = p_i$ is the logistic regression</strong>,  estimating the possiibility that if $x_i$ belongs to class 0 or class 1.<br> The log function here is used to simplify the likelihood function and convert the multiplication into summation. This gives us easier way to analyze it. Maximizing the log likelihood is equivalent to maximize the original likelihood function.</p>
<p> Notice that when $p_i$ predicted by the model is as same as its label $y_i$, then $p_i^{y_i}(1-p_i)^{1-y_i}$=1, the likelihood function is maximized.<br> The optimal solution of weight matrix in logistic regression is </p>
<p> $$W = argmax_W (\sum_i^n(y_ilog(h_w(x_i))+(1-y_i)log(1-h_w(x_i))))<br> $$</p>
 <br>


</li>
</ol>
<ol start="2">
<li><p><strong>Binary Cross-Entropy</strong><br> When we add the <strong>negative sign</strong> to the log likelihood function, we can get </p>
<p> $$ - \sum_i^n log(p_i^{y_i}(1-p_i)^{1-y_i})= -\sum_i^n(y_ilog(p_i)+(1-y_i)log(1-p_i))<br> $$</p>
<p> This form is also called <strong>binary cross-entropy</strong>. To maximize the likelihood function to real distribution of data, is actually equivalent to minimize the <strong>binary cross entropy</strong>. Cross entropy is to measure the uncertainty between $y_i$ and $p_i$, $1-y_i$ and $1-p_i$.  Higher the entropy is, less similar they are. <strong>So binary cross entropy assume the data is in Bernoulli distribution</strong></p>
<p> The goal to train the logisitic regression using binary cross-entropy is to let $h_w(x_i)$ get as close to $y_i$ as possible.</p>
</li>
</ol>
<h4 id="Properties-1"><a href="#Properties-1" class="headerlink" title="Properties"></a>Properties</h4><ol>
<li><p><strong>Logistic function actually projects the range of linear regression into range of [0,1]</strong>. Y-axis in logistic regression= possibility that input x belongs to one class, while the linear regression output range is ($-\infty, +\infty$).</p>
</li>
<li><p><strong>Logisitic regression is an bias model</strong>, since it considers the feature values X close to 0 are different and hence the gradient close to x=0 is large. However, when x is far away from x=0, such as $+\infty, -\infty $, the gradient of logistic function is close to 0 and it “considers” there is no much difference between two features x1 and x2 when both x1 and x2 $\to \infty$.</p>
</li>
<li><p><strong>Logistic Regression has vanishing gradient problem</strong>.<br> Since the gradient of logistic regression is :</p>
<p> $$(\frac{1}{1+e^{-wx}})’ =\frac{ e^{-wx}}{(1+e^{-wx})^2} = (\frac{1}{1+e^{-wx}})(1-\frac{1}{1+e^{-wx}})$$</p>
<p> when it predicts some sample X as a value very close to 0 or 1, the gradient will be very close to 0 and the weight is hard to update any more. The gradient is actually vanishing in this case. <strong>When it think a sample X is class y, then it won’t be willing to change its mind</strong>. This is a reason why it is “biased model” as well</p>
</li>
<li><p><strong>Logistic Regression is widely used for binary classification problem</strong>, it can be regarded as a two-classes version of softmax function. </p>
</li>
<li><p><strong>Using Maximum Likelihood estimation requires large data sample size in order to estimate the real distribution.</strong></p>
</li>
<li><p><strong>Assumption in Logistic Regression</strong> is that the logarithem value of the ratio of possibility of class =1 to possibility of class =0 is linear, which can be estimated by linear regression.<br>Proof:<br>$$<br> log(\frac{P(Y=1|X)}{P(Y=0|X)}) = log(\frac{P(Y=1|X)}{1- P(Y=1|X)})=wx \<br>$$</p>
</li>
</ol>
<p>$$<br>    (1- P(Y=1|X))*e^{wx} = P(Y=1|X)<br>$$</p>
<p>$$<br>    P(Y=1|X) = \frac{e^{wx}}{1+e^{wx}} = \frac{1}{1+e^{-wx}}<br>$$</p>
<ol start="7">
<li><strong>Logistic Regression is to find a linear separation boundary /a line that separate the data in sample space for classification problem</strong>. Since in logistic regression $\frac{1}{1+e^{-wx}}$, when $-wx&gt;0$, $h_w(x)&gt;0.5$ and it predicts class 1, otherwise, class 0. The line $-wx=0$ in 2-D space shown in the left figure below is actually the decision boundary. The data x with $-wx&gt;0$ is above the line and is classified as class1.</li>
</ol>
<img src=/images/ML-model/fitting.png>
<br>

<h2 id="Polynomial-Regression"><a href="#Polynomial-Regression" class="headerlink" title="Polynomial Regression"></a>Polynomial Regression</h2><p>$$<br>y’ = w_0 + w_1x_1 + w_2x_2^2 +w_3x_3^3+…w_nx_n^n<br>$$<br>where y’, $w_i$, $x_i$ are all scalar values in this case and y’ is the predicted value from model. Different from linear regression, polynomial regression involves higher order terms.</p>
<h4 id="Optimize-Model-2"><a href="#Optimize-Model-2" class="headerlink" title="Optimize Model"></a>Optimize Model</h4><p><strong>Sum square error</strong><br>$$ SSE = \sum_i^N{ (y_i - y’_i)^2<br>}$$<br>It is as same as the linear regression</p>
<h4 id="Properties-2"><a href="#Properties-2" class="headerlink" title="Properties"></a>Properties</h4><ol>
<li>It is very easy for Polynomial regression to over-fitting as the order increase. It is an biased model as well.</li>
<li>It predicts the expected value of Y, which is as same as linear regression</li>
<li>To implement polynomial regression, we can first construct polynomial features, like $x_1^2, x_1x_2, x_2^2$, etc. Then apply linear regression to polynomial features<br>


</li>
</ol>
<h2 id="Lasso-regression"><a href="#Lasso-regression" class="headerlink" title="Lasso regression"></a>Lasso regression</h2><p>The prediction model of lasso regression is still linear regression:<br>$$y’=\sum_{i=1}^nw_ix_i +w_0<br>$$</p>
<h4 id="Optimize-Model-3"><a href="#Optimize-Model-3" class="headerlink" title="Optimize Model"></a>Optimize Model</h4><ol>
<li><p><strong>Loss function to minimize in Lasso regression:</strong></p>
<p> $$min \sum_i^n(y_i - wx_i -w_0)^2 + \lambda||w||_{1}$$</p>
<p> where $y_i$ is a scalar value, a label, $x_i$ is a feacture column vector, $w$ is the weight vector.  $\sum_i^n(y_i - wx_i)^2$ is the least square error (or called L2 norm distance between y and x).<br> <strong>Note that mean square error is scaled version of sum square error. Minimize the sum square error is equivalent to minimize the mean square error, so this term is as same as loss function in Linear regression</strong></p>
<p> The second term in the loss function is the <strong>L1- regularization term</strong> used to reduce overfitting effect, in which $||w||_1 = \sum_i^n|w_i|$ is the L1 norm format.</p>
</li>
<li><p><strong>Principle of L1-Regularization term</strong></p>
<p> $$min \sum_i^n(y_i - wx_i -w_0)^2 + \lambda||w||_{1}<br> $$</p>
<p> is equivalent to optimize the problem:</p>
<p> $$min \sum_i^n(y_i - wx_i -w_0)^2  \<br> \text{ subject to }  ||w||_1 \leq C<br> $$</p>
<p> where C is a constant.<br> Assume there are two variables in feature vector $x_i$ and only two weight variables, then to visualize it in 2-D space, we have:</p>
 <img src =/images/ML-model/L1-regularization.png>

<p> The x, y axis represent the weight variables. The blue point is the optimal point of the minimum least square error term. The orange region describes the constraints $||w||_1 \leq C$, hence the intercepts on x, y axis are equal to $C$.<br> <strong>The main idea of L1 regularization term</strong> is that when the weight vector satisfies the constraint, then the weight vector must be inside the orange region. We need to find the point with the smallest distance to blue point inside this region. When C is small enough such that weight vector can not reach the optimal point, then the model can reduce over-fitting effect.</p>
<p> By using lagrange multiplier, the constraint problem can be converted to:</p>
<p> $$min \sum_i^n(y_i - wx_i-w_0)^2 + \lambda (||w||_{1}-C)<br> $$</p>
<p> where $\lambda$ is the lagrange multiplier coefficient used to weigh the regularization term and <strong>$\lambda$ is adjusted by user manually</strong>. Larger $\lambda$ is, more regularization term affect.  We can see that during minimization process, $C$ and $\lambda$ are actually constants, so we can ignore this term and get:</p>
<p> $$min \sum_i^n(y_i - wx_i)^2 + \lambda||w||_{1}<br> $$</p>
</li>
</ol>
<h4 id="Properties-3"><a href="#Properties-3" class="headerlink" title="Properties"></a>Properties</h4><ol>
<li>It involves L1 regularization term to limit the range of weights and reduce over-fitting effect</li>
<li><strong>Lasso regularization can be used to do feature selection</strong><br>Since in the figure above, we can see when the optimal solution to this constraint problem is at one of the angles of the square, some weight parameters are driven to be 0 and this leads to a sparse model. That is, the model filters out some features in $x_i$. As $\lambda$  increases, more weights are driven to 0 and more features are not selected.</li>
<li><strong>We can not decide which features should be excluded in Lasso regression</strong>, since we don’t know which parameters become 0.</li>
<li><strong>Feature selection with lasso regression is not stable.</strong> Since during training the model, some weights may be very close to 0, but not equal to 0. That is, the features that are expected to be filtered out are actually not filtered out and still affect the model performance.<br>In addition, different initialization of weights may lead to different feature selection.<br>

</li>
</ol>
<h2 id="Ridge-Regression"><a href="#Ridge-Regression" class="headerlink" title="Ridge Regression"></a>Ridge Regression</h2><p>The prediction model of Ridge regression is linear regression as well:</p>
<p>$$y =  \sum_{i=1}^nw_ix_i +w_0<br>$$</p>
<p>But the loss function is different. Read the following.</p>
<h4 id="Optimize-Model-4"><a href="#Optimize-Model-4" class="headerlink" title="Optimize Model"></a>Optimize Model</h4><ol>
<li><p><strong>Loss function to minimize in Ridge regression:</strong><br> $$min \sum_i^n(y_i - wx_i -w_0)^2+ \lambda||w||_{2}^2$$</p>
<p> Ridge regression is similar to Lasso regression, except that the regularization term here use L2 norm distance. Then the constraint region in 2-D space becomes a circle, shown in below.</p>
 <img src= /images/ML-model/L2-regularization.png>

<p> Since the constraint region becomes a circle, it is almost impossible to drive some weights to be zeros unless the blue point (optimal point of least square error) is on the axis. Hence, Ridge regression can not be used for feature selection.</p>
</li>
</ol>
<h4 id="Properties-4"><a href="#Properties-4" class="headerlink" title="Properties"></a>Properties</h4><ol>
<li>Ridge regression <strong>can not be used for feature selection</strong></li>
<li><strong>Ridge regression can be used to reduce multi-collinearity effect of features and soothe the over-fitting effect.</strong><br>multi-collinearity means that some features have a linear correlated relationship. For example, one feature value increases, another feature value may increase or decrease linearly. If model focuses on the similar features, it may become over-fitting on such feature.</li>
</ol>
<p><strong>The reason is that</strong> in the circle region, assume features $x_1$ and $x_2$ are collinear, if $w_1$ increases and the model weigh more on feature $x_1$, then $w_2$ will decrease and model weighs less on feature $x_2$. In this case, model could avoid focusing on learning similar features heavily and reduce the multi-collinearity effect.<br>3. <strong>Reduce the variance in model error and reduce over-fitting effect</strong></p>
<ol start="4">
<li><strong>Ridge regression is more stable than Lasso regression for avoiding over-fitting</strong><br>

</li>
</ol>
<h2 id="Elastic-Net-Regression"><a href="#Elastic-Net-Regression" class="headerlink" title="Elastic Net Regression"></a>Elastic Net Regression</h2><p>The prediction model of Ridge regression is linear regression as well:</p>
<p>$$y’ =  \sum_{i=1}^nw_ix_i +w_0$$</p>
<h4 id="Optimize-Model-5"><a href="#Optimize-Model-5" class="headerlink" title="Optimize Model"></a>Optimize Model</h4><ol>
<li><strong>Loss function to minimize</strong><br>$$min \sum_i^n(y_i - wx_i- w_0)^2 + \lambda_1||w||_{1} + \lambda_2 ||w||_{2}<br>$$</li>
</ol>
<p>where $y_i$ is the label, $w$ is the weight vector and $w_0$ is the scalar bias. $x_i$ is the feature vector.<br>Elastic net regression combines L1, L2 regularization terms together. Hence it can be regarded as the combination of weighed Ridge regression and weighed Lasso regression.</p>
<h4 id="Properties-5"><a href="#Properties-5" class="headerlink" title="Properties"></a>Properties</h4><ol>
<li>Combine the advantages of Lasso regression and Ridge regression.<br>

</li>
</ol>
<h2 id="Stepwise-Regression"><a href="#Stepwise-Regression" class="headerlink" title="Stepwise Regression"></a>Stepwise Regression</h2><p>In statistics, stepwise regression is a method of fitting regression models in which the choice of predictive variables is carried out by an automatic procedure.<br>Since there are multiple variable / features in feature vectors, we want to select the most important features to construct our regression model. Different combinations of variables lead to different regression model. <strong>Stepwise regression is to select the different feature variables and fit different models and then get the best one with least Residual sum square error</strong></p>
<h4 id="Stepwise-regression-approaches"><a href="#Stepwise-regression-approaches" class="headerlink" title="Stepwise regression approaches"></a>Stepwise regression approaches</h4><p>Assume there are p features  in dataset to pick. There are several types of approaches in stepwise regression for variable selection.</p>
<ol>
<li><p><strong>Forward Stepwise Regression /selection</strong></p>
<ul>
<li>Begin with the <strong>null model</strong> { a model that contains an intercept but no predictors.</li>
<li>Fit <strong>p simple linear regressions</strong> and add to the null model the predictor that results in the <strong>lowest RSS</strong>.</li>
<li>Add to that model the predictor that results in the lowest RSS among  all <strong>two-predictor models</strong>.</li>
<li>Continue until some stopping rule is satisfied, for example when all remaining variables have a p-value above some threshold. <strong>P-value is to measure how significantly different two models are using statistic technique</strong></li>
<li><strong>There are p(p+1)/2 models to evaluate</strong></li>
</ul>
</li>
<li><p><strong>Backward Stepwise Regression (Backward elimination)</strong></p>
<ul>
<li>Start with <strong>all predictors</strong> in the model.</li>
<li><strong>Remove</strong> the predictor with the largest p-value { that is, the predictor that is <strong>the least statistically significant</strong>.</li>
<li>The new (p -1 )-predictor model is fit, and the <strong>predictor with the largest p-value is removed</strong>.</li>
<li>Continue until a stopping rule is reached.</li>
<li><strong>There are p(p+1)/2 models to evaluate</strong> </li>
</ul>
</li>
<li><p><strong>Bidirectional Stepwise Regression</strong></p>
<ul>
<li>combine two methods above to see which predictor should be included or excluded from model</li>
</ul>
</li>
</ol>
<h4 id="Subset-Selection"><a href="#Subset-Selection" class="headerlink" title="Subset Selection"></a><strong>Subset Selection</strong></h4><p>Subset selection is another variable selection method, in addition to stepwise regression</p>
<ul>
<li>Assume there are p features  in dataset to pick</li>
<li>fit all models for all subset</li>
<li>pick the best model with smallest residual sum square error</li>
<li><strong>There are $2^p$ models to evaluate</strong><br>

</li>
</ul>
<h2 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h2><p>This article summarizes the seven regression methods and their properties.<br><strong>Key things to note:</strong></p>
<ul>
<li>Linear regression is to estimate the mean of normal distribution</li>
<li>MSE assumes data is in normal distribution</li>
<li>Logistic regression is an biased model to estimate the possibility that input x belongs to a class. Logistic regression assume data is in Bernoulli distribution.</li>
<li>Binary cross entropy assumes data distribution is Bernoulli distribution</li>
<li>Lasso Regression (L1 regularization) is good to do feature selection, but could be unstable. It can also reduce the over-fitting effect</li>
<li>Ridge Regression can not be used for feature selection, but is good to reduce multi-collinearity effect and avoid overfitting</li>
<li>Elastic Net Regression combines Lasso and Ridge regression</li>
<li>Stepwise Regression and Subset selection are used to combine and select feature variables to find the estimators with the least SSE error.</li>
</ul>
<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><p>[1] <a href="https://ncss-wpengine.netdna-ssl.com/wp-content/themes/ncss/pdf/Procedures/NCSS/Stepwise_Regression.pdf">Stepwise Regression</a></p>
<p>[2] <a href="https://www.analyticssteps.com/blogs/7-types-regression-technique-you-should-know-machine-learning">https://www.analyticssteps.com/blogs/7-types-regression-technique-you-should-know-machine-learning</a></p>
<p>[3] <a href="https://www.listendata.com/2018/03/regression-analysis.html">https://www.listendata.com/2018/03/regression-analysis.html</a></p>
<p>[4] <a href="http://users.isr.ist.utl.pt/~wurmd/Livros/school/Bishop%20-%20Pattern%20Recognition%20And%20Machine%20Learning%20-%20Springer%20%202006.pdf">BIshop-PatternRecognition-MachineLearning.pdf</a></p>
<p>[5] <a href="https://strata.uga.edu/8370/rtips/images/outlier.png">https://strata.uga.edu/8370/rtips/images/outlier.png</a></p>
</div><div class="article-tags size-small mb-4"><span class="mr-2">#</span><a class="link-muted mr-2" rel="tag" href="/tags/Regression/">Regression</a><a class="link-muted mr-2" rel="tag" href="/tags/Variable-Selection/">Variable Selection</a></div><div class="sharethis-inline-share-buttons"></div><script src="https://ppoffice.github.io/hexo-theme-icarus-categorites-Plugins/Share/" defer></script></article></div><!--!--><nav class="post-navigation mt-4 level is-mobile"><div class="level-start"><a class="article-nav-prev level level-item link-muted" href="/2020/10/12/PySpark-Note-1/"><i class="level-item fas fa-chevron-left"></i><span class="level-item">PySpark-Note-1</span></a></div><div class="level-end"><a class="article-nav-next level level-item link-muted" href="/2020/09/22/Data-Structure-BuckSort-Parallel-Computing/"><span class="level-item">Data Structure - BuckSort with Parallel Computing</span><i class="level-item fas fa-chevron-right"></i></a></div></nav><div class="card"><div class="card-content"><h3 class="title is-5">Comments</h3><div class="content" id="valine-thread"></div><script src="//cdn1.lncld.net/static/js/3.0.4/av-min.js"></script><script src="https://cdn.jsdelivr.net/npm/valine@1.4.14/dist/Valine.min.js"></script><script>new Valine({
            el: '#valine-thread' ,
            appId: "kvXCKmDNxnA2486N29e5cP7i-MdYXbMMI",
            appKey: "0Lv5b0SqotzkGHQvD64u4AKo",
            
            avatar: "mm",
            
            meta: ["nick","mail","link"],
            pageSize: 10,
            lang: "en",
            
            highlight: true,
            
            
            
            
            
            requiredFields: [],
        });</script></div></div></div><div class="column column-left is-4-tablet is-4-desktop is-3-widescreen  order-1"><div class="card widget"><div class="card-content"><div class="menu"><h3 class="menu-label">Links</h3><ul class="menu-list"><li><a class="level is-mobile is-mobile" href="https://github.com/wenkangwei" target="_blank" rel="noopener"><span class="level-left"><span class="level-item">Github</span></span><span class="level-right"><span class="level-item tag">github.com</span></span></a></li></ul></div></div></div><div class="card widget"><div class="card-content"><div class="menu"><h3 class="menu-label">Categories</h3><ul class="menu-list"><li><a class="level is-mobile is-marginless" href="/categories/Data-Collection/"><span class="level-start"><span class="level-item">Data Collection</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile is-marginless" href="/categories/Data-Structure/"><span class="level-start"><span class="level-item">Data Structure</span></span><span class="level-end"><span class="level-item tag">2</span></span></a><ul class="mr-0"><li><a class="level is-mobile is-marginless" href="/categories/Data-Structure/Sorting/"><span class="level-start"><span class="level-item">Sorting</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></li><li><a class="level is-mobile is-marginless" href="/categories/Deep-Learning/"><span class="level-start"><span class="level-item">Deep Learning</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile is-marginless" href="/categories/Linux/"><span class="level-start"><span class="level-item">Linux</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile is-marginless" href="/categories/Machine-Learning/"><span class="level-start"><span class="level-item">Machine Learning</span></span><span class="level-end"><span class="level-item tag">5</span></span></a><ul class="mr-0"><li><a class="level is-mobile is-marginless" href="/categories/Machine-Learning/Accuracy-Improvement/"><span class="level-start"><span class="level-item">Accuracy Improvement</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></li><li><a class="level is-mobile is-marginless" href="/categories/NLP/"><span class="level-start"><span class="level-item">NLP</span></span><span class="level-end"><span class="level-item tag">2</span></span></a><ul class="mr-0"><li><a class="level is-mobile is-marginless" href="/categories/NLP/Machine-Learning/"><span class="level-start"><span class="level-item">Machine Learning</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li></ul></li><li><a class="level is-mobile is-marginless" href="/categories/Parallel-Computing/"><span class="level-start"><span class="level-item">Parallel Computing</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile is-marginless" href="/categories/Programming/"><span class="level-start"><span class="level-item">Programming</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile is-marginless" href="/categories/PySpark/"><span class="level-start"><span class="level-item">PySpark</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile is-marginless" href="/categories/Recommendation-System/"><span class="level-start"><span class="level-item">Recommendation System</span></span><span class="level-end"><span class="level-item tag">6</span></span></a></li><li><a class="level is-mobile is-marginless" href="/categories/Report/"><span class="level-start"><span class="level-item">Report</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile is-marginless" href="/categories/Searching/"><span class="level-start"><span class="level-item">Searching</span></span><span class="level-end"><span class="level-item tag">1</span></span></a><ul class="mr-0"><li><a class="level is-mobile is-marginless" href="/categories/Searching/Data-Strucure/"><span class="level-start"><span class="level-item">Data Strucure</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></li><li><a class="level is-mobile is-marginless" href="/categories/Statistic/"><span class="level-start"><span class="level-item">Statistic</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile is-marginless" href="/categories/Web/"><span class="level-start"><span class="level-item">Web</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></div></div></div><div class="card widget"><div class="card-content"><div class="menu"><h3 class="menu-label">Tags</h3><div class="field is-grouped is-grouped-multiline"><div class="control"><a class="tags has-addons" href="/tags/Amdahl-s-Law/"><span class="tag">Amdahl&#039;s Law</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Backpropagation/"><span class="tag">Backpropagation</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Bagging/"><span class="tag">Bagging</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/BeautifulSoup/"><span class="tag">BeautifulSoup</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Binary-Tree/"><span class="tag">Binary Tree</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Blending/"><span class="tag">Blending</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Boosting/"><span class="tag">Boosting</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Boosting-Machine/"><span class="tag">Boosting Machine</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/BuckSort/"><span class="tag">BuckSort</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/C/"><span class="tag">C++</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Coding/"><span class="tag">Coding</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Collaborative-Filtering/"><span class="tag">Collaborative Filtering</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Confusion-Metric/"><span class="tag">Confusion Metric</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Convolution-Neural-Network/"><span class="tag">Convolution Neural Network</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Cross-Validation/"><span class="tag">Cross Validation</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/D3-js/"><span class="tag">D3.js</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/DIN/"><span class="tag">DIN</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Data-Analysis/"><span class="tag">Data Analysis</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Datawhale-Team-Learning/"><span class="tag">Datawhale Team Learning</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/DeepFM/"><span class="tag">DeepFM</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Dropout/"><span class="tag">Dropout</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Ensemble-Learning/"><span class="tag">Ensemble Learning</span><span class="tag is-grey-lightest">3</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Hexo/"><span class="tag">Hexo</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Holdout/"><span class="tag">Holdout</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Hypothesis-Test/"><span class="tag">Hypothesis Test</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Icarus/"><span class="tag">Icarus</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Independence-Test/"><span class="tag">Independence Test</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/JavaScript/"><span class="tag">JavaScript</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/K-Mean-Clustering/"><span class="tag">K-Mean Clustering</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/KMean-Clustering/"><span class="tag">KMean Clustering</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/LGBM/"><span class="tag">LGBM</span><span class="tag is-grey-lightest">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Leetcode/"><span class="tag">Leetcode</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Machine-Learning/"><span class="tag">Machine Learning</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Markdown/"><span class="tag">Markdown</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Model-Evaluation/"><span class="tag">Model Evaluation</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Model-Selection/"><span class="tag">Model Selection</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Natural-Language-Representations/"><span class="tag">Natural Language Representations</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Nature-Language-Processing/"><span class="tag">Nature Language Processing</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Negative-Sampling/"><span class="tag">Negative Sampling</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/NeuralFM/"><span class="tag">NeuralFM</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/P-value/"><span class="tag">P-value</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Parallel-Computing/"><span class="tag">Parallel Computing</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Parallel-Speedup/"><span class="tag">Parallel Speedup</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/PySpark/"><span class="tag">PySpark</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/ROC-curve/"><span class="tag">ROC curve</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Recommendation-System/"><span class="tag">Recommendation System</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Regression/"><span class="tag">Regression</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Regular-Expression/"><span class="tag">Regular Expression</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Sorting/"><span class="tag">Sorting</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Stacking/"><span class="tag">Stacking</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Traversal/"><span class="tag">Traversal</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Unsupervised-Learning/"><span class="tag">Unsupervised Learning</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Variable-Selection/"><span class="tag">Variable Selection</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Web-Scrapping/"><span class="tag">Web Scrapping</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Wide-and-Deep/"><span class="tag">Wide and Deep</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Wide-and-Deep-Model/"><span class="tag">Wide and Deep Model</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Wide-Deep-Model/"><span class="tag">Wide&amp;Deep Model</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Word-Embedding/"><span class="tag">Word Embedding</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Word-vector/"><span class="tag">Word vector</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/binary-search/"><span class="tag">binary search</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/bubble-sort/"><span class="tag">bubble sort</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/commands/"><span class="tag">commands</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/insertion-sort/"><span class="tag">insertion sort</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/merge-sort/"><span class="tag">merge sort</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/non-parametric-learning/"><span class="tag">non-parametric learning</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/quick-sort/"><span class="tag">quick sort</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/review/"><span class="tag">review</span><span class="tag is-grey-lightest">1</span></a></div></div></div></div></div><div class="card widget"><div class="card-content"><h3 class="menu-label">Recent</h3><article class="media"><div class="media-content size-small"><p><time dateTime="2021-05-11T17:53:43.000Z">2021-05-11</time></p><p class="title is-6"><a class="link-muted" href="/2021/05/11/ensemble-learning-blending/">Ensemble Learning - Blending</a></p><p class="is-uppercase"><a class="link-muted" href="/categories/Machine-Learning/">Machine Learning</a></p></div></article><article class="media"><div class="media-content size-small"><p><time dateTime="2021-05-07T04:54:08.000Z">2021-05-07</time></p><p class="title is-6"><a class="link-muted" href="/2021/05/07/LeetCode/">LeetCode 笔记</a></p><p class="is-uppercase"></p></div></article><article class="media"><div class="media-content size-small"><p><time dateTime="2021-03-27T17:59:45.000Z">2021-03-27</time></p><p class="title is-6"><a class="link-muted" href="/2021/03/27/Recommendation-System-5-DIN/">Recommendation-System-5-DIN</a></p><p class="is-uppercase"><a class="link-muted" href="/categories/Recommendation-System/">Recommendation System</a></p></div></article><article class="media"><div class="media-content size-small"><p><time dateTime="2021-03-24T17:14:15.000Z">2021-03-24</time></p><p class="title is-6"><a class="link-muted" href="/2021/03/24/Recommendation-System-4-NFM/">Recommendation-System-4-NFM</a></p><p class="is-uppercase"><a class="link-muted" href="/categories/Recommendation-System/">Recommendation System</a></p></div></article><article class="media"><div class="media-content size-small"><p><time dateTime="2021-03-21T05:57:56.000Z">2021-03-21</time></p><p class="title is-6"><a class="link-muted" href="/2021/03/21/Recommendation-System-3-DeepFM/">Recommendation-System-3-DeepFM</a></p><p class="is-uppercase"><a class="link-muted" href="/categories/Recommendation-System/">Recommendation System</a></p></div></article></div></div><div class="card widget"><div class="card-content"><div class="menu"><h3 class="menu-label">Archives</h3><ul class="menu-list"><li><a class="level is-mobile is-marginless" href="/archives/2021/05/"><span class="level-start"><span class="level-item">May 2021</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile is-marginless" href="/archives/2021/03/"><span class="level-start"><span class="level-item">March 2021</span></span><span class="level-end"><span class="level-item tag">5</span></span></a></li><li><a class="level is-mobile is-marginless" href="/archives/2021/02/"><span class="level-start"><span class="level-item">February 2021</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile is-marginless" href="/archives/2020/12/"><span class="level-start"><span class="level-item">December 2020</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile is-marginless" href="/archives/2020/11/"><span class="level-start"><span class="level-item">November 2020</span></span><span class="level-end"><span class="level-item tag">5</span></span></a></li><li><a class="level is-mobile is-marginless" href="/archives/2020/10/"><span class="level-start"><span class="level-item">October 2020</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile is-marginless" href="/archives/2020/09/"><span class="level-start"><span class="level-item">September 2020</span></span><span class="level-end"><span class="level-item tag">5</span></span></a></li><li><a class="level is-mobile is-marginless" href="/archives/2020/08/"><span class="level-start"><span class="level-item">August 2020</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile is-marginless" href="/archives/2020/07/"><span class="level-start"><span class="level-item">July 2020</span></span><span class="level-end"><span class="level-item tag">7</span></span></a></li><li><a class="level is-mobile is-marginless" href="/archives/2020/06/"><span class="level-start"><span class="level-item">June 2020</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li></ul></div></div></div><div class="card widget"><div class="card-content"><div class="menu"><h3 class="menu-label">Subscribe to Updates</h3><form action="https://feedburner.google.com/fb/a/mailverify" method="post" target="popupwindow" onsubmit="window.open(&#039;https://feedburner.google.com/fb/a/mailverify?uri=&#039;,&#039;popupwindow&#039;,&#039;scrollbars=yes,width=550,height=520&#039;);return true"><input type="hidden" value="" name="uri"><input type="hidden" name="loc" value="en_US"><div class="field has-addons"><div class="control has-icons-left is-expanded"><input class="input" name="email" type="email" placeholder="Email"><span class="icon is-small is-left"><i class="fas fa-envelope"></i></span></div><div class="control"><input class="button is-primary" type="submit" value="Subscribe"></div></div></form></div></div></div><div class="column-right-shadow is-hidden-widescreen"></div></div><div class="column column-right is-4-tablet is-4-desktop is-3-widescreen is-hidden-touch is-hidden-desktop-only order-3"><div class="card widget"><div class="card-content"><nav class="level"><div class="level-item has-text-centered flex-shrink-1"><div><figure class="image is-128x128 mx-auto mb-2"><img class="avatar is-rounded" src="/images/avatar.jpg" alt="Wenkang Wei"></figure><p class="title is-size-4 is-block line-height-inherit">Wenkang Wei</p><p class="is-size-6 is-block">computer engineering| machine learning</p><p class="is-size-6 is-flex justify-content-center"><i class="fas fa-map-marker-alt mr-1"></i><span>Clemson,SC</span></p></div></div></nav><nav class="level is-mobile"><div class="level-item has-text-centered is-marginless"><div><p class="heading">Posts</p><a href="/archives"><p class="title">33</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">Categories</p><a href="/categories"><p class="title">18</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">Tags</p><a href="/tags"><p class="title">67</p></a></div></div></nav><div class="level"><a class="level-item button is-primary is-rounded" href="https://github.com/wenkangwei" target="_blank" rel="noopener">Follow</a></div><div class="level is-mobile"><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Github" href="https://github.com/wenkangwei"><i class="fab fa-github"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="LinkedIn" href="https://www.linkedin.com/in/wenkang-wei-588811167"><i class="fab fa-linkedin"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Facebook" href="https://facebook.com"><i class="fab fa-facebook"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Twitter" href="https://twitter.com"><i class="fab fa-twitter"></i></a></div></div></div><div class="card widget" id="toc"><div class="card-content"><div class="menu"><h3 class="menu-label">Catalogue</h3><ul class="menu-list"><li><a class="is-flex" href="#Introduction"><span class="mr-2">1</span><span>Introduction</span></a></li><li><a class="is-flex" href="#Type-of-machine-learning-model"><span class="mr-2">2</span><span>Type of machine learning model</span></a><ul class="menu-list"><ul class="menu-list"><li><a class="is-flex" href="#Parametric-non-parametric-models"><span class="mr-2">2.1.1</span><span>Parametric/ non-parametric models</span></a></li><li><a class="is-flex" href="#linear-non-linear-model"><span class="mr-2">2.1.2</span><span>linear/non-linear model</span></a></li></ul></ul></li><li><a class="is-flex" href="#Linear-Regression"><span class="mr-2">3</span><span>Linear Regression</span></a><ul class="menu-list"><ul class="menu-list"><li><a class="is-flex" href="#Optimize-Model"><span class="mr-2">3.1.1</span><span>Optimize Model</span></a></li><li><a class="is-flex" href="#Properties"><span class="mr-2">3.1.2</span><span>Properties</span></a></li></ul></ul></li><li><a class="is-flex" href="#Logistic-Regression"><span class="mr-2">4</span><span>Logistic Regression</span></a><ul class="menu-list"><ul class="menu-list"><li><a class="is-flex" href="#Optimize-Model-1"><span class="mr-2">4.1.1</span><span>Optimize Model</span></a></li><li><a class="is-flex" href="#Properties-1"><span class="mr-2">4.1.2</span><span>Properties</span></a></li></ul></ul></li><li><a class="is-flex" href="#Polynomial-Regression"><span class="mr-2">5</span><span>Polynomial Regression</span></a><ul class="menu-list"><ul class="menu-list"><li><a class="is-flex" href="#Optimize-Model-2"><span class="mr-2">5.1.1</span><span>Optimize Model</span></a></li><li><a class="is-flex" href="#Properties-2"><span class="mr-2">5.1.2</span><span>Properties</span></a></li></ul></ul></li><li><a class="is-flex" href="#Lasso-regression"><span class="mr-2">6</span><span>Lasso regression</span></a><ul class="menu-list"><ul class="menu-list"><li><a class="is-flex" href="#Optimize-Model-3"><span class="mr-2">6.1.1</span><span>Optimize Model</span></a></li><li><a class="is-flex" href="#Properties-3"><span class="mr-2">6.1.2</span><span>Properties</span></a></li></ul></ul></li><li><a class="is-flex" href="#Ridge-Regression"><span class="mr-2">7</span><span>Ridge Regression</span></a><ul class="menu-list"><ul class="menu-list"><li><a class="is-flex" href="#Optimize-Model-4"><span class="mr-2">7.1.1</span><span>Optimize Model</span></a></li><li><a class="is-flex" href="#Properties-4"><span class="mr-2">7.1.2</span><span>Properties</span></a></li></ul></ul></li><li><a class="is-flex" href="#Elastic-Net-Regression"><span class="mr-2">8</span><span>Elastic Net Regression</span></a><ul class="menu-list"><ul class="menu-list"><li><a class="is-flex" href="#Optimize-Model-5"><span class="mr-2">8.1.1</span><span>Optimize Model</span></a></li><li><a class="is-flex" href="#Properties-5"><span class="mr-2">8.1.2</span><span>Properties</span></a></li></ul></ul></li><li><a class="is-flex" href="#Stepwise-Regression"><span class="mr-2">9</span><span>Stepwise Regression</span></a><ul class="menu-list"><ul class="menu-list"><li><a class="is-flex" href="#Stepwise-regression-approaches"><span class="mr-2">9.1.1</span><span>Stepwise regression approaches</span></a></li><li><a class="is-flex" href="#Subset-Selection"><span class="mr-2">9.1.2</span><span>Subset Selection</span></a></li></ul></ul></li><li><a class="is-flex" href="#Summary"><span class="mr-2">10</span><span>Summary</span></a></li><li><a class="is-flex" href="#Reference"><span class="mr-2">11</span><span>Reference</span></a></li></ul></div></div></div></div></div></div></section><footer class="footer"><div class="container"><div class="level"><div class="level-start"><a class="footer-logo is-block mb-2" href="/"><img src="/images/icon.png" alt="Wenkang&#039;s Blog" height="28"></a><p class="size-small"><span>&copy; 2021 Wenkang Wei</span>  Powered by <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a> &amp; <a href="https://github.com/ppoffice/hexo-theme-icarus" target="_blank" rel="noopener">Icarus</a></p></div><div class="level-end"><div class="field has-addons"><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Creative Commons" href="https://creativecommons.org/"><i class="fab fa-creative-commons"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Attribution 4.0 International" href="https://creativecommons.org/licenses/by/4.0/"><i class="fab fa-creative-commons-by"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/ppoffice/hexo-theme-icarus"><i class="fab fa-github"></i></a></p></div></div></div></div></footer><script src="https://cdn.jsdelivr.net/npm/jquery@3.3.1/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/moment@2.22.2/min/moment-with-locales.min.js"></script><script>moment.locale("en");</script><script>var IcarusThemeSettings = {
            site: {
                url: 'https://github.com/wenkangwei/wenkangwei.github.io`',
                external_link: {"enable":true,"exclude":[]}
            },
            article: {
                highlight: {
                    clipboard: true,
                    fold: 'unfolded'
                }
            }
        };</script><script src="https://cdn.jsdelivr.net/npm/clipboard@2.0.4/dist/clipboard.min.js" defer></script><script src="/js/animation.js"></script><a id="back-to-top" title="Back to Top" href="javascript:;"><i class="fas fa-chevron-up"></i></a><script src="/js/back_to_top.js" defer></script><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/lightgallery@1.6.8/dist/js/lightgallery.min.js" defer></script><script src="https://cdn.jsdelivr.net/npm/justifiedGallery@3.7.0/dist/js/jquery.justifiedGallery.min.js" defer></script><script>window.addEventListener("load", () => {
            if (typeof $.fn.lightGallery === 'function') {
                $('.article').lightGallery({ selector: '.gallery-item' });
            }
            if (typeof $.fn.justifiedGallery === 'function') {
                if ($('.justified-gallery > p > .gallery-item').length) {
                    $('.justified-gallery > p > .gallery-item').unwrap();
                }
                $('.justified-gallery').justifiedGallery();
            }
        });</script><!--!--><!--!--><script type="text/x-mathjax-config">MathJax.Hub.Config({
            'HTML-CSS': {
                matchFontHeight: false
            },
            SVG: {
                matchFontHeight: false
            },
            CommonHTML: {
                matchFontHeight: false
            },
            tex2jax: {
                inlineMath: [
                    ['$','$'],
                    ['\\(','\\)']
                ]
            }
        });</script><script src="https://cdn.jsdelivr.net/npm/mathjax@2.7.5/unpacked/MathJax.js?config=TeX-MML-AM_CHTML" defer></script><!--!--><script src="/js/main.js" defer></script><div class="searchbox"><div class="searchbox-container"><div class="searchbox-header"><div class="searchbox-input-container"><input class="searchbox-input" type="text" placeholder="Type something..."></div><a class="searchbox-close" href="javascript:;">×</a></div><div class="searchbox-body"></div></div></div><script src="/js/insight.js" defer></script><script>document.addEventListener('DOMContentLoaded', function () {
            loadInsight({"contentUrl":"/content.json"}, {"hint":"Type something...","untitled":"(Untitled)","posts":"Posts","pages":"Pages","categories":"Categories","tags":"Tags"});
        });</script></body></html>