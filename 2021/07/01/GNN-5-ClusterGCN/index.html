<!doctype html>
<html lang="en"><head><meta charset="utf-8"><meta name="generator" content="Hexo 4.2.1"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta><title>GNN-5-ClusterGCN - Wenkang&#039;s Blog</title><meta description=""><meta property="og:type" content="article"><meta property="og:title" content="GNN-5-ClusterGCN"><meta property="og:url" content="https://github.com/wenkangwei/"><meta property="og:site_name" content="Wenkang&#039;s Blog"><meta property="og:locale" content="en_US"><meta property="article:published_time" content="2021-07-01T06:18:47.000Z"><meta property="article:modified_time" content="2021-07-01T21:02:15.509Z"><meta property="article:author" content="Wenkang Wei"><meta property="article:tag" content="GNN"><meta property="article:tag" content="Graph"><meta property="twitter:card" content="summary"><script type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"https://github.com/wenkangwei/wenkangwei.github.io%60/2021/07/01/GNN-5-ClusterGCN/"},"headline":"Wenkang's Blog","image":[],"datePublished":"2021-07-01T06:18:47.000Z","dateModified":"2021-07-01T21:02:15.509Z","author":{"@type":"Person","name":"Wenkang Wei"},"description":""}</script><link rel="canonical" href="https://github.com/wenkangwei/wenkangwei.github.io%60/2021/07/01/GNN-5-ClusterGCN/"><link rel="icon" href="/images/icon.png"><link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.12.0/css/all.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@9.12.0/styles/atom-one-light.css"><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Ubuntu:wght@400;600&amp;family=Source+Code+Pro"><link rel="stylesheet" href="/css/default.css"><style>body>.footer,body>.navbar,body>.section{opacity:0}</style><!--!--><!--!--><!--!--><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/lightgallery@1.6.8/dist/css/lightgallery.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/justifiedGallery@3.7.0/dist/css/justifiedGallery.min.css"><!--!--><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/pace-js@1.0.2/pace.min.js"></script></head><body class="is-3-column"><nav class="navbar navbar-main"><div class="container"><div class="navbar-brand justify-content-center"><a class="navbar-item navbar-logo" href="/"><img src="/images/icon.png" alt="Wenkang&#039;s Blog" height="28"></a></div><div class="navbar-menu"><div class="navbar-start"><a class="navbar-item" href="/">Home</a><a class="navbar-item" href="/archives">Archives</a><a class="navbar-item" href="/categories">Categories</a><a class="navbar-item" href="/tags">Tags</a><a class="navbar-item" href="/about">About</a></div><div class="navbar-end"><a class="navbar-item" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/ppoffice/hexo-theme-icarus"><i class="fab fa-github"></i></a><a class="navbar-item is-hidden-tablet catalogue" title="Catalogue" href="javascript:;"><i class="fas fa-list-ul"></i></a><a class="navbar-item search" title="Search" href="javascript:;"><i class="fas fa-search"></i></a></div></div></div></nav><section class="section"><div class="container"><div class="columns"><div class="column order-2 column-main is-8-tablet is-8-desktop is-6-widescreen"><div class="card"><article class="card-content article" role="article"><div class="article-meta size-small is-uppercase level is-mobile"><div class="level-left"><time class="level-item" dateTime="2021-07-01T06:18:47.000Z" title="2021-07-01T06:18:47.000Z">2021-07-01</time><span class="level-item"><a class="link-muted" href="/categories/Deep-Learning/">Deep Learning</a></span><span class="level-item">40 minutes read (About 5980 words)</span></div></div><h1 class="title is-3 is-size-4-mobile">GNN-5-ClusterGCN</h1><div class="content"><img src=https://raw.githubusercontent.com/wenkangwei/Datawhale-Team-Learning/main/GNN/Task-5-ClusterGCN/neighborhood-expansion.png>

<a id="more"></a>

<h1 id="GCN-Task-5-Cluster-GCN"><a href="#GCN-Task-5-Cluster-GCN" class="headerlink" title="GCN-Task-5-Cluster GCN"></a>GCN-Task-5-Cluster GCN</h1><h2 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1. Introduction"></a>1. Introduction</h2><p>这篇文章的目的主要是理解 Cluster-GCN这篇文章的内容(要解决的问题，思路，等)和通过代码实现一下Cluster-GCN网络。 Cluster-GCN的文章可以查看: <a href="https://arxiv.org/pdf/1905.07953.pdf">https://arxiv.org/pdf/1905.07953.pdf</a><br>这篇blog的结构大概如下:</p>
<ul>
<li>解释Cluster-GCN的内容<ul>
<li>Cluster-GCN要解决的问题</li>
<li>基本思路</li>
<li>Cluster-GCN的特点和优缺点</li>
</ul>
</li>
<li>Coding实现<ul>
<li>数据集</li>
<li>Cluster-GCN模型</li>
<li>Training and Testing</li>
<li>Assignment from datawhale</li>
</ul>
</li>
<li>总结文章的重点</li>
<li>Reference 参考文献</li>
</ul>
<h2 id="2-Cluster-GCN"><a href="#2-Cluster-GCN" class="headerlink" title="2. Cluster-GCN"></a>2. Cluster-GCN</h2><p>Cluster GCN是由国立台湾大学Wei-Lin Chiang，Google research 的Yang Li和 Samy Bengio， Cho-Jui Hsieh 等人提出的GCN网络(看到Bengio等Google大牛的名字就知道这篇文章很值得一读)。文章的全称是Cluster-GCN: An Efficient Algorithm for Training Deep and Large Graph Convolutional Networks。 从名字就可以知道这个Cluster-GCN的目的是要优化深度学习和超大图网络的效率而提出来的(一般都会涉及时间和空间的复杂度分析)。这里先来看看这篇文章要解决的问题</p>
<h3 id="2-1-Problem-to-solve"><a href="#2-1-Problem-to-solve" class="headerlink" title="2.1 Problem to solve"></a>2.1 Problem to solve</h3><p>首先这篇文章提出和分析了在GNN学习里面的几个问题:</p>
<ul>
<li><p>Full-batch gradient descent:</p>
<p>  这里先做以下定义</p>
<ul>
<li><p>node的个数为N</p>
</li>
<li><p>Embedding dimension = F</p>
</li>
<li><p>GCN的layer层数为L.</p>
<p>那么在用Full batch GD 全梯度下降方法时所需的Space Complexity = O(NFL)并且计算梯度时如果node个数很多有上百万个，每个epoch里面梯度计算也是很慢的。因此这种方法不考虑</p>
</li>
</ul>
</li>
<li><p><strong>Mini-batch SGD</strong></p>
<p>  Mini-batch SGD算是对Full batch GD的方法通过随机采样降低要计算的梯度的存储空间以及加快了计算的速度。 但是mini-batch SGD会使得时间复杂度随着GCN的层数增加而指数级增长。paper原话是这么解释的:<br>  <code>  mini-batch SGD introduces a significant computational overhead due to the neighborhood expansion problem—to compute the loss on a single node at layer L, it requires that node’s neighbor nodes’ embeddings at layer L − 1, which again requires their neighbors’ embeddings at layer L − 2 and recursive ones in the downstream layers. This leads to time complexity exponential to the GCN depth.  </code><br>  用我自己的话来理解就是，以下图为例</p>
  <img src=https://raw.githubusercontent.com/wenkangwei/Datawhale-Team-Learning/main/GNN/Task-5-ClusterGCN/graph-1.png>

<pre><code>   如果在GCN第i层的第j个节点的embedding vector用$Z_ {i}^{j}$，而第i个节点的neighbor用$N(i)$表示， 那么在第0层输入层节点A的embedding就是$Z_ {A}^{0}$, $N(A)$ = {B,D}。那么我们就有以下的公式:
</code></pre>
<ul>
<li>在第1层 L1, $Z_ {A}^{1} = f(Z_ {B}^{0}, Z_ {D}^{0}, Z_ {A}^{0})$</li>
<li>在第2层 L2, $Z_ {A}^{2} = f(Z_ {B}^{1}, Z_ {D}^{1}, Z_ {A}^{1})$, 而其中又可以把$Z_ {A}^{1}$ 展开成L1层里面的式子，$Z_ {B}^{1},Z_ {D}^{1}$ 同理</li>
<li>第3层L3,  $Z_ {A}^{3} = f(Z_ {B}^{2}, Z_ {D}^{2}, Z_ {A}^{2})$, 计算时和第二步同理可以多次展开成用L1层的输入表示的形式，这么一来可以看到，随着GCN层数增加，neighborhood expansion problem 邻居展开问题就会使得梯度计算更加复杂</li>
<li>这个节点i在第j层梯度计算都取决于第j-1层前面一层的计算就是neighborhood expansion problem</li>
<li>如果GCN层数为L， 节点的平均degree为d，那么我们计算一个节点的梯度就需要$O(d ^L)$个节点embedding信息， 而由于要乘上权重矩阵W, 计算每个node的embedding需要O($F^2$) 的时间。 那么计算一个node的梯度为$O(d ^LF^2)$</li>
</ul>
</li>
</ul>
<ul>
<li>VR-GCN(variance reduction GCN), it uses variance reduction technique to reduce the size of neighborhood sampling nodes</li>
</ul>
<h3 id="2-2-How-Cluster-GCN-works"><a href="#2-2-How-Cluster-GCN-works" class="headerlink" title="2.2 How Cluster-GCN works"></a>2.2 How Cluster-GCN works</h3><h4 id="2-2-1-Cluster-GCN的思想"><a href="#2-2-1-Cluster-GCN的思想" class="headerlink" title="2.2.1 Cluster-GCN的思想"></a>2.2.1 Cluster-GCN的思想</h4><p>ClusterGCN的想法是我们能不能找到一把种将节点分成多个batch的方式，并将图划分成多个子图，使得表征利用率最大？我们通过将表征利用率的概念与图节点聚类的目标联系起来来回答这个问题。原文:<code>can we design a batch and the corresponding computation subgraph to maximize the embedding utilization?</code></p>
<p><strong>节点表征的利用率可以反映出计算的效率。</strong>考虑到一个batch有多个节点，时间与空间复杂度的计算就不是上面那样简单了，因为不同的节点同样距离远的邻接节点可以是重叠的，于是计算表征的次数可以小于最坏的情况$O(b d^{L})$。为了反映mini-batch SGD的计算效率，Cluster-GCN论文提出了**”表征利用率”<strong>的概念来描述计算效率。在训练过程中，如果节点$i$在$l$层的表征$z_{i}^{(l)}$被计算并在$l+1$层的表征计算中被重复使用$u$次，那么我们说$z_{i}^{(l)}$的表征利用率为$u$。</strong>对于随机抽样的mini-batch SGD，$u$非常小<strong>，因为图通常是大且稀疏的。假设$u$是一个小常数（节点间同样距离的邻接节点重叠率小），那么mini-batch SGD的训练方式对每个batch需要计算$O(b d^{L})$的表征，于是每次参数更新需要$O(b d^{L} F^{2})$的时间，</strong>每个epoch需要$O(N d^{L} F^{2})$的时间<strong>，这被称为</strong>邻域扩展问题**。</p>
<p>相反的是，<strong>全梯度下降训练具有最大的表征利用率</strong>——每个节点表征将在上一层被重复使用平均节点度次。因此，全梯度下降法在每个epoch中只需要计算$O(N L)$的表征，这意味着平均下来只需要$O(L)$的表征计算就可以获得一个节点的梯度。</p>
<h4 id="2-2-2-简单的Cluster-GCN方法"><a href="#2-2-2-简单的Cluster-GCN方法" class="headerlink" title="2.2.2 简单的Cluster-GCN方法"></a>2.2.2 简单的Cluster-GCN方法</h4><p>考虑到在每个batch中，我们计算一组节点（记为$\mathcal{B}$）从第$1$层到第$L$层的表征。由于图神经网络每一层的计算都使用相同的子图$A_{\mathcal{B}, \mathcal{B}}$（$\mathcal{B}$内部的边），所以表征利用率就是这个batch内边的数量，记为$|A_{\mathcal{B}, \mathcal{B}}|_{0}$。因此，<strong>为了最大限度地提高表征利用率，理想的划分batch的结果是，batch内的边尽可能多，batch之间的边尽可能少</strong>。基于这一点，我们将SGD图神经网络训练的效率与图聚类算法联系起来。</p>
<p><strong>现在我们正式学习Cluster-GCN方法</strong>。对于一个图$G$，我们将其节点划分为$c$个簇：$\mathcal{V}=[\mathcal{V}_ {1}, \cdots \mathcal{V}_ {c}]$，其中$\mathcal{V}<em>{t}$由第$t$个簇中的节点组成，对应的我们有$c$个子图：<br>$$<br>\bar{G}=[G</em>{1}, \cdots, G_{c}]=[{\mathcal{V}_ {1}, \mathcal{E}_ {1}}, \cdots,{\mathcal{V}_ {c}, \mathcal{E}_ {c}}]<br>\notag<br>$$<br>其中$\mathcal{E}<em>{t}$只由$\mathcal{V}</em>{t}$中的节点之间的边组成。经过节点重组，邻接矩阵被划分为大小为$c^{2}$的块矩阵，如下所示</p>
<p>$$<br>A=\bar{A}+\Delta=[\begin{array}{ccc}<br>A_{11} &amp; \cdots &amp; A_{1 c} \\<br>\vdots &amp; \ddots &amp; \vdots \\<br>A_{c 1} &amp; \cdots &amp; A_{c c}<br>\end{array}]<br>\tag{4}<br>$$</p>
<p>其中</p>
<p>$$<br>\bar{A}=[\begin{array}{ccc}<br>A_{11} &amp; \cdots &amp; 0 \\<br>\vdots &amp; \ddots &amp; \vdots \\<br>0 &amp; \cdots &amp; A_{c c}<br>\end{array}], \Delta=[\begin{array}{ccc}<br>0 &amp; \cdots &amp; A_ {1 c} \\<br>\vdots &amp; \ddots &amp; \vdots \\<br>A_{c 1} &amp; \cdots &amp; 0<br>\end{array}]<br>\tag{5}<br>$$</p>
<p>其中，对角线上的块$A_{t t}$是大小为$|\mathcal{V}<em>{t}| \times|\mathcal{V}</em>{t}|$的邻接矩阵，它由$G_{t}$内部的边构成。$\bar{A}$是图$\bar{G}$的邻接矩阵。$A_{s t}$由两个簇$\mathcal{V}_ {s}$和$\mathcal{V}_ {t}$之间的边构成。$\Delta$是由$A$的所有非对角线块组成的矩阵。同样，我们可以根据$[\mathcal{V}_ {1}, \cdots, \mathcal{V}_ {c}]$ 划分节点表征矩阵$X$和类别向量 $Y$，得到$[X_{1}, \cdots, X_{c}]$和$[Y_{1}, \cdots, Y_{c}]$，其中$X_{t}$和$Y_{t}$分别由$V_{t}$中节点的表征和类别组成。</p>
<p>接下来我们用块对角线邻接矩阵 $\bar{A}$ 去近似邻接矩阵$A$，这样做的好处是，<strong>完整的损失函数（公示(2）)可以根据batch分解成多个部分之和</strong>。以$\bar{A}^{\prime}$表示归一化后的$\bar{A}$，最后一层节点表征矩阵可以做如下的分解：</p>
<p>$$<br>\begin{aligned}<br>Z^{(L)} &amp;=\bar{A}^{\prime} \sigma(\bar{A}^{\prime} \sigma(\cdots \sigma(\bar{A}^{\prime} X W^{(0)}) W^{(1)}) \cdots) W^{(L-1)} \\<br>&amp;=[\begin{array}{c}<br>\bar{A}_ {11}^{\prime} \sigma(\bar{A}_ {11}^{\prime} \sigma(\cdots \sigma(\bar{A}_ {11}^{\prime} X_{1} W^{(0)}) W^{(1)}) \cdots) W^{(L-1)} \\<br>\vdots \\<br>\bar{A}_ {c c}^{\prime} \sigma(\bar{A}_ {c c}^{\prime} \sigma(\cdots \sigma(\bar{A}_ {c c}^{\prime} X_{c} W^{(0)}) W^{(1)}) \cdots) W^{(L-1)}<br>\end{array}]<br>\end{aligned}<br>\tag{6}<br>$$</p>
<p>由于$\bar{A}$是块对角形式（$\bar{A}_ {t t}^{\prime}$是$\bar{A}^{\prime}$的对角线上的块），于是损失函数可以分解为<br>$$<br>\mathcal{L}_ {\bar{A}^{\prime}}=\sum_{t} \frac{|\mathcal{V}_ {t}|}{N} \mathcal{L}_ {\bar{A}_ {t t}^{\prime}} \text { and } \mathcal{L}_ {\bar{A}_ {t t}^{\prime}}=\frac{1}{|\mathcal{V}_ {t}|} \sum_{i \in \mathcal{V}_ {t}} \operatorname{loss}(y_{i}, z_{i}^{(L)})<br>\tag{7}<br>$$</p>
<p>基于公式(6)和公式(7)，在训练的每一步中，Cluster-GCN首先采样一个簇 $\mathcal{V}_ {t}$，然后根据$\mathcal{L}_ {\bar{A}^{\prime}_ {tt}}$ 的梯度进行参数更新。这种训练方式，只需要用到子图 $A_{t t}$, $X_{t}$, $Y_{t}$ 以及神经网络权重矩阵 ${W^{(l)}}_{l=1}^{L}$。 实际中，主要的计算开销在神经网络前向过程中的矩阵乘法运算（公式(6)的一个行）和梯度反向传播。</p>
<p>我们使用图节点聚类算法来划分图。<strong>图节点聚类算法将图节点分成多个簇，划分结果是簇内边的数量远多于簇间边的数量</strong>。如前所述，每个batch的表征利用率相当于簇内边的数量。直观地说，每个节点和它的邻接节点大部分情况下都位于同一个簇中，因此 $L$ 跳（L-hop）远的邻接节点大概率仍然在同一个簇中。由于我们用块对角线近似邻接矩阵$\bar{A}$代替邻接矩阵$A$，产生的误差与簇间的边的数量$\Delta$成正比，所以<strong>簇间的边越少越好</strong>。综上所述，使用图节点聚类算法对图节点划分多个簇的结果，正是我们希望得到的。</p>
<p>在下图，我们可以看到，<strong>Cluster-GCN方法可以避免巨大范围的邻域扩展</strong>（图右），因为Cluster-GCN方法将邻域扩展限制在簇内。</p>
<img src=https://raw.githubusercontent.com/wenkangwei/Datawhale-Team-Learning/main/GNN/Task-5-ClusterGCN/neighborhood-expansion.png>

<h4 id="2-2-3-Cluster-GCN实现过程"><a href="#2-2-3-Cluster-GCN实现过程" class="headerlink" title="2.2.3 Cluster-GCN实现过程"></a>2.2.3 Cluster-GCN实现过程</h4><img src=https://raw.githubusercontent.com/wenkangwei/Datawhale-Team-Learning/main/GNN/Task-5-ClusterGCN/alg-1.png>

<p>从上图可以看到, Cluster-GCN的实现流程基本是</p>
<ol>
<li>用METIS partition算法对图的节点进行分解成c个cluster</li>
<li>不断迭代， 每次迭代都随机选取q个cluster进行无放回采样node，links并形成subgraph</li>
<li>对subgraph进行预测和gradient计算</li>
<li>用adam进行node的更新和学习</li>
</ol>
<p>另外 Cluster-GCN方法提出了一个修改版的公式(9)，以更好地保持邻接节点信息和数值范围。首先给原始的$A$添加一个单位矩阵$I$，并进行归一化处理<br>$$<br>\tilde{A}=(D+I)^{-1}(A+I)<br>\tag{10}<br>$$<br>然后考虑，<br>$$<br>X^{(l+1)}=\sigma((\tilde{A}+\lambda \operatorname{diag}(\tilde{A})) X^{(l)} W^{(l)})<br>\tag{11}<br>$$</p>
<p>以上就是Cluster-GCN的每层layer的更新输出公式</p>
<h3 id="2-3-Properties"><a href="#2-3-Properties" class="headerlink" title="2.3 Properties"></a>2.3 Properties</h3><ul>
<li>Advantage<ul>
<li><p>先来看看时间和空间复杂度。 在时间上它只和layer层数 L, embedding feature的大小F以及邻接矩阵的非零的行数||A||和节点个数有关， 而空间上和batch的大小相关，相对于传统的GCN，它把指数次降到1次</p>
<img src=https://raw.githubusercontent.com/wenkangwei/Datawhale-Team-Learning/main/GNN/Task-5-ClusterGCN/Complexity.png>
</li>
<li><p>除了Time, Space complexity外, paper里面提及在大型的图数据里面如PPI, Reddit是最好的(这个可能有调参的因素在里面)</p>
<img src=https://raw.githubusercontent.com/wenkangwei/Datawhale-Team-Learning/main/GNN/Task-5-ClusterGCN/SOTA-result.png>
</li>
</ul>
</li>
<li>Shortage<ul>
<li>在收敛性上面, Cluster-GCN在layer数量超过3层之后Accuracy性能没有明显变大，反而layer到了6层之后，开始收敛不好性能变差,如下图所示<img src=https://raw.githubusercontent.com/wenkangwei/Datawhale-Team-Learning/main/GNN/Task-5-ClusterGCN/convergence.png>
</li>
</ul>
</li>
<li>Other properties<ul>
<li>ClusterGCN在做clustering对节点进行cluster partition时特意对比了 random partition和METIS clustering partition两种方法, 以及batch设计时是用多个cluster还是一个cluster作为一个batch。它表明了用METIS和 multiple clusters as a batch 更能使性能提升，loss降低更多。</li>
</ul>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h3 id="3-1-官方测试源码"><a href="#3-1-官方测试源码" class="headerlink" title="3.1 官方测试源码"></a>3.1 官方测试源码</h3><p>这里用了Reddit的dataset进行测试</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">from</span> torch.nn <span class="keyword">import</span> ModuleList</span><br><span class="line"><span class="keyword">from</span> tqdm <span class="keyword">import</span> tqdm</span><br><span class="line"><span class="keyword">from</span> torch_geometric.datasets <span class="keyword">import</span> Reddit</span><br><span class="line"><span class="keyword">from</span> torch_geometric.data <span class="keyword">import</span> ClusterData, ClusterLoader, NeighborSampler</span><br><span class="line"><span class="keyword">from</span> torch_geometric.nn <span class="keyword">import</span> SAGEConv</span><br><span class="line"></span><br><span class="line">dataset = Reddit(<span class="string">'./data/Reddit'</span>)</span><br><span class="line">data = dataset[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">cluster_data = ClusterData(data, num_parts=<span class="number">1500</span>, recursive=<span class="literal">False</span>,</span><br><span class="line">                           save_dir=dataset.processed_dir)</span><br><span class="line">train_loader = ClusterLoader(cluster_data, batch_size=<span class="number">20</span>, shuffle=<span class="literal">True</span>,</span><br><span class="line">                             num_workers=<span class="number">12</span>)</span><br><span class="line"></span><br><span class="line">subgraph_loader = NeighborSampler(data.edge_index, sizes=[<span class="number">-1</span>], batch_size=<span class="number">1024</span>,</span><br><span class="line">                                  shuffle=<span class="literal">False</span>, num_workers=<span class="number">12</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Net</span><span class="params">(torch.nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, in_channels, out_channels)</span>:</span></span><br><span class="line">        super(Net, self).__init__()</span><br><span class="line">        self.convs = ModuleList(</span><br><span class="line">            [SAGEConv(in_channels, <span class="number">128</span>),</span><br><span class="line">             SAGEConv(<span class="number">128</span>, out_channels)])</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x, edge_index)</span>:</span></span><br><span class="line">        <span class="keyword">for</span> i, conv <span class="keyword">in</span> enumerate(self.convs):</span><br><span class="line">            x = conv(x, edge_index)</span><br><span class="line">            <span class="keyword">if</span> i != len(self.convs) - <span class="number">1</span>:</span><br><span class="line">                x = F.relu(x)</span><br><span class="line">                x = F.dropout(x, p=<span class="number">0.5</span>, training=self.training)</span><br><span class="line">        <span class="keyword">return</span> F.log_softmax(x, dim=<span class="number">-1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">inference</span><span class="params">(self, x_all)</span>:</span></span><br><span class="line">        pbar = tqdm(total=x_all.size(<span class="number">0</span>) * len(self.convs))</span><br><span class="line">        pbar.set_description(<span class="string">'Evaluating'</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Compute representations of nodes layer by layer, using *all*</span></span><br><span class="line">        <span class="comment"># available edges. This leads to faster computation in contrast to</span></span><br><span class="line">        <span class="comment"># immediately computing the final representations of each batch.</span></span><br><span class="line">        <span class="keyword">for</span> i, conv <span class="keyword">in</span> enumerate(self.convs):</span><br><span class="line">            xs = []</span><br><span class="line">            <span class="keyword">for</span> batch_size, n_id, adj <span class="keyword">in</span> subgraph_loader:</span><br><span class="line">                edge_index, _, size = adj.to(device)</span><br><span class="line">                x = x_all[n_id].to(device)</span><br><span class="line">                x_target = x[:size[<span class="number">1</span>]]</span><br><span class="line">                x = conv((x, x_target), edge_index)</span><br><span class="line">                <span class="keyword">if</span> i != len(self.convs) - <span class="number">1</span>:</span><br><span class="line">                    x = F.relu(x)</span><br><span class="line">                xs.append(x.cpu())</span><br><span class="line"></span><br><span class="line">                pbar.update(batch_size)</span><br><span class="line"></span><br><span class="line">            x_all = torch.cat(xs, dim=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">        pbar.close()</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> x_all</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">device = torch.device(<span class="string">'cuda'</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">'cpu'</span>)</span><br><span class="line">model = Net(dataset.num_features, dataset.num_classes).to(device)</span><br><span class="line">optimizer = torch.optim.Adam(model.parameters(), lr=<span class="number">0.005</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train</span><span class="params">()</span>:</span></span><br><span class="line">    model.train()</span><br><span class="line"></span><br><span class="line">    total_loss = total_nodes = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> batch <span class="keyword">in</span> train_loader:</span><br><span class="line">        batch = batch.to(device)</span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line">        out = model(batch.x, batch.edge_index)</span><br><span class="line">        loss = F.nll_loss(out[batch.train_mask], batch.y[batch.train_mask])</span><br><span class="line">        loss.backward()</span><br><span class="line">        optimizer.step()</span><br><span class="line"></span><br><span class="line">        nodes = batch.train_mask.sum().item()</span><br><span class="line">        total_loss += loss.item() * nodes</span><br><span class="line">        total_nodes += nodes</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> total_loss / total_nodes</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="meta">@torch.no_grad()</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">test</span><span class="params">()</span>:</span>  <span class="comment"># Inference should be performed on the full graph.</span></span><br><span class="line">    model.eval()</span><br><span class="line"></span><br><span class="line">    out = model.inference(data.x)</span><br><span class="line">    y_pred = out.argmax(dim=<span class="number">-1</span>)</span><br><span class="line"></span><br><span class="line">    accs = []</span><br><span class="line">    <span class="keyword">for</span> mask <span class="keyword">in</span> [data.train_mask, data.val_mask, data.test_mask]:</span><br><span class="line">        correct = y_pred[mask].eq(data.y[mask]).sum().item()</span><br><span class="line">        accs.append(correct / mask.sum().item())</span><br><span class="line">    <span class="keyword">return</span> accs</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> range(<span class="number">1</span>, <span class="number">31</span>):</span><br><span class="line">    loss = train()</span><br><span class="line">    <span class="keyword">if</span> epoch % <span class="number">5</span> == <span class="number">0</span>:</span><br><span class="line">        train_acc, val_acc, test_acc = test()</span><br><span class="line">        print(<span class="string">f'Epoch: <span class="subst">&#123;epoch:<span class="number">02</span>d&#125;</span>, Loss: <span class="subst">&#123;loss:<span class="number">.4</span>f&#125;</span>, Train: <span class="subst">&#123;train_acc:<span class="number">.4</span>f&#125;</span>, '</span></span><br><span class="line">              <span class="string">f'Val: <span class="subst">&#123;val_acc:<span class="number">.4</span>f&#125;</span>, test: <span class="subst">&#123;test_acc:<span class="number">.4</span>f&#125;</span>'</span>)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        print(<span class="string">f'Epoch: <span class="subst">&#123;epoch:<span class="number">02</span>d&#125;</span>, Loss: <span class="subst">&#123;loss:<span class="number">.4</span>f&#125;</span>'</span>)</span><br></pre></td></tr></table></figure>

<pre><code>Downloading https://data.dgl.ai/dataset/reddit.zip
Extracting data/Reddit/raw/reddit.zip
Processing...
Done!
Computing METIS partitioning...
Done!


/home/wenkanw/.conda/envs/mlenv/lib/python3.8/site-packages/torch/utils/data/dataloader.py:474: UserWarning: This DataLoader will create 12 worker processes in total. Our suggested max number of worker in current system is 8, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(


Epoch: 01, Loss: 1.0684
Epoch: 02, Loss: 0.4532
Epoch: 03, Loss: 0.3874
Epoch: 04, Loss: 0.3552


Evaluating: 100%|██████████| 465930/465930 [00:42&lt;00:00, 10886.52it/s]

Epoch: 05, Loss: 0.3361, Train: 0.9568, Val: 0.9523, test: 0.9509





Epoch: 06, Loss: 0.3220
Epoch: 07, Loss: 0.3259
Epoch: 08, Loss: 0.3068
Epoch: 09, Loss: 0.2899


Evaluating: 100%|██████████| 465930/465930 [00:43&lt;00:00, 10825.21it/s]

Epoch: 10, Loss: 0.2844, Train: 0.9639, Val: 0.9517, test: 0.9523





Epoch: 11, Loss: 0.2850
Epoch: 12, Loss: 0.2700
Epoch: 13, Loss: 0.2705
Epoch: 14, Loss: 0.2696


Evaluating: 100%|██████████| 465930/465930 [00:43&lt;00:00, 10803.24it/s]

Epoch: 15, Loss: 0.2793, Train: 0.9637, Val: 0.9524, test: 0.9506





Epoch: 16, Loss: 0.2699
Epoch: 17, Loss: 0.2556
Epoch: 18, Loss: 0.2656
Epoch: 19, Loss: 0.2642


Evaluating: 100%|██████████| 465930/465930 [00:43&lt;00:00, 10797.29it/s]


Epoch: 20, Loss: 0.2491, Train: 0.9686, Val: 0.9551, test: 0.9537
Epoch: 21, Loss: 0.2450
Epoch: 22, Loss: 0.2449
Epoch: 23, Loss: 0.2456
Epoch: 24, Loss: 0.2491


Evaluating: 100%|██████████| 465930/465930 [00:43&lt;00:00, 10737.69it/s]


Epoch: 25, Loss: 0.2518, Train: 0.9572, Val: 0.9433, test: 0.9389
Epoch: 26, Loss: 0.2430
Epoch: 27, Loss: 0.2342
Epoch: 28, Loss: 0.2297
Epoch: 29, Loss: 0.2270


Evaluating: 100%|██████████| 465930/465930 [00:43&lt;00:00, 10824.00it/s]


Epoch: 30, Loss: 0.2319, Train: 0.9716, Val: 0.9514, test: 0.9517
</code></pre>
<h3 id="3-2-自己调整的代码"><a href="#3-2-自己调整的代码" class="headerlink" title="3.2 自己调整的代码"></a>3.2 自己调整的代码</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line">os.environ[<span class="string">"WITH_METIS"</span>] =<span class="string">"1"</span></span><br><span class="line">print(os.getenv(<span class="string">'WITH_METIS'</span>))</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> tqdm <span class="keyword">import</span> tqdm</span><br><span class="line"><span class="keyword">from</span> torch.nn <span class="keyword">import</span> ModuleList, functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">from</span> torch_geometric.nn <span class="keyword">import</span> SAGEConv</span><br><span class="line"><span class="keyword">from</span> torch_geometric.datasets <span class="keyword">import</span> Reddit</span><br><span class="line"><span class="keyword">from</span> torch_geometric.data <span class="keyword">import</span> ClusterData, ClusterLoader, NeighborSampler</span><br><span class="line"></span><br><span class="line">dataset = Reddit(<span class="string">'./data/Reddit'</span>)</span><br><span class="line">data = dataset[<span class="number">0</span>]</span><br><span class="line">print(dataset.num_classes)</span><br><span class="line">print(data.num_nodes)</span><br><span class="line">print(data.num_edges)</span><br><span class="line">print(data.num_features)</span><br></pre></td></tr></table></figure>

<pre><code>1
41
232965
114615892
602
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ClusterGCNNet</span><span class="params">(torch.nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, in_channels, out_channels,hidden_dim= <span class="number">128</span>,num_layers=<span class="number">4</span>)</span>:</span></span><br><span class="line">        super(ClusterGCNNet, self).__init__()</span><br><span class="line">        <span class="comment"># GraphSAGE layer</span></span><br><span class="line">        <span class="comment"># 这里参考了 原paper里面的4-layer的设定 + 128 hidden units</span></span><br><span class="line">        layer_ls=[SAGEConv(in_channels, hidden_dim)]</span><br><span class="line">        <span class="keyword">if</span> num_layers &lt;=<span class="number">2</span>:</span><br><span class="line">            layer_ls += [SAGEConv(hidden_dim, hidden_dim) <span class="keyword">for</span> i <span class="keyword">in</span> range(num_layers<span class="number">-2</span>)]</span><br><span class="line">        layer_ls.append(SAGEConv(hidden_dim, out_channels))</span><br><span class="line">        self.convs = ModuleList(layer_ls)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x, edge_index)</span>:</span></span><br><span class="line">        <span class="keyword">for</span> i, conv <span class="keyword">in</span> enumerate(self.convs):</span><br><span class="line">            x = conv(x, edge_index)</span><br><span class="line">            <span class="keyword">if</span> i != len(self.convs) - <span class="number">1</span>:</span><br><span class="line">                x = F.relu(x)</span><br><span class="line">                x = F.dropout(x, p=<span class="number">0.5</span>, training=self.training)</span><br><span class="line">        <span class="keyword">return</span> F.log_softmax(x, dim=<span class="number">-1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">inference</span><span class="params">(self, x_all, subgraph_loader,device)</span>:</span></span><br><span class="line">        pbar = tqdm(total=x_all.size(<span class="number">0</span>) * len(self.convs))</span><br><span class="line">        pbar.set_description(<span class="string">'Evaluating'</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Compute representations of nodes layer by layer, using *all*</span></span><br><span class="line">        <span class="comment"># available edges. This leads to faster computation in contrast to</span></span><br><span class="line">        <span class="comment"># immediately computing the final representations of each batch.</span></span><br><span class="line">        </span><br><span class="line">        </span><br><span class="line">        <span class="keyword">for</span> i, conv <span class="keyword">in</span> enumerate(self.convs):</span><br><span class="line">            xs = []</span><br><span class="line">            <span class="keyword">for</span> batch_size, n_id, adj <span class="keyword">in</span> subgraph_loader:</span><br><span class="line">                edge_index, _, size = adj.to(device)</span><br><span class="line">                x = x_all[n_id].to(device)</span><br><span class="line">                x_target = x[:size[<span class="number">1</span>]]</span><br><span class="line">                x = conv((x, x_target), edge_index)</span><br><span class="line">                <span class="keyword">if</span> i != len(self.convs) - <span class="number">1</span>:</span><br><span class="line">                    x = F.relu(x)</span><br><span class="line">                xs.append(x.cpu())</span><br><span class="line"></span><br><span class="line">                pbar.update(batch_size)</span><br><span class="line"></span><br><span class="line">            x_all = torch.cat(xs, dim=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">        pbar.close()</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> x_all</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train</span><span class="params">(model,optimizer)</span>:</span></span><br><span class="line">    model.train()</span><br><span class="line"></span><br><span class="line">    total_loss = total_nodes = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> batch <span class="keyword">in</span> train_loader:</span><br><span class="line">        batch = batch.to(device)</span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line">        out = model(batch.x, batch.edge_index)</span><br><span class="line">        loss = F.nll_loss(out[batch.train_mask], batch.y[batch.train_mask])</span><br><span class="line">        loss.backward()</span><br><span class="line">        optimizer.step()</span><br><span class="line"></span><br><span class="line">        nodes = batch.train_mask.sum().item()</span><br><span class="line">        total_loss += loss.item() * nodes</span><br><span class="line">        total_nodes += nodes</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> total_loss / total_nodes</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="meta">@torch.no_grad()</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">test</span><span class="params">(data, model)</span>:</span>  <span class="comment"># Inference should be performed on the full graph.</span></span><br><span class="line">    model.eval()</span><br><span class="line"></span><br><span class="line">    out = model.inference(data.x,subgraph_loader,device)</span><br><span class="line">    y_pred = out.argmax(dim=<span class="number">-1</span>)</span><br><span class="line"></span><br><span class="line">    accs = []</span><br><span class="line">    <span class="keyword">for</span> mask <span class="keyword">in</span> [data.train_mask, data.val_mask, data.test_mask]:</span><br><span class="line">        correct = y_pred[mask].eq(data.y[mask]).sum().item()</span><br><span class="line">        accs.append(correct / mask.sum().item())</span><br><span class="line">    <span class="keyword">return</span> accs</span><br></pre></td></tr></table></figure>



<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">from</span> torch.nn <span class="keyword">import</span> ModuleList</span><br><span class="line"><span class="keyword">from</span> tqdm <span class="keyword">import</span> tqdm</span><br><span class="line"><span class="keyword">from</span> torch_geometric.datasets <span class="keyword">import</span> Reddit</span><br><span class="line"><span class="keyword">from</span> torch_geometric.data <span class="keyword">import</span> ClusterData, ClusterLoader, NeighborSampler</span><br><span class="line"><span class="keyword">from</span> torch_geometric.nn <span class="keyword">import</span> SAGEConv</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">import</span> datetime</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Net</span><span class="params">(torch.nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, in_channels, out_channels)</span>:</span></span><br><span class="line">        super(Net, self).__init__()</span><br><span class="line">        self.convs = ModuleList(</span><br><span class="line">            [SAGEConv(in_channels, <span class="number">128</span>),</span><br><span class="line">             SAGEConv(<span class="number">128</span>, out_channels)])</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x, edge_index)</span>:</span></span><br><span class="line">        <span class="keyword">for</span> i, conv <span class="keyword">in</span> enumerate(self.convs):</span><br><span class="line">            x = conv(x, edge_index)</span><br><span class="line">            <span class="keyword">if</span> i != len(self.convs) - <span class="number">1</span>:</span><br><span class="line">                x = F.relu(x)</span><br><span class="line">                x = F.dropout(x, p=<span class="number">0.5</span>, training=self.training)</span><br><span class="line">        <span class="keyword">return</span> F.log_softmax(x, dim=<span class="number">-1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">inference</span><span class="params">(self, x_all)</span>:</span></span><br><span class="line">        pbar = tqdm(total=x_all.size(<span class="number">0</span>) * len(self.convs))</span><br><span class="line">        pbar.set_description(<span class="string">'Evaluating'</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Compute representations of nodes layer by layer, using *all*</span></span><br><span class="line">        <span class="comment"># available edges. This leads to faster computation in contrast to</span></span><br><span class="line">        <span class="comment"># immediately computing the final representations of each batch.</span></span><br><span class="line">        <span class="keyword">for</span> i, conv <span class="keyword">in</span> enumerate(self.convs):</span><br><span class="line">            xs = []</span><br><span class="line">            <span class="keyword">for</span> batch_size, n_id, adj <span class="keyword">in</span> subgraph_loader:</span><br><span class="line">                edge_index, _, size = adj.to(device)</span><br><span class="line">                x = x_all[n_id].to(device)</span><br><span class="line">                x_target = x[:size[<span class="number">1</span>]]</span><br><span class="line">                x = conv((x, x_target), edge_index)</span><br><span class="line">                <span class="keyword">if</span> i != len(self.convs) - <span class="number">1</span>:</span><br><span class="line">                    x = F.relu(x)</span><br><span class="line">                xs.append(x.cpu())</span><br><span class="line"></span><br><span class="line">                pbar.update(batch_size)</span><br><span class="line"></span><br><span class="line">            x_all = torch.cat(xs, dim=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">        pbar.close()</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> x_all</span><br><span class="line"></span><br><span class="line">    </span><br><span class="line">    </span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ClusterGCNNet</span><span class="params">(torch.nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, in_channels, out_channels,hidden_dim= <span class="number">128</span>,num_layers=<span class="number">4</span>)</span>:</span></span><br><span class="line">        super(ClusterGCNNet, self).__init__()</span><br><span class="line">        <span class="comment"># GraphSAGE layer</span></span><br><span class="line">        <span class="comment"># 这里参考了 原paper里面的4-layer的设定 + 128 hidden units</span></span><br><span class="line">        layer_ls=[SAGEConv(in_channels, hidden_dim)]</span><br><span class="line">        <span class="keyword">if</span> num_layers &lt;=<span class="number">2</span>:</span><br><span class="line">            layer_ls += [SAGEConv(hidden_dim, hidden_dim) <span class="keyword">for</span> i <span class="keyword">in</span> range(num_layers<span class="number">-2</span>)]</span><br><span class="line">        layer_ls.append(SAGEConv(hidden_dim, out_channels))</span><br><span class="line">        self.convs = ModuleList(layer_ls)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x, edge_index)</span>:</span></span><br><span class="line">        <span class="keyword">for</span> i, conv <span class="keyword">in</span> enumerate(self.convs):</span><br><span class="line">            x = conv(x, edge_index)</span><br><span class="line">            <span class="keyword">if</span> i != len(self.convs) - <span class="number">1</span>:</span><br><span class="line">                x = F.relu(x)</span><br><span class="line">                x = F.dropout(x, p=<span class="number">0.5</span>, training=self.training)</span><br><span class="line">        <span class="keyword">return</span> F.log_softmax(x, dim=<span class="number">-1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">inference</span><span class="params">(self, x_all)</span>:</span></span><br><span class="line">        pbar = tqdm(total=x_all.size(<span class="number">0</span>) * len(self.convs))</span><br><span class="line">        pbar.set_description(<span class="string">'Evaluating'</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Compute representations of nodes layer by layer, using *all*</span></span><br><span class="line">        <span class="comment"># available edges. This leads to faster computation in contrast to</span></span><br><span class="line">        <span class="comment"># immediately computing the final representations of each batch.</span></span><br><span class="line">        </span><br><span class="line">        </span><br><span class="line">        <span class="keyword">for</span> i, conv <span class="keyword">in</span> enumerate(self.convs):</span><br><span class="line">            xs = []</span><br><span class="line">            <span class="keyword">for</span> batch_size, n_id, adj <span class="keyword">in</span> subgraph_loader:</span><br><span class="line">                edge_index, _, size = adj.to(device)</span><br><span class="line">                x = x_all[n_id].to(device)</span><br><span class="line">                x_target = x[:size[<span class="number">1</span>]]</span><br><span class="line">                x = conv((x, x_target), edge_index)</span><br><span class="line">                <span class="keyword">if</span> i != len(self.convs) - <span class="number">1</span>:</span><br><span class="line">                    x = F.relu(x)</span><br><span class="line">                xs.append(x.cpu())</span><br><span class="line"></span><br><span class="line">                pbar.update(batch_size)</span><br><span class="line"></span><br><span class="line">            x_all = torch.cat(xs, dim=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">        pbar.close()</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> x_all</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train</span><span class="params">()</span>:</span></span><br><span class="line">    model.train()</span><br><span class="line"></span><br><span class="line">    total_loss = total_nodes = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> batch <span class="keyword">in</span> train_loader:</span><br><span class="line">        batch = batch.to(device)</span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line">        out = model(batch.x, batch.edge_index)</span><br><span class="line">        loss = F.nll_loss(out[batch.train_mask], batch.y[batch.train_mask])</span><br><span class="line">        loss.backward()</span><br><span class="line">        optimizer.step()</span><br><span class="line"></span><br><span class="line">        nodes = batch.train_mask.sum().item()</span><br><span class="line">        total_loss += loss.item() * nodes</span><br><span class="line">        total_nodes += nodes</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> total_loss / total_nodes</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="meta">@torch.no_grad()</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">test</span><span class="params">()</span>:</span>  <span class="comment"># Inference should be performed on the full graph.</span></span><br><span class="line">    model.eval()</span><br><span class="line"></span><br><span class="line">    out = model.inference(data.x)</span><br><span class="line">    y_pred = out.argmax(dim=<span class="number">-1</span>)</span><br><span class="line"></span><br><span class="line">    accs = []</span><br><span class="line">    <span class="keyword">for</span> mask <span class="keyword">in</span> [data.train_mask, data.val_mask, data.test_mask]:</span><br><span class="line">        correct = y_pred[mask].eq(data.y[mask]).sum().item()</span><br><span class="line">        accs.append(correct / mask.sum().item())</span><br><span class="line">    <span class="keyword">return</span> accs</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">dataset = Reddit(<span class="string">'./data/Reddit'</span>)</span><br><span class="line">data = dataset[<span class="number">0</span>]</span><br><span class="line">num_clusters = [<span class="number">500</span>, <span class="number">1000</span>,<span class="number">1500</span>, <span class="number">2000</span>]</span><br><span class="line">result = &#123;<span class="string">"num_cluster"</span>:[],<span class="string">"partition_t"</span>:[],<span class="string">"train_t"</span>:[],<span class="string">"train_acc"</span>:[] ,<span class="string">"val_acc"</span>:[],<span class="string">"test_acc"</span>:[]&#125;</span><br><span class="line"><span class="keyword">for</span> num_part <span class="keyword">in</span> num_clusters:</span><br><span class="line">    print(<span class="string">f"Using num cluster: <span class="subst">&#123;num_part&#125;</span>"</span>)</span><br><span class="line">    </span><br><span class="line">    start_t = time.time()</span><br><span class="line">    cluster_data = ClusterData(data, num_parts=num_part, recursive=<span class="literal">False</span>,</span><br><span class="line">                               save_dir=dataset.processed_dir)</span><br><span class="line">    end_t = time.time()</span><br><span class="line">    partition_t =end_t - start_t</span><br><span class="line">    print(<span class="string">f"Partition Time: <span class="subst">&#123;partition_t&#125;</span> s"</span>)</span><br><span class="line">    train_loader = ClusterLoader(cluster_data, batch_size=<span class="number">20</span>, shuffle=<span class="literal">True</span>,</span><br><span class="line">                                 num_workers=<span class="number">12</span>)</span><br><span class="line"></span><br><span class="line">    subgraph_loader = NeighborSampler(data.edge_index, sizes=[<span class="number">-1</span>], batch_size=<span class="number">1024</span>,</span><br><span class="line">                                      shuffle=<span class="literal">False</span>, num_workers=<span class="number">12</span>)</span><br><span class="line"></span><br><span class="line">    device = torch.device(<span class="string">'cuda'</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">'cpu'</span>)</span><br><span class="line">    model = ClusterGCNNet(dataset.num_features, dataset.num_classes,hidden_dim= <span class="number">128</span>,num_layers=<span class="number">2</span>).to(device)</span><br><span class="line">    optimizer = torch.optim.Adam(model.parameters(), lr=<span class="number">0.005</span>)</span><br><span class="line"></span><br><span class="line">    start_t = time.time()</span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> range(<span class="number">1</span>, <span class="number">31</span>):</span><br><span class="line">        loss = train()</span><br><span class="line">        <span class="keyword">if</span> epoch % <span class="number">5</span> == <span class="number">0</span>:</span><br><span class="line">            train_acc, val_acc, test_acc = test()</span><br><span class="line">            print(<span class="string">f'Epoch: <span class="subst">&#123;epoch:<span class="number">02</span>d&#125;</span>, Loss: <span class="subst">&#123;loss:<span class="number">.4</span>f&#125;</span>, Train: <span class="subst">&#123;train_acc:<span class="number">.4</span>f&#125;</span>, '</span></span><br><span class="line">                  <span class="string">f'Val: <span class="subst">&#123;val_acc:<span class="number">.4</span>f&#125;</span>, test: <span class="subst">&#123;test_acc:<span class="number">.4</span>f&#125;</span>'</span>)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            print(<span class="string">f'Epoch: <span class="subst">&#123;epoch:<span class="number">02</span>d&#125;</span>, Loss: <span class="subst">&#123;loss:<span class="number">.4</span>f&#125;</span>'</span>)</span><br><span class="line">    end_t = time.time()</span><br><span class="line">    train_t =end_t - start_t</span><br><span class="line">    train_t = datetime.timedelta(seconds=train_t)</span><br><span class="line">    print(<span class="string">f"Training Time: <span class="subst">&#123;train_t&#125;</span> s"</span>)</span><br><span class="line">    result[<span class="string">"num_cluster"</span>].append(num_part)</span><br><span class="line">    result[<span class="string">'partition_t'</span>].append(partition_t)</span><br><span class="line">    result[<span class="string">'train_t'</span>].append(train_t)</span><br><span class="line">    result[<span class="string">"train_acc"</span>].append(train_acc) </span><br><span class="line">    result[<span class="string">"val_acc"</span>].append(val_acc)</span><br><span class="line">    result[<span class="string">"test_acc"</span>].append(test_acc)</span><br><span class="line">    </span><br><span class="line">df_result= pd.DataFrame(result)</span><br><span class="line">df_result</span><br></pre></td></tr></table></figure>

<pre><code>Using num cluster: 500
Partition Time: 1.7125461101531982 s

Epoch: 01, Loss: 1.8346
Epoch: 02, Loss: 0.6928
Epoch: 03, Loss: 0.4838
Epoch: 04, Loss: 0.4009


Evaluating: 100%|██████████| 465930/465930 [00:57&lt;00:00, 8127.96it/s] 

Epoch: 05, Loss: 0.3632, Train: 0.9541, Val: 0.9542, test: 0.9531

Epoch: 06, Loss: 0.3425
Epoch: 07, Loss: 0.3161
Epoch: 08, Loss: 0.3140
Epoch: 09, Loss: 0.3006

Evaluating: 100%|██████████| 465930/465930 [00:56&lt;00:00, 8212.24it/s] 

Epoch: 10, Loss: 0.2787, Train: 0.9637, Val: 0.9586, test: 0.9571

Epoch: 11, Loss: 0.2684
Epoch: 12, Loss: 0.2558
Epoch: 13, Loss: 0.2522
Epoch: 14, Loss: 0.2455


Evaluating: 100%|██████████| 465930/465930 [01:00&lt;00:00, 7730.12it/s] 

Epoch: 15, Loss: 0.2421, Train: 0.9667, Val: 0.9577, test: 0.9565

Epoch: 16, Loss: 0.2649
Epoch: 17, Loss: 0.2377
Epoch: 18, Loss: 0.2277
Epoch: 19, Loss: 0.2190

Evaluating: 100%|██████████| 465930/465930 [00:54&lt;00:00, 8495.90it/s] 

Epoch: 20, Loss: 0.2151, Train: 0.9698, Val: 0.9568, test: 0.9559

Epoch: 21, Loss: 0.2138
Epoch: 22, Loss: 0.2101
Epoch: 23, Loss: 0.2084
Epoch: 24, Loss: 0.2062

Evaluating: 100%|██████████| 465930/465930 [00:58&lt;00:00, 7914.75it/s] 

Epoch: 25, Loss: 0.2057, Train: 0.9713, Val: 0.9558, test: 0.9545

Epoch: 26, Loss: 0.2061
Epoch: 27, Loss: 0.2097
Epoch: 28, Loss: 0.2099
Epoch: 29, Loss: 0.2035

Evaluating: 100%|██████████| 465930/465930 [00:59&lt;00:00, 7891.56it/s] 

Epoch: 30, Loss: 0.1938, Train: 0.9736, Val: 0.9567, test: 0.9560
Training Time: 0:09:43.498993 s
Using num cluster: 1000
Partition Time: 3.144443988800049 s
Epoch: 01, Loss: 1.4359
Epoch: 02, Loss: 0.5340
Epoch: 03, Loss: 0.4172
Epoch: 04, Loss: 0.3630

Evaluating: 100%|██████████| 465930/465930 [00:58&lt;00:00, 7984.94it/s] 

Epoch: 05, Loss: 0.3458, Train: 0.9358, Val: 0.9350, test: 0.9327

Epoch: 06, Loss: 0.3326
Epoch: 07, Loss: 0.3068
Epoch: 08, Loss: 0.2879
Epoch: 09, Loss: 0.2861

Evaluating: 100%|██████████| 465930/465930 [00:59&lt;00:00, 7885.83it/s] 

Epoch: 10, Loss: 0.2757, Train: 0.9618, Val: 0.9536, test: 0.9506
Epoch: 11, Loss: 0.2700
Epoch: 12, Loss: 0.2535
Epoch: 13, Loss: 0.2523
Epoch: 14, Loss: 0.2461

Evaluating: 100%|██████████| 465930/465930 [00:59&lt;00:00, 7813.02it/s] 

Epoch: 15, Loss: 0.2412, Train: 0.9688, Val: 0.9568, test: 0.9548

Epoch: 16, Loss: 0.2470
Epoch: 17, Loss: 0.2456
Epoch: 18, Loss: 0.2403
Epoch: 19, Loss: 0.2296

Evaluating: 100%|██████████| 465930/465930 [00:54&lt;00:00, 8482.81it/s] 

Epoch: 20, Loss: 0.2284, Train: 0.9696, Val: 0.9546, test: 0.9545

Epoch: 21, Loss: 0.2276
Epoch: 22, Loss: 0.2219
Epoch: 23, Loss: 0.2224
Epoch: 24, Loss: 0.2233

Evaluating: 100%|██████████| 465930/465930 [00:57&lt;00:00, 8037.32it/s] 

Epoch: 25, Loss: 0.2241, Train: 0.9698, Val: 0.9533, test: 0.9520

Epoch: 26, Loss: 0.2226
Epoch: 27, Loss: 0.2129
Epoch: 28, Loss: 0.2171
Epoch: 29, Loss: 0.2312

Evaluating: 100%|██████████| 465930/465930 [00:58&lt;00:00, 7972.01it/s] 

Epoch: 30, Loss: 0.2149, Train: 0.9739, Val: 0.9559, test: 0.9539
Training Time: 0:09:46.712217 s
Using num cluster: 1500
Partition Time: 1.5299558639526367 s
Epoch: 01, Loss: 1.1529
Epoch: 02, Loss: 0.4863
Epoch: 03, Loss: 0.3942
Epoch: 04, Loss: 0.3567

Evaluating: 100%|██████████| 465930/465930 [01:00&lt;00:00, 7716.31it/s] 

Epoch: 05, Loss: 0.3439, Train: 0.9559, Val: 0.9524, test: 0.9513

Epoch: 06, Loss: 0.3230
Epoch: 07, Loss: 0.3062
Epoch: 08, Loss: 0.3013
Epoch: 09, Loss: 0.3049

Evaluating: 100%|██████████| 465930/465930 [01:00&lt;00:00, 7741.65it/s] 

Epoch: 10, Loss: 0.2984, Train: 0.9609, Val: 0.9518, test: 0.9501

Epoch: 11, Loss: 0.2839
Epoch: 12, Loss: 0.2775
Epoch: 13, Loss: 0.2720
Epoch: 14, Loss: 0.2701

Evaluating: 100%|██████████| 465930/465930 [01:01&lt;00:00, 7567.86it/s] 

Epoch: 15, Loss: 0.2634, Train: 0.9633, Val: 0.9513, test: 0.9495

Epoch: 16, Loss: 0.2851
Epoch: 17, Loss: 0.2721
Epoch: 18, Loss: 0.2635
Epoch: 19, Loss: 0.2489

Evaluating: 100%|██████████| 465930/465930 [01:00&lt;00:00, 7680.40it/s] 

Epoch: 20, Loss: 0.2617, Train: 0.9645, Val: 0.9495, test: 0.9494

Epoch: 21, Loss: 0.2517
Epoch: 22, Loss: 0.2424
Epoch: 23, Loss: 0.2411
Epoch: 24, Loss: 0.2370

Evaluating: 100%|██████████| 465930/465930 [01:00&lt;00:00, 7762.74it/s] 

Epoch: 25, Loss: 0.2379, Train: 0.9702, Val: 0.9521, test: 0.9517

Epoch: 26, Loss: 0.2414
Epoch: 27, Loss: 0.2358
Epoch: 28, Loss: 0.2325
Epoch: 29, Loss: 0.2406

Evaluating: 100%|██████████| 465930/465930 [01:00&lt;00:00, 7753.34it/s] 

Epoch: 30, Loss: 0.2327, Train: 0.9633, Val: 0.9450, test: 0.9433
Training Time: 0:10:01.358439 s
Using num cluster: 2000
Computing METIS partitioning...
Done!
Partition Time: 298.0074031352997 s
</code></pre>
<br>


<h3 id="3-3-Result"><a href="#3-3-Result" class="headerlink" title="3.3 Result"></a>3.3 Result</h3><p>这里因为训练时我尝试了不同的cluster的数目，但是都是试了3种不同cluster数目之后就内存溢出，所以这里我尝试跑了2次，分别对比500,1000,1500以及1000,1500,2000两种情况。可以看到随着cluster数目的增多 test accuracy的的变化是先大后小，而且变化的幅度不大一般都在1%左右</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">df_result = pd.DataFrame(result)</span><br><span class="line">df_result</span><br></pre></td></tr></table></figure>



<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

<pre><code>.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
</code></pre>
<p></style></p>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>num_cluster</th>
      <th>partition_t</th>
      <th>train_t</th>
      <th>train_acc</th>
      <th>val_acc</th>
      <th>test_acc</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>500</td>
      <td>1.712546</td>
      <td>0 days 00:09:43.498993</td>
      <td>0.973591</td>
      <td>0.956695</td>
      <td>0.955999</td>
    </tr>
    <tr>
      <th>1</th>
      <td>1000</td>
      <td>3.144444</td>
      <td>0 days 00:09:46.712217</td>
      <td>0.973949</td>
      <td>0.955898</td>
      <td>0.953916</td>
    </tr>
    <tr>
      <th>2</th>
      <td>1500</td>
      <td>1.529956</td>
      <td>0 days 00:10:01.358439</td>
      <td>0.963299</td>
      <td>0.945030</td>
      <td>0.943306</td>
    </tr>
  </tbody>
</table>
</div>




<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pd.DataFrame(result)</span><br></pre></td></tr></table></figure>




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

<pre><code>.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
</code></pre>
<p></style></p>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>num_cluster</th>
      <th>partition_t</th>
      <th>train_t</th>
      <th>train_acc</th>
      <th>val_acc</th>
      <th>test_acc</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>2000</td>
      <td>2.519914</td>
      <td>0 days 00:04:56.476637</td>
      <td>0.966519</td>
      <td>0.948219</td>
      <td>0.947866</td>
    </tr>
    <tr>
      <th>1</th>
      <td>1500</td>
      <td>3.753822</td>
      <td>0 days 00:06:28.267962</td>
      <td>0.971231</td>
      <td>0.953674</td>
      <td>0.952803</td>
    </tr>
    <tr>
      <th>2</th>
      <td>1000</td>
      <td>2.336999</td>
      <td>0 days 00:07:26.611871</td>
      <td>0.971479</td>
      <td>0.951534</td>
      <td>0.950075</td>
    </tr>
  </tbody>
</table>
</div>




<h2 id="4-Conclusion-and-Take-away"><a href="#4-Conclusion-and-Take-away" class="headerlink" title="4. Conclusion and Take-away"></a>4. Conclusion and Take-away</h2><ul>
<li><p>ClusterGCN的成果</p>
<ul>
<li>对不同的batch，graph partition的方法进行研究</li>
<li>通过batch的设计和Clustering partition(METIS) 对GCN算法的Time, Space Complexity 在大型图数据里面有很大的提升(解决了neighborhood expansion problem问题)</li>
<li>相对于VR-GCN， 训练时间随着DNN 层数变多而增加的幅度不大。 Cluster-GCN的训练时间随着层数增多几乎是线性的。</li>
<li>能够用于训练大型embedding的特征</li>
</ul>
</li>
<li><p>Note</p>
<ul>
<li>Vanilla 在CS里面的含义<br> vanilla is the term used to refer when computer software and sometimes also other computing-related systems like computer hardware or algorithms are not customized from their original form</li>
</ul>
</li>
</ul>
<h2 id="5-Reference"><a href="#5-Reference" class="headerlink" title="5. Reference"></a>5. Reference</h2><p>[1] Datawhale: <a href="https://github.com/datawhalechina/team-learning-nlp/blob/master/GNN/Markdown%E7%89%88%E6%9C%AC/7-%E8%B6%85%E5%A4%A7%E5%9B%BE%E4%B8%8A%E7%9A%84%E8%8A%82%E7%82%B9%E8%A1%A8%E5%BE%81%E5%AD%A6%E4%B9%A0.md">https://github.com/datawhalechina/team-learning-nlp/blob/master/GNN/Markdown%E7%89%88%E6%9C%AC/7-%E8%B6%85%E5%A4%A7%E5%9B%BE%E4%B8%8A%E7%9A%84%E8%8A%82%E7%82%B9%E8%A1%A8%E5%BE%81%E5%AD%A6%E4%B9%A0.md</a><br>[2] Cluster-GCN 原文: <a href="https://arxiv.org/pdf/1905.07953.pdf">https://arxiv.org/pdf/1905.07953.pdf</a></p>
<p>[3] torch_geometric source code 参考: <a href="https://github.com/rusty1s/pytorch_geometric/blob/master/examples/cluster_gcn_reddit.py">https://github.com/rusty1s/pytorch_geometric/blob/master/examples/cluster_gcn_reddit.py</a></p>
</div><div class="article-tags size-small mb-4"><span class="mr-2">#</span><a class="link-muted mr-2" rel="tag" href="/tags/GNN/">GNN</a><a class="link-muted mr-2" rel="tag" href="/tags/Graph/">Graph</a></div><div class="sharethis-inline-share-buttons"></div><script src="https://ppoffice.github.io/hexo-theme-icarus-categorites-Plugins/Share/" defer></script></article></div><!--!--><nav class="post-navigation mt-4 level is-mobile"><div class="level-start"><a class="article-nav-prev level level-item link-muted" href="/2021/07/04/GNN-6-GIN-GraphRepresentation/"><i class="level-item fas fa-chevron-left"></i><span class="level-item">GNN-6-GIN-GraphRepresentation</span></a></div><div class="level-end"><a class="article-nav-next level level-item link-muted" href="/2021/06/27/GNN-4-EdgePrediction/"><span class="level-item">GNN-4-EdgePrediction</span><i class="level-item fas fa-chevron-right"></i></a></div></nav><div class="card"><div class="card-content"><h3 class="title is-5">Comments</h3><div class="content" id="valine-thread"></div><script src="//cdn1.lncld.net/static/js/3.0.4/av-min.js"></script><script src="https://cdn.jsdelivr.net/npm/valine@1.4.14/dist/Valine.min.js"></script><script>new Valine({
            el: '#valine-thread' ,
            appId: "kvXCKmDNxnA2486N29e5cP7i-MdYXbMMI",
            appKey: "0Lv5b0SqotzkGHQvD64u4AKo",
            
            avatar: "mm",
            
            meta: ["nick","mail","link"],
            pageSize: 10,
            lang: "en",
            
            highlight: true,
            
            
            
            
            
            requiredFields: [],
        });</script></div></div></div><div class="column column-left is-4-tablet is-4-desktop is-3-widescreen  order-1"><div class="card widget"><div class="card-content"><div class="menu"><h3 class="menu-label">Links</h3><ul class="menu-list"><li><a class="level is-mobile is-mobile" href="https://github.com/wenkangwei" target="_blank" rel="noopener"><span class="level-left"><span class="level-item">Github</span></span><span class="level-right"><span class="level-item tag">github.com</span></span></a></li></ul></div></div></div><div class="card widget"><div class="card-content"><div class="menu"><h3 class="menu-label">Categories</h3><ul class="menu-list"><li><a class="level is-mobile is-marginless" href="/categories/Data-Collection/"><span class="level-start"><span class="level-item">Data Collection</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile is-marginless" href="/categories/Data-Structure/"><span class="level-start"><span class="level-item">Data Structure</span></span><span class="level-end"><span class="level-item tag">2</span></span></a><ul class="mr-0"><li><a class="level is-mobile is-marginless" href="/categories/Data-Structure/Sorting/"><span class="level-start"><span class="level-item">Sorting</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></li><li><a class="level is-mobile is-marginless" href="/categories/Deep-Learning/"><span class="level-start"><span class="level-item">Deep Learning</span></span><span class="level-end"><span class="level-item tag">10</span></span></a></li><li><a class="level is-mobile is-marginless" href="/categories/Linux/"><span class="level-start"><span class="level-item">Linux</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile is-marginless" href="/categories/Machine-Learning/"><span class="level-start"><span class="level-item">Machine Learning</span></span><span class="level-end"><span class="level-item tag">7</span></span></a><ul class="mr-0"><li><a class="level is-mobile is-marginless" href="/categories/Machine-Learning/Accuracy-Improvement/"><span class="level-start"><span class="level-item">Accuracy Improvement</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></li><li><a class="level is-mobile is-marginless" href="/categories/NLP/"><span class="level-start"><span class="level-item">NLP</span></span><span class="level-end"><span class="level-item tag">2</span></span></a><ul class="mr-0"><li><a class="level is-mobile is-marginless" href="/categories/NLP/Machine-Learning/"><span class="level-start"><span class="level-item">Machine Learning</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li></ul></li><li><a class="level is-mobile is-marginless" href="/categories/Parallel-Computing/"><span class="level-start"><span class="level-item">Parallel Computing</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile is-marginless" href="/categories/Programming/"><span class="level-start"><span class="level-item">Programming</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile is-marginless" href="/categories/PySpark/"><span class="level-start"><span class="level-item">PySpark</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile is-marginless" href="/categories/Recommendation-System/"><span class="level-start"><span class="level-item">Recommendation System</span></span><span class="level-end"><span class="level-item tag">7</span></span></a></li><li><a class="level is-mobile is-marginless" href="/categories/Report/"><span class="level-start"><span class="level-item">Report</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile is-marginless" href="/categories/Searching/"><span class="level-start"><span class="level-item">Searching</span></span><span class="level-end"><span class="level-item tag">1</span></span></a><ul class="mr-0"><li><a class="level is-mobile is-marginless" href="/categories/Searching/Data-Strucure/"><span class="level-start"><span class="level-item">Data Strucure</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></li><li><a class="level is-mobile is-marginless" href="/categories/Statistic/"><span class="level-start"><span class="level-item">Statistic</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile is-marginless" href="/categories/Web/"><span class="level-start"><span class="level-item">Web</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></div></div></div><div class="card widget"><div class="card-content"><div class="menu"><h3 class="menu-label">Tags</h3><div class="field is-grouped is-grouped-multiline"><div class="control"><a class="tags has-addons" href="/tags/Amdahl-s-Law/"><span class="tag">Amdahl&#039;s Law</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Backpropagation/"><span class="tag">Backpropagation</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Bagging/"><span class="tag">Bagging</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/BeautifulSoup/"><span class="tag">BeautifulSoup</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Binary-Tree/"><span class="tag">Binary Tree</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Blending/"><span class="tag">Blending</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Boosting/"><span class="tag">Boosting</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Boosting-Machine/"><span class="tag">Boosting Machine</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/BuckSort/"><span class="tag">BuckSort</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/C/"><span class="tag">C++</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Collaborative-Filtering/"><span class="tag">Collaborative Filtering</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Confusion-Metric/"><span class="tag">Confusion Metric</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Convolution-Neural-Network/"><span class="tag">Convolution Neural Network</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Cross-Validation/"><span class="tag">Cross Validation</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/D3-js/"><span class="tag">D3.js</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/DCN/"><span class="tag">DCN</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/DIN/"><span class="tag">DIN</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Data-Analysis/"><span class="tag">Data Analysis</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Data-Mining/"><span class="tag">Data Mining</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Datawhale-Team-Learning/"><span class="tag">Datawhale Team Learning</span><span class="tag is-grey-lightest">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/DeepFM/"><span class="tag">DeepFM</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Dropout/"><span class="tag">Dropout</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Ensemble-Learning/"><span class="tag">Ensemble Learning</span><span class="tag is-grey-lightest">5</span></a></div><div class="control"><a class="tags has-addons" href="/tags/GNN/"><span class="tag">GNN</span><span class="tag is-grey-lightest">8</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Graph/"><span class="tag">Graph</span><span class="tag is-grey-lightest">8</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Hexo/"><span class="tag">Hexo</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Holdout/"><span class="tag">Holdout</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Hypothesis-Test/"><span class="tag">Hypothesis Test</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Icarus/"><span class="tag">Icarus</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Independence-Test/"><span class="tag">Independence Test</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/JavaScript/"><span class="tag">JavaScript</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/K-Mean-Clustering/"><span class="tag">K-Mean Clustering</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/KMean-Clustering/"><span class="tag">KMean Clustering</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/LGBM/"><span class="tag">LGBM</span><span class="tag is-grey-lightest">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Machine-Learning/"><span class="tag">Machine Learning</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Markdown/"><span class="tag">Markdown</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Model-Evaluation/"><span class="tag">Model Evaluation</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Model-Selection/"><span class="tag">Model Selection</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Natural-Language-Representations/"><span class="tag">Natural Language Representations</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Nature-Language-Processing/"><span class="tag">Nature Language Processing</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Negative-Sampling/"><span class="tag">Negative Sampling</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/NeuralFM/"><span class="tag">NeuralFM</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/P-value/"><span class="tag">P-value</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Parallel-Computing/"><span class="tag">Parallel Computing</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Parallel-Speedup/"><span class="tag">Parallel Speedup</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/PySpark/"><span class="tag">PySpark</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/ROC-curve/"><span class="tag">ROC curve</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Recommendation-System/"><span class="tag">Recommendation System</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Regression/"><span class="tag">Regression</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Regular-Expression/"><span class="tag">Regular Expression</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Sorting/"><span class="tag">Sorting</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Stacking/"><span class="tag">Stacking</span><span class="tag is-grey-lightest">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Traversal/"><span class="tag">Traversal</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Unsupervised-Learning/"><span class="tag">Unsupervised Learning</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Variable-Selection/"><span class="tag">Variable Selection</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Web-Scrapping/"><span class="tag">Web Scrapping</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Wide-and-Deep/"><span class="tag">Wide and Deep</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Wide-and-Deep-Model/"><span class="tag">Wide and Deep Model</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Wide-Deep-Model/"><span class="tag">Wide&amp;Deep Model</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Word-Embedding/"><span class="tag">Word Embedding</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Word-vector/"><span class="tag">Word vector</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/binary-search/"><span class="tag">binary search</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/bubble-sort/"><span class="tag">bubble sort</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/commands/"><span class="tag">commands</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/insertion-sort/"><span class="tag">insertion sort</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/merge-sort/"><span class="tag">merge sort</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/non-parametric-learning/"><span class="tag">non-parametric learning</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/quick-sort/"><span class="tag">quick sort</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/review/"><span class="tag">review</span><span class="tag is-grey-lightest">1</span></a></div></div></div></div></div><div class="card widget"><div class="card-content"><h3 class="menu-label">Recent</h3><article class="media"><div class="media-content size-small"><p><time dateTime="2021-07-13T01:43:40.000Z">2021-07-12</time></p><p class="title is-6"><a class="link-muted" href="/2021/07/12/Java-1-Installation/">Java-1-Setup</a></p><p class="is-uppercase"></p></div></article><article class="media"><div class="media-content size-small"><p><time dateTime="2021-07-09T19:23:13.000Z">2021-07-09</time></p><p class="title is-6"><a class="link-muted" href="/2021/07/09/GNN-8-Conclusion/">GNN-8-Conclusion</a></p><p class="is-uppercase"><a class="link-muted" href="/categories/Deep-Learning/">Deep Learning</a></p></div></article><article class="media"><div class="media-content size-small"><p><time dateTime="2021-07-09T06:11:53.000Z">2021-07-09</time></p><p class="title is-6"><a class="link-muted" href="/2021/07/09/GNN-7-Mini-Batching-Practice/">GNN-7-Mini-Batching-Practice</a></p><p class="is-uppercase"><a class="link-muted" href="/categories/Deep-Learning/">Deep Learning</a></p></div></article><article class="media"><div class="media-content size-small"><p><time dateTime="2021-07-04T22:08:09.000Z">2021-07-04</time></p><p class="title is-6"><a class="link-muted" href="/2021/07/04/GNN-6-GIN-GraphRepresentation/">GNN-6-GIN-GraphRepresentation</a></p><p class="is-uppercase"><a class="link-muted" href="/categories/Deep-Learning/">Deep Learning</a></p></div></article><article class="media"><div class="media-content size-small"><p><time dateTime="2021-07-01T06:18:47.000Z">2021-07-01</time></p><p class="title is-6"><a class="link-muted" href="/2021/07/01/GNN-5-ClusterGCN/">GNN-5-ClusterGCN</a></p><p class="is-uppercase"><a class="link-muted" href="/categories/Deep-Learning/">Deep Learning</a></p></div></article></div></div><div class="card widget"><div class="card-content"><div class="menu"><h3 class="menu-label">Archives</h3><ul class="menu-list"><li><a class="level is-mobile is-marginless" href="/archives/2021/07/"><span class="level-start"><span class="level-item">July 2021</span></span><span class="level-end"><span class="level-item tag">5</span></span></a></li><li><a class="level is-mobile is-marginless" href="/archives/2021/06/"><span class="level-start"><span class="level-item">June 2021</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li><li><a class="level is-mobile is-marginless" href="/archives/2021/05/"><span class="level-start"><span class="level-item">May 2021</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li><li><a class="level is-mobile is-marginless" href="/archives/2021/03/"><span class="level-start"><span class="level-item">March 2021</span></span><span class="level-end"><span class="level-item tag">5</span></span></a></li><li><a class="level is-mobile is-marginless" href="/archives/2021/02/"><span class="level-start"><span class="level-item">February 2021</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile is-marginless" href="/archives/2020/12/"><span class="level-start"><span class="level-item">December 2020</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile is-marginless" href="/archives/2020/11/"><span class="level-start"><span class="level-item">November 2020</span></span><span class="level-end"><span class="level-item tag">5</span></span></a></li><li><a class="level is-mobile is-marginless" href="/archives/2020/10/"><span class="level-start"><span class="level-item">October 2020</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile is-marginless" href="/archives/2020/09/"><span class="level-start"><span class="level-item">September 2020</span></span><span class="level-end"><span class="level-item tag">5</span></span></a></li><li><a class="level is-mobile is-marginless" href="/archives/2020/08/"><span class="level-start"><span class="level-item">August 2020</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile is-marginless" href="/archives/2020/07/"><span class="level-start"><span class="level-item">July 2020</span></span><span class="level-end"><span class="level-item tag">7</span></span></a></li><li><a class="level is-mobile is-marginless" href="/archives/2020/06/"><span class="level-start"><span class="level-item">June 2020</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li></ul></div></div></div><div class="card widget"><div class="card-content"><div class="menu"><h3 class="menu-label">Subscribe to Updates</h3><form action="https://feedburner.google.com/fb/a/mailverify" method="post" target="popupwindow" onsubmit="window.open(&#039;https://feedburner.google.com/fb/a/mailverify?uri=&#039;,&#039;popupwindow&#039;,&#039;scrollbars=yes,width=550,height=520&#039;);return true"><input type="hidden" value="" name="uri"><input type="hidden" name="loc" value="en_US"><div class="field has-addons"><div class="control has-icons-left is-expanded"><input class="input" name="email" type="email" placeholder="Email"><span class="icon is-small is-left"><i class="fas fa-envelope"></i></span></div><div class="control"><input class="button is-primary" type="submit" value="Subscribe"></div></div></form></div></div></div><div class="column-right-shadow is-hidden-widescreen"></div></div><div class="column column-right is-4-tablet is-4-desktop is-3-widescreen is-hidden-touch is-hidden-desktop-only order-3"><div class="card widget"><div class="card-content"><nav class="level"><div class="level-item has-text-centered flex-shrink-1"><div><figure class="image is-128x128 mx-auto mb-2"><img class="avatar is-rounded" src="/images/avatar.jpg" alt="Wenkang Wei"></figure><p class="title is-size-4 is-block line-height-inherit">Wenkang Wei</p><p class="is-size-6 is-block">computer engineering| machine learning</p><p class="is-size-6 is-flex justify-content-center"><i class="fas fa-map-marker-alt mr-1"></i><span>Clemson,SC</span></p></div></div></nav><nav class="level is-mobile"><div class="level-item has-text-centered is-marginless"><div><p class="heading">Posts</p><a href="/archives"><p class="title">44</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">Categories</p><a href="/categories"><p class="title">18</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">Tags</p><a href="/tags"><p class="title">69</p></a></div></div></nav><div class="level"><a class="level-item button is-primary is-rounded" href="https://github.com/wenkangwei" target="_blank" rel="noopener">Follow</a></div><div class="level is-mobile"><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Github" href="https://github.com/wenkangwei"><i class="fab fa-github"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="LinkedIn" href="https://www.linkedin.com/in/wenkang-wei-588811167"><i class="fab fa-linkedin"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Facebook" href="https://facebook.com"><i class="fab fa-facebook"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Twitter" href="https://twitter.com"><i class="fab fa-twitter"></i></a></div></div></div><div class="card widget" id="toc"><div class="card-content"><div class="menu"><h3 class="menu-label">Catalogue</h3><ul class="menu-list"><li><a class="is-flex" href="#GCN-Task-5-Cluster-GCN"><span class="mr-2">1</span><span>GCN-Task-5-Cluster GCN</span></a><ul class="menu-list"><li><a class="is-flex" href="#1-Introduction"><span class="mr-2">1.1</span><span>1. Introduction</span></a></li><li><a class="is-flex" href="#2-Cluster-GCN"><span class="mr-2">1.2</span><span>2. Cluster-GCN</span></a><ul class="menu-list"><li><a class="is-flex" href="#2-1-Problem-to-solve"><span class="mr-2">1.2.1</span><span>2.1 Problem to solve</span></a></li><li><a class="is-flex" href="#2-2-3-Cluster-GCN实现过程"><span class="mr-2">1.2.2</span><span>2.2.3 Cluster-GCN实现过程</span></a></li><li><a class="is-flex" href="#2-3-Properties"><span class="mr-2">1.2.3</span><span>2.3 Properties</span></a></li><li><a class="is-flex" href="#3-1-官方测试源码"><span class="mr-2">1.2.4</span><span>3.1 官方测试源码</span></a></li><li><a class="is-flex" href="#3-2-自己调整的代码"><span class="mr-2">1.2.5</span><span>3.2 自己调整的代码</span></a></li><li><a class="is-flex" href="#3-3-Result"><span class="mr-2">1.2.6</span><span>3.3 Result</span></a></li></ul></li><li><a class="is-flex" href="#4-Conclusion-and-Take-away"><span class="mr-2">1.3</span><span>4. Conclusion and Take-away</span></a></li><li><a class="is-flex" href="#5-Reference"><span class="mr-2">1.4</span><span>5. Reference</span></a></li></ul></li></ul></div></div></div></div></div></div></section><footer class="footer"><div class="container"><div class="level"><div class="level-start"><a class="footer-logo is-block mb-2" href="/"><img src="/images/icon.png" alt="Wenkang&#039;s Blog" height="28"></a><p class="size-small"><span>&copy; 2021 Wenkang Wei</span>  Powered by <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a> &amp; <a href="https://github.com/ppoffice/hexo-theme-icarus" target="_blank" rel="noopener">Icarus</a></p></div><div class="level-end"><div class="field has-addons"><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Creative Commons" href="https://creativecommons.org/"><i class="fab fa-creative-commons"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Attribution 4.0 International" href="https://creativecommons.org/licenses/by/4.0/"><i class="fab fa-creative-commons-by"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/ppoffice/hexo-theme-icarus"><i class="fab fa-github"></i></a></p></div></div></div></div></footer><script src="https://cdn.jsdelivr.net/npm/jquery@3.3.1/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/moment@2.22.2/min/moment-with-locales.min.js"></script><script>moment.locale("en");</script><script>var IcarusThemeSettings = {
            site: {
                url: 'https://github.com/wenkangwei/wenkangwei.github.io`',
                external_link: {"enable":true,"exclude":[]}
            },
            article: {
                highlight: {
                    clipboard: true,
                    fold: 'unfolded'
                }
            }
        };</script><script src="https://cdn.jsdelivr.net/npm/clipboard@2.0.4/dist/clipboard.min.js" defer></script><script src="/js/animation.js"></script><a id="back-to-top" title="Back to Top" href="javascript:;"><i class="fas fa-chevron-up"></i></a><script src="/js/back_to_top.js" defer></script><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/lightgallery@1.6.8/dist/js/lightgallery.min.js" defer></script><script src="https://cdn.jsdelivr.net/npm/justifiedGallery@3.7.0/dist/js/jquery.justifiedGallery.min.js" defer></script><script>window.addEventListener("load", () => {
            if (typeof $.fn.lightGallery === 'function') {
                $('.article').lightGallery({ selector: '.gallery-item' });
            }
            if (typeof $.fn.justifiedGallery === 'function') {
                if ($('.justified-gallery > p > .gallery-item').length) {
                    $('.justified-gallery > p > .gallery-item').unwrap();
                }
                $('.justified-gallery').justifiedGallery();
            }
        });</script><!--!--><!--!--><script type="text/x-mathjax-config">MathJax.Hub.Config({
            'HTML-CSS': {
                matchFontHeight: false
            },
            SVG: {
                matchFontHeight: false
            },
            CommonHTML: {
                matchFontHeight: false
            },
            tex2jax: {
                inlineMath: [
                    ['$','$'],
                    ['\\(','\\)']
                ]
            }
        });</script><script src="https://cdn.jsdelivr.net/npm/mathjax@2.7.5/unpacked/MathJax.js?config=TeX-MML-AM_CHTML" defer></script><!--!--><script src="/js/main.js" defer></script><div class="searchbox"><div class="searchbox-container"><div class="searchbox-header"><div class="searchbox-input-container"><input class="searchbox-input" type="text" placeholder="Type something..."></div><a class="searchbox-close" href="javascript:;">×</a></div><div class="searchbox-body"></div></div></div><script src="/js/insight.js" defer></script><script>document.addEventListener('DOMContentLoaded', function () {
            loadInsight({"contentUrl":"/content.json"}, {"hint":"Type something...","untitled":"(Untitled)","posts":"Posts","pages":"Pages","categories":"Categories","tags":"Tags"});
        });</script></body></html>