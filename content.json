{"pages":[{"title":"","text":"","link":"/index.html"},{"title":"Wenkang Wei","text":"Who am I? My ResumeMachine Learning Fan and Researcher - I’m a second-year master student of computer engineering with minor of Computer Science. So far, I’m doing researches about transfer learning and GAN. I’m working with Dr. Adam hoover on Human activity (Eating) pattern recognition research. I’m also a research assistant in Clemson-Clair Lab of Dr. Kai Liu with research topic of machine learning optimization. Job Seeker - I’m actively looking for a full-time job related to Data Scientist or Machine Learning Engineer that starts from Summer 2021 after my graduation in May 2021My Resume can be found Here or LinkedInYou are welcomed to email me if you have any suggestion or reference to me. blogger - I find that writing blogs can help me manage my tehcnical notes and problems I solved in the past. What’s more, it provides a platform for me to communicate techniques with more people.So, right now, I’m trying to manage my technical notes and my thoughts on some problems I wrote in the past. Geek - I usually find some technical projects to do once I’m leisure. Some projects are inspired by my daily life, my classes on campus and my researches. To know more about it, Click HERE Dreamer - I usually imagine how our brains work and ponder how AI will collaborate with human in the future and how can we borrow human learning behaviors or architectures of human brain to design a new AI algorithm. Hopefully, the secret behine human brain can be revealed one day. Amateur of paintingI’m a fan of japanese animes and hence sometimes do some paintings related to anime characters. Educational BackgroundI’m a master student of computer engineering with the focus area of intelligent system and pattern recogniton. I have received the BS degree with EE major and named to Dean’s List as well in Clemson Univeristy. If you are interested in my academic achievements, Click HERE Work ExperienceMachine Learning Research Assistant - Summer 2020-Current Proof of convergence and convergence rate of Multiple Update Algorithm (MUA) in Non-Negative Matrix Factorization Problem Formulated Matrix Factorization Problem into Constraint Optimization Problem Applied Linear Algebra, Lagrange multiplier to simplify problem and utilized Lipschitz gradient, convex optimization to prove the convergence and convergence rate of MUA algorithm Implemented MUA and ALS (alternative least square) algorithm in Google Colab and Matlab to verify convergence result Collaborated and communicated with CS professor Dr Kai Liu to present mathematic proof process orally Wrote a paper in AAAI format using Latex (unpublished due to copyright reason) Technical Skills Programming: Python/Jupyter Notebook PostgreSQL C/C++ Matlab HTML, Markdown, Latex Tools: Deep Learning Framework: PyTorch / Tensorflow Distributed Machine Learning: PySpark, Hadoop MapReduce,MPI Data Analysis toolkits: sklearn, pandas, seaborn, etc Platform: Raspberry Pi, Linux, Google Colab, Git, AWS (RDS, EC2) Theory and Analysis Techniques: Feature Engineering, Data Visualization and Preprocessing Techniques, PCA, NLP text processing: Word Embedding, TF-IDF, etc Machine Learning Modeling: Collaborate Filtering, Matrix Factorization, SVM, Decision Tree, Clustering, Convolution Neural Network etc Techniques for Model Evaluation and Improvement: Cross-Validation, Ensemble Learning, ROC, AUC, Feature Importance, etc. Selected ProjectsImage Classification Car Classification using Transfer Learning - Fall 2020 Constructed data pipeline by PyTorch to extracted and transformed Stanford car images dataset (1.96GB dataset with 196 classes) Modified and tuned pre-trained models Google-Net, VGG-16 , Res-Net 50 to fit car dataset using early stopping, weight decay techniques Improved test accuracy of the best model to 85% using cross-validation model selection techniques Recommendation System Recommendation System based on MovieLens 25M dataset ((PySpark, Hadoop, HDFS, SQL) Utilized PySpark to load movielens 25M dataset (25 million ratings) and used SQL to query and analyze data in databrick cluster platform Implemented and applied Mapper, Reducer functions in Hadoop File system to analyze contribution of different movie genres to ratings Applied Collaborative Filtering and Matrix Factorization methods to construct a recommendation system with PySpark Achieved 0.67 mean square error score and deployed recommendation system using IPython widget KKBox Music Recommendation System Utilized Exploratory Data Analysis (EDA) techniques and data visualization to analyze relationship between features Constructed Data pipeline to clean data by filling missing values, converting data type and transform data using OneHot encoding, Embedding, etc. Implemented and applied Light gradient boosting machine and wide and deep neural network for recommendation system Data Science and NLP Bank Churn Prediction Visualized and analyzed data related to customer churn by using visualization toolkits: seaborn, matplotlib Preprocessed and transforms categorical data for Machine Learning model training using pandas toolkit and normalization techniques Established Data Pipeline and ML Models: Random Forest, Logistic Regression, SVM, etc. and Evaluated Models using ROC,AUC Improved Models Accuracy from 80% to 86% by using Model Selection, Cross Validation and Feature Selection, L1 Regularization techniques Youtube Comments Analysis and Pet Owners Classification (PySpark, SQL, Databrick Cluster) Utilized PySpark and PostgreSQL to load, query and explore Youtube comment text data (about 1GB after decompression) Built data pipeline and applied Term-Frequency-Inverse Document-Frequency(TF-IDF) to transform text data into numerical data Applied Logistic Regression, Random Forest, Gradient Boosting machine in PySpark to classify cat or dog owners from comments Achieved 92% prediction accuracy on test set using grid search and cross validation Software Development Real-time Signal Visualization System (C++, Qt, GDB) Designed a visualization software system based on Qt toolkit, Arduino using C/C++ to solve the problem of visualizing voltage signal data in real time with self-motivation and initiative Designed GUI components and class modules for software interface in Qt and software framework to control data visualization behaviors in C++ using data structure (queue) and Object-Oriented Programming (OOP) techniques Integrated, tested and debugged GUI components with software framework using GDB toolkit and Qt IDE Wrote technical document for software system in Github with video demo. Link to demo: https://github.com/wenkangwei/SerialPlot More projects will be uploaded soon. They are mentioned in my resume or LinkedIn. Please feel free to contact me via email: wenkanw@g.clemson.edu or LinkedIn if you have any questions or any job opportunities for me. Thanks! My InterestsResearch Interests:Data mining, Recommendation System, NLP, machine learning,deep learning and their applications. Other Interests:Anime, painting, music, badminton,swimming… My Framework for solving problems / researching You are welcomed to contact me by wenkangwei917@gmail.com or by https://github.com/wenkangwei if you have any suggestion on my projects or my blog.","link":"/about/index.html"},{"title":"categories","text":"","link":"/categories/index.html"},{"title":"images","text":"","link":"/images/index.html"},{"title":"src","text":"","link":"/src/index.html"},{"title":"tags","text":"","link":"/tags/index.html"}],"posts":[{"title":"Cpp review 1","text":"Important Keywords const Type Meaning Explanation const int x constant variable Value of x must be initialized first const int *x constant content; mutable pointer; d constant content pointer x points to. address of pointer x is mutable. x can be initialized after defintion int * const x constant pointer; mutable content; pointer x must be initialized first; const int* const x Constant pointer; Constant content x must be initialized first const Class a constant class var content of member var of a can not be changed; can not call not-constant functions;(avoid changing member values) void func(const int x) const var as input function can not change the local variable; const int&amp; funct(x) return constant reference pointer using this reference can not change its content const int funct(x) return constant var return a constant variable int func() const constant member function Function is unable to change member values in the class const MyClass c constant Class variable variable c can not call non-const member function and c can not change member variable values define Macro preprocesing keyword. Different from const, #define doesn’t have data type Compiler replaces the words only without checking the variable and its memory. E.g #define A B. Replace A with B only, compiler doesn’t check the type of A. In C++ program, usually use const rather than define for security. sizeof(x) / sizeof x check number of bytes of variable based on data type sizeof class_variable = sum of bytes of all member variables in class in 64-bit machine, sizeof pointer = 8 bytes = 64 bits static static global var only visiable to current file data is stored in global memory, whereas local var is stored in stack and heap. Automatically initialized as 0 Eg: 12static int a;int main(){...} static local var It is initialized once stored in global memory without releasing memory until program terminates can be used in local code block only. static function visible to current file only can be defined in public, protected, private region. static class member: used for private class info, rather than instance info Different from common private member (which can be called and modified via this pointer), static class member DO NOT have this pointer Only private and protected member can define static. Public member can not define static.However, static function can be defined in public region as well. Can be called only by private/protected class function Initialization for static member inside class requires const definition Initialization for static member outside class requires that it can be initialized once. Initialization must be outside code block, like global variable Example code: 12345678910111213class myclass{ public: void func(){cout&lt;&lt;b;} // call private var inside member function private: static int a; //or static const int a =0; static int a=0;//wrong, a must be const static int b;}int myclass::b = 10; // way to initialize staic class member, like global var, but won't conflict global var.int main(){...} external Tell the compiler that there is an function, whose implement of function is in other file. inline inline function tell the compiler to embed the code of the function into every place where the function is called. So, inline function reduce the time used to expand the function by using more memory. Normal function in C/C++ is expanded by compiler only when they are call. This requires longer time to run program Inline function has higher memory complexity and lower time complexity than normal function Not suitable for construtor function (since it will generate lots of codes ) Not suitable for virtual function inline function is unable to use render/loop operation Note: Inline function doesn’t support loop and switch Usually, inline function is used when function has a few codes. (usually less than 10 lines) typedef and #define It is a sentence not command for compiler, so it requires “;” compared with #define Compiler check the grammar and type of typedef, but doesn’t check type of #define #define simply replaces the notation with new one, it doesn’t check the grammar or operation in definition Example: 123typdef struct A{...} B; //define A as type B#define A B // define / replace B with A#define func(x) x*x explicit Require constructor to run explictlyEg:1234567c = myclass(1);//orc= (myclass) 12; // convert type explictlymyclass c;c = 1; // Not allowed, need (myclass) 1 Using Declaration of “using” keyword: It indicates the class we use is from std library. Example: 1using std::vector; It indicates the class vector is from std Compile command: It tells the compiler to use the whole namespace (every element from this namespace) Example: 1using namespace std; Variable type global local static Operators Operator computes from right to left: &lt;condition-expression&gt; ? &lt;return if condition=__True__&gt; : &lt;return if condition =__False__&gt;It returns value from right to left = Operator Priority arithmetic operator: *, % , / are higher than +, - Relationship operator: &gt;, &lt; , &gt;=，&lt;= are higher than !=, == logic operator: ! higher than &amp;&amp;, &amp;&amp; higher than || logic bit operator: ~ higher than &amp;, &amp; higher than | Grammar a++ and ++a a++: return another local variable contain value of a. Then a=a+1 ++a: a=a+1 and then return a. Speed: a++ &gt; a+=1 &gt; a=a+1. Switch input must be one of char,short, int 123456switch(a):{ case x1: ....; break; case x2:....; break; default: ....; break} Pointer and reference Pointer Reference Pointer is a variable Reference is an essentially implicit pointer or the representation of the address of a memory piece Pointer can be empty or uninitialized; Reference requires variable to be initialized before using reference. nullptr virtual pointer Pointer and Array Array Pointer: the pointer pointing to the address of an array Array Name: array name is not a pointer, but just return the address of the array (address of the first element) Pointer for array element: 123int arr[] = {1,2,3,4,5};int *p = arrcout&lt;&lt;*p++; *p++ : return element *p, then p++ ++*p: (*p) element +1 and then return. Without changing the address pointer p pointing to Array Pointer 123int arr[] = {1,2,3,4,5};size =5;int (*p)[size] = &amp;arr; (*p)[size] : array pointer, pointing to array NOT element size of (*p)[size] must be number of element in arr p++: address skip the whole array size (5 * 4 byteshere), not element size (*p)[0]: return arr[0] Wild Pointer(野指针) pointer that has not been initialized before using Dangling Pointer(悬空指针) pointer that point to invalid memory address (after memory released) Macro and PreprocesssorDifference between Class and StructureObject-Oriented DesignBasic Features of OOD in C++ Three Traits Encapsulation(封装性)A mechanism of bundling the data, and the functions that use them and data abstraction is a mechanism of exposing only the interfaces and hiding the implementation details from the user Polymorphism(多态)Different classes inherent from the same parent class can have different methods, class members. It diversifies features of classes. Polymorphims in C++ is achieved by using virtual keyword. Inheritance(继承)The members of the base class become members of thederived class. It helps save our time and avoid re-writing the same codes. Publich, Protected, private Properties Private Protected Public Accessibility only Base Class member/method only Base Class and Child Class member/method Any instance and Class member Inheritance Properties: Private Inheritance: Base Class’s private members won’t be inherited by Child Class Base Class’s public and protected members will be converted to be private in Child Class Protected Inheritance: Base Class’s private members won’t be inherited by Child Class Base Class’s protected, public members wil become protected in Child Class, Base Class’s members can not be accessed by Child Class of Base Class’s Child Class. Public Inheritance: Base Class’s private members won’t be inherited by Child Class Base Class’s protected, public members don’t change (Still protected, or public) Types of overloading overloading the amount of parameter Example 12345Class A{ publich: void func(int a) {} void func(int a, int b) {}} overloading the data types of parameter Example 12345Class A{ publich: void func(int a) {} char func(char a) {}} overloading of const and non-const methods 123456Class A{ publich: void func(int a) {} // the const method can not change class member values void func(int a) const {}} overloading of Operators Example: 1234Class A{publich: int operator + (const int a, const int b) {}} Operators that can not be overloaded: :: , pointer operator . *, condition operator ?: , sizeof, typeof const_cast static_cast dynamic_cast reinterpret_cast Methods that can not be inherent Constructor Deconstructor Friend method Operators Methods that can be inherited static functions non-static functions non-static members (Note: static members must be private or protected that are usually unable to be inherent) Virtual function (used for polymorphism) Interface and Virtual class, method and pointer Pure virtual functionit is applied by using virtual keyword and =0 behind the function. 1234class A{ public: virtual int func(int x) =0;} Abstract ClassAbstract class provide an appropriate base class from which other classes can inheritAn abstract class must contain at least one pure virtual function InterfaceInterface describes the behavior a class without a particular implementation of that class.Interface in C++ is implemented by using Abstract class. Virtual Function and PolymorphismA Child of base class can overload the virtual function and give particular implement of the function to achieve polymorphism Virtual Function mechanism: using virtual pointer and virtual table In base class with virtual function, Compiler creates an virtual pointer vptr and a virtual table vtbl vptr points to the address of vtbl to write and store the addresses of all virtual functions in this base class. The pointer vptr is stored at the beginning of the memory of the base class. (the first element of the Class is vptr ) Example: For class A and Class B, each of them have virtual pointer __vptr__ pointing to a virtual table, which stores addresses of virtual functions. In addition, the virtual pointer of each class is stored at the beginning of the class. 4. In __single inheritance__ (a class is allowed to inherit only one class), child class will inherit the vptr from base class, but __create a new vtbl table to store addresses of new virtual functions and inherited/overrided virtual functions__ 5. In __multiple inheritance__: + child class inherits multiple vptr, then creates multiple vtbls, in which each vtbl corresponds to each base class (Hence __the number of vtbl = number of base class inherited__). + __All addresses of new virtual functions will be stored at the end of the first vtbls__ Template and Generic Function A generic function defines a general set of operations that will be applied to various types of data. Memory allocation and Types of memory new and malloc new malloc A keyword a function it calls the operator operator new() -&gt; malloc()-&gt; then call constructor in class directly allocate memory without using constructor return class pointer return void pointer can be overloading can not be overloading No need to tell size of memory Need to tell size of memory delete and free delete free A keyword a function it calls the operator operator delete() -&gt; free()-&gt; then call deconstructor in class directly free memory without using constructor return class pointer return void pointer can be overloading can not be overloading No need to check if memory exists Need to check memory first create and delete Class array 123456int size=10;ClassA* a = new ClassA[size]delete[] a; // input is an array of int class int ClassA::func(int[] a){....} malloc, calloc, realloc and alloca malloc: allocate memory without initialization calloc: allocate memory and initialize them to zero realloc: extend the memory with bigger size than before. alloca: allocate tempory memory on stack on local scope. Memory will be released outside the scope. Hence no need to use free() link and compile in C++ Dynamic Link Static Link Regular Expression (re)STL containerReference[1] https://www.tutorialspoint.com/cplusplus/cpp_overloadinging.htm [2] https://blog.csdn.net/csdn_chai/article/details/78041050","link":"/2021/03/08/Cpp-review/"},{"title":"DeepLearning-1 ConvolutionNetwork","text":"IntroductionConvolution neural network has been applied to different domains widely for feature extractor / filter. One of the most successful area is computer vision, in which convolution network is trained and used to extract the general feature we are interested in. This article summarizes how forward passing and backpropagation works in CNN to train the CNN filters. Then some properties of CNN are mentioned. Terms in CNN N: number of data points/ samples in dataset C: number of channel in one sample. Example: in RGB image, it has 3 channels: Red, Green, Blue H: height of one data point matrix / amount of rows in one sample W: width / amount of columns in one sample x: input with shape (N, C, H, W) y: output of convolution network kernel/filter/weight: filter with trainable weights and shape of (KH, KW), where KH: height / rows of kernel, KW: width/ columns of kernel b: bias term. One kernel correponds to one bias stride: The number of pixel between adjacent receptive fields in horizonal, vertical directions. padding: the number of rows, columns added to the boundaries/edges of a sample matrix. Usually, we set the padding row/column values to zeros. We usually set padding = (1,1) or (2,2) to add 1 or 2 row(s)/column(s) to each edge of sample matrix Ho: height/ amount of rows of output fromm convolution Wo: width/ amount of columns of output from convolution Output shape from convolution network: Ho = 1 + (H + 2*padding - KH )/stride Wo = 1 + (W + 2*padding - KW )/stride if Ho, Wo are not integer, that means when moving the kernel along input, index out of range occurs. In this case, we can simply drop the last column/row that make index out of range. Or, we can add zero columns/rows to fill the out of range pieces in matrix. How does CNN workForward pass in CNN Convolution with 1-D Input: without padding, stride =1, channel =1After we define some terms above, let consider there is 1-D input X with shape: C=1, H=1, W=4 and 1-D kernel K with shape KH=1, KW = 3.$$X = [x1, x2, x3, x4], K = [w1, w2,w3]$$ Then the output of convolution is$$Y = \\begin{bmatrix}y_1 \\\\y_2\\end{bmatrix}$$ where $y_1 = w_1 * x_1 + w_2 * x_2 + w_3 * x_3 +b$ and $y_2 = w_1 * x_2 + w_2 * x_3 + w_3 * x_4 +b$ where b is the bias term in convolution Convolution with 2-D Input: stride =1, no padding, channel =1$$X = \\begin{bmatrix}x1 &amp; x2 &amp; x3 \\\\x4 &amp; x5 &amp; x6 \\\\x7 &amp; x8 &amp; x9\\end{bmatrix} ,K = \\begin{bmatrix}w1 &amp; w2 \\\\w3 &amp; w4\\end{bmatrix}$$ Based on these equations, Ho = 1 + (H + 2*padding - KH )/stride Wo = 1 + (W + 2*padding - KW )/stride we know that the output Y has shape H =2, W= 2: $$Y = \\begin{bmatrix}y1 &amp; y2 \\\\y3 &amp; y4\\end{bmatrix}$$ $$y1 = sum( \\begin{bmatrix}x1 &amp; x2 \\\\x4 &amp; x5\\end{bmatrix} * \\begin{bmatrix}w1 &amp; w2 \\\\w3 &amp; w4\\end{bmatrix} ) +b = w1x1 + w2x2 + w3x4 + w4x5 + b$$ where * is element-wise multiplicationSimilarly, we have $$\\begin{matrix}y2 = w1x2 + w2x3 + w3x5 + w4x6 + b, \\\\y3 = w1x4 + w2x5 + w3x7 + w4x8 + b, \\\\y4 = w1x5 + w2x6 + w3x8 + w4x9 + b \\\\\\end{matrix}$$ Convolution with 2-D Input: with padding = (1,1) stride =1, channel =1Here I add 1 row, 1 column zeros pad to four edges of 2-D matrix. Then the sample X becomes 4 by 4 matrix. After padding, we will do the same forward pass process as step 2. $$X = \\begin{bmatrix}0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\\\0 &amp; x1 &amp; x2 &amp; x3 &amp; 0 \\\\0 &amp; x4 &amp; x5 &amp; x6 &amp; 0 \\\\0 &amp; x7 &amp; x8 &amp; x9 &amp; 0 \\\\0 &amp; 0 &amp; 0 &amp; 0 &amp; 0\\end{bmatrix}$$ and$$K = \\begin{bmatrix}w1 &amp; w2 &amp; w3 \\\\w4 &amp; w5 &amp; w6 \\\\w7 &amp; w8 &amp; w9 \\\\\\end{bmatrix}$$ The output Y becomes:$$Y = \\begin{bmatrix}y1 &amp; y2 &amp; y3 \\\\y4 &amp; y5 &amp; y6 \\\\y7 &amp; y8 &amp; y9 \\\\\\end{bmatrix}$$ where$$y1 = w_1 * 0 + w_2 * 0 + w_3 * 0 + w_4 * 0 + w_5 * x_1 + w_6 * x_2 + w_7 * 0 +w_8 * x_4 + w_9 * x_5$$ Let denote the $i^{th}$ row and $j^{th}$ column entry in output Y as $y_{i,j}$, the $h^{th}$ row and $w^{th}$ column entry in kernel K as $w_{h,w}$, the number of stride as $s$. We can write down the formula to compute every entry in output Y:$$y_{i,j} = \\sum_{h=1}^{KH}\\sum_{w = 1}^{KW} w_{h,w} * x_{s * i+h-1, s * j+w -1}$$ Convolution with 2-D Input: with channel =3, padding = (1,1) stride =1when channel of sample X is more than 1, we will need the amount of kernels K equal to the number of input channel $C_{in}$.Example: Input X with Channel C=3$$X_R = \\begin{bmatrix}xr1 &amp; xr2 &amp; xr3 \\\\xr4 &amp; xr5 &amp; xr6 \\\\xr7 &amp; xr8 &amp; xr9\\end{bmatrix}$$ $$X_G = \\begin{bmatrix}xg1 &amp; xg2 &amp; xg3 \\\\xg4 &amp; xg5 &amp; xg6 \\\\xg7 &amp; xg8 &amp; xg9\\end{bmatrix}$$ $$X_B = \\begin{bmatrix}xb1 &amp; xb2 &amp; xb3 \\\\xb4 &amp; xb5 &amp; xb6 \\\\xb7 &amp; xb8 &amp; xb9\\end{bmatrix}$$ Then we will need one kernel for each input channel. Hence we will have 3 kernels $K_R, K_G,K_B$. The output of this convolution network is the sum of all convolution output$$Y = conv(X_R, K_R) + conv(X_G, K_G) + conv(X_B, K_B)$$ In this cases, the output channel is still C= 1. If we want the output channel to be more than 1. Let say the output channel $C_{out}$ =3, then we have $C_{out} * C_{in}$ kernels in total and each kernel has shape of $W_o * H_o$. Hence we will have parameters with amount of $C_{out} * C_{in} * W_o * H_o$ or $C_{out} * C_{in} * (1 + (H + 2 * pad - KH) / stride ) * (1 + (W + 2 * pad - KW) / stride )$ Chain RuleBefore talking about how to update the weight in kernel, let talk about the chain rule first.Denote loss function as $L(y_p, y_t)$, where $y_p$ is prediction / output from estimator and $y_p =f_w(x)$ is a function of input x, $y_t$ is target, where x, y,w are all scalar values Let: $y_p = f_w(x) =wx$$$L(w) = L(y_p, y_{t}) = (y_p - y_{t})^2 = (f_w(x) - y_{t})^2 = (wx - y_{t})^2$$ Then in chain rule to find the gradient of weight w, we have$$\\nabla_w L(w) = \\frac{dL(w)}{dw} = \\frac{dL}{dy_p} * \\frac{dy_p}{dw} = [2 *(y_p - y_t)] * [x] = 2x(wx - y_t)$$ By chain rule, we can extend $ \\frac{dL}{dy_p} * \\frac{dy_p}{dw}$ to $\\frac{dL}{dy_p} * \\frac{dy_p}{dy_1} * \\frac{dy_1}{dy_2} *…\\frac{dy_i}{dw}$ in many terms Backpropagation in CNNLet’s go back the feed forward step in CNN, we have equation$$y_{i,j} =f_w(x) = \\sum_{h=1}^{KH}\\sum_{w = 1}^{KW} w_{h,w} * x_{s * i+h-1, s * j+w -1}$$ Where the output vector of convolution is $Y$ and $y_{i,j}$ is the $i^{th}$ row and $j^{th}$ column entry of Y. $Y^t$ is the target. $x_{i,j}$ is the $i^{th}$ row and $j^{th}$ column entry of input X. Define loss function $L(w)$ . Then gradient of weight $w_{h,w}$ in kernel is obtained by$$\\frac{dL(w)}{dw_{h,w}} = \\sum_{i=1}^{H}\\sum_{j=1}^{W}\\frac{dL}{dy_{i,j}} * \\frac{dy_{i,j}}{dw_{h,w}}$$ Example: $$X = \\begin{bmatrix}x_{11} &amp; x_{12} &amp; x_{13} \\\\x_{21} &amp; x_{22} &amp; x_{23} \\\\x_{31} &amp; x_{32} &amp; x_{33}\\end{bmatrix} , K = \\begin{bmatrix}w_{11} &amp; w_{12} \\\\w_{21} &amp; w_{22}\\end{bmatrix}$$ $$Y = \\begin{bmatrix}y_{11} &amp; y_{12} \\\\y_{21} &amp; y_{22}\\end{bmatrix}$$ Let Loss function be $L(w) = 0.5* \\sum_i\\sum_j (y_{i,j} - y^{t}_{i,j})^2$and we have $$\\frac{dL}{dy_{i,j}} = (y_{i,j} - y^{t}_{i,j})$$ $$\\frac{dL}{dy} = \\begin{bmatrix}\\frac{dL}{dy_{11}} &amp; \\frac{dL}{dy_{12}} \\\\\\frac{dL}{dy_{21}} &amp; \\frac{dL}{dy_{22}}\\end{bmatrix}$$ $$\\frac{dy_{11}}{dw_{11}} = \\frac{d(w_{11} * x_{11} + w_{12} * x_{12} + w_{21} * x_{21} + w_{22} * x_{22} +b)}{dw_{11}}$$ $$\\frac{dy_{11}}{dw_{11}} = x_{11}$$ Similarly, we have $$\\frac{dy}{dw_{11}} = \\begin{bmatrix}\\frac{dy_{11}}{dw_{11}} &amp; \\frac{ddy_{12}}{dw_{11}} \\\\\\frac{ddy_{21}}{dw_{11}} &amp; \\frac{ddy_{22}}{dw_{11}}\\end{bmatrix} = \\begin{bmatrix}x_{11} &amp; x_{12} \\\\x_{21} &amp; x_{22}\\end{bmatrix}$$ Finally, gradient of weight $w_{11}$ becomes$$\\frac{dL}{dw_{11}} = sum(\\frac{dL}{dy} * \\frac{dy}{dw_{11}} ) = \\sum_{i=1}^{2}\\sum_{j=1}^{2}\\frac{dL}{dy_{i,j}} * \\frac{dy_{i,j}}{dw_{1,1}}$$ $$\\frac{dL}{dw_{11}} = \\sum_{i=1}^{2}\\sum_{j=1}^{2} (y_{i,j}-y^t_{i,j})*x_{i,j}$$ where * is element-wise multiplication Repeat doing this, we can find the gradient for all weights in all kernels. Properties in CNN the weight matrix in CNN is small and save memory compared with traditional dense network.For example, if we have an 1818 input matrix we want to output a 3x3 matrix, we can either use a 16x16 convolution kernel with stride =1, or 9 3x3 kernels to do this.In this case, 933 is smaller than 1616. Hence CNN with deeper convolution network and small filter can save memory Updating weights in CNN is fast, as the weight / kernel is small CNN can be used for transfer learning by transferring learned kernels CNN can be used for down-sampling data, reducing data size.For example, in ResNet, it uses pixel convolution: kernel size =1 * 1 and stride =2 to down sample data by half of original size in each channel of matrix Reference[1] https://towardsdatascience.com/backpropagation-in-a-convolutional-layer-24c8d64d8509 [2] https://arxiv.org/pdf/1512.03385.pdf","link":"/2020/11/02/DL-ConvolutionNetwork/"},{"title":"DeepLearning -2 DropOut","text":"Introduction and Problem of OverfittingIn deep learning, Overfitting is a common problem, in which model fits the training data very well, but perform worse in test data / unseen data. This is due to that when the model learns general features of the dataset, it also learns some specific features in some specific samples well. It makes generalization error increase as well. Dropout is a regularization method that approximates training a large number of neural networks with different architectures in parallel. Or we can regard DropOut as a method of sampling sub-neural network within a full neural network by randomly selecting some neurons to feedforward and update weight. How DropOut WorkDropOut can be applied to any hidden layer. It is to randomly select some neurons in the hidden layer to output to the next layer. As the image shows here, in the second hidden layer, we randomly select some neurons to output and disable other neurons during feedforward process. Then in back propagation process, we only update the weights connected to those selected neurons. We can also regard DropOut as Sampling training technique in training weight. In back propagation, since we randomly sample neurons to output, it can be regarded as a 0,1 mask to multiply the output in hidden layer. So in back propagation, the gradient to the weights of ignored neurons will be zeros. Note that DropOut is applied in training step only. We DO NOT use dropout in prediction step as it can make the prediction unstable when randomly choosing different neurons for output. DropOut RateDropOut rate is the possibility of training a given node in a hidden layer. If dropout rate is large, then it is more likely to select and train the node in hidden layer. For example, if dropout rate = 0.1, then each node in a hidden layer has only 0.1 possibility of being trained (enabled to feedforward and back propagation) in training step. If dropout rate = 1, then all neurons in network will be trained. Code of dropout12345678910111213141516171819202122def forward(X): p = 0.5 # p is dropout rate out1 = np.maximum(0, np.dot(w1,X)+b1) mask1 = np.random.rand(*out1.shape) &lt; p # create dropout mask out1 *= mask # since we select parts of neurons for output, # the scale of out1 has changed, we need to use /p to # re-scale the output out1 /= p out2 = np.maximum(0, np.dot(w2,out1)+b2) mask2 = (np.random.rand(*out2.shape) &lt; p)/p out2 *= mask2 out3 = np.maximum(0, np.dot(w3,out2)+b3) return out3def predict(x): # In prediction, we don't need dropout as dropout make the output # difference and unstable out1 = np.maximum(0, np.dot(w1,X)+b1) out2 = np.maximum(0, np.dot(w2,out1)+b2) out3 = np.maximum(0, np.dot(w3,out2)+b3) return out3 Properties of DropOut dropout is one way to regularize neural network and avoid overfitting dropout is extremely effective and simple It can be applied to any hidden layer output Reference[1] https://machinelearningmastery.com dropout-for-regularizing-deep-neural-networks/ [2] https://blog.csdn.net/qq_28888837/article/details/84673884","link":"/2020/11/13/DL-DropOut/"},{"title":"Data Structure 3 - BinaryTree","text":"Binary TreeDefinition of Binary TreeThe settings of binary tree are following: Each node in the tree contains no more than 2 children nodes (left node, right node) Leaf nodes of the tree are the nodes that contain no children nodes Traversal of Binary TreeType of traversals of binary treeConsider this example of binary tree Pre-order traversalThe order of visiting nodes: current node -&gt; left children node -&gt; right children node.In the binary tree above, we start from the root node (current node) and follow the rule to visit each node. Then have cur_node= 10, left_node =5. when we goes to left_node 5, 5 becomes the new current node and hence print 5 and then its left_node 2. When it finds there is no left node after 2, it goes back to 5 and visit its right node.Repeat doing this, we have Pre-Order traversal of this binary tree: 10-&gt; 5-&gt;2-&gt; 7-&gt; 15-&gt;20 In-order traversalThe order of visiting nodes: left children node -&gt; current node -&gt; right children nodeSimilar to Pre-Order traversal, except the traversal order, In-order requires the current node is the second node to be visited. Hence, In-Order traversal of this example is: 2-&gt;5-&gt;7-&gt;10-&gt;15-&gt;20 Post-order traversalThe order of visiting nodes: left children node -&gt; right children node -&gt; current nodeSimilarly, Post-Order traversal of the example is: 2-&gt;7-&gt;5-&gt;15-&gt;20-&gt;10 Level-order traversalIt traverses the binary tree from top level to lower level. In each level of tree, it iterates the nodes from left to right. Level-Order traversal of this example: 10-&gt;5-&gt;15-&gt;2-&gt;7-&gt;20 Implementation of Traversal of Binary Tree Recursive Method Pre-OrderTime Complexity: O(n)Memory Complexity: O(h), h is the height of the tree. It is used by recursion to store address of function in stack 12345678910111213141516class TreeNode(): def __init__(self): self.left = None self.right = None self.val = Noneclass Solution(): def Pre_Order(self, root): if not root: return [] result = [] result.append(root.val) left_list = self.Pre_Order(root.left) right_list = self.Pre_Order(root.right) result.extend(left_list) result.extend(right_list) return result In-OrderTime Complexity: O(n)Memory Complexity: O(h), h is the height of the tree. It is used by recursion to store address of function in stack 1234567891011class Solution(): def In_Order(self, root): if not root: return [] result = [] left_list = self.In_Order(root.left) result.extend(left_list) result.append(root.val) right_list = self.In_Order(root.right) result.extend(right_list) return result Post-OrderTime Complexity: O(n)Memory Complexity: O(h), h is the height of the tree. It is used by recursion to store address of function in stack 1234567891011class Solution(): def Post_Order(self, root): if not root: return [] result = [] left_list = self.Post_Order(root.left) result.extend(left_list) right_list = self.Post_Order(root.right) result.extend(right_list) result.append(root.val) return result Iterative Method Pre-OrderTime Complexity: O(n)Memory Complexity: O(h), h is the height of the tree123456789101112131415161718192021222324class Solution(object): def PreOrder(self, root): \"\"\" input: TreeNode root return: Integer[] \"\"\" # write your solution here if not root: return [] stack = [(root, 1)] result = [] while len(stack)&gt;0: node, count = stack.pop() if count ==1: result.append(node.val) stack.append((node,2)) if node.left: node = node.left stack.append((node, 1)) if count == 2: if node.right: node= node.right stack.append((node,1)) return result In-OrderTime Complexity: O(n)Memory Complexity: O(h), h is the height of the tree123456789101112131415161718192021222324class Solution(object): def InOrder(self, root): \"\"\" input: TreeNode root return: Integer[] \"\"\" # write your solution here if not root: return [] stack = [(root, 1)] result = [] while len(stack)&gt;0: node, count = stack.pop() if count ==1: stack.append((node,2)) if node.left: node = node.left stack.append((node, 1)) if count == 2: result.append(node.val) if node.right: node= node.right stack.append((node,1)) return result Post-OrderTime Complexity: O(n)Memory Complexity: O(h)1234567891011121314151617181920212223242526class Solution(object): def PostOrder(self, root): \"\"\" input: TreeNode root return: Integer[] \"\"\" # write your solution here if not root: return [] stack = [(root, 1)] result = [] while len(stack)&gt;0: node, count = stack.pop() if count ==1: stack.append((node,2)) if node.left: node = node.left stack.append((node, 1)) if count == 2: stack.append((node, 3)) if node.right: node= node.right stack.append((node,1)) if count==3: result.append(node.val) return result Level-OrderTime Complexity: O(n)Memory Complexity: O(h)1234567891011121314151617class Solution(object): def LevelOrder(self,root): if not root: return [] queue = [root] result = [] while len(queue) &gt;0: # dequeue node = queue[0] del queue[0] result.append(node.val) # enqueue if node.left: queue.append(node.left) if node.right: queue.append(node.right) return result Special Binary Tree1. Balanced Binary TreeBalanced Binary Tree is a tree that the depth of left and right subtrees of every node differ by 1 or less.Hence, for each node in the tree, we need to check the heights of left, right subtrees.In the examples below: Example 1 is a balanced tree, but Example 2 is not, since left and right subtrees of the node of 20 in example 2 has the height difference 2-0 = 2 &gt;1. 2. Complete Binary TreeA Binary Tree is a complete Binary Tree if all the levels are completely filled except possibly the last level and the last level has all keys as left as possibleConsider the following example: 12345 10 / \\ 5 1 / \\ / 2 4 2 This is an complete as well as balanced tree.However, the following one is balanced but not complete tree, since in the last level, all keys are not as left as possible as the node 2 should be in the left node, but it doesn’t. 12345 10 / \\ 5 1 / \\ \\2 4 2 4. Perfect Binary TreePerfect Binary Tree A Binary tree is a Perfect Binary Tree in which all the internal nodes have two children and all leaf nodes are at the same level. Example of a perfect tree 12345 10 / \\ 5 1 / \\ / \\2 4 3 2 Example of not a perfect tree 12345 10 / \\ 5 1 / \\ /2 4 3 This is a complete, balanced binary tree, but not a perfect tree 5. Binary Search TreeBinary search tree is a tree that for every node in the tree, the values in left subtree are smaller than its value, the values in right subtree are greater than its.if we consider the duplicated values in the tree, the values in right subtree can be greater than or equal to the node value. This case should be discussed if duplicated values exist. This difference should be determined when discussing with the hiring manager Example of Binary Search Tree 12345 10 / \\ 5 11 / \\ \\2 7 21 Example of Not a Binary Search Tree 12345 10 / \\ 5 1 / \\ \\2 7 21 6. AVL TreeAVL tree is a self-balancing Binary Search Tree (BST) where the difference between heights of left and right subtrees cannot be more than one for all nodes.When inserting each element in the AVL tree, it will re-balance the tree automatically.The operations of this data structure will be demonstrated in future 7. Red-Black TreeRed-Black tree is also a self-balancing tree. It has the following constraints: Each node has a color either red or black Root of tree is always black No adjacent red nodes. The parent/ children of a red node could not be red. But there could be adjacent black nodes Every path from a node (including root) to any of its descendant NULL node has the same number of black nodes.Example: Every path starting from node 18 to NiL/ None, have the same amount of black node 1. SummaryIn future, I may write the notes about more operations about binary tree, AVL tree, Red-Black Tree, just like computing height of tree, isBalanced, isSymmetric, insertion and deletion of element in AVL tree/Red-Black tree Reference[1] https://www.geeksforgeeks.org/red-black-tree-set-1-introduction-2/[2] https://www.geeksforgeeks.org/avl-tree-set-1-insertion/[3] https://zh.wikipedia.org/wiki/%E4%BA%8C%E5%85%83%E6%90%9C%E5%B0%8B%E6%A8%B9","link":"/2020/09/05/Data-Structure-BinaryTree/"},{"title":"Data Structure - BuckSort with Parallel Computing","text":"IntroductionThis article introduces the basic bucket sort algorithm and the parallel version of bucket sort Simple Bucket SortStep of Bucket Sort Create n buckets and each bucket has a range, such as [0,4) Assign every element of unsorted array to the corresponding bucket based on the range of bucket. Ex: element 3 should be assigned to the bucket with range [0,4) Sort every bucket using insertion sort or other sorting method Merge all buckets together based on range to get the overall sorted arrayExampleUsing two buckets with range: [0,5), [5, 10]12345678unsorted array: [10, 2 , 5, 9, 4, 6, 1] / \\assign element to buckets: [2,4,1] [10,5,9,6] | | Sort every bucket [1,2,4] [5,6,9,10]with insertion sortMerge buckets: [1,2,4,5,6,9,10] Analysis Time complexity: O(n) for assigning element to buckets. O(n^2) for insertion sort in insertion. When merging buckets to a new buckets: O(n). Depending on the insertion operation, the time complexity can be different. Average case: O(n) if we think insertion time is O(1), else O(n)+O(n^2) = O(n^2) when using insertion sortif using quicksort, mergesort, it becomes O(nlog(m))+O(n) = O(nlogm) Memory complexity: O(n), since we use buckets to store element, where n = number of bucket * size of bucket Bucket Sort with Parallel Computing methodStep of Bucket Sort of parallel version There are p computing nodes/processors For every computing node, it has 1 large bucket and m small bucket and p=m In processor 0, it divides unsorted array into pieces evenly to every processors Every small bucket / large bucket has its range Every processor breaks its own piece of array into its small buckets based on the range of small bucket Sort every small bucket using some sorting (quicksort, insertion sort, etc) Gather all small buckets that have the same range into the large bucket that have that range Sort every large bucket Merge all large buckets into a sorted array ExampleUsing two small buckets: b1, b2 and two large buckets: lb1, lb2, with range: [0,5), [5, 10].There are two computing nodes/processor 1234567891011121314unsorted array: [10, 2 , 5, 9, 4, 6, 1, 7] / \\2 nodes: node1: [10,2,5,9] node2: [4,6,1,7] | | Divide them into b1: [2] b2:[10,5,9] b1:[4,1] b2:[6,7]Small buckets | |Sort: b1:[2], b2:[5,9,10] b1:[1,4] b2:[6,7] | |Send to large buckets: node1: [2,1,4] node2: [5,9,10,6,7] | |Sort: node1: [1,2,4] node2: [5,6,7,9,10]Merge buckets: [1,2,4,5,6,7, 9,10] Analysis Time complexity: Average Case O(n) if we consider sorting time as O(1). Or O(nlogm) if we use quicksort/mergesort Memory Complexity: O(n) since we use p computing nodes , large buckets and m smaller buckets to store data Reference[1] https://media.geeksforgeeks.org/wp-content/uploads/BucketSort.png","link":"/2020/09/22/Data-Structure-BuckSort-Parallel-Computing/"},{"title":"Data Structure 1 - Binary Search","text":"Summary of Binary Search (also called half-interval search)Given an array A = [a0, a1,….an], where elements are in increasing order, that is, a1&lt; a2 &lt;…&lt; an. We are going to find the index of an element ak inside the array.Let consider the array below, where a0=1, a1=3… index 0 1 … k … n a[i] a[0] = 1 a[1] =3 … a[k] = ak =10 … a[n]=an= 20 let set left variable L store index of a1 and right variable R stores index of an. Then Let L=0, R=n Check if a[L]=ak or a[R]=ak. If yes, return index of ak. Otherwise, step3 Find middle index M =$\\frac{(L+R)}{2}$ and check if a[M]=ak. If no, continue if a[M]&gt;ak, let R=M and search subarray between index L~ MReason:we know array is in increasing order and elements between M and R must be larger than a[M] and ak. Hence we only need to search elements between L and M if a[M]&lt;ak, let L=M search subarray between index M~ RThe logic is similar to step 4 Back to Step 3 until ak is found, Or R = L+1, that is, ak is not found. Analysis Computational Complexity: O($log_2(n)$) since it makes $log_2 n$ iterations. Space complexity of binary search is O(1) My Thoughts Its main idea is to reduce the searching space by half each time by using mid-intervel Assumption It requires we know L and R It assumes the array has been sorted. If the array has not been sorted, we can use Binary-Search-Tree to sort and store data and then use in-order traversal to obtain index of each element When to Apply It is good to search specific values that can be found by comparison of values However, in my opinion, It is not good enough to explore the combination of elements. For example, Finding the max sum of N numbers in an array. Then the searching space would be too large to find when using binary search. Extension Extensive Problem 1:Given a very very large (may be infinite) array and we don’t know how large it is. Use binary search to find the index of a given element ak. Example: Given an array like this, a1 a2 … ak …(we don’t know the end of the array) 1 3 … a[k] … Analysis: since we don’t know the size of array, we are hardly to find the right boundary R we only know the left boundary L=0 Idea: Jump Out to find R: We can first find the smallest integer X, such that a[$z^X$]$\\geq$ ak, where z is positive integer, maybe 2, 10… Set R= $z^X$ Jump in to find ak: Apply binary search on the region between $z^{X-1}$ and $z^X$. Evaluate Time Complexity since jump out step is $z^X$, then the time required to find the region is O($log_Z$(n)). Time required to search ak insdie $z^{X-1}$ and $z^X$ is O($log_2$(n)) Total complexity is O($log_Z$(n) + $log_2$(n)) Compare the value of Z to find the best performance. __Draw the picture of $log_z(X)$ !__ Reference[1] “https://www.cdn.geeksforgeeks.org/wp-content/uploads/binary-tree-to-DLL.png&quot;","link":"/2020/07/19/Data-Structure-binary-search/"},{"title":"Distributed Computing Basic Note - 1","text":"IntroductionThis blog introduces basic types of parallel computing architecture, memory, and basic evaluation metrics of parallel computing. Parallel Computing Features of Parallel computing Features of Problem: problem task can be broken into discrete pieces of work Problem can be solved faster with multiple computing resource than single resource Features of Computing resource Single computer with multiple processors Multiple computers connected in network Combine 1,2 Special computing component (GPU) Execution Execute multiple instructions concurrently in time Relationship between Parallel, Distributed, Cluster, Concurrency Computing:$ Cluster Computing \\subset Distributed Computing \\subset Parallel computing \\subset Concurrencty Computing$ Evaluation of Computing performanceAmdahl’s LawAssume:$f_s$: fraction that the program is not parallelizable (the ratio of series part to whole program)Or called portion of series in the programp: Number of processor$t_p$: runtime of parallelizable operation$t_s$: runtime of series operationS(p): Parallel SpeedupThenAmdahl’s Law: $t_p = f_s S(p) + \\frac{(1-p)f_s}{p} $ Parallel SpeedupParallel Speedup: $S(p) = \\frac{t_s}{t_p} = \\frac{p}{(p-1)*f+1}$. Usually Parallel Speedup $S(p) \\leq p$ Limiting Factors affecting Parallel Speedup: Not parallelizable code (or Series operation runtime): $t_s$. Longer runtime series code takes, larger $f_s$ is and Smaller the Parallel Efficiency is Communication Overhead: more time is spent on communication, then it also increases $t_s$ Parallel EfficiencyParallel Efficiency: $E = \\frac{S(p)}{p} = \\frac{1}{(p-1)*f + 1}$Parallel Efficiency measures how efficient the parallelization is. Higher Efficiency is, more busy each processor is and Faster the operations are executed. So we want to increase parallel Efficiency and maximize the ratio between S(p) and p. Type of Parallel Speedup Linear Speedup (normal case): $S(p) &lt; p$ Sublinear Speedup: $S(p) = p$ Superlinear Speedup: $S(p) &gt; p$ limiting Factors lead to superlilnear Speedup: Poor sequential reference implementation, leading to $t_s$ very large and series runtime very large 2. Memory caching: when memory cache is small, it will increase $t_s$ and lead to lower processing speed such that $t_s &gt;t_p$ and S(p) &gt;p3. I/O blocking : block I/O leads to delay of runtime, $t_s$ will increase Types of ComputersFlynne’s TaxonomyThis taxonomy classifies the computer architecture into four types: SISD: Single Instruction Single Datastream SIMD: Single Instruction Multiple Datastreams MISD: Multiple Instructions Single Datastream MIMD: Multiple Instructions Multiple Datastreams Note: SIMD, MISD, MIMD architecture belongs to parallel computer since they satisfy the requirements of feature in parallel computing ( break task into piece, execute multiple(same or different )instrutions concurrently, etc) Type of Memory Shared MemoryMultiple processors share the same memory Distributed Memory and Message PassingMultiple processors have its own memory but use message passing method to communicate with each other distributedly across network. Hyper-ModelCombining Shared memory and distributed Memory together. Heterogeneous computing (accelerators) GPU FPGA Benchmarking and Ranking supercomputers LINPACK (Linear Algebra Package): Dense Matrix Solver HPCC: High-Performance Computing Challenge SHOC: Scalable Heterogeneous Computing - Non-traditional systems (GPU) TestDFSIO - I/O Performance of MapReduce/Hadoop Distributed File System","link":"/2020/09/20/Distributed-Computing-note-1/"},{"title":"Quick tutorial JavaScript with D3.js","text":"IntroductionAbout this passageThis passage simply introduces how to combine HTML, CSS and Javascript together to design a web page. Then D3.js, a javascript package for data visualization is introduced and applied to visualize some simple data. About HTML, CSS, JavaScriptHTML: HTML stands for Hyper Text Markup Language. It is the standard markup language for Web pages. Elements in HTML are the building blocks of HTML pages, represented by &lt;&gt; tags CSS: it stands for Cascading Style Sheets. It describes how HTML elements are to be displayed. JavaScript: JavaScript is the Programming Language for the Web. It can update and change both HTML and CSS. It can calculate, manipulate and validate data Combining HTML, CSS, JavaScript for a webpageTo start with, Let’s create a folder that will contain all HTML, CSS, JavaScript files we will create. Write a HTML fileFirst, we write an empty HTML file with title “Demo” and empty content for passage &lt;p&gt; &lt;\\p&gt; and heading &lt;h1&gt;&lt;\\h1&gt; 12345678910&lt;html&gt; &lt;head&gt; &lt;meta charset=\"utf-8\"&gt; &lt;title&gt; Demo&lt;/title&gt; &lt;/head&gt; &lt;body&gt; &lt;h1&gt;&lt;/h1&gt; &lt;p&gt; &lt;/p&gt; &lt;/body&gt; &lt;/html&gt; Then we put Javascript into HTML directly 1234567891011121314151617181920212223&lt;html&gt; &lt;head&gt; &lt;meta charset=\"utf-8\"&gt; &lt;title&gt; Demo&lt;/title&gt; &lt;/head&gt; &lt;body&gt; &lt;h1&gt;&lt;/h1&gt; &lt;p&gt; &lt;/p&gt; &lt;script&gt; var headers = document.getElementsByTagName(\"h1\"); for (var i = 0; i &lt; headers.length; i++) { var header = headers.item(i); header.innerHTML = \"Hello I'm Header\"; } var paragraphs = document.getElementsByTagName(\"p\"); for (var i = 0; i &lt; paragraphs.length; i++) { var paragraph = paragraphs.item(i); paragraph.innerHTML = \"I'm Passage\"; } &lt;/script&gt; &lt;/body&gt; &lt;/html&gt; The tag &lt;\\script&gt;&lt;\\script&gt; tells HTML the code inside this block is Javascript. In this script, we select the element with tag name ‘h1’ and ‘p’ using document.getElementsByTagName() and then input text “Hello I’m Header” and “I’m Passage”. This will let HTML display such texts in corresponding elements. Using Javascript file instead of internal scriptTo use JavaScript file rather than script inside HTML, we first create a JavaScript file called “myjs.js” in the same folder. Then copy the JavaScript code from HTML to Javascirpt. 12345678910var headers = document.getElementsByTagName(\"h1\"); for (var i = 0; i &lt; headers.length; i++) { var header = headers.item(i); header.innerHTML = \"Hello I'm Header\"; }var paragraphs = document.getElementsByTagName(\"p\"); for (var i = 0; i &lt; paragraphs.length; i++) { var paragraph = paragraphs.item(i); paragraph.innerHTML = \"I'm Passage\"; } In HTML file, we replace the script with a sentence, shown as below. 1234567891011&lt;html&gt; &lt;head&gt; &lt;meta charset=\"utf-8\"&gt; &lt;title&gt;D3 Demo&lt;/title&gt; &lt;/head&gt; &lt;body&gt; &lt;p&gt;Hello World &lt;/p&gt; &lt;h1&gt;&lt;/h1&gt; &lt;script id=\"myjs\" src=\"myjs.js\" type=\"text/javascript\"&gt;&lt;/script&gt; &lt;/body&gt; &lt;/html&gt; “id”，”type” attributes tell HTML we are looking for a file with name “myjs” and file type of JavaScript. Define style with CSS file Internal CSS code Elements’ styles, like color, font size, etc can be defined using CSS code. To add CSS inside HTML, here is an example: 1234567891011121314151617181920212223242526272829303132333435363738394041424344 &lt;html&gt; &lt;head&gt; &lt;meta charset=\"utf-8\"&gt; &lt;title&gt;D3 Demo&lt;/title&gt; &lt;/head&gt; &lt;style&gt; h2 { color: blue; font-size: 12px; font-family: \"Open Sans\"; text-anchor: middle; } &lt;/style&gt; &lt;body&gt; &lt;p&gt;Hello World &lt;/p&gt; &lt;h1&gt;&lt;/h1&gt; &lt;script id=\"myjs\" src=\"myjs.js\" type=\"text/javascript\"&gt;&lt;/script&gt; &lt;/body&gt; &lt;/html&gt; ~~~ The tag \\&lt;style&gt; tells the HTML the style settings (blue color, font-size = 12 pixel, ...) for all h2 element. &lt;br&gt;- __External CSS code__Usually, using a separated css file, rather than internal css code inside HTML file, can help further improvement for our project. It is more flexible to use css for element style design. To use external CSS file, replace the style codes in HTML with \\&lt;link&gt; tag~~~HTML&lt;html&gt; &lt;head&gt; &lt;meta charset=\"utf-8\"&gt; &lt;title&gt;D3 Demo&lt;/title&gt; &lt;/head&gt; &lt;link id =\"mystyle\" href=\"mystyle.css\" rel=\"stylesheet\" type=\"text/css\" /&gt; &lt;body&gt; &lt;p&gt;Hello World &lt;/p&gt; &lt;h1&gt;&lt;/h1&gt; &lt;script id=\"myjs\" src=\"myjs.js\" type=\"text/javascript\"&gt;&lt;/script&gt; &lt;/body&gt; &lt;/html&gt; Then Create a CSS file, called “mystyle.css” and copy the style code to CSS file 123456h2 { color: blue; font-size: 12px; font-family: \"Open Sans\"; text-anchor: middle; } The CSS file is linked to the HTML file by &lt;link&gt; tags in HTML. “id” is the identity of element “link” and “href” is set to be “mystyle.css” file. Attribute “type” is the file type of “mycss.css” file, hence it is css or text type. What is D3.jsD3.js is a JavaScript library for manipulating documents based on data. D3 helps you bring data to life using HTML, SVG, and CSS. D3’s emphasis on web standards gives you the full capabilities of modern browsers without tying yourself to a proprietary framework, combining powerful visualization components and a data-driven approach to DOM manipulation Import D3.jsTo import D3.js library, we can either add this line to the HTML file, then the D3.js library from the website will be applied. 1&lt;script src=\"http://d3js.org/d3.v3.min.js\" charset=\"utf-8\"&gt;&lt;/script&gt; Or download D3.js library from https://d3js.org/ to the folder directly.Here is an example of importing D3.js library and my javascript (D3-demo.js), css files (mystyle.css) 1234567891011121314&lt;html&gt; &lt;link id=\"mystyle\" href= 'mystyle.css' rel=\"stylesheet\" type=\"text/css\" /&gt; &lt;script src=\"http://d3js.org/d3.v3.min.js\" charset=\"utf-8\"&gt;&lt;/script&gt; &lt;head&gt; &lt;meta charset=\"utf-8\"&gt; &lt;title&gt;D3 Demo&lt;/title&gt; &lt;/head&gt; &lt;body&gt; &lt;p&gt;Hello World &lt;/p&gt; &lt;h1&gt;&lt;/h1&gt; &lt;script id=\"myjs\" src='D3-demo.js' type=\"text/javascript\"&gt;&lt;/script&gt; &lt;div id=\"chart\"&gt;&lt;/div&gt; &lt;/body&gt; &lt;/html&gt; Selection, Insertion, Remove for elements Selection and Modification 123456789101112131415161718192021222324252627// Using JavaScript to change URL to stylesheet and JavaScript in HTML// #mystyle ： element with id \"mystyle\"// #myjs ： element with id \"myjs\"// Note: to Select element by ID, we need to add # ahead id namecss_src= \"mystyle.css\"js_src=\"D3-demo.js\"var mycss= d3.select(\"#mystyle\").attr(\"href\",css_src)var myjs = d3.select(\"#myjs\").attr (\"src\",js_src)// Change Passage textvar p = d3.select(\"body\") .selectAll(\"p\") .text(\"d3js.org\");// Change color of passage text//修改段落的颜色和字体大小p.style(\"color\",\"red\") .style(\"font-size\",\"72px\");//Change &lt;h1&gt; element's text and text style: color, font sizevar h = d3.select(\"body\") .selectAll(\"h1\") .text(\"Heading\"); h.style(\"color\",\"red\") .style(\"font-size\",\"72px\");// Append &lt;h2&gt; tag to &lt;p&gt; section, with text \"Here is h2\"var h2 = d3.select(\"body\").select(\"p\").append(\"h2\").text(\"Here is h2\"); Bar Plot 1234567891011121314151617181920212223242526// Setup Canvas element by appending tag &lt;svg&gt; into &lt;body&gt; section// with width=300 pixels and height= 300 pixelsvar width = 300; //画布的宽度var height = 300; //画布的高度var svg = d3.select(\"body\") //选择文档中的body元素 .append(\"svg\") //添加一个svg元素 .attr(\"width\", width) //设定宽度 .attr(\"height\", height); // Append Rectangle bars to Canvas// Set width of each bar = each dataset value var dataset = [ 250 , 210 , 170 , 130 , 90 ]; var rectHeight = 25; //每个矩形所占的像素高度(包括空白)svg.selectAll(\"rect\") .data(dataset) .enter() .append(\"rect\") .attr(\"x\",50) .attr(\"y\",function(d,i){ return i * rectHeight; }) .attr(\"width\",function(d){ return d; }) .attr(\"height\",rectHeight-2) .attr(\"fill\",\"steelblue\"); Copy all codes above and open our HTML file in browser, we can see something like this. Reference[1] https://wiki.jikexueyuan.com/project/d3wiki/selection.html[2] https://d3js.org/[3] https://www.w3schools.com/","link":"/2020/07/12/JS-tutorial-with-D3js/"},{"title":"Some Useful Linux-commands","text":"Here is some useful linux commands du -h file check the file size pwd show the current directory location locate file find the file name alias name=”…” alias some commands tar -xzvf file.tar.gz -x extract file, -v show extraction process, -f output to given file name, -z using gzip or unzip to extract gz format cat /proc/cpuinfo display information of cpus nvidia-smi -L list the gpus. need to install nvidia first diff a b show difference between a and b files chmod ugo file-nameugo is a oct number used to set the access mode of the file, indicating r (read), w (write), x (executive) mode of the file to different groups of users.For example if ugo = 666, then it means user, group and other groups can read, write the file, but can not execute the file. 12345678910111213permission to: user(u) group(g) other(o) /¯¯¯\\ /¯¯¯\\ /¯¯¯\\octal: 6 6 6binary: 1 1 0 1 1 0 1 1 0what to permit: r w x r w x r w xbinary - 1: enabled, 0: disabledwhat to permit - r: read, w: write, x: executepermission to - user: the owner that create the file/folder group: the users from group that owner is member other: all other users Palmetto Commands: Need to be in login node (name node), not computing node or shell in jupyter Hub checkquota: check my disk space quota module avail : list available packages qstat -xf jobid: check the status of job with id: jobid qstat -Qf queuename : check status of a queue References[1] https://www.runoob.com/linux/linux-comm-tar.html","link":"/2020/11/12/Linux-commands/"},{"title":"ML-K-mean-Cluster","text":"Introduction to K-mean ClusteringIn Chinese, we usually say “物以类聚”， which means somethings in the same class can be grouped together based on their similar attributes. For example, when we group different types of fruit, like apple, cherry, blackberry, together based on their color, then we can group apple and cherry into the same class and the blackberry into another class. In K-mean clustering, it uses the same idea, by assigning different data points to the same cluster center based on the distance between data point and different cluster centers. By grouping data together simply, we can explore the similarity among data points and potential data pattern, which can help us analyze the internal features between data. Note that K-mean clustering is an unsupervised, non-parameter learning method, since it doesn’t use labels created by human or any numerical parameters/ weights like linear, logistic regression to estimate the distribution of data. How K-mean Clustering works?The steps in K-Mean-Clustering are following: Pick the number of clusters K Randomly assign K cluster centers in feature space In the feature space, each data point is assigned to the cluster, whose center is closest to this point, based on distance measurement (Usually Euclidean Distance) In each cluster, re-compute and update the center/mean value of this cluster based on data points in the cluster. Repeat Step 3 to 4 until the cluster of each data point doesn’t change, or called Converged. Here is an image to demonstrate the process of K-mean clustering. Distributed Version of K-mean ClusteringAs K-mean clustering is a simple straight-forward unsupervised learning method, it can be extended to distributed version easily.In distributed version of K-mean Clustering, we have: Partition big data data points evenly into n processes. Mapper Function: Each process uses its local data points to do K-mean clustering. Each process has the same K value Shuffle / Sort K cluster centers from each process. Then we have K * n cluster centers Reducer Function: In the Kn cluster center points, we do K-mean clustering again to find K cluster centers of Kn center points Send the K cluster centers found in step 4 back to each process (then each process has the same K centers) and repeat step 2 to 5 until K centers don’t move (Converge) Properties of K-mean ClusteringAlthough K-Mean Clustering is very easy to compute, it has different advantages and disadvantages. Advantage Computing mean of data is easy and fast Can explore potential similarity on low-dimensional data points Disadvantage Need to specify the K value and we don’t know which K is the best. Cluster Pattern can be affected by scale of data, Since K-mean clustering is distance-based method. If different attributes are not in same scale, the cluster pattern will be distorted and some data points may be assigned to wrong clusterSo K-mean Cluster need normalization of data Cluster is Sensitive to outlier data point (data outside the normal range of cluster) and cluster will shift a lot when computing mean.For example:I have a dataset like [-100, -1, 0, 1, 5,6,7], where -100 is a outlier point as it is pretty far away from other data points. If I have two cluster centers 0, 8, then [-100, -1, 0, 1] will belong to cluster 0 and [5,6, 7] belongs to cluster 8.When I update the centers of two clusters, they become (-100-1+0+1) / 4 = -25, and (5+6+7)/3 = 6. We can see that -25 this center is actually very far away from data points (-1, 0, 1) due to outlier -100. In the next assignment of points, all (-1, 0, 1) points will be assigned to another cluster based on distance. Hence the clustering pattern will be distorted. K-mean Clustering needs to store all data points to compute. It can lead to large space complexity when handling large data. K-mean Clustering can detect Convex patterns only, such as circle, retangle, triangle. But for non-convex patterns, like U-shape, V-shape, patterns, it can not make cluster correctly. Not work directly in high dimensional data and unstructured data like image. When to Use When we want to simply visualize the data distribution, we can use PCA, t-SNE to reduce dimension of data and use K-mean clustering to visualize them When the data is in low dimension Some extension of clustering K-medoid clustering: using median rather than mean to update clustring Hierarchical Clustering: use distance matrix as clustering criteria. Without the need of value K but need terminal state BIRCH CF-Tree: tree-based hierarchical clustering Density clustering DBSCAN OPTICS … Reference[1] https://i.ytimg.com/vi/_aWzGGNrcic/hqdefault.jpg","link":"/2020/11/23/ML-K-mean-Cluster/"},{"title":"ML Model - Regression models","text":"IntroductionThis article is to first introduce types of machine learning models and summarize properties of seven regression models: Linear Regression and Logistic Regression, Lasso Regression, Ridge Regression, Elastic Net Regression and Stepwise regression. Type of machine learning modelParametric/ non-parametric modelsIn general, Machine Learning model can be classified into two types of models based on the critic that if model uses parameters to estimate model or not.In this case, machine learning model can be divided into two types: parametric model and non-parametric model.The following are some useful machine models: parametric: Linear regression, Logistic regression Neural Network non-parametric: K-Nearest neighbor (K-NN) K-Mean Clustering Decision Tree Random Forest Naive Bayesian etc…Note that the “parametric” here is the parameters the model use to estimate the function, distribution. The parameters that control the complexity, performance of model are excluded. For example, K-Mean clustering and K-NN requires us to choose a k value to do clustering/ classification tasks. This k value is not the parameter we consider here. linear/non-linear model Linear modelThe linear model is the model that can be expressed by the formula$y’ = w_0 + w_1x_1 +….+w_nx_n = w_0+ \\sum_{i}^nw_ix_i$where x is the data point from dataset, $w_i$ is the parameters used to adjust the model and $w_0$ is the bias term. $y’$ is the prediction from the estimator.This formula is linear since it just involves first order terms and linear combination.Note that the real label is$y = \\epsilon + w_0+ \\sum_{i}^nw_ix_i$where $\\epsilon$ is the irreducible error between prediction $y’$ and true label $y$. In the following discussion, I discuss $y’$ prediction from model rather than label $y$. Non-linear modelNon-linear model in other word is the model that can not be expressed by the linear formula above. If there are second order term like $x^2, x^3, x_1x_2$, it is non-linear model as wellHence we can easly know that models like Naive Bayesian, K-NN, K-mean clustering, logistic regression, etc are non-linear models Linear RegressionThe formula of Linear Regression is$$y’ = w_0 + w_1x_1 +….+w_nx_n $$where $w_0$ is a constant and $y’$ is the prediction from model. To simplify it, we have: $$y’ = \\sum_{i=1}^nw_ix_i + w_0$$Obviously, the linear regression model is a linear model. Optimize Model Maximum Likelihood Estimation (MLE) Assume the data distribution is normal distribution, then the likelihood function used to estimate the real data distribution is: $$P(y=y_i|x) = \\frac{1}{\\sigma\\sqrt{2\\pi}} e^{-\\frac{(y_i-f(x))^2}{2\\sigma}}$$ where f(x) is the linear regression estimator, estimating the mean of normal distribution $\\mu$. Then the weight solution of the regression model is $$W = argmax_w P(Y|X)$$ Loss Function to minimize The loss function used to update the parameter in Linear regression is mean square error (MSE) $$ MSE = \\frac{1}{N}\\sum_i^N{ (y_i - y’_i)^2 }$$ or sum square error (SSE) $$ SSE = \\sum_i^N{ (y_i - y’_i)^2 }$$ where $y_i$ is the $i^{th}$ label and $y’_i$ is the $i^{th}$ prediction from model Properties of MSE/SSE: SSE / MSE error actually assumes that the distribution of data is Normal distribution and the Linear regression model is actually estimating the mean of Normal distribution with a fixed variance $\\sigma$. The negative log likelihood of normal distribution is as follow: $$ -log(\\Pi_i^nP(y=y_i|x_i)) \\ \\propto -log(e^{\\sum_i^n\\frac{-(y_i-f(x_i)^2)}{2\\sigma}}) \\ \\propto \\sum_i^n(y_i-f(x_i))^2 $$ where $y_i$ is the $i^{th}$ label and $x_i$ is the $i^{th}$ feature vector. It is easy to see that maximize the likelihood function is equivalent to minimize the negative log likelihood function or the SSE /MSE. Hence to minimize the MSE error is actually estimating the normal distribution function $P(Y|X)$. Properties Linear regression is an unbias model, which is sensitive to outliers.For example, In the following linear regression There is an outlier at x= 50, y= 30.During update with gradient descent w = w - 2x(y - wx). The outlier value changes the weight a lot and shift the line away from the $y=4x$. This is because in the update of parameter $w$, each data point is given equal weight to change the parameter $w$.Hence linear regression is sensitive to outliers. The output/prediction from linear regression is to estimate the expected value/ mean of normal distribution Logistic RegressionLogistic regression is an non-linear model as it can not be expressed into the linear form.The formula of Logistic Regression model: $$ P(y=y_i|x) = p_i = \\frac{1}{1+e^{-wx}} $$ Optimize Model Maximum Likelihood Estimation (MLE): we want to maximize the log likelihood function (The distribution estimated by the model) to get close to the real distribution function. In binary classification problem, we assume the data distribution is Bernoulli distribution, $ P(y=y_i|x) = p^{y_i}(1-p)^{1-y_i}$， since this distribution considers the possibility of P(y=0|x) and P(y=1|x). Then the log likelihood function / log Bernoulli distribution is following: $$ logP(y_1,y_2,..y_n| x_1, x_2..x_n) =log(P(y_1|x_1)P(y_2|x_2)..P(y_n|x_n))$$ $$ =\\sum_i^n log(p_i^{y_i}(1-p_i)^{1-y_i})= \\sum_i^n[y_ilog(p_i)+(1-y_i)log(1-p_i)]$$ Since we only care the effect of weight vector and data X to the likelihood function, it can be re-written as $$ \\sum_i^n[y_ilog(h_w(x_i))+(1-y_i)log(1-h_w(x_i))]$$ where $x_i, y_i$ are feature vector and binary label, $h_w(x_i) = p_i$ is the logistic regression, estimating the possiibility that if $x_i$ belongs to class 0 or class 1. The log function here is used to simplify the likelihood function and convert the multiplication into summation. This gives us easier way to analyze it. Maximizing the log likelihood is equivalent to maximize the original likelihood function. Notice that when $p_i$ predicted by the model is as same as its label $y_i$, then $p_i^{y_i}(1-p_i)^{1-y_i}$=1, the likelihood function is maximized. The optimal solution of weight matrix in logistic regression is $$W = argmax_W (\\sum_i^n(y_ilog(h_w(x_i))+(1-y_i)log(1-h_w(x_i)))) $$ Binary Cross-Entropy When we add the negative sign to the log likelihood function, we can get $$ - \\sum_i^n log(p_i^{y_i}(1-p_i)^{1-y_i})= -\\sum_i^n(y_ilog(p_i)+(1-y_i)log(1-p_i)) $$ This form is also called binary cross-entropy. To maximize the likelihood function to real distribution of data, is actually equivalent to minimize the binary cross entropy. Cross entropy is to measure the uncertainty between $y_i$ and $p_i$, $1-y_i$ and $1-p_i$. Higher the entropy is, less similar they are. So binary cross entropy assume the data is in Bernoulli distribution The goal to train the logisitic regression using binary cross-entropy is to let $h_w(x_i)$ get as close to $y_i$ as possible. Properties Logistic function actually projects the range of linear regression into range of [0,1]. Y-axis in logistic regression= possibility that input x belongs to one class, while the linear regression output range is ($-\\infty, +\\infty$). Logisitic regression is an bias model, since it considers the feature values X close to 0 are different and hence the gradient close to x=0 is large. However, when x is far away from x=0, such as $+\\infty, -\\infty $, the gradient of logistic function is close to 0 and it “considers” there is no much difference between two features x1 and x2 when both x1 and x2 $\\to \\infty$. Logistic Regression has vanishing gradient problem. Since the gradient of logistic regression is : $$(\\frac{1}{1+e^{-wx}})’ =\\frac{ e^{-wx}}{(1+e^{-wx})^2} = (\\frac{1}{1+e^{-wx}})(1-\\frac{1}{1+e^{-wx}})$$ when it predicts some sample X as a value very close to 0 or 1, the gradient will be very close to 0 and the weight is hard to update any more. The gradient is actually vanishing in this case. When it think a sample X is class y, then it won’t be willing to change its mind. This is a reason why it is “biased model” as well Logistic Regression is widely used for binary classification problem, it can be regarded as a two-classes version of softmax function. Using Maximum Likelihood estimation requires large data sample size in order to estimate the real distribution. Assumption in Logistic Regression is that the logarithem value of the ratio of possibility of class =1 to possibility of class =0 is linear, which can be estimated by linear regression.Proof:$$ log(\\frac{P(Y=1|X)}{P(Y=0|X)}) = log(\\frac{P(Y=1|X)}{1- P(Y=1|X)})=wx \\$$ $$ (1- P(Y=1|X))*e^{wx} = P(Y=1|X)$$ $$ P(Y=1|X) = \\frac{e^{wx}}{1+e^{wx}} = \\frac{1}{1+e^{-wx}}$$ Logistic Regression is to find a linear separation boundary /a line that separate the data in sample space for classification problem. Since in logistic regression $\\frac{1}{1+e^{-wx}}$, when $-wx&gt;0$, $h_w(x)&gt;0.5$ and it predicts class 1, otherwise, class 0. The line $-wx=0$ in 2-D space shown in the left figure below is actually the decision boundary. The data x with $-wx&gt;0$ is above the line and is classified as class1. Polynomial Regression$$y’ = w_0 + w_1x_1 + w_2x_2^2 +w_3x_3^3+…w_nx_n^n$$where y’, $w_i$, $x_i$ are all scalar values in this case and y’ is the predicted value from model. Different from linear regression, polynomial regression involves higher order terms. Optimize ModelSum square error$$ SSE = \\sum_i^N{ (y_i - y’_i)^2}$$It is as same as the linear regression Properties It is very easy for Polynomial regression to over-fitting as the order increase. It is an biased model as well. It predicts the expected value of Y, which is as same as linear regression To implement polynomial regression, we can first construct polynomial features, like $x_1^2, x_1x_2, x_2^2$, etc. Then apply linear regression to polynomial features Lasso regressionThe prediction model of lasso regression is still linear regression:$$y’=\\sum_{i=1}^nw_ix_i +w_0$$ Optimize Model Loss function to minimize in Lasso regression: $$min \\sum_i^n(y_i - wx_i -w_0)^2 + \\lambda||w||_{1}$$ where $y_i$ is a scalar value, a label, $x_i$ is a feacture column vector, $w$ is the weight vector. $\\sum_i^n(y_i - wx_i)^2$ is the least square error (or called L2 norm distance between y and x). Note that mean square error is scaled version of sum square error. Minimize the sum square error is equivalent to minimize the mean square error, so this term is as same as loss function in Linear regression The second term in the loss function is the L1- regularization term used to reduce overfitting effect, in which $||w||_1 = \\sum_i^n|w_i|$ is the L1 norm format. Principle of L1-Regularization term $$min \\sum_i^n(y_i - wx_i -w_0)^2 + \\lambda||w||_{1} $$ is equivalent to optimize the problem: $$min \\sum_i^n(y_i - wx_i -w_0)^2 \\ \\text{ subject to } ||w||_1 \\leq C $$ where C is a constant. Assume there are two variables in feature vector $x_i$ and only two weight variables, then to visualize it in 2-D space, we have: The x, y axis represent the weight variables. The blue point is the optimal point of the minimum least square error term. The orange region describes the constraints $||w||_1 \\leq C$, hence the intercepts on x, y axis are equal to $C$. The main idea of L1 regularization term is that when the weight vector satisfies the constraint, then the weight vector must be inside the orange region. We need to find the point with the smallest distance to blue point inside this region. When C is small enough such that weight vector can not reach the optimal point, then the model can reduce over-fitting effect. By using lagrange multiplier, the constraint problem can be converted to: $$min \\sum_i^n(y_i - wx_i-w_0)^2 + \\lambda (||w||_{1}-C) $$ where $\\lambda$ is the lagrange multiplier coefficient used to weigh the regularization term and $\\lambda$ is adjusted by user manually. Larger $\\lambda$ is, more regularization term affect. We can see that during minimization process, $C$ and $\\lambda$ are actually constants, so we can ignore this term and get: $$min \\sum_i^n(y_i - wx_i)^2 + \\lambda||w||_{1} $$ Properties It involves L1 regularization term to limit the range of weights and reduce over-fitting effect Lasso regularization can be used to do feature selectionSince in the figure above, we can see when the optimal solution to this constraint problem is at one of the angles of the square, some weight parameters are driven to be 0 and this leads to a sparse model. That is, the model filters out some features in $x_i$. As $\\lambda$ increases, more weights are driven to 0 and more features are not selected. We can not decide which features should be excluded in Lasso regression, since we don’t know which parameters become 0. Feature selection with lasso regression is not stable. Since during training the model, some weights may be very close to 0, but not equal to 0. That is, the features that are expected to be filtered out are actually not filtered out and still affect the model performance.In addition, different initialization of weights may lead to different feature selection. Ridge RegressionThe prediction model of Ridge regression is linear regression as well: $$y = \\sum_{i=1}^nw_ix_i +w_0$$ But the loss function is different. Read the following. Optimize Model Loss function to minimize in Ridge regression: $$min \\sum_i^n(y_i - wx_i -w_0)^2+ \\lambda||w||_{2}^2$$ Ridge regression is similar to Lasso regression, except that the regularization term here use L2 norm distance. Then the constraint region in 2-D space becomes a circle, shown in below. Since the constraint region becomes a circle, it is almost impossible to drive some weights to be zeros unless the blue point (optimal point of least square error) is on the axis. Hence, Ridge regression can not be used for feature selection. Properties Ridge regression can not be used for feature selection Ridge regression can be used to reduce multi-collinearity effect of features and soothe the over-fitting effect.multi-collinearity means that some features have a linear correlated relationship. For example, one feature value increases, another feature value may increase or decrease linearly. If model focuses on the similar features, it may become over-fitting on such feature. The reason is that in the circle region, assume features $x_1$ and $x_2$ are collinear, if $w_1$ increases and the model weigh more on feature $x_1$, then $w_2$ will decrease and model weighs less on feature $x_2$. In this case, model could avoid focusing on learning similar features heavily and reduce the multi-collinearity effect.3. Reduce the variance in model error and reduce over-fitting effect Ridge regression is more stable than Lasso regression for avoiding over-fitting Elastic Net RegressionThe prediction model of Ridge regression is linear regression as well: $$y’ = \\sum_{i=1}^nw_ix_i +w_0$$ Optimize Model Loss function to minimize$$min \\sum_i^n(y_i - wx_i- w_0)^2 + \\lambda_1||w||_{1} + \\lambda_2 ||w||_{2}$$ where $y_i$ is the label, $w$ is the weight vector and $w_0$ is the scalar bias. $x_i$ is the feature vector.Elastic net regression combines L1, L2 regularization terms together. Hence it can be regarded as the combination of weighed Ridge regression and weighed Lasso regression. Properties Combine the advantages of Lasso regression and Ridge regression. Stepwise RegressionIn statistics, stepwise regression is a method of fitting regression models in which the choice of predictive variables is carried out by an automatic procedure.Since there are multiple variable / features in feature vectors, we want to select the most important features to construct our regression model. Different combinations of variables lead to different regression model. Stepwise regression is to select the different feature variables and fit different models and then get the best one with least Residual sum square error Stepwise regression approachesAssume there are p features in dataset to pick. There are several types of approaches in stepwise regression for variable selection. Forward Stepwise Regression /selection Begin with the null model { a model that contains an intercept but no predictors. Fit p simple linear regressions and add to the null model the predictor that results in the lowest RSS. Add to that model the predictor that results in the lowest RSS among all two-predictor models. Continue until some stopping rule is satisfied, for example when all remaining variables have a p-value above some threshold. P-value is to measure how significantly different two models are using statistic technique There are p(p+1)/2 models to evaluate Backward Stepwise Regression (Backward elimination) Start with all predictors in the model. Remove the predictor with the largest p-value { that is, the predictor that is the least statistically significant. The new (p -1 )-predictor model is fit, and the predictor with the largest p-value is removed. Continue until a stopping rule is reached. There are p(p+1)/2 models to evaluate Bidirectional Stepwise Regression combine two methods above to see which predictor should be included or excluded from model Subset SelectionSubset selection is another variable selection method, in addition to stepwise regression Assume there are p features in dataset to pick fit all models for all subset pick the best model with smallest residual sum square error There are $2^p$ models to evaluate SummaryThis article summarizes the seven regression methods and their properties.Key things to note: Linear regression is to estimate the mean of normal distribution MSE assumes data is in normal distribution Logistic regression is an biased model to estimate the possibility that input x belongs to a class. Logistic regression assume data is in Bernoulli distribution. Binary cross entropy assumes data distribution is Bernoulli distribution Lasso Regression (L1 regularization) is good to do feature selection, but could be unstable. It can also reduce the over-fitting effect Ridge Regression can not be used for feature selection, but is good to reduce multi-collinearity effect and avoid overfitting Elastic Net Regression combines Lasso and Ridge regression Stepwise Regression and Subset selection are used to combine and select feature variables to find the estimators with the least SSE error. Reference[1] Stepwise Regression [2] https://www.analyticssteps.com/blogs/7-types-regression-technique-you-should-know-machine-learning [3] https://www.listendata.com/2018/03/regression-analysis.html [4] BIshop-PatternRecognition-MachineLearning.pdf [5] https://strata.uga.edu/8370/rtips/images/outlier.png","link":"/2020/09/30/ML-Model-LR/"},{"title":"Markdown Quick Tutorial","text":"Source File For this tutorial: Click Here Example 1: Markdown Code: 123456_Italic_ *Italic* __Bold__ **Bold**&lt;span style=\"color:blue\"&gt;Blue&lt;/span&gt;_&lt;span style=\"color:red\"&gt;Red and Italic&lt;/span&gt;____&lt;span style=\"color:red\"&gt;Red and Italic and Bold&lt;/span&gt;___**_&lt;span style=\"color:red\"&gt;Red and Italic and Bold&lt;/span&gt;_** Output: Italic ItalicBold BoldBlueRed and ItalicRed and Italic and BoldRed and Italic and Bold Example 2:Markdown Code: 123456789~~Strickout~~&lt;u&gt;Underline&lt;/u&gt;创建脚注格式类似这样 [^Footnote]。[^Footnote]: Here is footnoteoutput \"*\" \\* \\_ Output StrickoutUnderline 创建脚注格式类似这样 [^Footnote]。 [^Footnote]: Here is footnote output ““ \\ _ Example 3Markdown Code: 12345678910111213141516__Here is check list__- [ ] my- [ ] check- [x] list__Here is Emoji__:blush::smile::angry::cry::joy: &gt;Block&gt;1. Block 1&gt;2. Blokc 2 Output Here is check list my check list Here is Emoji:blush::smile::angry::cry::joy: Block Block 1 Blokc 2 Example 4Here is code function() 1Code Block 1int text= C Code Block Col1 Col2 Col3 a b c 左对齐(left) 居中(center) 右对齐(right) 默认左对齐(default) a b c d list 1 list 2 list 3 list 1 list 2 list 3 Example 5Markdown Code: 1234567891011121314151617181920&lt;image src=\"https://gst-online.com/wp-content/uploads/2018/07/16679084-abstract-word-cloud-for-representation-with-related-tags-and-terms.jpg\"&gt;[Here is a link](https://google.com/)[1]: http://static.runoob.com/images/runoob-logo.png&lt;h1&gt;Embeded HTML&lt;/h1&gt;&lt;h2&gt;Hearder&lt;/h2&gt;&lt;h3&gt;Hearder&lt;/h3&gt;使用 &lt;kbd&gt;Ctrl&lt;/kbd&gt;+&lt;kbd&gt;Alt&lt;/kbd&gt;+&lt;kbd&gt;Del&lt;/kbd&gt; 重启电脑## math formula： Need to be loaded on browser$$\\mathbf{V}_1 \\times \\mathbf{V}_2 = \\begin{vmatrix} \\mathbf{i} &amp; \\mathbf{j} &amp; \\mathbf{k} \\\\\\frac{\\partial X}{\\partial u} &amp; \\frac{\\partial Y}{\\partial u} &amp; 0 \\\\\\frac{\\partial X}{\\partial v} &amp; \\frac{\\partial Y}{\\partial v} &amp; 0 \\\\\\end{vmatrix}${$tep1}{\\style{visibility:hidden}{(x+1)(x+1)}}$$ Output Here is a link[1]: http://static.runoob.com/images/runoob-logo.png Embeded HTML Hearder Hearder 使用 Ctrl+Alt+Del 重启电脑 math formula： Need to be loaded on browser$$\\mathbf{V}_1 \\times \\mathbf{V}_2 = \\begin{vmatrix}\\mathbf{i} &amp; \\mathbf{j} &amp; \\mathbf{k} \\\\frac{\\partial X}{\\partial u} &amp; \\frac{\\partial Y}{\\partial u} &amp; 0 \\\\frac{\\partial X}{\\partial v} &amp; \\frac{\\partial Y}{\\partial v} &amp; 0 \\\\end{vmatrix}${$tep1}{\\style{visibility:hidden}{(x+1)(x+1)}}$$ Hello","link":"/2020/07/08/Markdown-Tutorial/"},{"title":"Accuracy Improvement-Ensemble Methods","text":"IntroductionIn our life, we know that the powerful of a single person is much weaker than a group of people. One simple example is tug-of-war. A group of people have stronger power than a single person and much easier to win the competition. Similarly, when making a decision about if a person should be punished, the judgement from a group of jurors is less biased than that from a single juror, since they consider different aspects of the case.In machine learning, ensemble method applies such idea to combine decisions from different models and improve the accuracy. Three common ensemble methods are: bagging, boosting and stacking. Bagging (bootstrap aggregation) Main Idea: Its goal is to reduce variance by using multiple classifiers, like decision tree. It uses boostrap method to sample data and train multiple classifiers and then average the prediction over a collection of classifiers (for continuous value prediction, regression), or return the prediction with maximum votes (for classification) Random forest is a bagging approach, which bags a set of decision trees together. Assumptions we have training set with size of D and a set of models with size of K Training: Similar to Bootstrap, at each iteration i, a training set Di of d tuples is sampled with replacement from training set. A classifier model Mi is learned for each training set Di Prediction: Each Classifier Mi returns prediction for input X. Discrete value output: The bagged classifier counts the votes and assigns the class with the most votes to X Continous value output: take the average value of each prediction for a given test sample. Advantages: Better than a single classifier from the classifier set. More robust in noisy data and hence smaller variance Disadvantages: 1.Its training depends on sampling techniques, which could affect the accuracy The prediction may be not precise, since it uses average value of classifiers’ predictions. For example, if valid prediction values are 1,2,or 3, then the average of predictions from different model could lead to a floating point number. Features of Random Forest Comparable in accuracy to Adaboost, but more robust to errors and outliers. Insensitive to the number of attributes selected for consideration at each split, and faster than boosting Boosting Main Idea: Its goal is to improve accuracy, let models better fit training set. It uses weighted votes from a collection of classifiers Training: Each training sample/tuple is given a weight, eg $w_i$ for the $i^{th}$ tuple. Then we have training set {(X0,y0, w0), … ,(Xi,yi, wi)}where $X_i$ and $y_i$ are training sample and target We have k classifiers {M0, M1,…Mk}. Each classifier is learned from the whole training set iteratively. That is, if we have k classifiers, then we need to iterate the training set at least k times (at $i^{th}$ iteration, we train the $i^{th}$ classifier), so that each classifier can learn the training set. After classifier $M_i$ is learned on training set, classifier $M_{i+1}$ pays more attention to the training samples that are misclassified by $M_i$ Prediction: The final Model combines the votes of each individual classifier. Either find the prediction with largest sum of weights (Classification), or find the average of all prediction values (Regression) The weight of each classifier's vote is a function of its accuracy Advantages Boosting can be extended to numeric prediction Better fit the training set since it adjusts the weights of training set and gives more attention to the misclassified sample. Disadvantages Easy to overfit. Need Additional techniques to avoid overfitting. (I will discuss the methods dealing with Overfitting ). Questions: How to pay more attention to misclassified samples? give Higher weights? But How to compute weights?Answer: This depends on the actual boosting algorithm, like GradientBoosting, AdaBoosting AdaBoosting Assumption: Assume we have training set with size of D and a set of classifier models with size of T __Error of model $M_i$__ Error($M_i$) = $\\sum_i^D (w_i \\times err(X_i))$ if using normalized weight (weight in range [0,1]), then Error($M_i$) = $\\frac{\\sum_i^D (w_i \\times err(X_i))}{(\\sum_j^D w_j)} $ Note: In classification, $err(X_i)= 1(C_i(X_i) !=Y_i)$, $C_i(X_I)$ means the prediction of model $M_i$ on sample $X_i$. If the prediction is correction $error(X_i) =0$, otherwise 1. Weight of model $M_i$’s voting: $\\alpha_i$ $\\alpha_i = log\\frac{1-error(M_i)}{error(M_i)} + log(K-1)$. Note: K = the number of classes in dataset. When K=1, log(K-1) term can be ignored Update of weight $w_i = w_i \\cdot exp(\\alpha_i \\cdot 1(M_j(X_i) != Y_i))$ The weight $w_i$ of the $i^{th}$ training tuple $X_i$ is updated by timing exponential value of weight of model only when this model $M_j$ misclassifies the $X_i$ ( That is $M_j(X_i) !=Y_i$ and hence $1(M_j(X_i) !=Y_i) =1 $). Prediction $C(X_i) = argmax_{k} \\sum_{j=1}^T \\alpha_{m}\\cdot 1(M_j(X_i)== Y_i)$ where $1(M_j(X_i)== Y_i)$ is equal to 1 if prediction is correct, otherwise, 0. The prediction of the whole model has the largest sum of weight of models, NOT the weight of training tuple! More detail for AdaBoosting, Read this paper Stacking Main Idea: It combines and trains a set of heterogeneous classifiers in parallel. It consists of 2-level models: level-0: base model Models fit on the training data and whose predictions are compiled. level-1: Meta-Model It learns how to best combine the predictions of the base models. Training: split the training data into K-folds one of base models is fitted on the K-1 parts and predictions are made for Kth part. Repeat step 2 for each fold Fit the base model on the whole train data set to calculate its performance on the test set. repeat step 2~4 for each base model Predictions from the train set are used as features for the second level model. Classification: Second level model is used to make a prediction on the test set. Advantage: It harness the capabilities of a range of well-performing models on a classification or regression task and make predictions that have better performance than any single model in the ensemble. Disadvantage: It could be computational expensive since it uses k-fold method and use multiple level models. Comparison among Ensembling, boosting and bagging Goal of bagging is to reduce variance and noise while boosting is to improve accuracy using weighted models. Stacking is to improve accuracy of model using hetergenerous models. Adaboost let classifiers pay more attention to the misclassified samples, but if those misclassified samples are outlier or noisy data, it will affect a lot and lead to larger variance. However, bagging and ensemble uses averaging and voting methods and each classifier has equal weight, which is less sensitive to the noise data and outlier. Reference[1] https://blog.csdn.net/weixin_37352167/article/details/85028835[2] https://machinelearningmastery.com/stacking-ensemble-machine-learning-with-python/[3] https://miro.medium.com/max/497/0*Bbf8eeslDtVKog7U.png","link":"/2020/07/23/Model-Acc-Improvement/"},{"title":"Model Evaluation and Selection","text":"IntroductionIn order to do something better in our study, our jobs or even our life, evaluation step is an indispensable part of improvement. Otherwise, How do we know our work is good or bad? In addition, “good” and “bad” are ambiguous terms if we don’t have any evaluation methods. Similarly, in machine learning, in order to training a “better” model, we need concrete evaluation metrics to tell us if our models perform well, so that we can choose the best one. The goal of this passage is to conclude the common useful ways to evaluate and improve our machine learning models. My Thoughts are also provided. Evaluation Metrics Confusion MatrixIn classification task, we have predictions from our model on the test set and ground truth labels/targets indicating the real class of each sample in dataset.Let consider we are predicting if data belongs to class “C” or not. Then Confusion matrix is Actual class\\prediction C not C sum C TP FN actual P= TP+FN Not C FP TN actual N = FP+TN sum predicted P’ = TP+FP predicted N’ = FN+TN All True positive (TP): the amount of samples that are predicted as class “C” and they actually belong to class “C”. False positive (FP): the amount of samples that are predicted as class “C” and they actually DON’T belong to class “C”. False Negative (FN): the amount of samples that are predicted as Not class “C” and they actually belong to class “C”. True Negative (TN): the amount of samples that are predicted as not class “C” and they actually don’t belong to class “C”. Accuracy:measure how accurate the predictions are $acc = \\frac{TP+TN}{TP+TN+FP+FN}$ Error Rate: 1-acc Sensitivity/Recall: (Precentage of correct positive prediction on Actual positive )Recognition rate on True positive:$sens = \\frac{TP}{predicted P’} = \\frac{TP}{TP+FN}$ Specificity:(Precentage of correct negative prediction on Actual negative )How many positive predictions in true data are recognized by model?Recognition rate on True Negative: $spec = \\frac{TN}{predicted N’} = \\frac{TN}{TN+FP}$ Precision: (Percentage of correct positive prediction on all positive prediction) Measure what % of tuples that the classifier labeled as positive are actually positive How many positive predictions of your model are correct? $precision = \\frac{TP}{predicted P’} = \\frac{TP}{TP+FP}$ __F-score: $F_\\beta$__: It weighs precision and recall $$ F_\\beta = \\frac{(1+\\beta^2) * precision*recall}{\\beta^2 * precision+recall}$$ $\\beta$ controls the weight of precision. Higher $\\beta$ weigh more to precision than recall and hence precision is given more attention. F-1 score ($\\beta =1$): $$ F_1 = \\frac{2 * precision * recall}{precision+recall}$$ Both precision and recall are given equal weights. When to use Recall, Sensitivity and F-1 to evaluate model? Sensitivity/Recall:When we want the model to be more sensitive to positive cases and would like to predict more False Positive than False Negative.Example:In disease detection, we would like to use recall more than precision, since we want to detect disease and cure earily, we only care if we get disease or not (Positive or not)Cybersecurity, fault detection. We care if fault occurs/ prediction is positive or not only. Even if more FP than FN are in prediction, it help us reduce the possibility of missing faults (FN) Precision:when we care real positive cases, or both FN and TP. That is, we don’t want the model to predict more FP than FN. False negative also matters.Example:Spam detection: when mailbox detects a spam, it will directly delete/remove that email. However, if that email is actually not a spam (False positive), but an important email, then deleting it leads to an unexpected result.In this case, we want to reduce the false positive cases to avoid deleting an important email. Hence, we don’t use FP, but use FN and precision instead. F-Score:when we care both precision and recall, but want to weigh them.This depends on the assumption in the real cases. If the real case assumes both precision and recall are important, then we use F-score. Score used for GAN:Inception Score, FID. For more details, Please read this article Estimation of Model Accuracy Holdout The purpose of holdout is to evaluate how well the model performs on the unseen dataset. It is used to evaluate the both the framework of model and the hyperparameters of model Process: Split dataset into two independent datasets randomly: training set (usually 80%) and test set (usually 20%) Train the initial model with training set and then test model with test set Repeat steps 1~2 k times and calculate average accuracy of the k accuracies obtained Note: The test sets in k iterations may be repeated Accuracy of model could be unstable due to the split method on dataset.For example, if after randomly spliting dataset into training set and test set, all samples belonging to class “C” are in test set, then model can not learn features from class “C” and hence perform worse. Otherwise, it could perform better if it learn features from class “C” from training set. Cross Validation The purpose of Cross Validation is to evaluate the framework of the model, rather than how good the hyper parameters of the model is. K-fold 1. Split the training set into k-subsets: {D1, D2,..Dk} with (almost) equal size. 2. Loop through k iterations. At $i^th$ iteration, select subset Di as validation set and the remaining k-1 subsets as training set. 3. Compute accuracy of each iteration 4. Compute the mean of accuracies obtained in k iterations. Leave One out It is a case of k-fold cross validation with setting K= N, the number of data point in training set. It hence takes N-1 iterations to evaluate model. Stratified Cross validation It is a case of K-fold cross validation with each setting: each set contains approximately the same percentage of samples of each target class as the complete set. __For example:__ In a dataset S= {A1,A2,A3,B1,B2,B3,C1,C2,C3}, which has classes A,B,C. Each class occupies 30% of total dataset.In Stratified 3-fold Cross-validation, S= {K1,K2,K3}. To make each class of data in each fold have the same percentage, we have: K1 ={A1,B1,C1}, K2 ={A2,B2,C2}, K3 ={A3,B3,C3} such that the percentage of each class of sample in each fold is still 30%. Note for Cross-Validation When K is small, the variance in performance of model could be large, since each fold contains more data and hence becomes noisy. When K is large, eg. in LeaveOneOut, K=N, variance of performance of model would be smaller. In each fold of training set, we need to start training the initial model again, rather than training the model from the last fold. Cross validation is to evaluate the performance of the framework of the model, rather than of the parameters of the model. Bootstrap (Sample-based) Samples the given training tuples uniformly with replacement d times as training set. The remaining data tuples form the test set (or validation set).That is,the tuples/samples that have been chosen can still be chosen with equal possibility Train model with training set Compute average accuracy of model on both training set and test set Repeat steps 1~3 k times, then compute average accuracy of all accuracy obtained in k iterations. Comparison among methods Above holdout Purpose Holdout is mainly used to evaluate how well the model performs on the unseen dataset. It evaluates both framework and hyperparameter of the model. Different from cross-validation, the holdout data can be any size (usually 20% of the training dataset), while cross-validation requires validation data has the same size as each fold. When the amount of fold is small (like 2) in cross-validation, it could waste the training set. Advantages Less expensive in computation, Easy to compute, compared with cross-validation, since it splits test set randomly Disadvantages Estimated accuracy may be unstable (Accuracy is easy to change), since holdout depends on the dataset split methods on training set and test set. When to Use When the dataset is very large and hard to compute multiple subset Cross-validation Advantages The estimated accuracy is much stable than holdout , since it trains model on multiple different train-test set splits. It guarantees to train model with all samples. Disadvantages The Cost for computation is expensive when dataset is very large. When value K is smaller, the variance in performance of model will be larger and model is easier to overfit (training with more data samples could be more noisy). When value K is smaller, it could waste the training set. For example, in 2-fold, we use only half of training set to train the model, which is a waste of training data. When to Use when dataset is not very large (10000 samples or even more) when you have powerful computational device. Bootstrap Advantages Performance of model doesn’t depend on the split method on dataset When dataset is small or insufficient, or imbalanced (some classes are more than other significantly), it may reduce overfitting effect by sampling with replacement Less expensive on computation compared with cross validation Disadvantages Need to determine what sampling method to use When to Use when dataset is small or insufficient (In this case, we may use Over-sampling to repeat some data) when dataset is imbalanced when dataset is pretty large (In this case, we may use down-sampling to select part of data for training) Comparison of performances among different modelHow do we know the performances between models are similar? t-Test / Student’s t test Read this paper Model Selection ROC curve: receiver operating charatistic curve (true positive rate -VS- False positive rate)we first let model output possibility of each class and then set the threshold to convert possibility to 0, 1 binary classification labels.The threshold is usually default as 0.5.Based on the binary classification, compute true positive rate TPR/recall (TP/(TP+FN)) and false positive rate/FPR (FP/(FP+TN) = FP/real negative)Since threshold is set to 0.5, during training of model, the performance of model will change and hence TPR and FPR will change.The changing TPR and FPR leads to the ROC curve AUC: area under curve ( ROC curve)Since TPR and FPR range from 0 to 1, hence AUC has range [0, 1]. AUC usually is inside [0.5, 1] because a good model usually can classify sample correctly with 0.5 possibility. Physical Meaning of AUC:AUC is the possibility that a random-chosen positive sample is ranked more highly than a randomly-chosen negative sample.In other words, AUC is the possibility that a model’s prediction possibility of a random positive sample is more higher than a random negative sample. Then when AUC of a model is larger, it is more likely for the model to predict positive sample as 1 , rather than negative sample as 1. Example: when threshold =0.5 Case 1 Sample A B C D Possibility 0.9 0.8 0.51 0.3 Prediction 1 1 1 0 Ground Truth 1 1 0 0 In case 1 we see that possibility that positive sample is ranked higher than negative sample, since all positive sample has possibility greater than negative sample and TPR = 2/2=1 as FPR = 1/2 =0.5 &lt; TPR. Hence in this case, it is a good model. Case 2 Sample A B C D Possibility 0.9 0.3 0.51 0.6 Prediction 1 0 1 1 Ground Truth 1 1 0 0 In case 2, we see that TPR = 1/2 &lt; FPR= 2/2=1 and AUC is small. The possibility that positive sample ranked higher than negative sample is small. The prediction possibilty of positive sample is also small than negative sample in sample B,D. Hence the model performance is not good enough. Improvement on Accuracy: Ensembling method Bagging (bootstrap aggregation) Main Idea:Its goal is to reduce variance by using multiple classifiers, like decision tree. It uses boostrap method to sample data and train multiple classifiers and then average the prediction over a collection of classifiers (for continuous value prediction, regression), or return the prediction with maximum votes (forclassification)Random forest is a bagging approach, which bags a set of decision trees together. Assumptions we have training set with size of D and a set of models with size of K Training: Similar to Bootstrap, at each iteration i, a training set Di of d tuples is sampled with replacement from training set. A classifier model Mi is learned for each training set Di Prediction: Each Classifier Mi returns prediction for input X. Discrete value output: The bagged classifier counts the votes and assigns the class with the most votes to X Continous value output:take the average value of each prediction for a given test sample. Advantages: Better than a single classifier from the classifier set. More robust in noisy data and hence smaller variance Disadvantages: 1.Its training depends on sampling techniques, which could affect the accuracy The prediction may be not precise, since it uses average value of classifiers’ predictions.For example, if valid prediction values are 1,2,or 3, then the average of predictions from different model could lead to a floating point number. Properties of Random Forest Comparable in accuracy to Adaboost, but more robust to errors and outliers. Insensitive to the number of attributes selected for consideration at each split, and faster than boosting Boosting Main Idea:Its goal is to improve accuracy, let models better fit training set. It uses weighted votes from a collection of classifiers Training: Each training sample/tuple is given a weight, eg $w_i$ for the $i^{th}$ tuple. Then we have training set {(X0,y0, w0), … ,(Xi,yi, wi)}where $X_i$ and $y_i$ are training sample and target We have k classifiers {M0, M1,…Mk}. Each classifier is learned from the whole training set iteratively. That is, if we have k classifiers, then we need to iterate the training set at least k times (at $i^{th}$ iteration, we train the $i^{th}$ classifier), so that each classifier can learn the training set. After classifier $M_i$ is learned on training set, classifier $M_{i+1}$ paies more attention to the training samples that are misclassified by $M_i$ Prediction: The final Model combines the votes of each individual classifier. Either find the prediction with largest sum of weights (Classification), or find the average of all prediction values (Regression) The weight of each classifier’s vote is a function of its accuracy Advantages Boosting can be extended to numeric prediction Better fit the training set since it adjusts the weights of training set and gives more attention to the misclassified sample. Disadvantages Easy to overfit. Need Additional techniques to avoid overfitting. (I will discuss the methods dealing with Overfitting ). Questions: How to pay more attention to misclassified samples? give Higher weights? But How to compute weights?Answer: This depends on the actual boosting algorithm, like GradientBoosting, AdaBoosting AdaBoosting Assumption:Assume we have training set with size of D and a set of classifier models with size of T __Error of model $M_i$__ Error($M_i$) = $\\sum_i^D (w_i \\times err(X_i))$ if using normalized weight (weight in range [0,1]), then Error($M_i$) = $\\frac{\\sum_i^D (w_i \\times err(X_i))}{(\\sum_j^D w_j)} $ Note: In classification, $err(X_i)= 1(C_i(X_i) !=Y_i)$, $C_i(X_I)$ means the prediction of model $M_i$ on sample $X_i$. If the prediction is correction $error(X_i) =0$, otherwise 1. Weight of model $M_i$’s voting: $\\alpha_i$ $\\alpha_i = log\\frac{1-error(M_i)}{error(M_i)} + log(K-1)$. Note: K = the number of classes in dataset. When K=1, log(K-1) term can be ignored Update of weight $w_i = w_i \\cdot exp(\\alpha_i \\cdot 1(M_j(X_i) != Y_i))$ The weight $w_i$ of the $i^{th}$ training tuple $X_i$ is updated by timing exponential value of weight of model only when this model $M_j$ misclassifies the $X_i$ ( That is $M_j(X_i) !=Y_i$ and hence $1(M_j(X_i) !=Y_i) =1 $). Prediction $C(X_i) = argmax_{k} \\sum_{j=1}^T \\alpha_{m}\\cdot 1(M_j(X_i)== Y_i)$ where $1(M_j(X_i)== Y_i)$ is equal to 1 if prediction is correct, otherwise, 0. The prediction of the whole model has the largest sum of weight of models, NOT the weight of training tuple! More detail for AdaBoosting, Read this paper Stacking Main Idea:It combines and trains a set of heterogeneous classifiers in parallel.It consists of 2-level models:level-0: base modelModels fit on the training data and whose predictions are compiled.level-1: Meta-ModelIt learns how to best combine the predictions of the base models. Training: split the training data into K-folds one of base models is fitted on the K-1 parts and predictions are made for Kth part. Repeat step 2 for each fold Fit the base model on the whole train data set to calculate its performance on the test set. repeat step 2~4 for each base model Predictions from the train set are used as features for the second level model. Classification: Second level model is used to make a prediction on the test set. Advantage: It harness the capabilities of a range of well-performing models on a classification or regression task and make predictions that have better performance than any single model in the ensemble. Disadvantage: It could be computational expensive since it uses k-fold method and use multiple level models. Comparison among Ensembling, boosting and bagging Goal of bagging is to reduce variance and noise while boosting is to improve accuracy using weighted models. Stacking is to improve accuracy of model using hetergenerous models. Adaboost let classifiers pay more attention to the misclassified samples, but if those misclassified samples are outlier or noisy data, it will affect a lot and lead to larger variance.However, bagging and ensemble uses averaging and voting methods and each classifier has equal weight, which is less sensitive to the noise data and outlier. Reference[1] https://blog.csdn.net/weixin_37352167/article/details/85028835[2] https://machinelearningmastery.com/stacking-ensemble-machine-learning-with-python/[3] https://en.wikipedia.org/wiki/Student%27s_t-test[4] https://media.springernature.com/lw685/springer-static/image/art%3A10.1038%2Fnmeth.3945/MediaObjects/41592_2016_Article_BFnmeth3945_Fig1_HTML.jpg","link":"/2020/08/13/Model-Eval-Selection/"},{"title":"NLP Word Representations","text":"1. Representation:1.1 WordNeta thesaurus containing lists of synonyms and hpernyms, using “is-a” to denote the relationship among words. Advantages: easy to understand the relationship. It is interpretable. Disadvantages: good as resource, but missing nuance in different context. Eg: “proficient” doesn’t always mean “good” Hard to update with new meanings of words, since the meanings of words may change along time subjective and bias Require Human to label Can’t compute accurate word similarity 1.2 One-hotRepresent word as discrete symbol, eg. 0 or 1Example: In sentence “I like dog”, the vector for “I” =[1,0,0], the vector for “like” =[0,1,0],the vector for “dog” =[0,0,1] word one-hot vector “I” 1 0 0 “like” 0 1 0 “Dog” 0 0 1 Note: Usually, the number of of vocabulary is equal to the number of entry in the one-hot vector. This enables the (Manhatton) distance between any two words/labels is same and the feature of each word is independent with each other.The feature of a word contains no information about another word and hence don’t affect other words. Moreover, this method also has physical meaning and easy to understand.For example:Assume in linear regression,we have $y =w_1x_1 + w_2x_2 + w_3x_3 $, in vector representation it is $Y = [w_1, w_2, w_3] \\cdot [x_1, x_2, x_3]^T = WX$Let vector X be a one-hot vector. That is, one of $x_1, x_2, x_3$ must be one. In this case, $WX$ becomes a lookup table to choose which weight $w_1, w_2, w_3$ should be learned. In this case, weight $w_1$ is affected by feature $x_1$ only, without being affected other features. This can lead to faster convergence of $w_1$ in learning . Advantage: able to represent words into number for computing Easy to understand, since it has physical meaning Features/ labels are independent from each other Disadvantage: Vector dimension equal to the number of words in vocabulary, it could be very big No natural notion of similarity, since word vectors are orthogonal. Hard to extend the representation of labels when more labels are added. Could be memory expensive when a large group of labels involved. Eg, represent vocabulary in a book as one-hot vector, the vectors could be very large and sparse. 1.3 Bag of Words (BOW)Similar to one-hot, it represents each word as orthogonal vector, but the number of vector is the frequency the word appears. In BOW, a text (such as a sentence or a document) is represented as the bag (multiset)/set of its words, disregarding grammar and even word order but keeping multiplicity. Advantage: Easy to realize and compute Easy to use for simple case (the order of words don’t matter a lot) Disadvantage: Without considering similarity among different words it has the same drawbacks as one-hot vector 1.4 n-gram modelN-gram model is an extension of bag of word model. While bag-of-word model considers each word only, N-gram model considers a tuple of n consecutive words as an element in the collection of words. Example: Consider a sentence: “a boy is playing grames”. When using 3-gram model, the collection of words becomes {(a boy is), (boy is playing),(is playing games)} Then if we have a 3-gramm vector for a document: [1, 0,0 ], it means the element (a boy is) appear once in the document. Advantage: More flexible and robust than bag-of-world model, since it could considers different structure of words. Some names like “deep learning”, “machine learning” with more than one words can be detected easier. Disadvantage: The collection could be large when using different grams. Usually, bigram and trigram are used. 1.5 Word Vectors/ Word Embeddings/Word distributed representationsWord Vectors represent words by context. Context of a word is a set of words that appear nearby, or in a fixed window A word meaning is given by the words that frequently appear close-by. Each entry in a word vector for a word is the similarity between this center word and the words in context. Eg: In “I like dog”, vector for “like” = [0.01,0.7 ,0.5], where 0.5 is the possibility “dog” will occur, given center word “like”. More general, given the center word $w_{t}$, the possibility of context word $w_{t+1}$ will occur is $P(w_{t+1}|w_{t} )$. Then in word vector, our goal is to find the best condition distribution to represent the vector. There are many frameworks to find word vector, such as word2Vec, Glove, Fasttext. Advantage: Able to compute similarity between words No need to label by human Disadvantage: Dimension of vector is equal to the number of words in vocabulary. It could be very large Need to learn the similarity by word2vector framework. Hard to compute similarity when there is a large corpus of words 2. Word2Vector: Skip-gramA framework to learn and find dense word vectors, rather than sparse orthogonal vector like bag-of-word model. Here is details about word2vector The dense word vectors measure similarity between two words. If two words are similar, then they have similar word vectors. Goal: to learn the word vector that measure the similarity between context and center word, Or possibility that the next word will occur, given the center word. 2.1 Summary of Idea in Word2Vector: Collect a large corpus of text Represent each word in text as a vector (count vector or one-hot model) Go through each position t in text, find center word c and outside words o (context words) Find similarity of the word vectors for c and o (c and o are actually from the output of the neural network), compute possibility of context o, given center word vector c Keep adjusting word vectors / train the network to maximize possibility/likelihood function and find the optimal word vectors that contains similarity between c and o. Note: input to network is one-hot vector , output to network is similarity/possibility vector. The output is the vector we want 2.2 One-hot Vector In Skip gram, it first find the set of vocabularies and then converts each word into its one-hot vector. In one-hot vector, 1 represents the word Example:consider a sentence “I like dog and you like cat”.Then there is a set of words [“I”,”you” ,”like””,”dog”,”cat”,”and”]. word one-hot vector “I” [1,0,0,0,0,0] “like” [0,1,0,0,0,0] “dog” [0,0,1,0,0,0] “and” [0,0,0,1,0,0] … … Then one-hot vector for “like” = [0,0,1,0,0,0], where 1 at the corresponding position represents “like” in the word set. Dimension of vector = |V| , where V is vocabulary set of the text. 2.3 Skip-gram neural network The neural network takes one-hot vector as input, with size |V|. Let denote one-hot vector as v for convenience There are only one hidden layer. There is no activation function in this layer and hence it is linear. The number of neurons N is defined by user. Softmax activation function for output with |V| neurons There is a input weight matrix W with dimension |V|-by-N between input layer and hidden layer (|V| rows and N columns) There is a input weight matrix W ‘ with dimension N-by-|V| between hidden layer and output layer Since input is 0,1 vector, when it times input matrix W, it actually selects a row of the matrix.Hence, we can consider W as a look-up table and output from hidden layer c = Wv is the real “word vector”. “Word vector” is acutually one-hot vector or bag-of-word vector times input weight matrix W The i^th entry in the output vector from softmax function is the possibility that if you pick up a word nearby the input word, that is the i^th word in the vocabulary. Note: when training neural network, its output is one-hot vector ( set the maximum softmax output value as 1, others as 0s), indicating the predicted nearby context words When evaluating network, using softmax output value, possibility as output, to compute the cost value __Since there are different actual context words $w_{t-1}, w_{t-2}, …$, corresponding to the single input vector $w_{t}$.__If we consider P($w_{t-1},w_{t-2},w_{t-3}|w_t$) with window with size of 4 containing central word $w_t$ and context words $w_{t-1},w_{t-2},w_{t-3}$,then the output look something like this: 2.4 Evaluation of Skip-gramSince the input to the network is a single word, it is also regarded as center word $w_t$, at the t position in text. The words nearby it are called context, or outside words. The context word at t+k position, is denoted as $w_{t+k}$. Hence our goal, or objective is to estimate the distribution $P(w_{t+k}| w_{t})$.The distribution has the meaning that given center word $w_t$, the possibility that context word $w_{t+k}$ will appears. In order to estimate the distribution, we need to maximize the likelihood function, or its log value, called loss, or cost function, shown as below. Advantages It is unsupervised learning hence can work on any raw text given. It requires less memory comparing with other words to vector representations. It requires two weight matrix of dimension [N, |v|] each instead of [|v|, |v|]. And usually, N is around 300 while |v| is in millions. So, we can see the advantage of using this algorithm. Disadvantages Finding the best value for N and the context position is difficult. Softmax function is computationally expensive. The time complexity for training is high Models for Word2vector Skip-gramPredict context (outside) words (surrounding the center word) given centerword Continuous Bag of words （CBOW）Predict center word from (bag of) context words. It is an inverse version of skip-gram. Reference:[1] http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/[2] http://web.stanford.edu/class/cs224n/slides/cs224n-2020-lecture02-wordvecs2.pdf[3] https://towardsdatascience.com/skip-gram-nlp-context-words-prediction-algorithm-5bbf34f84e0c[4] https://zhuanlan.zhihu.com/p/50243702[5] http://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf","link":"/2020/07/07/NLP-Word-Representation/"},{"title":"NLP Improvement on Word2Vector","text":"IntroductionAlthough word2Vec for word embedding has been widely used, it has some obvious shortages that affect computation of word vector. This passage is to identify those shortages and introduce a solution called Negative Sampling to solve the problems. Review to Word2VecWord2Vec is a framework to convert vocabulary in texts into dense numeric vector representation such that our machine learning models can realize the vocabulary and human language and learn something.The main idea of word2Vec (skip-gram) is following: Start with random word vectors in neural network Iterate each world in word corpus Predict the context words (surrounding words) of a center word by computing posterior distribution: $P(w_{t+1}|w_t)$, where $w_t$ is the center word at position t and $w_{t+1}$ is the surrounding word. $$P(o|c) = \\frac{exp(\\textbf{u}_o^T\\textbf{v}c )}{\\sum{w\\in V} exp(\\textbf{u}_w^T\\textbf{v}_c)}$$ where $\\textbf{v}_c$ is the word vector of center word and $\\textbf{u}_o$ is the word vector of surrounding words (or weights). Update word vector using Gradient Descent based on cost function Note: $\\theta$ in loss function here is the weight matrix used in softmax. The weight matrix in the network is $\\textbf{u}_w$. Disadvantages of Word2VectorWe can notice that bigger vocabulary it is, larger the word vector becomes. Usually, there are thousands of different words in text, using gradient descent to update the whole weight matrix leads to expensive computation cost and each update become super slow. We need to repeat updating each weight using the following equation and the time complexity increases linearly as the amount of words increases. ImprovementStochastic Gradient Descent with sampled windowAssume we are using a window centered at center word and hence it has size of 2m+1. The the update is Repeatedly sample windows and iterate each window, rather than iterate all windows. Compute the gradient of the words that actually appear. The graident of words in dictionary that don’t appear in text won’t be updated. Notes: the gradient of the words that don’t appear in text, but in vector is all 0. In this case the vector would be very sparse (many zeros in vectors). It is a waste of time to compute those 0 update. We need a sampling technique to sample windows and words for updating part of weights. This leads to our next section Negative Sampling Negtive SamplingIn word2vec (skip-gram), the input to the neural network is one-hot word vector of the center word. In training step, output of neural network is a vector of possibility that each word can appear given the center word.In prediction step, the output is converted from possibility vector to one-hot vectors of the predcited context words that are most likely to appear given the center word. For example, if we have possibility output [0.1, 0.1, 0.5, 0.3] and expect predictions of 2 context words. Then we output one-hot vectors of the words with possibility of 0.5 and 0.3.The targets corresponding to the given center word are one-hot vector of context words surrounding this center word in the window. Negative wordsIn one-hot vector output from neural network, we call the word with value equal to 1 as postive word and those words with values equal to 0 as negative word. For example. Assume we have a word vector with dimension of 100 (100 words in dictionary). Then in a window such as “I like my dog”, I like my dog if “like” is center word $w_t$, as the input to the neural network. Then “I” , “my”, “dog” are context words, or positive words that are expected to output “1” in the output one-hot vector of neural network(it is expected to output 3 one-hot vectors). Then other words that don’t appear in this window / context and are expected to be 0 in one-hot vector are negative words. Selection of negative wordsHowever, there would be a large amount of negative words that don’t appear in context. If we update weights of all negative words, the update could be very inefficient.In negative sampling, we sample negative words based on the possiblity that word may occur. The possibility is given by: $$P(w_i)= \\frac{f(w_i)}{\\sum_j^Nf(w_j)}$$ where $f(w_i)$ is the number of word $w_i$ appears in corpus and the denominator is the number of all words appear/ the amount of words in corpus. However, the author proposes this equation since it gives the best performance. $$P(w_i)= \\frac{f(w_i)^{3/4}}{\\sum_j^Nf(w_j)^{3/4}}$$ Based on the possibility of the occurence of words, we can sample a number of negative words that are most likely to appear and update the corresponding weights by backward update in neural network. The number of samples is set by user.For example, If we have a dictionary with size of 100 words (hence one-hot vector with size of 100) and each word has corresponding weights with size of 100. Then there are 100x100 weights to update when updating all weights in network. However, if we only sample 20 negative words to update, we need to update 20x100 weights only. This allows us to speed up training step. References[1] http://mccormickml.com/2017/01/11/word2vec-tutorial-part-2-negative-sampling/ [2] https://arxiv.org/pdf/1310.4546.pdf","link":"/2020/07/29/NLP-Word2Vec-Improvement/"},{"title":"PySpark-Note-1","text":"IntroductionThis note is to introduce some useful functions in PySpark and also do some practice with them to analyze SF crime dataset. Then KMean Clustering model is applied to show how to use ML model in PySpark. List of Useful Functions Create Spark Session SparkSession Create spark session, the main entry to create DataFrame, register DataFrame as tables, execute SQL over tables, cache tables, and read parquet files. 123456from pyspark.sql import SparkSessionspark = SparkSession.builder \\.master(\"local\") \\.appName(\"Word Count\") \\.config(\"spark.some.config.option\", \"some-value\") \\.getOrCreate() Explanation: .master(‘local’): sets the Spark master URL to connect to. “local” mean run spark locally. Just like a local server .appName(“Word Count”) : application name .config(“spark.some.config.option”, “some-value”): configure some key-value pair in application .getOrCreate(): Gets an existing SparkSession or, if there is no existing one, creates a new one SQLContext As of Spark 2.0, this is replaced by SparkSession. However, this class is kept here for backward compatibility. A SQLContext can be used create DataFrame, register DataFrame as tables, execute SQL over tables, cache tables,and read parquet files. Load data After we create a SparkSession, we can use the following code to read data from CSV or JSON file. Returned Object: PySpark DataFrame object csv12345678# Set delimiter = \";\" between columns# Skip header of file if header=True. # Otherwise, load header as data recordspark.read.options(header= True,delimiter=\";\") \\.csv(\"path-to-dataset/dataset.csv\")# orspark.read.options(header= True,delimiter=\";\") \\.format('csv').load(\"path-to-dataset/dataset.csv\") json123spark.read.json(\"path-to-dataset/dataset.json\")#orspark.read.format('json').load(\"path-to-dataset/dataset.json\") Useful functions of pyspark.sql.DataFrame with SQL Create DataFrame and SQL table df.createDataFrame(data, schema=None): create PySpark dataframe data: a list of tuples,each tuple contains the data of a row schema: a list of column names of dataframe 12345spark = SparkSession.builder\\.master(\"local\").appName(\"Word Count\")\\.config(\"spark.some.config.option\", \"some-value\").getOrCreate()l1 = [('Alice', 1)]new_row1 = spark.createDataFrame(l1,[\"name\",\"age\"]) df.createGlobalTempView(view_name) Creates a global temporary view with this DataFrame. df.createOrReplaceTempView(view_name): Creates or replaces a local temporary view with this DataFrame. df.registerDataFrameAsTable(table_name): Registers the given DataFrame as a temporary table in the catalog. Show Columns, Rows, Statistic description df.columns: a list of column names df.summary(): Computes specified statistics for numeric and string columns, with count - mean - stddev - min - max. df.describe(): Computes basic statistics for numeric and string columns. Similar to summary() df.printSchema(): print Schema / decriptions of each column, like data type Query and Selection df.head(10): return the top 10 rows in Row type, not DataFrame df.tail(10):return the last 10 rows in Row type, not DataFrame df.where(condition) or df.filter(condition) : select the rows which satisfy the conditions. Return a DataFrame 12345678# Select ages that are &lt; 100 and City name = \"SF\"# Using SQL expression in conditionsdf.where(\"age &lt; 100 and city in ('SF') \")#Using Spark APIfrom pyspark.sql import Rowfrom pyspark.sql.functions import coldf.where(col(\"age\")&lt;100 &amp; col('city').isin([\"SF\"])) df.column.isin([‘a’,’b’])Check if the value in a column is in the list or not df.column.between(lower_bound, upper_bound)check if the value is within a range or not df.select([“column-name”]):select the columns based on a list of given column-names df.take(N):Return a list the top N rows in Row() format df.collect():convert pyspark dataframe to a list of row in Row() format Handle data df.withColumn(‘column_name’, col):append a column to dataframe with name “column_name” col: a Column expression for the new column. we can use UDF (user defined function) to it as well. 1df = df.withColumn(\"age\", df.age+2) df.withColumnRenamed(“old name”,”new name”):rename a column in the dataframe df.drop([“column-name”]), df.dropna(subset =[“column-name”])):drop columns and drop the rows with NaN in selected columns df.dropDuplicates([“column-name-to-remove-duplicated-value”]):drop duplicated rows in selected coumns df.fillna(), df.fill_duplicates(): fill na with given values df.spark.sql.Row(age=10, name=’A’):Return a row with elements: age=10, name=’A’ df.orderBy([“column-name”], ascending=False), df.sortBy():orderBy is an alias of sortBy, they sort the dataframe along given column names df.groupby().agg() / df.groupby().count()Apply aggregation function, such as count() to a group 123# First select group based on column A, B. Then count the amount of group \"A\"from pyspark.sql import functions as Fdf.groupby([\"A\"]).agg(F.count(df.A)) Append New Rows df.union():Return a new DataFrame containing union of rows in this and another DataFrame, based on position. df.unionByName([“colunm-name”])The difference between this function and union() is that this function resolves columns by name (not by position) 12new_row = spark.createDataFrame([(\"A\",1)],[\"name\",\"count\"])df = df.union() SQL df = SparkSession.sql(“select * from table”) Display Data df.show(n) Data Types Convertion with Pandas df.toPandas()convert dataframe to pandas dataframe df.toDF()convert a list of Rows to PySpark dataframe Convert Pandas DataFrame to PySpark DataFrame using Schema 12345from pyspark.sql.types import *mySchema = StructType([ StructField(\"col name1\", IntegerType(), True)\\ ,StructField(\"col name2\", StringType(), True)\\ ,StructField(\"col name3\", IntegerType(), True)])spark_df = spark.createDataFrame(df, mySchema) Using Resilient distributed Dataset (RDD) RDD represents an immutable, partitioned collection of elements that can be operated on in parallel. Please refer to the official website about RDD Practice with Example: SF Crime data Requirements: Find a platform for distributed computing, like databricks Install PySpark Download SF Crime Data for Demo 12345import requestsr = requests.get(\"https://data.sfgov.org/api/views/tmnf-yvry/rows.csv?accessType=DOWNLOAD\")with open(\"sf_crime.csv\",\"w\") as f f.write(r.content) f.close() Load Data with PySpark 12345678910111213from pyspark.sql import SparkSessionfrom pyspark.sql.functions import to_date, to_timestamp, hourfrom pyspark.sql.functions import year, month, dayofmonth, date_formatfrom pyspark.sql.functions import from_unixtime, unix_timestamp# create SparkSession entry to handle dataspark = SparkSession \\ .builder \\ .appName(\"crime analysis\") \\ .config(\"spark.some.config.option\", \"some-value\") \\ .getOrCreate()# load csv datadf_opt1 = spark.read.option(\"header\", \"true\").csv(data_path) Visualize Data 1234567891011# Create a view called \"sf_crime\" to the dataframe, so that we can use SQL# to query data laterdf_opt1.createOrReplaceTempView('sf_crime')# if using databricks, we can use display function to see the datadisplay(df_opt1 )# ordf_opt1.show()# or # show the schema of dataframedf_opt1.printSchema() Clean DataChange the string data type to date type, integer type and other suitable data type 12345678910# convert data type and replace datadf_opt1 = df_opt1.withColumn('Hour', hour(df_opt1['Time']))# convert string to date type using unix_timestampdf_opt1 = df_opt1.withColumn(\"Date\", to_date( from_unixtime(unix_timestamp(df_opt1['Date'], 'MM/dd/yyy'))))df_opt1 = df_opt1.withColumn(\"Year\", year(df_opt1.Date))df_opt1 = df_opt1.withColumn(\"Month\", month(df_opt1.Date))df_opt1 = df_opt1.withColumn(\"Day\", dayofmonth(df_opt1.Date))df_opt1 = df_opt1.withColumn('HasCriminal', (df_opt1[\"category\"]!=\"NON-CRIMINAL\"))df_opt1 = df_opt1.withColumn(\"X\", df_opt1[\"X\"].cast(\"double\"))df_opt1 = df_opt1.withColumn(\"Y\", df_opt1[\"Y\"].cast(\"double\")) Query and Select data I’m interested inI want to analyze the count of crime of each category here, so there are two ways to do this. 12345678910# Display Count of cime using SQLcrimeCategory = spark.sql(\"SELECT category, COUNT(*) AS crime_counts FROM sf_crime GROUP BY category ORDER BY crime_counts DESC\")crimes_pd_df = crimeCategory.toPandas()display(crimeCategory)# Display Count of crime of each category Using PySparkcrime_category_df = df_opt1.groupby('category').count().orderBy('count',ascending=False)crime_category_df = crime_category_df.withColumnRenamed('count', 'crime_counts')crime_category_df = crime_category_df.toPandas()display(crime_category_df) Result Advance Topic: Machine Learning ModelUsing KMean Clustering to find the 5 centers in which crimes occur frequently Select Position data X,Y and then Use Interquantile Range method to find outliers of positionSince KMean Clustering is sensitive to the outlier as it uses mean method to find the center of clusters, we need to remove outliers first. Quantile Based method to remove outlier: The outlier is defined as the data point that drop outside the range [Q1-1.5IQR , Q3+1.5IQR],where Q1 and Q3 are the first and third quantile of dataset and IQR = Q3-Q1 is the interquantile range. API in PySpark to find quantile: df.approxQuantile(col, probabilities, relativeError):col: column to find quantileprobabilities: a list of quantile probabilities we want to find. Here I want to find Q1 =0.25 and Q3=0.75return: the a list values that correspond to the quantile in probabilities list. 1234567891011121314151617# select the positions where crimes occurcrime_cluster_df = df_opt1.where(\"hasCriminal =true\").select([\"Hour\", \"PdDistrict\", \"X\",\"Y\", \"DayOfWeek\", \"category\",\"Resolution\",\"hasCriminal\"])# crime_cluster_df.show()#Find the Q1, Q 3 Quantilebounds = { c: dict( zip([\"q1\", \"q3\"], crime_cluster_df.approxQuantile(c, [0.25, 0.75], 0)) ) for c in crime_cluster_df.select([\"X\",\"Y\"]).columns}# compute lower bound and upper bound of normal datafor c in bounds: iqr = bounds[c]['q3'] - bounds[c]['q1'] bounds[c]['lower'] = bounds[c]['q1'] - (iqr * 1.5) bounds[c]['upper'] = bounds[c]['q3'] + (iqr * 1.5) Remove Outliers based on upper bound and lower bound of quantile 1234from pyspark.sql.functions import *crime_cluster_df = crime_cluster_df.select([\"X\",\"Y\"])\\.where(col(\"X\").between(bounds['X']['lower'], bounds['X']['upper'])) \\.where(col(\"Y\").between(bounds['Y']['lower'], bounds['Y']['upper'])) Assemble columns into one feature column before trainingIn PySpark, we need to put all feature columns into one single feature column before we train the model. VectorAssembler provides a way to assemble those features into one column. 1234# ensemble multiple columns into one single feature column for training KMean Clusteringfrom pyspark.ml.feature import VectorAssemblervecAssembler = VectorAssembler(inputCols=[\"X\", \"Y\"], outputCol=\"features\")crime_cluster_df = vecAssembler.transform(crime_cluster_df.select([\"X\",\"Y\"])) KMean Clustering to learn dataIn Machine Learning of PySpark, we need to set the name of feature column to “features”, otherwise, set featuresCol=&quot;column-name&quot; to select which feature column to learn in dataframe 123456789101112# Training KMean clusteringfrom pyspark.ml.clustering import BisectingKMeansK=5bkm = BisectingKMeans(k=K, minDivisibleClusterSize=1.0)model = bkm.fit(crime_cluster_df)# predict at one single point# print(model.predict(crime_cluster_df.head().features))# predict clusters# Output the prediction to the column called \"Prediction\"model.setPredictionCol(\"Prediction\")transformed = model.transform(crime_cluster_df).select(\"X\",\"Y\", \"Prediction\") Result SummaryThis tutorial introduce: List of useful PySpark functions and their basic usage Use SF Crime dataset as a demo to see how to use PySpark to manipulate data and visualize them How to use machine learning model: KMean Clustering in PySpark to learn data. Note: the APIs of ML in PySpark are different from sklearn. Need to Pay attention to the difference. Reference[1] SparkReader,Writer[2] PySpark,SQL_module[3] PySpark,ML_module[4] PySpark,outlier_detection[5] logo","link":"/2020/10/12/PySpark-Note-1/"},{"title":"Recommendation-System-1- Collaborative Filtering and Content-based Filtering","text":"Introduction to Recommendation systemIn recommendation system, we want to let it give recommendations of items to users based on users’ visit history or other data.if we represent the items and users in a table, it would be something like this: users\\items item 1 item 2 item 3 user 1 1 - - user 2 - 1 - user 3 - - 1 user 4 1 1 - In this table, each row represent a user and column represent an item. The cell with value 1 means that user buys the corresponding item. For example, the cell of user1 - item 1 =1 means user 1 buys item 1. Then recommendation system is to predict if the empty cells in the table will become 1 or not. Or predict if that user will buy the item or not. Building such recommendation system can help encourage users to consume items by recommending somethings attracting them and let companies make profits Collaborative Filtering is one type of common methods in constructing recommendation system. Its main idea is to find the similarity between users or between items to rank the items and then pick and recommend the top K items to users.In content-based filtering, it utilizes the content features of items and the contents users may like to compute similarity between item and user and make recommendation. There are three common ways to do recommendation user-based It first computes similarity between users, then pick top K items that the most similar user like to the current user Item-based It recommends new items to current user based on the similarity between new items and items that user has brought before content-based It computes the similarity between user and item based on the tags/topics the user like and the tags/topics that item contains. Then it recommends the most similar item to the user. Cold Start ProblemIn recommendation system, we also face a problem that when a new user or a new item comes to the platform, we don’t have enough information (like the preference of new users, tags/contents of new items) about them to make recommendation. Hence Cold start problem mainly can be divided into two types: Cold start of user: We don’t have enough information about new users and can not determine the preference of users to do recommendation Cold start of item: We don’t have enough information of new items and don’t know the tags and contents those items may have User-based filtering (similarity between users)The main idea in user-based filtering is that we first find the similarity between each two users based on the item vectors of the two users.Then pick the user who is most similar to the current user. In the item list of the selected user, we pick the K top frequent visited items to the current user. How it works AssumptionAssume we have a user-item table users\\items item 1 item 2 item 3 item 4 user 1 1 - 1 1 user 2 1 1 1 - user 3 - - 1 1 user 4 1 1 - 1 where 1 represent the ratign from the user to the item the user purchase, - mean the user doesn’t buy the item. Denote the $i^{th}$ user and the $j^{th}$ user as ui, uj, respectively and N(ui), N(uj) are the set of items the $i^{th}$ user purchased and the set of items $j^{th}$ user purchased respectively. |N(ui)| is the number of items the $i^{th}$ user purchased. Step 1: Compute Similarity between a pair of usersIn order to compute the similarity between a pair of users, we need a way to measure the distance between users.One common way is cosine distance, denoted as cos(ui, uj): $$cos(ui, uj) = \\frac{ |N(ui) \\cap N(uj) |}{\\sqrt{|N(ui)| \\times|N(uj)|} }$$ where $|N(ui) \\cap N(uj)|$ means the size of the intersection set between the set of items purchased by ui and the set of items purchased by uj. Example:In the table above, we want to compute cos(u1, u2), then we can see the intersection of the items from u1 and items from u2 is [item1, item3], so $|N(ui) \\cap N(uj)|=2$. Both u1 and u2 users purchase 3 items, so $|N(u1)|=|N(u2)|=3$ Then similarity between user 1 and user 2 is:$$cos(u1, u2) = \\frac{ 2}{\\sqrt{3 \\times 3} }$$ Based on this, we can compute similarity between every pair of users in table.There are other choices to compute similarity, such as Euclidean distance, dot product, Pearson’s Correlation. Step 2: Predict the interest of the $i^{th}$ user, ui, on the $x^{th}$ item, ix In this step, we first pick the top K users who are most similar to ui. Then We pick the users who rated the item ix from the K users.Finally we compute the estimated rating of ui on item ix using user similarity and ratings from those users. Estimated rating of ui on item ix is $$R(ui, ix) = \\sum_{v\\in M} { cos(ui, v) \\times R(v, ix)}$$ where M is the intersection between the set of the top K similar users and users who rated the item ix. The $R(v, ix)$ is the rating of user v on item ix. Note that we can also use K-Nearest Neighbor method, rather than cosine similarity to find the top K simiar users as well. In this case, we may use Euclidean distance or other distance to measure similarity. Properties Advantages Easy to implement It is good to explore group interests on items, since it measures similarity between users in a group Good to use this method when the update of incoming new items is faster than the update of incoming new users(Computing similarity between new user and other existing users is time expensive when there is millions of users. But updating similarity after adding new items is much easy, we just need to update N(ui)) Disadvantages Sparsity. The percentage of rating from users is low User-based collaborative filtering is also a memory-based mehtod, since it requires memory to store similarity between users. when update frequency of user is faster than the update frequency of items, it is time-expensive, especially there are millions of users Cold start problem. When there is no sufficient information about users or item, it is hard to estimate similarity efficiently update is slow when there are large amount of users or items When to use when we want to explore group interests or similarity between users in a group The number of users is smaller than the number of items. Or Incoming new users amount is smaller than incoming new items amount. Item-based filtering (similarity between items)The main idea in Item-based is to compute the similarity between new item and the items purchased by users to predict if users will buy the new item. How it worksUsing the user-item table above users\\items item 1 item 2 item 3 item 4 user 1 1 - 1 1 user 2 1 1 1 - user 3 - - 1 1 user 4 1 1 - 1 Denote the $i^{th}$ item as xi. M(xi) is the set of users who purchased item xi and |M(xi)| is the size of this set Step 1: Compute similarity between a pair of itemsThe cosine similarity of item xi and item xj is similarity to the similarity between users: $$cos(xi, xj) = \\frac{ |M(xi) \\cap M(xj) |}{\\sqrt{|M(xi)| \\times|M(xj)|} }$$ In this example, $cos(x1, x2) = \\frac{ |M(x1) \\cap M(x2) |}{\\sqrt{|M(x1)| \\times|M(x2)|} } = \\frac{2}{\\sqrt{3\\times 2} } $ Step 2: Predict Rating of user ui on item xiWe pick the K items which are most similar to item xi based on computed item similarity. Then find the intersection Q between these K items and the set of items user ui purchased.Then estimated rating of user ui on item xi is $$R(ui, xi) = \\sum_{y \\in Q} { cos(xi, y) \\times R(ui, y)}$$ where Q is the intersection set between K items and the set of items user ui purchased. y is item from Q. $R(ui, y)$ is rating of user ui on item y. Properties Advantages Item-based Collaborative filtering is good for personalized recommendation, since it is based on similarity of items user purchased, which shows user’s preference information. Good to use this method when there are a large amount of users. Easy to compute as well Good Explainability Disadvantages It is memory-based model as well. Hence it needs to store information of items, users. If there are millions of items, the computation is expensive Cold Start problem. For new item, it is hard for us to compute similarity between items since there is lack of users using the new item Sparsity. The number of users using new items could be small and it is hard to compute similarity Time complexity is high when there are millions of items and users When to use when we want to make personalized recommendations to users when the amount of items is not pretty large. Or the update frequency of items is smaller than update frequency of users’ information. When we have enough information about items and cold start effect is small. Content-based filtering (based on tags/content of items)Let’s consider that there are some tags/contents in items and users may prefer some tags/contents and hence like the items that contain such tags/contents. Based on this setting, we can construct an item vector $v_i$ and an user vector $u_i$. $v_i$ and $u_i$ have the same shape Each entry in the vector represents whether this item contains the corresponding tags/contents, or whether this user like or dislike the corresponding content / tag. The user vector is given by this formula:$$u_i = \\frac{1}{n}\\sum_i^nu_i - \\frac{1}{m}\\sum_j^mu_j$$ where $u_i$ is the item vector that user likes and $u_j$ is the item vector that user dislikes. Each entry in user vector is the difference between the level of prefering this contents and the level of disliking this contents. If entry is negative, then user may dislike that content. After computing the user vector, we can compute the similarity between user vector and item vectors to see which item is most likely to be purchased by user based on user’s interests. The similarity can be computed by dot product similarity: $$similarity(ui, vi) = &lt;ui, vi&gt;$$ where &lt;ui, vi&gt; is dot product of ui, vi. Note that for ui, vi, we don’t use normalization here, since when there are large amount of contents but item has only a few contents, the vector will be very sparse. Normalization in a sparse vector will make the non-zero values pretty small and even close to 0, which make computation difficult and may loss precision. Steps in content-based filtering are as follow: Compute item-content vectors and user vectors Compute similarity between user and items Rank items to recommend Examplein a item-content table: item tag1 tag2 tag3 v1 1 0 1 v2 1 0 0 v3 0 1 0 v4 0 1 1 in a user-item table: user item1 item2 item3 item4 u1 1 1 -1 -1 Step1: compute item vectors and user vectorin item-content table, 1 represents item vi contains this tag/content, otherwise, it doesnt. In user-item table, 1 indicates this user like this item and -1 means user dislike this item.Hence item vectors are v1 = [1, 0, 1], v2 = [1, 0, 0], v3= [0, 1, 0], v4=[0, 1, 1]Then user vector u1 = $\\frac{v1+v2}{2} - \\frac{v3+v4}{2}$ = [1, 0, 0.5] - [0, 1, 0.5] = [1, -1, 0]. In this case, we can see user1 like tag1 and dislike tag2 and be neutral about tag3. Step2: compute similarity between user and itemsSim(u1, v1) = &lt;u1, v1&gt; = 1 * 1 + 0 * (-1) + 1 * 0 = 1Sim(u1, v2) = &lt;u1, v2&gt; = 1 * 1 + 0 * (-1) + 0 * 0 = 1Sim(u1, v3) = &lt;u1, v3&gt; = 0 * 1 + 1 * (-1) + 0 * 0 = -1Sim(u1, v4) = &lt;u1, v4&gt; = 0 * 1 + 1 * (-1) + 1 * 0 = -1 Step3: ranking and recommenduser likes v1, v2 and dislikes v3, v4. Update of user vectorSince user vector is just the difference between the average of like item vectors and the average of dislike item vectors. When the $k^{th}$ item comes, we can update either the like vector or dislike vector to update the user vector. Let the average of K1 item vectors that user likes as $$u^+_{k-1}= \\frac{1}{k1-1}\\sum^{k1-1}_iv_i$$ the average of K2 item vectors that user dislikes as $$u^-_{k-1} = \\frac{1}{k2-1}\\sum^{k2-1}_jv_j$$ and K1 + K2 = k-1 user vector for k-1 items is $$u_{k-1} = u_{k-1}^+ -u_{k-1}^-$$ When the $K^{th}$ item comes, if user like this item, then we update like vector $$u_{k}^+ = \\frac{(k1-1)u_{k-1}^+ + v_k }{k1} , u_{k}^- = u_{k-1}^-$$ if user dislikes the item, just update dislike vector $$u_{k}^- = \\frac{(k2-1)u_{k-1}^- + v_k }{k2} , u_{k}^+ = u_{k-1}^+$$ This enable us to update the user vector on the fly. Properties Advantages Easy and very fast to compute similarity even when there are large amount of items or users. Utilize the content information to explore potential contents that user likes Compared with Item-based and user-based method, it is less sensitive to Cold Start problem, since contents of item can be defined easily. Disadvantages Sparsity of item vectors. Item vector could be sparse when there are a lot of contents and content follow long-tail distribution (Note long-tail distribution of feature/item can also lead to the sparsity) Very easy to converge to certain scope. Since content/tag could be long-tail distribution and most of items have similar content, then a small change in the item vector will lead to large change of similarity.For example, a user vector u= [100, 0, 1, -100], due to long-tail distribution of contents, the values between contents are quick different. For item vectors v1 = [1, 0, 0, 0 ] and v2= [0, 0, 1, 0], sim(u1,v1) and sim(u1,v2) would be very different. When to useWhen we have more information about items and want to use those contents in recommendation. Reference[1] https://medium.com/@cfpinela/recommender-systems-user-based-and-item-based-collaborative-filtering-5d5f375a127f [2] https://zh.wikipedia.org/wiki/%E5%8D%94%E5%90%8C%E9%81%8E%E6%BF%BE [3] https://ars.els-cdn.com/content/image/1-s2.0-S1110866515000341-gr3.jpg [4] https://towardsdatascience.com/introduction-to-recommender-systems-1-971bd274f421","link":"/2020/12/15/Recommendation-System-1/"},{"title":"Recommendation-System-2-WideDeep","text":"Background2016 年Google通过用wide&amp; Deep+ FTRL 优化器对数据稀疏性sparsity和特征组合学习的问进行研究原文： In the experiments, we used Follow- the-regularized-leader (FTRL) algorithm with L1 regularization as the optimizer for the wide part of the model, and AdaGrad for the deep part. 来自 https://zhuanlan.zhihu.com/p/142958834 Motivation目的 CTR预估任务里面需要精细的特征工程对特征进行组合使模型对出现频率高的特征进行学习。但问题是特征工程需要花费时间长而且麻烦 对于sparse的数据进行特征组合进行训练容易模型过拟合，泛化性差，因此需要一种generalization的方法对数据进行处理 Wide&amp;Deep模型就是围绕记忆性和泛化性进行讨论的，模型能够从历史数据中学习到高频共现的特征组合的能力，称为是模型的Memorization。能够利用特征之间的传递性去探索历史数据中从未出现过的特征组合，称为是模型的Generalization。Wide&amp;Deep兼顾Memorization与Generalization并在Google Play store的场景中成功落地。 特点 用了Embedding的deep model对学习general的feature为了解决sparse 数据的问题，Google把DNN，embedding结合用于处理sparse的数据。通过dense embedding的方法把sparse的高维数据进行降维的同时，也把0,1的sparse的数据变成连续的数据使得梯度下降学习的时候不会因为梯度消失的问题而变得模型难以训练。而这个模块也成为deep model 用了wide model对特征进行自动组合。为了解决特征组合的问题，google通过利用wide model方法直接把其他类别的数据进行线性投影从而达到特征选择的作用。因为线性投影的时候每个特征都有一个weight进行权衡。而模型学习的时候就是把这些weight进行调整来选择。这样可解释性也比较强 联合训练W&amp;D模型是将两部分输出的结果结合起来联合训练，将deep和wide部分的输出重新使用一个逻辑回归模型做最终的预测，输出概率值。联合训练的数学形式如下：需要注意的是，因为Wide侧的数据是高维稀疏的，所以作者使用了FTRL算法优化，而Deep侧使用的是 Adagrad 原理 我们可以看到 Wide&amp; Deep model基本就是wide model 和deep model的输出的相加，再通过logistics 或softmax(如果是多分类)，输出的结果是CTR (click or not click)的概率 Wide modelwide部分是一个广义的线性模型，输入的特征主要有两部分组成，一部分是原始的部分特征，另一部分是原始特征的交叉特征(cross-product transformation)，对于交互特征可以定义为 Deep modelDeep部分是一个DNN模型，输入的特征主要分为两大类，一类是数值特征(可直接输入DNN)，一类是类别特征(需要经过Embedding之后才能输入到DNN中)，DNN模型随着层数的增加，中间的特征就越抽象，也就提高了模型的泛化能力。对于Deep部分的DNN模型作者使用了深度学习常用的优化器AdaGrad，这也是为了使得模型可以得到更精确的解。 W&amp;D模型是将两部分输出的结果结合起来联合训练，将deep和wide部分的输出重新使用一个逻辑回归模型做最终的预测，输出概率值。联合训练的数学形式如下：需要注意的是，因为Wide侧的数据是高维稀疏的，所以作者使用了FTRL算法优化，而Deep侧使用的是 Adagrad 优缺点 优点 通过结合wide和deep model同时解决sparse data和memorization， generalization的问题 不用特意人工选择特征进行组合 数据特征可以通过embedding方式降维降低参数的数目 缺点 可能需要考虑把哪些特征放到deep model哪些放到wide model 需要通过联合训练方式对不同模型优化6. 思考 在你的应用场景中，哪些特征适合放在Wide侧，哪些特征适合放在Deep侧，为什么呢？个人认为categorical的数据如果distinct的value很多的feature应该用于deep model，比如说像电影类别id的特征，或者用户地址，如果细分的话会有上百个value，而每个value这样需要one-hot来表示的话就会变得很大这样需要deep model的embedding进行降维。 而如果value个数不多的categorical feature，比如性别，又或者连续数据，比如年龄，可以通过wide的 model进行处理。不过感觉也可以放到deep model里面 为什么Wide部分要用L1 FTRL训练？Wide model 用L1 - FTRL 的优化器主要原因是 L1-FTRL 在结合L1-FOBOS 和 L1-RDA的优化方法后再梯度下降有较高的精度同时，也能产生稀疏性进行特征的筛选，而这个在wide-model里有利于wide mode通过weight的稀疏性进行特征组合和选择。比如说通过L1-FTRL 我们可以把 y=w1x1 +w2x2 +w3x3 里面的w1， w2的weight降到接近0从而达到排除特征x1和x2 而选择x3的效果 为什么Deep部分不特别考虑稀疏性的问题？Deep model 之所以不用太考虑了稀疏性是因为embedding已经把稀疏的特征进行降维成dense的vector，而对于本来就不sparse的feature如年龄也就更加不用考虑稀疏性了 Future work 有没有可以不用专门分开两种训练方法，更加general的训练方法对不同的模型进行训练但不会影响效果？ 能不能不专门考虑什么feature要放到deep model什么feature放到wide model达到更好的自动化特征选择的效果？ Reference[1] https://github.com/datawhalechina/team-learning-rs/blob/master/DeepRecommendationModel/Wide%26Deep.md [2] https://zhuanlan.zhihu.com/p/142958834","link":"/2021/03/18/Recommendation-System-2-WideDeep/"},{"title":"Recommendation-System-3-DeepFM","text":"Background在推荐系统里面学习低阶和高阶的feature，将feature 进行交叉组合挖掘数据的信息一直需要花费大量的时间精力进行feature engineering。组合特征大牛们研究过组合二阶特征，三阶甚至更高阶，但是面临一个问题就是随着阶数的提升，复杂度就成几何倍的升高。这样即使模型的表现更好了，但是推荐系统在实时性的要求也不能满足了。所以很多模型的出现都是为了解决另外一个更加深入的问题：如何更高效的学习特征组合？ 为了进行通过特征交叉更加高效学习高阶和低阶的特征，大牛们曾经对研究过FM(factorization machine), FFM (field-aware factorization machine) 对特征进行交叉和研究更加高阶的特征(注意FM的特征交叉相对于直接在logistics regression对特征进行相乘交叉而已，通过用embedding的思想先把sparse feature $x_j$ 进行embedding转换成dense的vector 然后再对不同的特征交叉， 这样的好处在避免了$x_ix_j$特征交叉时只要有一个特征是0就会变成0这样更加容易稀疏的缺陷).然而FM， FFM的方法把特征进行交叉的问题在于随着交叉的order越高，计算复杂度越大。 另外一种方法是利用DNN对更加高阶的非线性特征进行学习。但是这种方法会导致model的参数因为sparse的feature维度太高而导致参数过多的难以训练的问题。 为了解决这几个问题，研究人员曾经把FM， DNN和embedding进行结合。先通过embedding把sparse feature进行降维学习到dense的vector同时也泛化了feature，之后再通过类似FM的feature intersection 特征交叉的形式(比如inner product和outer product)将feature 进行组合并得到高阶的feature，最后再用DNN进行特征学习，而这也引向了PNN的思想。下图为PNN的结构。 不过PNN的问题在于它通过串行的形式把feature变成高阶feature之后，低阶的feature不能很好表达(因为所有输入的feature都被投影转换后丢失了原来低阶特征的信息)。 为了解决这个问题，后来研究人员把Google的wide&amp;deep的model的并行结构(wide model+ deep model)和 类似FM的feature crossing的方式进行结合，把高阶特征和低阶特征同时并行分开学习而这个也就是DeepFM的思想 Motivation在DeepFM (deep factorization machine) 里面，它通过把Factorization Machine， Embedding，Wide&amp;Deep model 的思想进行结合从而达到以下的效果： No Feature Engineering和wide&amp;deep 相比， DeepFM 不用做feature engineering对feature进行组合，DeepFM可以直接通过FM方式把数据特征自动组合 Learn low-order Feature Intersection and high-order Feature Intersection相对于PNN， DeepFM通过利用并行的方式对高阶和低阶特征进行组合，而不会影响低阶特征的表达和学习 Share the same input and embedding vector对于不同的用户的数据输入，DeepFM用相同的embedding vectors进行转换，用户的sparse的数据相对于是对embedding的vector进行选择，这样可以有效降低模型的空间复杂度 How DeepFM work模型的结构与原理 前面的Field和Embedding处理是和前面的方法是相同的，如上图中的绿色部分；DeepFM将Wide部分替换为了FM layer如上图中的蓝色部分 这幅图其实有很多的点需要注意，很多人都一眼略过了，这里我个人认为在DeepFM模型中有三点需要注意： Deep模型部分 FM模型部分 Sparse Feature中黄色和灰色节点代表什么意思 FM model详细内容参考FM模型部分的内容，下图是FM的一个结构图，从图中大致可以看出FM Layer是由一阶特征和二阶特征Concatenate到一起在经过一个Sigmoid得到logits（结合FM的公式一起看），所以在实现的时候需要单独考虑linear部分和FM交叉特征部分。 $$\\hat{y}{FM}(x) = w_0+\\sum_{i=1}^N w_ix_i + \\sum{i=1}^N \\sum_{j=i+1}^N v_i^T v_j x_ix_j$$ Deep modelDeep架构图 Deep Module是为了学习高阶的特征组合，在上图中使用用全连接的方式将Dense Embedding输入到Hidden Layer，这里面Dense Embeddings就是为了解决DNN中的参数爆炸问题，这也是推荐模型中常用的处理方法。 Embedding层的输出是将所有id类特征对应的embedding向量concat到到一起输入到DNN中。其中$v_i$表示第i个field的embedding，m是field的数量。$$z_1=[v_1, v_2, …, v_m]$$上一层的输出作为下一层的输入，我们得到：$$z_L=\\sigma(W_{L-1} z_{L-1}+b_{L-1})$$其中$\\sigma$表示激活函数，$z, W, b $分别表示该层的输入、权重和偏置。 最后进入DNN部分输出使用sigmod激活函数进行激活：$$y_{DNN}=\\sigma(W^{L}a^L+b^L)$$ Properties优点 结合了Wide&amp;Deep, FM的特点，在通过特征进行交叉(feature intersection)来得到更高阶的特征并同时学习高阶特征和低阶特征 不用进行特别精细的feature engineering （wide&amp;Deep 在wide的模型里面还是需要人工特征组合，比较wide model里面feature intersection不够） 通过embedding和field input的思想将sparse的特征进行降维同时可以share相同的embedding vector，降低模型的复杂度 Comparison of deep models for CTR prediction model No Feature Pre-training High-order Low-order No Features Engineering FNN × √ × √ PNN √ √ × √ Wide &amp; Deep √ √ √ × DeepFM √ √ √ √ 缺点 对于 FM部分的训练会相对较慢 FM的公式是一个通用的拟合方程，可以采用不同的损失函数用于解决regression、classification等问题，比如可以采用MSE（Mean Square Error）loss function来求解回归问题，也可以采用Hinge/Cross-Entropy loss来求解分类问题。当然，在进行二元分类时，FM的输出需要使用sigmoid函数进行变换，该原理与LR是一样的。直观上看，FM的复杂度是 $O(kn^2)$ 。但是FM的二次项可以化简，其复杂度可以优化到 $O(kn)$ 。由此可见，FM可以在线性时间对新样本作出预测。 证明推理见 link： $\\begin{array}{cc}\\sum_{i=1}^{n-1}{\\sum_{j=i+1}^{n}{&lt;v_i,v_j&gt;x_ix_j}}\\\\= \\frac{1}{2}\\sum_{i=1}^n\\sum_{j=1}^n &lt;v_i,v_j&gt;x_ix_j -\\frac{1}{2}\\sum_{i=1}^n &lt;v_i,v_i&gt;x_ix_i\\\\= \\frac{1}{2}\\sum_{i=1}^n\\sum_{j=1}^n\\sum_{f=1}^k v_{if}v_{jf}x_ix_j - \\frac{1}{2}\\sum_{i=1}^n\\sum_{f=1}^k v_{if}v_{if}x_ix_i\\\\= \\frac{1}{2}\\sum_{f=1}^k [(\\sum_{i=1}^nv_{if}x_i)(\\sum_{j=1}^nv_{jf}x_j) - \\sum_{i=1}^nv_{if}^2x_{i}^2]\\\\\\end{array}$由于第一个sum的loop时间是O(k), 而计算$(\\sum_{j=1}^nv_{jf}x_j)$只需要1次就可以得到中括号里面的term所以是O(n)最后相乘起来时间复杂度变成O(kn) 其中 没有考虑到用户的兴趣和过去浏览的历史的问题，只是单纯在对feature进行交叉，没有考虑用户在时间上的行为变化，也不能进行个性化推荐 FM的部分可能需要FTRL优化器进行更新 QuestionsCode12345678910111213141516171819202122232425262728293031323334def DeepFM(linear_feature_columns, dnn_feature_columns): # 构建输入层，即所有特征对应的Input()层，这里使用字典的形式返回，方便后续构建模型 dense_input_dict, sparse_input_dict = build_input_layers(linear_feature_columns + dnn_feature_columns) # 将linear部分的特征中sparse特征筛选出来，后面用来做1维的embedding linear_sparse_feature_columns = list(filter(lambda x: isinstance(x, SparseFeat), linear_feature_columns)) # 构建模型的输入层，模型的输入层不能是字典的形式，应该将字典的形式转换成列表的形式 # 注意：这里实际的输入与Input()层的对应，是通过模型输入时候的字典数据的key与对应name的Input层 input_layers = list(dense_input_dict.values()) + list(sparse_input_dict.values()) # linear_logits由两部分组成，分别是dense特征的logits和sparse特征的logits linear_logits = get_linear_logits(dense_input_dict, sparse_input_dict, linear_sparse_feature_columns) # 构建维度为k的embedding层，这里使用字典的形式返回，方便后面搭建模型 # embedding层用户构建FM交叉部分和DNN的输入部分 embedding_layers = build_embedding_layers(dnn_feature_columns, sparse_input_dict, is_linear=False) # 将输入到dnn中的所有sparse特征筛选出来 dnn_sparse_feature_columns = list(filter(lambda x: isinstance(x, SparseFeat), dnn_feature_columns)) fm_logits = get_fm_logits(sparse_input_dict, dnn_sparse_feature_columns, embedding_layers) # 只考虑二阶项 # 将所有的Embedding都拼起来，一起输入到dnn中 dnn_logits = get_dnn_logits(sparse_input_dict, dnn_sparse_feature_columns, embedding_layers) # 将linear,FM,dnn的logits相加作为最终的logits output_logits = Add()([linear_logits, fm_logits, dnn_logits]) # 这里的激活函数使用sigmoid output_layers = Activation(\"sigmoid\")(output_logits) model = Model(input_layers, output_layers) return model Reference[1] Datawhale: https://github.com/datawhalechina/team-learning-rs/blob/master/DeepRecommendationModel/DeepFM.md[2] zhihu: https://zhuanlan.zhihu.com/p/50692817[3] CSDN: https://blog.csdn.net/ISMedal/article/details/100578354[4] https://blog.csdn.net/a819825294/article/details/51218296[5] DeepFM Paper: https://arxiv.org/pdf/1703.04247.pdf","link":"/2021/03/21/Recommendation-System-3-DeepFM/"},{"title":"Recommendation-System-4-NFM","text":"BackgroundNeuralFM (Neural Factorization Machines)是2017年由新加坡国立大学的何向南教授等人在SIGIR会议上提出的一个模型，这个模型可以看成是直接把FM，Neural network 和embedding的简单粗暴的结合 （原来论文这么好发的吗？） Motivation这个模型考虑的问题是 FM 模型只考虑了一阶和二阶的特征，然而对更加高阶的特征没有学习到，这样无法对生活中更加复杂和有规律的数据进行挖掘和学习。FM的公式如下， 它只考虑到一阶和二阶的特征组合。第二个term是inner product。$$y_{N F M}(x)=w_{0}+\\sum_{i=1}^{n} w_{i} x_{i}+ \\sum_{i=1}^{n}\\sum_{j=i+1}^{n} &lt;v_iv_j , x_{i}x_{j}&gt;$$ 为了解决这个局限性问题，何向南教授通过简单粗暴的方法直接把FM的二阶特征的组合的部分换成神经网络，并将sparse的feature在网络中先进行embedding降维再交叉结合从而得到学习更加高阶非线性特征的效果， 更新后的公式如下： $$y_{N F M}(x)=w_{0}+\\sum_{i=1}^{n} w_{i} x_{i}+f(x)$$ 其中f(x) 是DNN的部分. NeuralFM 原理Neural FM 的模型结构如下：橙色框框代表FM里面linear feature 的一阶特征组合，而对于sparse的特征先通过embedding生成dense vector然后把dense vector用于一阶的特征组合(橙色框框)以及bi-interactoin pooling layer 的高阶特征交叉进行高阶特征的学习（绿色框框） NeuralFM 的$f(x)$ 部分的结构如下：NeuralFM的DNN部分由 Sparse input, Embedding layer, B-intersection Layer, Hidden layers (Deep model), prediction output组成 Input Layer输入层的特征里面 每一个cell相当于一个 sparse feature， 每个feature一般是先one-hot, 然后会通过embedding，生成纬度低的dense vector，假设$v_i \\in R^{k}$为第$i$个特征的embedding向量， 那么$V_{x}={x_{1} v_{1}, \\ldots, x_{n} v_{n}}$表示的下一层的输入特征。这里带上了$x_i$是因为很多$x_i$转成了One-hot之后，出现很多为0的， 这里的$x_iv_i$ 是一个embedding的vector 而 $x_i$不等于0的那些特征向量，相当于$x_i$ 通过lookup table 方式选择 embedding vector $v_i$. Bi-Interaction Pooling layer在Embedding层和神经网络之间加入了特征交叉池化层是本网络的核心创新了，正是因为这个结构，实现了FM与DNN的无缝连接， 组成了一个大的网络，且能够正常的反向传播。假设$V_{x}$是所有特征embedding的集合， 那么在特征交叉池化层的操作： $$f_{B I}(V_{x})=\\sum_{i=1}^{n} \\sum_{j=i+1}^{n} x_{i} v_{i} \\odot x_{j} v_{j}$$ $\\odot$表示两个向量的元素积操作，即两个向量对应维度相乘得到的元素积向量（可不是点乘呀），其中第$k$维的操作：$(v_{i} \\odot v_{j})k= v{i k} v_{j k}$ 因此个人认为在 $x_{i} v_{i} \\odot x_{j} v_{j}$ 里面它的结构是和$v_i,v_j$相同大小的特征dense vector。$f_{B I}(V_{x})$是多个dense vector的简单直接交叉element-wise相乘后相加的结果。这个方法其实也是挺直接Bi-Interaction层不需要额外的模型学习参数，更重要的是它在一个线性的时间内完成计算，和FM一致的，即时间复杂度为$O(k N_{x})$，$N_x$为embedding向量的数量。参考FM，可以将上式转化为证明推理见 link： $$f_{B I}(V_{x})=\\frac{1}{2}[(\\sum_{i=1}^{n} x_{i} v_{i})^{2}-\\sum_{i=1}^{n}(x_{i} v_{i})^{2}]$$ Hidden Layer这一层就是全连接的神经网络， DNN在进行特征的高层非线性交互上有着天然的学习优势，公式如下： $$\\begin{array}{cc}a_1 = \\sigma(W_1f_{BI}(V_x) +b_1) \\\\a_2 = \\sigma(W_2a_1 +b_2) \\\\… \\\\a_{i+1} = \\sigma(W_ia_i +b_i) \\\\\\end{array}$$ 这里的$\\sigma_i$是第$i$层的激活函数，hidden layer里面一般是ReLu函数而不是logistics 来防止梯度消失问题， $f_{BI}(V_x)$是交叉后的embedding的dense vector。 Prediction Layer这个就是最后一层的结果直接过一个隐藏层，但如果这里是回归问题，没有加sigmoid激活，如果是分类问题，需要加上logistic 或softmax的 activation function： $$f({x})={h}^{T} {z}_{L}$$ 在NeuralFM的DNN这一部分，为了减少DNN的很多负担，一般只需要很少的隐藏层就可以学习到高阶特征信息。当然可以像其他深度模型一样通过添加Dropout, Batch normalization, ResBlock 等方法进行进行更加深度的feature的学习已经抑制过拟合，梯度消失等问题。 Properties优点 NFM 通过简单粗暴直接的方式把FM的二阶特征组合部分换成DNN的方式学习到更加高阶非线性的特征，很容易理解 计算简单直接linear logit + embedding 的DNN的部分就完事了，而且由于DNN一般来说比较浅就能学习高阶的特征，训练也不难 能够学习低阶特征和高阶特征 能够通过embedding有效解决sparse feature带来的训练看你的问题，不用像wide&amp;deep那样需要额外的FTRL的optimization的方法进行优化 缺点 和DeepFM相比，NeuralFM 有点像把DeepFM里面的FMmodel和Deep model 串起来的感觉。相对于DeepFM, NFM没有把低阶特征直接交叉，只是要么把低阶特征直接线性相加，要么把他们通过embedding进行投影到高阶general的特征再直接相加，所以感觉对低阶特征交叉组合方面不太好。 Code123456789101112131415161718192021222324252627282930313233343536def NFM(linear_feature_columns, dnn_feature_columns): \"\"\" 搭建NFM模型，上面已经把所有组块都写好了，这里拼起来就好 :param linear_feature_columns: A list. 里面的每个元素是namedtuple(元组的一种扩展类型，同时支持序号和属性名访问组件)类型，表示的是linear数据的特征封装版 :param dnn_feature_columns: A list. 里面的每个元素是namedtuple(元组的一种扩展类型，同时支持序号和属性名访问组件)类型，表示的是DNN数据的特征封装版 \"\"\" # 构建输入层，即所有特征对应的Input()层， 这里使用字典的形式返回， 方便后续构建模型 # 构建模型的输入层，模型的输入层不能是字典的形式，应该将字典的形式转换成列表的形式 # 注意：这里实际的输入与Input()层的对应，是通过模型输入时候的字典数据的key与对应name的Input层 dense_input_dict, sparse_input_dict = build_input_layers(linear_feature_columns+dnn_feature_columns) input_layers = list(dense_input_dict.values()) + list(sparse_input_dict.values()) # 线性部分的计算 w1x1 + w2x2 + ..wnxn + b部分，dense特征和sparse两部分的计算结果组成，具体看上面细节 linear_logits = get_linear_logits(dense_input_dict, sparse_input_dict, linear_feature_columns) # DNN部分的计算 # 首先，在这里构建DNN部分的embedding层，之所以写在这里，是为了灵活的迁移到其他网络上，这里用字典的形式返回 # embedding层用于构建FM交叉部分以及DNN的输入部分 embedding_layers = build_embedding_layers(dnn_feature_columns, sparse_input_dict, is_linear=False) # 过特征交叉池化层 pooling_output = get_bi_interaction_pooling_output(sparse_input_dict, dnn_feature_columns, embedding_layers) # 加个BatchNormalization pooling_output = BatchNormalization()(pooling_output) # dnn部分的计算 dnn_logits = get_dnn_logits(pooling_output) # 线性部分和dnn部分的结果相加，最后再过个sigmoid output_logits = Add()([linear_logits, dnn_logits]) output_layers = Activation(\"sigmoid\")(output_logits) model = Model(inputs=input_layers, outputs=output_layers) return model Reference[1] datawhale: https://github.com/datawhalechina/team-learning-rs/blob/master/DeepRecommendationModel/NFM.md [2] FM https://www.jianshu.com/p/152ae633fb00 [3] 原paper https://arxiv.org/pdf/1708.05027.pdf","link":"/2021/03/24/Recommendation-System-4-NFM/"},{"title":"Statistic- 1 Hypothesis-Testing","text":"Introduction: Hypothesis testingThis article will introduce what is hypothesis testing and some teminologies, like p-value, significant level, confidence level, etc.The outline of this article is as follow: what is hypothesis testing What components are in Hypothesis testing and How Hypothesis testing Distribution of the null hypothesis (Center limit theorem) Significant level and confidence level P-value Overall Steps in Hypothesis testing When to use hypothesis testing What is Hypothesis testingHypothesis testing is a procedure to determine if a hypothesis about an estimated difference is statistically meaningful and should be rejected or not. If the hypothesis is rejected, then we will choose alternative competing hypothesis For example, when we are designing an APP and have two different styles of the interface: Red and Green. Then we make an hypothesis that Red color style can attract more users than the Green style. The alternative competing hypothesis is that Red color style can not attract more users than Green style. Then in hypothesis testing is to determine if we should reject the hypothesis that red style can attract more users than green and choose another competing hypothesis that there is no difference between two styles. Main idea behind Hypothesis testingComponents in Hypothesis testingThe components in Hypothesis testing are as follow: Null Hypothesis H0: the original hypothesis assuming that the estimated difference is slight enough and can be regarded as no difference. Alternative Hypothesis H1: the alternative competing hypothesis that we use to challenge the null hypothesis. Usually H1 assumes that observed difference between groups should be considered.Note that alternative hypothesis is mutually exclusive from Null hypothesis. Evidence/ Observation that is used to challenge the Null hypothesis. Note that if Null hypothesis is statistically meaningful and held, then the possibility of the occurence of this evidence will be small. Idea behind Hypothesis testingThe idea behind hypothesis testing is to use proof of contradiction to reject the null hypothesis. It is to use the evidence/observation, which seems like an extreme case under hypothesis H0 and may be able to challenge the null hypothesis, to work as a contradiction to reject H0. However, whether the evidence/ observation is strong enough to work as an contradiction to reject H0 should be determined. Example 1:H0: All bird can fly. H1: Not all bird can fly. Evidence: penguin can not fly.We can see that in this case the observation that penguin can not fly is a strong enough to be a contradiction to H0, since all penguins can not fly. Example2:H0: bird eats food. H1: bird doesn’t eat foodEvidence: one bird in the zoo didn’t eat my food when I try to feed it.Here we can see the evidence is just a specific case while that all birds eat food is a fact. Hence the evidence is not strong enough to reject H0. In order to test whether an evidence is strong enough to reject H0, P-value, significant level, confidence level, power will be introduced later. One important note is that in hypothesis testing, we can only determine if we should reject the hypothesis or not. We CAN NOT prove that the hypothesis we specify is correct or the hypothesis is accepted.If the hypothesis is not rejected, it just means that the possibility that the hypothesis goes wrong is statistically small enough so that we can not observe the wrong cases using current observation method. It doesn’t mean the hypothesis is actually correct or should be accepted. Center limit theoremThe central limit theorem states that if you have a population with mean μ and standard deviation σ and take sufficiently large random samples from the population with replacementtext annotation indicator, then the distribution of the sample means will be approximately normally distributed， even the real distribution of samples is not Normal distribution. Due to Center Limit theorem, when we samples large amount data for H0 and H1, the distribution of H0 and H1 are normal distribution, shown as below When we plot the distribution of H0 and H1 in the same space, it would look like this (assume the left one is H0, and the right one is H1) where the x axis represent feature X and y-axis represent the possibility density function value.We can see that two distributions have the overlapping region. If we want to determine when to reject H0, we need a threshold of feature x (a vertical line) to separate H0 and H1.Then to find such threshold, it involves the concept of significant level and p-value P-valueWhat is p-valueP-value is the probability under the assumption of no effect or no difference (null hypothesis H0), of obtaining a result equal to or more extreme than what was actually observed. (在H0=true的假设前提下/H0=True的distribution下,取得比目前的evidence/observation 同等极端或更加极端更加极端的事件的概率). The P stands for probability and measures how likely it is that any observed difference between groups is due to chance. In other words, P-value actually measure how strong the evidence is to reject H0. If P-value is larger, then the strenght of evidence to reject H0 is weaker. Otherwise, the strenght is stronger. Example: Assume I am sampling a random number from a range [-1000, 1000] and assumes that the distributions of number in this range is normal distribution with mean =0. An observation is a sample with value =900. Then the P-value of this observation is the possibility of obtaining sample &gt;=900. In the plot of possbility density function, P-value represents the area under curve in X&gt;=900. Significant level and confidence levelIn order to determine whether reject H0 or not, we need to define a threshold for P-value and this leads to the concepts of significant level and confidence level. Significant level“Significant” means how significant the difference between groups is so that we should consider such difference is meaningful. Significant level is usually notated as $\\alpha$ and $\\alpha$=P(reject H0 | H0 =true). significant level is the possibility of rejecting H0 when H0 is true. Moreover, the error that rejecting H0 when H0 is true is called Type I error. Hence Significant level is also the possibility of the occurence of Type I error / Type I error rate (Significant error = Type I error) confidence levelConfidence level is the possibility of retaining H0 when H0 is true. Hence we know that confidence level = 1- significant level = $1-\\alpha$Confidence interval is the region outside the shadow region of $\\alpha$ in the distribution of H0=true. In the possibility density function, We can see that the shadow region in the figure represent a single side significant level (as it is possibility, in density function, it represents an area). Then the area of shadow region = $\\alpha$ When to reject H0if P-value &lt; $\\alpha$, we reject H0 and choose H1. if P-value &gt;=$\\alpha$, retain H0 if an observation X on the x-axis is outside of shadow region, that is, P-value, P(x&gt;=X) is greater than or equal to significant level, then we think observation X is common under null hypothesis H0 and we can not use X to reject H0. if P-value is smaller than significant level / observation X fall inside shadow region, then the difference between groups will be considered and we reject H0 and choose H1. In other words, the observation X is too rare so that we don’t consider it satisfies null hypothesis and hence it becomes a contradiction to reject H0. Note: significant level and confidence level is set by user and the value of significant level is usually 0.05 or 5%. This also means that users assume the possibility of rejecting a true hypothesis H0 due to some extreme cases under current observation method should be smaller than significant level $\\alpha$. Retain region and powerNow, we are considering the alternative hypothesis H1 rather than H0. Assume the distribution from H1 is normal distribution as well. Beta/retain regionThe yellow region of distribution of H1=True is the retain region for H0 and the area of the yellow region is $\\beta$, which represents the possibility of retaining H0 when H1=True. If the observation value X on x-axis falls into the yellow region, then we retain Hypothesis H0.$\\beta$ = P(retain H0 | H0 = False) = Type II error rate ( possibility to retain H0 when H0 is false under assumption H1=True) Powerpower is possibility of rejecting null hypothesis H0 and power = 1- $\\beta$.if power is set to be larger, then testing could be easier to detect and reject false Null hypothesis. But if power is too large, it would reject a good hypothesis as well, despite the null hypothesis is true. Note that there is a tradeoff between $\\alpha$ and $\\beta$. When we set smaller $\\alpha$ value to decrease the Type I error rate (reject H0 when H0 is True), then $\\beta$ / Type II Error rate (retain H0 when H0 is False)will increase . Usually $\\beta$ is not set by user, but it can be calculated using p-value and $\\alpha$. Usually $\\beta$ is around 20% Type I and Type II ErrorNow Let’s consider the distributions of hypothesis that H0=True (left) and the hypothesis that H1 = True (right) Together. Both distributions are estimated by sampling. Since the yellow region $\\beta$ in distribution of H1= True and confidence region of distribution of H0 = True represent the regions to retain H0, we can put two distribution together. Then the vertical line separates the region of retaining H0 (left) and rejecting H0 (right). Remember that $\\alpha$ = Type I error rate = P(reject H0 | H0 = True) and $\\beta$ = Type II error rate = P(retain H0 | H0 = False) and there is trade-off between Type I error rate and Type II error rate. When we lower significant level $\\alpha$, $\\beta$ will increase. In other words, lowering threshold to reject H0 can increase possibility of retaining H0, but also null hypothesis H0 may not be true, in this case, rate of accepting False H0 will increase. Factors affecting Power and $\\beta$ Size of the effect / effect size The size of the effect is the distance between the mean of null hypothesis distributions and the mean of true distribution.If $\\alpha$ is fixed, then Greater the size of effect is, smaller $\\beta$ / Type II error rate is and more different two distributions are. We can see that the first figure has the effect size smaller than the effect size in second figure, as the $\\beta$ in the top figure is larger than the bottom one. Assume distribution of H0=True is estimated from samples and the real distribution of population is distribution of H1=True. Then Equation to compute size of effect is: effect size = $\\frac{\\mu_{H0} -\\mu_{H1} }{\\sigma_{H1}}$where $\\mu_{H0}$, $\\mu_{H1}$ are mean of H0, H1 $\\sigma_{H1}$ is standard variance of distribution of H1=True. Note that effect size can be computed, but can not be controlled by users directly. It depends on the samples and population. Size of samplesThe sample size controls the variance of a distribution (width of distribution) and hence can affect $\\beta$ / Type II error rate. In the following figure, when $\\alpha$ and means of two distributions H0=True, H1 = True are fixed, large sample size is, smaller variance is and smaller $beta$ / Type II error is Control Type I error $\\alpha$ To control Type I error rate/ $\\alpha$, we can change $\\alpha$ directly. Decrease $\\alpha$ is to decrease Type I error rate (rejecting H0 when H0 =True) Usually $\\alpha$ is set to be 0.05 or 0.01 Control Type II error $\\beta$ or (1- power) Since there is trade-off between Type I error rate $\\alpha$ and Type II error rate $\\beta$, we can increase $\\alpha$ to decrease $\\beta$ and increase power Increase sample size to decrease variance of distribution to decrease $\\beta$, the rate of accepting H0 when H0=False. Two-tailed Hypthesis TestingThe hypothesis testing above is called single-tailed hypothesis testing, since we only consider the significant level $\\alpha$ in on one side only. In the figure below $H_a: \\mu &lt;\\mu_0 $ means the sample mean $\\mu$ is smaller than population mean $\\mu_0$. In single-tailed test, we only consider one-side of relationship and disregard another side Now we consider the Two-tailed Hypothesis testing with the sum of area of left tail and right tail in distribution of H0 equal to $\\alpha$. The figure is shown below. In two-tailed test, we regard the extreme cases on both sides. *We reject H0 when 2P-value&lt; $\\alpha$ or P-value &lt; $\\frac{\\alpha}{2}$ in Two-tailed Hypothesis Testing** For example, we want to analysis how tall the people are in a region. Then Example 1:H0: the mean tall of population = 1.75 mH1: the mean tall of population is not equal to 1.75 m.Observation: the mean of a sample group of people = 1.9m.In this case, if we care whether tall mean of people is &gt;1.75m or &lt;=1.75m, then we use two-tailed test and compare P-value with $\\frac{\\alpha}{2}$ Example 2:H0: the mean tall of population = 1.75 mH1: the mean tall of population is greater than 1.75 m.Observation: the mean of a sample group of people = 1.9m.If we care if tall mean of people &gt;1.75m or not only, then we use one-tailed test and compare P-value with $\\alpha$. Example 3:H0: the mean tall of population = 1.75 mH1: the mean tall of population is less than or equal to 1.75 m.Observation: the mean of a sample group of people = 1.5m.In this case, we care if mean &lt;=1.75m or not only, then we use one-tailed test again. Choosing which tail in one-tailed test depends on the observation and your alternative hypothesis H1. When to use single-tailed test or two-tailed test Choosing two-tailed test when: when you care extreme cases on both sides of distribution without direction when effects exist on both sides based on your observations. Note that if effect exists on two sides, but we choose single-tailed test, then the expected significant level =$\\alpha$ but we choose $\\frac{\\alpha}{2}$, this may increase Type II error rate and decrease power as we decrease $\\alpha$ Choosing one-tailed test when: when you care one side of effect / extreme cases on one side only when effects exist on only one side based on your observations Note that if effect exists on two tails and we choose one-tailed test, then expected significant level =$\\frac{\\alpha}{2}$, but we use $\\alpha$ instead, this may increase Type I error rate. Methods to compute p-valueThe common methods to compute P-value as follow, I will introduce them in the next article z-test Z-distribution/normal distribution Assume data follow normal distribution usually used with continuous variables t-test Use t-distribution usually used with continuous variables Pearson’s Chi-Square score Use Chi sqare distribution (Square Sum of multiple normal distribution) Used to measure independence between two variables Can be used for continuous variables or discrete variables Factors affecting Hypothesis testing Sample size As mentioned before, sample size can affect $\\beta$, type II error rate (retain H0/ can not reject H0 when H0 is false). Randomized Experiment In order to obtain representative samples from population in hypothesis testing to compute the p-value, we need to avoid sampling in bias. This leads to randomized experiment. There are some important terms in randomized experiment: Random sampling Random sampling is to obtain get some representative samples from the population (full traffic) randomly. If the samples are not obtained randomly, it could be biased which can affect the distribution of sample data even sample size is large Randomization / Random assignment Randomization is how you allocate samples to groups in experiment. It alleviates the bias that might cause a systematic difference between groups unrelated to the treatment itself. For example, If I want to test if the new UI design of a software can get higher ClickThrough rate (CRT) from users. Then I compare a group of users who use the new UI, called treatment group with another group of users who use original UI, called controlled group. Then After I randomly sample the users from different region, the way that I randomly assign those users to treatment group or controlled group is randomization/ random assignment. Confounding factor /variable Confounding factor is the factor that correlates with the independent and dependent variable confuse the effects and impact the results of experiments. Randomization of experiments is the key to controlling for confounding variables in machine learning experiments. Summary to use Hypothesis testing Specify a Null Hypothesis H0 and alternative Hypothesis H1 based on some observations (H0 and H1 should be mutually exclusive). Choose one-tailed test or two-tailed test based on H0 and H1 Specify significant level $\\alpha$ Compute P-value based on dataset (t-test/t-statistic, z-test, F-statistic,Chi-square test) Compare P-value with significant level $\\alpha$. If P-value &gt;= $\\alpha$, retain H0 (NOT accept !). If P-value &lt; $\\alpha$, reject H0 and retain H1 Reference[1] https://towardsdatascience.com/a-complete-guide-to-hypothesis-testing-2e0279fa9149 [2] https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4111019/ [3] http://web.mnstate.edu/malonech/Psy633/Notes/hyp.%20testing,%20effect%20size,%20power%20GW8.htm [4] https://statisticsbyjim.com/hypothesis-testing/one-tailed-two-tailed-hypothesis-tests/ [5] http://us.sagepub.com/sites/default/files/upm-binaries/83321_Chapter_8_Hypothesis_Testing_Significance,_Effect_Size,_and_Power.pdf [6] https://zhuanlan.zhihu.com/p/86178674 [7] https://machinelearningmastery.com/confounding-variables-in-machine-learning/ [8] https://online.stat.psu.edu/stat100/lesson/10","link":"/2021/02/18/Statistic-Hypothesis-Testing/"},{"title":"Statistic-P-value","text":"IntroductionThis article introduces how to compute the P-value using different distributions / statistic methods, including Z-distribution, t-distribution, F-test ,Chi-Square $X^2$ Distribution.In all distributions, we first determine the random variable value in possibility density function (CDF) and then use the corresponding distribution look-up table to check the P-value (Possibility) of that distribution. Finally compare P-value with the significant level, $\\alpha$ to check should we reject Null hypothesis H0 or not. NotationsBefore talking about different testing for P-value, let’s denote some notations: x: sampled data , a small group of observations P: population size. Population is the total set of observations that can be made n: sample size $\\mu_0$: population mean $\\sigma_0$: population variance $\\mu’$: sample mean $\\sigma’$: sample variance Z-test / Z-statistic (standard normal distribution) AssumptionZ-test assumes distributions of both population and sample are Normal distributionNull Hypothesis in z-test is that two groups have no difference Computation of Z-Score of continuous random variable $$ z-score = \\frac{\\mu’ - \\mu_0 }{\\sigma_0/ \\sqrt{n}}$$ if we don’t know population standard variance, but know population mean only, we can use sample variance deviation to estimate the population variance deviation. Then it becomes $$ z-score = \\frac{\\mu’ - \\mu_0 }{\\sigma’/ \\sqrt{n}}$$ if compare two groups: $$ z-score = \\frac{\\mu_1 - \\mu_2 }{\\sqrt{(\\sigma_1^2/n_1)+(\\sigma_2^2/n_2)}}$$ Computation of Z-Score of binary random variable Denote $p_0$ as the Population possibility of obtaining current case X, $p’$ as the Sample possibility of current case X $$ z score = \\frac{p’ - p_0 }{ (p_0(1-p_0))/ \\sqrt{n}}$$ When to use when we know the population is normal distribution when the sample size is small, usually smaller than 30. When we know some distribution settings, like $\\mu_0$ = 0, $\\sigma_0$=1 in standard normal distribution ExampleTo test if two groups have no difference, we can let sample mean, variance of group A as $\\mu_A$ $\\sigma_A$, sample mean of group B as $\\mu_B$, $\\sigma_B$. Then let $\\mu’ = \\mu_A - \\mu_B$ if we assume population mean of group difference is $\\mu_0$ = 0, then we can model the group difference like this $$z-score = \\frac{(\\mu_A - \\mu_B) - 0 }{\\sigma_0/ \\sqrt{n}}$$ Or $$z-score = \\frac{(\\mu_A - \\mu_B) - 0 }{\\sqrt{(\\sigma_1^2/n_1)}}$$ t-test / t-statistic (similar to normal distribution) Assumptiont-test also assumes Normal distribution as the population distribution and sample distribution. Different from z-test, t-test doesn’t assume we know the parameters (mean, variance) of normal distribution.. Null Hypothesis in t-test is that two groups have no difference Note that as degree of freedom in t-distribution increases, it is more similar to normal distribution. Computation of t-Score for continuous random variable $$ t-score = \\frac{\\mu_1 - \\mu_2}{\\sqrt{(\\sigma_1^2/n_1)+(\\sigma_2^2/n_2)}}$$ since both t-test and z-test assume normal distribution, they have the same formula Find P-value of t-score After we determine the t-score in CDF, we can find the corresponding P-value in t-distribution using this table When to use t-test is used when population parameters (population mean and population variance) are not known when sample size is very large, usually &gt;30. (So usually in model evaluation, we use t-test with large dataset) when we assume data is normal distribution Note that since in t-test we don’t know the population distribution parameters, so the variances in t-test are sample variance. But in Z-test, we know the population distribution parameters, we usually use population variance and population mean instead In my word, z-test assumes we know normal distribution parameters, so we can use those parameters for testing and use less samples. But t-test doesn’t assume we know anything about distribution, so it uses samples parameters (sample mean and variance) to estimate the normal distribution for testing. Chi-Square-test / $X^2$-statistic AssumptionIn Chi-Square test, it is to test the independence/correlation between categorical variablesNote that Chi-Square test is a one(right)-tail test and the we can not use it for two-tail test. It meausres the correlation of two categorical variable. Null Hypothesis in Chi-Square test is that two groups are independent from each other Computation of Pearson’s Chi-Square valueConsider we have two binary categorical variable with total sample size of n. The table is shown as follow, we want to test their correlation. Then the Pearson’s Chi-Square Correlation is computed by where i, j mean the $i^{th}$ row and $j^{th}$ column. n is the total sample amount$$E_{i,j} = \\frac{X_iX_j}{n}$$ Find P-value from Chi-Square CDF function based on the Chi Square-value we find We can find the Chi-square distribution in this linkand the distribution table from here When to use when we are testing categorical variable and we need one-tail test only when we are testing independence between two variables Example: Measuring the independence between gender and salary, independence between smoking and cancer. F-test / F-statistic AssumptionF-score is to measure the ratio between explained variance over the unexplained variance Computation of F-Score for P-ValueTo understand the F-score, let consider a real linear regression model: $$ y = \\beta_0 + \\beta_1X_1 + \\beta_2X_2 + \\epsilon$$ and an estimated linear regression$$ y’ = \\beta_0 + \\beta_1X_1 + \\beta_2X_2$$where $\\epsilon$ is the unexplainable, irreducible error Then the Residual Sum Square Error (RSS) between two linear models among n samples is $$ RSS = \\sum_i^n(y_i - y’_i)^2$$ RSS Error is to measure the variability between two model. In F-statistic, we want to check if there is relationship between feature $X_i$ and target. If there is no relationship, then weight $\\beta_i$ = 0.(i&gt;0). Then Null hypothesis in F-statistic is that $\\beta_1 = \\beta_2=..=\\beta_p=0$ consider there is no relationship between features and targetThen alternative hypothesis is that $\\beta_i \\neq 0$ for some i&gt;0. In this case, for null hypothesis, we have a Restricted Linear Model$$ \\bar{y} = \\beta_0$$ and a Full Linear model $$ y_i’ = \\beta_0 + \\beta_1X_{1i}$$ where $X_{1i}$, $\\bar{y_i}$ means the the $1^{st}$ feature in the $i^{th}$ sample and the prediction of the $i^{th}$ sample. Denote the degree of freedom of restrict model as $df_r$, and the degree of freedom of Full model as $df_F$. There are n sample data points and p+1 weight cofficients $\\beta_i$ ($\\beta_0$ to $\\beta_p$ )to pick. Then degree of freedom = n -p.Hence in this case, we care about choosing $\\beta_1$ or not. We have $df_r$ = n-1 and $df_F$ = n-2 Total Sum Square Error (TSS) of Restrict Linear model:$$TSS = \\sum_i^n(y_i - \\bar{y})^2$$ Residual Sum Square Error (RSS) of Full Linear model:$$RSS = \\sum_i^n(y_i - y’_i)^2$$ The formula to compute F-score is $$F-score = \\frac{ (TSS-RSS)/(df_r - df_F)}{RSS/df_F} = \\frac{ (TSS-RSS)/1}{RSS/(n-2)}$$ More general, If there are p+1 weight coefficient in Full model, then we have$$F-score = \\frac{ (TSS-RSS)/p}{RSS/(n-p-1)}$$ where $TSS- RSS$ is the variability explained by model and $RSS$ measure the variability left unexplained after the regression (which is also the irreducible error mentioned before) If F-value &gt; 0 and &lt;= 1, we don’t reject H0. If F-value &gt;1, then we reject H0 and choose H1.If F-value &lt;0, then F-test fails, it doesn’t mean anything.Note that F-test is also a One-tail hypothesis test. When to use when we are measuring if some features matters in prediction / there is correlation between features and target F-test can be adjusted by the number of predictors, but individual t-test doesn’t Note that when the number of predictors p &gt; the number of samples n, F-test will fail since value $RSS/(n-p-1)$ becomes negative and this F-value is out of the range of random variable value in F-distribution ExampleThere are some examples from this link: https://online.stat.psu.edu/stat501/lesson/6/6.2 $R^2$ test Assumption$R^2$ measures the proportion of variability of prediction y that can be explained by feature X Computation of $R^2$-Score for P-Value$$ R^2 - value = \\frac{TSS- RSS}{RSS}$$ where RSS and TSS are computed as same as those in F-test $$TSS = \\sum_i^n(y_i - \\bar{y})^2$$ where $\\bar{y} =\\beta_0$ = mean of $y_i$since when we estimate $\\bar{y} =\\beta_0$ using samples to minimize the sum square loss, let gradient of $\\sum_i^n(y_i-\\bar{y})^2$ = 0, we have $-2\\sum_i^n(y_i-\\bar{y}) =0$ and $\\bar{y} = (\\sum_i^ny_i)/n$ Residual Sum Square Error (RSS) of Full Linear model:$$RSS = \\sum_i^n(y_i - y’_i)^2$$ Random Variable $R^2$ has range [0,1]. If $R^2$ fall outside this range, then the test fails. If $R^2$ value is close to 1, then reject H0 and consider H1 and there is relationship between features and y. if $R^2$ value is close to 0, then model is wrong and don’t fit well, then can not reject H0. When to use when we want to measures the proportion of variability of prediction y that can be explained by feature X Reference[1] https://towardsdatascience.com/statistical-tests-when-to-use-which-704557554740 [2] https://www.jmp.com/en_us/statistics-knowledge-portal/t-test/t-distribution.html [3] https://www.jmp.com/en_us/statistics-knowledge-portal/t-test/t-distribution.html [4] https://www2.palomar.edu/users/rmorrissette/Lectures/Stats/ttests/ttests.htm [5] https://online.stat.psu.edu/stat501/lesson/6/6.2 [6] https://stats.stackexchange.com/questions/130069/what-is-the-distribution-of-r2-in-linear-regression-under-the-null-hypothesis [7] https://www.statisticshowto.com/wp-content/uploads/2014/01/p-value1.jpg","link":"/2020/11/22/Statistic-P-value/"},{"title":"Web Scrapping and Regular Expression - 1","text":"IntroductionThis Quick tutorial is to use an web scrapping example to introduce how to use BeautifulSoup and Regular expression to mine the web data quickly and easily. For details, Please refer to the BeautifulSoup document. BeautifulSoupAPI Requests Import requests package 1import requests requests.get():using get(url) function from requests package, it allows us to send a request to the url webstite.It returns the response from that website, but this response is not HTML file, but a response packet from server response.content:By calling content after we get the response packet from get(), we can extract the HTML page from the packet and analyze it. BeautifulSoup soup = BeautifulSoup(html …) 12from bs4 import BeautifulSoupsoup = BeautifulSoup(html, 'html.parser') This creates beautifulsoup object and parse the HTML text with a html parser to store html content to the beautifulsoup object. .tag_nameAfter obtaining a beautiful soup object of a html file, we can use soup.tag_name to get name of current tag.Example: 12345678910111213 body = soup.b print(body.name) ~~~ This extracts the tag *\\&lt;b\\&gt;* &lt;br&gt;+ *t.get_text() / t.text* After extracting a tag, we can use .text or .get_text() function to extract all texts value under current tag ~~~Python soup = BeautifulSoup(\"&lt;html&gt;&lt;h1&gt;Head 1&lt;/h1&gt; &lt;h2&gt;Head 2&lt;/h2&gt;&lt;html&gt;\") soup.get_text() #or soup.text It returns “Head 1 Head 2” directly t.attrs[“href”] or t[“href”]:In HTML, every tag could have its attributes inside the tag. We can simply use tag.[“attribute-name”] or tag.attr[“attribute-name”] to extract the attributesThis example extracts the href link from tag &lt;a href=…&gt;&lt;/a&gt;Example: 1234567html = \"https://www.baidu.com\"soup = BeautifulSoup(html, \"html.parser\")# find a tag call \"a\", &lt;a href= ....&gt;tag= soup.find(\"a\")print(tag[\"href\"])#orprint(tag.attrs[\"href\"]) t.contents and t.children:A tag’s children are available in a list called .contents .contents:it stores all children into a list .children:it is a list_generator type object, we can not get child directly. We should use iteration method to get child from childrens123456789head_tag# &lt;head&gt;&lt;title&gt;The Dormouse's story&lt;/title&gt;&lt;/head&gt;head_tag.contents#[&lt;title&gt;The Dormouse's story&lt;/title&gt;]title_tag = head_tag.contents[0]title_tag# &lt;title&gt;The Dormouse's story&lt;/title&gt;title_tag.contents# [u'The Dormouse's story'] .string:If current tag doesn’t have children tags, but just have a string, then we can call .stringIf current tag doesn’t have text, but have children tags, we can not call .string (it returns nothing)Example: 123456head_tag# &lt;head&gt;&lt;title&gt;The Dormouse's story&lt;/title&gt;&lt;/head&gt;head_tag.string# Nothinghead_tag.title.string#The Dormouse's story find(…):It returns the first tag or string that satisfies the requirements in inputExample: 12tag = BeautifulSoup(\"&lt;html&gt; &lt;a&gt;text&lt;/a&gt; &lt;a&gt;text2&lt;/a&gt;&lt;html&gt;\")tag.find('a', string=\"text\") It returns the first tag with type of &lt;a&gt; which contains string “text”.soup.find() find_all()Similar to find(), but return a list of all tags that satisfy requirements find().find_next()Find the first tag and then find the next tag that satisfies the requirements inside current tag 12soup = BeautifulSoup(\"&lt;html&gt;&lt;h1&gt;Head 1 &lt;a href=www.baidu.com&gt;&lt;/h1&gt; &lt;h2&gt;Head 2&lt;/h2&gt;&lt;html&gt;\")soup.find('h1').find_next(attrs={'href':\"www.baidu.com\"}) it returns: &lt;a href=”www.baidu.com&quot;\\&gt;\\&lt;/a&gt; Example: Capture roster of football team in ESPN website 1234567import requestsfrom bs4 import BeautifulSoupteam_url = 'https://www.espn.com/college-football/team/roster/_/id/228/clemson-tigers'response = requests.get(team_url)soup = BeautifulSoup(response.content, \"html.parser\")tags= soup.find_all('h1') It finds all h1 tags in the HTML page and return them in a list Regular Expressionregular expression is a way to find any string pattern that match the expression we design. It helps us find the string pattern easier. API Import Regular expression package 1import re *Causes the resulting RE to match 0 or more repetitions.Example: ab*: match when 0 or more b follows a +Causes the resulting RE to match 1 or more repetitions.Example: ab+: match when 1 or more b follows a ?Causes the resulting RE to match 0 or 1 repetitions (…)Matches whatever regular expression is inside the parentheses, and indicates the start and end of a group […]Used to indicate a set of characters[a-z]: characters from a to z[a-zA-Z]: characters from a to z and from A to Z[a-zA-Z0-9]: characters from a to z and from A to Z and from 0 to 9 A|BMatch either A and B pattern (?=…):Matches if … matches next, but doesn’t consume any of the string (?!…):Matches if … doesn’t match next. (?&lt;=…), (?&gt;…)Matches if the current position in the string is preceded (the first one) or after (second one) by a match for … that ends at the current position re.search(“(text)”, input):Search a “text” pattern from input. The first input is regular expressionIt return a re object *re.compile(“([a-z]text)”):compile the regular expression object. This regular expression object can use match(), search() without inputing regular expression parameters re.group(0):It extracts matched string from re object and return the first pattern. Example: 123456import res = \"Hello World. Hello Everyone\"# match a set of character a-z or A-Z that repeat 0 or more before pattern \"one\".obj = re.search(\"([a-zA-Z]*one)\",s)if obj: print(obj.group(0))This example return pattern “Everyone” Example to Find Specific pattern from HTML fileSearch for all strings that contain pattern “SC” and end with “SC” 123456789import requestsfrom bs4 import BeautifulSoupimport reteam_url = 'https://www.espn.com/college-football/team/roster/_/id/228/clemson-tigers'response = requests.get(team_url)soup = BeautifulSoup(response.content, \"html.parser\")tags= soup.find_all(string= re.compile(\"[a-zA-Z ]*SC\"))tags Further workWe can mine more information from any websites. However, we also need to know the architecture and tag names or even some functions in webpage in the website we want to mine. To get to know how to know what functions or tags we have in the BeautifulSoup object, we can use a package called inspect in python to explore the structure of beautiful soup object Reference[1] BeautifulSoup[2] Regular Expression[3] Inspect package in Python","link":"/2020/09/20/Web-Scrapping/"},{"title":"Data Structure 2 -sorting","text":"Bubble sort (冒泡排序法)Main ideaBubble sort compares the array elements $(n-1) + (n-2)+…+1 = \\frac{n(n-1)}{2}$ times. At $k^{th}$ iteration, iterate (n-k) elements and swap two elements if previous one is larger than the next one in ascending sorting.It guarantees that in each iteration, there must be at least one element sorted into the correct positionsExample:in an array [5 1 4 2 8] with length = 5Begin:$1^{st}$ iteration: iteration starts from arr[0] to arr[4] and swap two element if arr[i] &gt; arr[i+1].[5 1 4 2 8] $\\to$ [1 5 4 2 8] $\\to$ [1 4 5 2 8] $\\to$ [1 4 2 5 8] $\\to$ [1 4 2 5 8] $2^{nd}$ iteration: iteration starts from arr[0] to arr[3] and swap two element if arr[i] &gt; arr[i+1]. Process is similar to the first iteration.The result is [1 4 2 5 8] $\\to$ [1 2 4 5 8] $3^{rd}$ iteration: iteration starts from arr[0] to arr[2].$4^{rd}$ iteration: iteration starts from arr[0] to arr[1].End Process Pesudo Code 1234Loop through n-1 iteration: at the k^th iteration, loop through arr[0] to arr[n-1 - k] element: if arr[i]&gt; arr[i+1] swap arr[i], arr[i+1] Python Code 1234567def bubble_sort(array): n = len(array) for i in range(n-1): for j in range(n-1-i): if array[j] &gt; array[j+1]: array[j], array[j+1] = array[j+1], array[j] return array Complexity Memory Complexity: O(1), since there is no memory used in algorithm Time Complexity: O(n^2), since there are two inner loops in function and need $\\frac{n(n-1)}{2}$ comparison. Selection Sort (选择排序法)Main IdeaSelection sort is to find the maximum value of current array and put it to the end of array and then exclude this sorted element to get the sub-array and repeat these two steps to sort the array. If find minimum value, need to put it to the beginning. Process Pesudo Code 1234Loop through n-1 iterations at k^th iteration find max of array[0:n-1-k] put max into array[n-1-k] Python Code 123456789def selection_sort(array): n = len(array) for i in range(n): max_v = 0 for j in range(n-i): if array[max_v] &lt; array[j]: max_v = j array[n-i-1] , array[max_v] =array[max_v], array[n-i-1] return array Complexity Memory Complexity: O(1) since in this code, we don’t use additional memory Time Complexity:: O(n^2), since selection sort use two loops to sort. One inner loop is used to find max value and the other loop is to sort the max values. Insertion sort (插入排序法)Main IdeaMain idea of Insertion Sort is to compare the current element with elements before this element and then insert this element to the correct position by moving some elements backward. Start from arr[1], i=1. Iterate elements arr[i] with index i from 1 to N-1 For each element arr[i], compare arr[i] with the element arr[k] right before arr[i]. If we found element arr[k] &gt; arr[i], swap arr[i] and arr[k]. Repeat Step 2 until the element arr[k] before arr[i] is less than or equal to arr[i]Example:Given an array arr = [4,3,2,10,12,1,5,6] with array length N= 8, initial index =0, we want to sort it in ascending order.iteration 1: [4,3,2,10,12,1,5,6] $\\to$ [3,4,2,10,12,1,5,6]iteration 2: [3,4,2,10,12,1,5,6] $\\to$ [3,2,4,10,12,1,5,6] $\\to$ [2,3,4,10,12,1,5,6]… Process Pseudo code12345678Let i = 1Loop until i ==N-1 j= i-1 // swap arr[i] and arr[j] if arr[j]&gt;arr[i] while arr[i]&lt; arr[j] and j&gt;0: swap(arr[i], arr[j]) j-- i++ Python Code1234567def insertion_sort(array): for i in range(1,len(array)): j = i-1 while j&gt;=0 and array[j] &gt;array[j+1]: array[j],array[j+1] = array[j+1], array[j] j -= 1 return array Complexity Memory Complexity: O(1) Speed Complexity: worst case: O(n^2) Extension Binary Search + Insertion sort. Reference Quick sort (快速排序法)Main IdeaIn quick sort, the main idea is that we first choose a pivot/key element used for comparison. Usually we choose the last element in the partition/array as the pivot (we can also choose middle element or the first element as well) Move the elements smaller than pivot to the left to form a subarray/partition P1 containing all elements smaller than pivot. Move the elements larger than pivot to the right to form a subarraysubarray/partition P2 containing all elements larger than pivot Move pivot to the position between P1 and P2 such that elements ahead pivot are smaller than pivot, elements after pivot are after than pivotNote: This structure is actually as same as binary search tree, in which elements on the left side of the parent node is smaller than parent as elements on the right side of the parent node is greater than parent Repeat steps 1~4 for each partition recursively until subarray can not be partitioned (only one element) Process In Partition step set the last element arr[len(arr)-1] to be pivot set pointer pointing to values smaller than pivot: left_pt =0 and pointer pointing to values greater than pivot: right_pt = len(arr)-2 Loop until reaching terminal state: left_pt&gt; right_pt left_pt++ until it finds arr[left_pt]&gt;pivot right_pt– until it finds arr[right_pt]&lt; pivot swap arr[left_pt] and arr[right_pt] after reaching terminal, swap pivot and arr[left_pt] such that pivot is at the correct position. Pseudo Code 12345678910111213141516171819202122partition(array, low, high): // check if input range is valid if begin-end &lt; 1: return end pivot = array[high] left_pt = low right_pt = high-1 //move left_pt to right, right_pt to left until left_pt&gt;right_pt //when left_pt&gt;right_pt, we know array[left_pt] &gt;= array[right_pt] //then we need to swap pivot with either array[left_pt] or array[right_pt] to set the boundary Loop if left_pt&lt;= right_pt: Loop if left_pt&lt; len(array) and array[left_pt] &lt;= pivot: left_pt++ Loop if right_pt&gt; 0 and array[right_pt]&gt;= pivot: right_pt-- if left_pt&lt; right_pt: swap array[left_pt] and array[right_pt] if array[left_pt] &gt; pivot swap array[left_pt], pivot return left_pt Note: array may have multiple values as same as pivot. In order to iterate each element in position 0~n-1, need to use &lt;= pivot, and &gt;= pivot to avoid trapping at some positions that doesn’t satisfy terminal conditioneg. arr[left_pt] a[1] arr[right_pt] arr[pivot] 2 1 2 2 In this case, if we use “&lt; pivot” and “&gt;pivot” rather than “&gt;=”,”&lt;=”, then left_pt and right_pt won’t check “1” in the array. __Need to compare array[left_pt] and pivot__, since when the input array has only 2 element, then left_pt = right_pt, it directly skips the loop and swap data without comparing the values, this could be wrong. Pseudo Code 12QuickSort( array): return quicksort(array, 0, len(array)-1) 1234567891011quicksort(array, begin,end) //Check if array is empty or length &gt;1 if end-begin&lt;1: return array // find the correct position of pivot mid = Partition(array, 0, len(array)-1) // sort the subarray with elements smaller than pivot array = quicksort(array, 0, mid-1) // sort the subarray with elements greater than pivot array = quicksort(array, mid+1,len(array)-1) return array Python Code 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748class Solution(object): def quickSort(self, array): \"\"\" input: int[] array return: int[] \"\"\" # write your solution here if array == None or len(array)==0: return array begin = 0 end = len(array)-1 return self.quick_sort(array, begin,end) def quick_sort(self, array, begin, end): # Check base case if begin &gt;=end: return array #pre-order operation # partition l_begin, l_end, r_begin, r_end = self.partition(array, begin, end) # recursively go to left and right sub-array array = self.quick_sort(array, l_begin, l_end) array = self.quick_sort(array, r_begin, r_end) return array def partition(self, array, begin, end): # check base case if begin &gt;= end: return begin, begin, begin,begin # choose pivot import random # randomly choose a pivot in array # then move the pivot to the end of the array, pivot=end pivot = random.randint(begin, end) array[pivot], array[end] = array[end], array[pivot] pivot = end stored_index = begin for i in range(begin, end+1): if array[i] &lt; array[pivot]: array[i], array[stored_index] = array[stored_index], array[i] stored_index += 1 array[pivot] , array[stored_index] = array[stored_index], array[pivot] l_begin = begin l_end = stored_index-1 r_begin = stored_index +1 r_end = end return l_begin, l_end, r_begin, r_end Complexity Memory Complexity: O(1) if we don’t consider the memory in stack. Otherwise, O(logn) Time Complexity: best case/average case；O(nlogn) worst case: O(n^2), since it is possible that every time we select the pivot of sub-array is the largest element of this sub-array, which lead to the case that the recursion tree becomes linked list.When the Recursion tree becomes linked list, time complexity becomes O(nh) = O(nn) rather than O(nh) = O(nlogn), where h is the height of tree. To solve this problem, we can use random selection method to select the pivot, rather than always pick the last element as pivot Note when it always picks the greatest/ smallest value as pivot or the array is already sorted, it comes to the worst case O(n^2) since the recursion tree becomes a list-like data structure with depth of recursion tree = O(n) and take O(n) to iterate every node and O(n) to sort at each node. Then it comes to O(n*n) time complexity Quick Sort use Pre-Order, Top-down, in-place operation to first partition array and then do recursion Merge Sort use Post-Order, Bottom-up, not in-place method to return array and then merge returned array Merge sort （归并排序）Main Idea：Merge sort is one of the divide and conquer algorithm like quicksort. Its main ideas is that Find the middle of the array Divide the array into two sub-array based on middle Repeat step 1~2 recursively until only 1 element in the array Merge left array and right array and return merged array recusively Example: In the array below, we recurively divide the array into left sub-array 2 and right sub-array 12, then sort the left array first by dividing it until step 4,5. We then merge two elements in step 6. After that, we go back to step 2 to sort the right sub-array. We do the same thing for the whole array. Process Pesudo Code 123456789101112131415161718192021222324252627282930MergeSort( array) check if array is valid array, if not ,return compute left , right boundary merge_subarray(array, left, right)merge_subarray(array, lef, right): check if left &lt; right. If not, return array[left] compute middle = (left+right)//2 // sort left array left_array = merge_subarray(array, left, middle) // sort right array right_array = merge_subarray(array, middle+1, right) // merge and sort left and right arrays sorted_array = merge(left_array, right_array) return sorted_arraymerge(left_array, right_array): use linear method to merge the left array and right array. // Note that since left_array and right array have //been sorted, merging two sorted array actually //lead to linear time complexity O(n) only create new array A left_index = 0 right_index = 0 while left_index &gt;0 and right_index&gt;0: add the min(left_array[left_index], right_array[right_index]) index += 1 Add the remaining elements in array to A return A Python code 12345678910111213141516171819202122232425262728293031323334353637383940414243class Solution(object): def merge_sort(self, array, left, right): if len(array)&lt;2 or left&gt;=right: return [array[left]] middle = left+ (right-left)//2 left_array = self.merge_sort(array, left, middle) right_array = self.merge_sort(array, middle+1, right) sorted_array = self.merge(left_array, right_array) return sorted_array def merge(self, left_array, right_array): if left_array == None or len(left_array) &lt;1: return right_array elif right_array == None or len(right_array) &lt;1: return left_array l_index = 0 r_index = 0 merged_array = [] while l_index &lt; len(left_array) and r_index&lt;len(right_array): if left_array[l_index] &lt; right_array[r_index]: merged_array.append(left_array[l_index]) l_index += 1 else: merged_array.append(right_array[r_index]) r_index += 1 if l_index &lt; len(left_array): merged_array.extend(left_array[l_index:]) elif r_index &lt; len(right_array): merged_array.extend(right_array[r_index:]) return merged_array def mergeSort(self, array): \"\"\" input: int[] array return: int[] \"\"\" # write your solution here if array == None or len(array) &lt;2: return array return self.merge_sort(array, 0, len(array)-1) Complexity Memory Complexity: if consider the memory used in stack during recursion, it is O(logn) Time Complexity: O(nlogn). In each layer of recursion tree, it takes O(n) to merge and sort two sub-arrays at a level of the tree. Since merge sort uses division method, that is, Assume that are n elements to sort and take k steps (depth of the recursion tree) to sort. There are $2^0+2^1+…+2^k = n$ elements. When only consider the largest term, we have $2^k = n$, and $k = O(log_2n) or O(logn)$. Since each level of recursion tree take O(n) to sort array, then have $O(nlogn)$ Compared with quick sort Both merge sort and quick sort use divide and conquer method (分治算法) to divide array and then solve each subset recursively. Hence both of them involve logn time complexity As for memory complexity, merge sort has O(nlogn) while quicksort has memory complexity O(logn). Quicksort is better than mergesort As for time complexity, merge sort has average/worst case time complexity: O(nlogn) while quick sort has O(nlogn) in average case and O(n^2) in worst case. Quick sort is less stable than merge sort. Quicksort and merge sort are much more useful than linear sorting(insertion, bubble, selection) due to its O(nlogn) time complexity without increasing memory complexity Notes: Usually, Recursion method is less optimal than iteration method (using while loop), since recursion method requires stack memory to store and return solved subset back to last sub-problem. Hence memory complexity of recursion is usually O(n) unless there are no memory used in recursion function and reduce n to 1 Recursion method is easy to implement, but usually costs more memory. Selection sort can be viewed as a version of bubble sort with more explicit physic meaning, since it explicitly finds the min/max value and put them to the beginning/end of subarray. Insertion sort compares current element with previous elements while bubble/selection sort compare current element with the next elements In-place algorithm: an algorithm which transforms input using no auxiliary data structure. However a small amount of extra storage space is allowed for auxiliary variables. The input is usually overwritten by the output as the algorithm executes. Source codeFor C++ source code, Please read my github repository here Reference[1] https://www.geeksforgeeks.org/bubble-sort/[2] https://www.geeksforgeeks.org/insertion-sort/?ref=lbp[3] https://www.geeksforgeeks.org/quick-sort/?ref=lbp","link":"/2020/08/24/data-structure-sorting/"},{"title":"Hello World","text":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new \"My New Post\" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment","link":"/2020/06/28/hello-world/"},{"title":"Use Hexo + ICarus to build a website","text":"IntroductionRecently, I realize that there are many benefits to write technical blog on a personal websites. It allows us to summarize the techniques we’ve learned, helping review what we did before, and to learn with each other.Hence, this blog is to show how to build a personal site using static website generator, such as Hugo, Hexo and Jekyll. Static site generate actually generates the general html, css and javascript,etc files we need in the framework of website. Comparison among Hugo, Hexo, JekyllHugo: Language GoLang Advantage The fastest framework for constructing website Good flexibility and no need to install many packages since Using Go language many built-in functions/plugins Templates for GoLang International Use Support Markdown Disadvantage Need to be familiar with GoLang Without default themes for website Lack of extensive plugins Hexo: Language Node.js Advantage Fast generating speed Support markdown Easy to setup Github page Good Chinese support (Most of users are Chinese) Many different themes for website Stable Disadvantage May be lack of supports for other languages, since most of users are Chinese Need to install some packages for plugins Jekyll: Language Ruby Advantage Simple to use Many free themes and plugins Easy to publish to Github Disadvantage Not international (may be blocked in some countries) Some plugins are not supported by Github Page Speed of constructing website could be slow as more documents/passages are included in website. Setup HexoI choose Hexo as the first generator for my website since it provides many different themes and plugins. To setup Hexo and publish your website, we first need to setup our work environment (Node.js) for Hexo and associate our website to our server (Github page). Setup Node.js and Git environment Install Node.js . Here is Chinese Reference Install Git to manage your repository of your website and open Git bash terminal. Open the git bash terminal and Install Hexo using command: 1$ npm install hexo-cli -g Initialize the folder that stores your website after install hexo: 1$ hexo init new-folder-name Then hexo will create the folder with the name “new-folder-name” and setup the environment. Inside the folder 1234567├── _config.yml├── package.json├── scaffolds├── source| ├── _drafts| └── _posts└── themes _config.yml contains the common configurations for the website. source contains the passages you are writing. themes is the folder containing all different themes for your website. Associate our website to server (I choose Gitpage here) Select your server There are many different servers for your website. To read more about choosing server for your websites, click here Since GithubPage provides a free way to publish our website and Github is a very powerful open source repository website. I use GithubPage for me website in this passage. create Github account create a new repository with name: &lt;Your github name&gt;. github.io Install git plugin for Hexo 1$ npm install hexo-deployer-git --save create SSH key 1$ ssh-keygen -t rsa -C \"your email address\" Copy your generated SSH key in the _id_rsa.pub file_ in the path you set like (C:\\Users\\Administrator.ssh\\id_rsa.pub) to your New ssh key Modify the deployment setting in _config.yml file in the root directory of your website. 123456# Deployment## Docs: https://hexo.io/docs/deployment.html deploy: type: git repo: git@github.com:&lt;Github账号名称&gt;/&lt;Github账号名称&gt;.github.io.git branch: master Upload your website to github Note: the default theme of your website in Hexo is landscape. so you don’t need to add any file to your website during testing. To learn more about how to write and post your passage, click here Choosing theme for your blogIf you dislike the default theme (landscape) in hexo, you can install other themes and change the _config.yml file setting to update your website.For example, if you want to install icarus theme, Go back to the root of your website, install icarus them using this command 1$ git clone https://github.com/ppoffice/hexo-theme-icarus.git themes/icarus Modify your config.yml file 12345# Extensions## Plugins: https://hexo.io/plugins/## Themes: https://hexo.io/themes/#theme: landscapetheme: icarus I annotate the theme of landscape and choose icarus here. Note: do not add any space before the keyword “theme”, otherwise, the theme will not update Generate static site files and update them to github generate static site files 1$ hexo g upload the files to github directory &lt;your github name&gt;.github.io 1$ hexo d Then you can type &lt;your github name&gt;.github.io in your browser to visit your website. Setup Comment pluginIn hexo, there are many different plugins for comment function, like valine, gitalk, gitment. Take valine as example here. Create an account on Leancloud Create a new App, then click the app you created -&gt; Settings-&gt;App keys Copy AppID and AppKey Modify the comment setting in _&lt;your hexo’s root&gt;/theme/&lt;directory name of icarus&gt;/config.yml 12345678910# Comment plugin configurations# https://ppoffice.github.io/hexo-theme-icarus/categories/Plugins/Comment/comment: # type: disqus # # Disqus shortname # shortname: 'Disqus' # Name of the comment plugin type: valine app_id: 'AppID' app_key: 'AppKey' Generate static site files and deploy them again 1$ hexo g -d Additional Notes Useful commands in Hexo 123456789101112131415161718192021222324#in the root directory of hexo$ hexo new page &lt;page-name&gt; #default generate a directory in source/$ hexo new &lt;file-name without postfix \".md\"&gt; #generate &lt;file-name&gt;.md file in /source/__post$ hexo clean #clean all static site files$ hexo g #generate static site files# or $ hexo generate$ hexo d #deploy website, push it to the server#or$ hexo deploy$ hexo g -d # generate and deploy website#move file from __drafts to __posts and publish the passage to website$ hexo publish draft &lt;filename without postfix \".md\"&gt; # Start local server and display your websiteat http://localhost:4000$ hexo s # or $ hexo server To enable catalogue in icarus to enable the passage to show up in catergory, tags, at the top of markdown file add -–type: [“category”, “tags”]-– To let catalogue show up in the page, add toc: true to the top of the file: -–toc: truetype: [“category”, “tags”]-– To change th background of your blog go to themes\\next\\source\\images folder to put your background image Then go to the file themes\\next\\source\\css_custom\\custom.styl to addthe following codes, where bg.jpg is your background image. 1234567body{ background:url(/images/bg.jpg); background-size:cover; background-repeat:no-repeat; background-attachment:fixed; background-position:center;} Then rebuild your HTML blog by commands 1hexo clean &amp;&amp; hexo g -d To change the text of button “next” and “previous” of page to be “next &gt;” and “&lt; prev”:In file: themes\\next\\layout_partials\\pagination.swig , Change 123456789&lt;nav class=&quot;pagination&quot;&gt; {{ paginator({ prev_text: '&lt;i class=&quot;fa fa-angle-left&quot;&gt;&lt;/i&gt;', next_text: '&lt;i class=&quot;fa fa-angle-right&quot;&gt;&lt;/i&gt;', mid_size: 1 }) }}&lt;/nav&gt; to 123456789&lt;nav class=&quot;pagination&quot;&gt; {{ paginator({ prev_text: '&lt; prev', next_text: 'next &gt;', mid_size: 1 }) }}&lt;/nav&gt; This enable you to change the text of next, prev button References https://blog.zhangruipeng.me/hexo-theme-icarus/Widgets/icarus%E7%94%A8%E6%88%B7%E6%8C%87%E5%8D%97-%E6%8C%82%E4%BB%B6/ https://lexcao.github.io/zh/posts/jekyll-hugo-hexo https://juejin.im/post/5bebfe51e51d45332a456de0#heading-9 https://www.vincentqin.tech/posts/build-a-website-using-hexo/ https://zsh2401.top/post/configure-theme-icarus/","link":"/2020/06/30/hexo-gitpage-website/"},{"title":"CPSC 6300 Final Report","text":"TITLE: KKBox Music Recommendation SystemTeam Members: Wenkang Wei, ShuLe ZhuDate: 12/5/2020Problem Statement and MotivationProblem statementAs the public is listening to all kinds of music and there is a diversity of the favors of different individuals in listening music, music users may choose different music platforms based on which platform can provide the kinds of music closest to their tastes. In order to help music platforms figure out how to recommend suitable kinds of music and songs to individuals or groups so that they can provide better services and retain users, it is necessary to analyze the tastes of individuals and the public in listening to music and explore the chances that users may repeat listening some songs based on data analysis. In addition, users, who listen to a large number of songs and add many songs to their favorite albums, usually forget what songs they have collected and which songs they like when trying to re-play those songs. In this case, it is necessary to build a recommendation system to help us predict the songs users may repeat listening to after the user’s a very first observable listening event. Motivation and GoalWe are to construct a recommendation system to predict the chances of a user listening to a song repetitively after the first observable listening event within a time window was triggered. If there are recurring listening event(s) triggered within a month after the user’s very first observable listening event, its target is marked 1, and 0 otherwise in the training set. By constructing such a recommendation system, we are able to remind music users of their favorite songs and increase the rate of the audience as well as the benefit to the music platform. Introduction and Description of DataMusic is important in our daily life and the role of music is determined by people and the environment. Music has a lot of help, for example, it can cultivate sentiment and reduce stress. However, some people don’t need music in their lives or live in music. People who never listen to music or harmony feel that music has a low sense of existence. But in fact, in their everyday lives, they actually have songs, ringing alarms, wind, and rain. So, the role of music performance is one of the most important companions in life. In order to recommend suitable music to users in music platforms efficiently, a recommendation system is an essential part of music platforms. A recommendation system is a subclass of an information filtering system that seeks to predict the “rating” or “preference” a user would give to an item. Due to the fact that there are millions of kinds of music, composers, artists, and other information from users, it is hard for users to pick the kinds of music they like from millions of songs and remember which songs they like and want to re-play. Usually, users forget the songs they have added to their favorite song album due to the large number of songs they have a play. Hence it is necessary to build a music recommendation system to recommend kinds of music, which users prefer and are likely to repeat listening to. The dataset we collected is KKBox Music 1.7GB dataset. Our dataset is from Kaggle KKBox-Music-Recommendation-Challenge. You can also get the dataset from our GoogleDrive . The description of the dataset is as follow: Descriptions of dataset: train.csv msno: user id song_id: song id source_system_tab: the name of the tab where the event was triggered. System tabs are used to categorize KKBOX mobile apps functions. For example, tab my library contains functions to manipulate the local storage, and tab search contains functions relating to search. source_screen_name: name of the layout a user sees. source_type: an entry point a user first plays music on mobile apps. An entry point could be album, online-playlist, song .. etc. target: this is the target variable. target=1 means there are recurring listening event(s) triggered within a month after the user’s very first observable listening event, target=0 otherwise . test.csv id: row id (will be used for submission) msno: user id song_id: song id source_system_tab: the name of the tab where the event was triggered. System tabs are used to categorize KKBOX mobile apps functions. For example, tab my library contains functions to manipulate the local storage, and tab search contains functions relating to search. source_screen_name: name of the layout a user sees. source_type: an entry point a user first plays music on mobile apps. An entry point could be album, online-playlist, song .. etc. sample_submission.csv sample submission file in the format that we expect you to submit id: same as id in test.csv target: this is the target variable. target=1 means there are recurring listening event(s) triggered within a month after the user’s very first observable listening event, target=0 otherwise . songs.csv The songs. Note that data is in unicode. song_id song_length: in ms genre_ids: genre category. Some songs have multiple genres and they are separated by | artist_name composer lyricist language members.csv user information. msno city bd: age. Note: this column has outlier values, please use your judgement. gender registered_via: registration method registration_init_time: format %Y%m%d expiration_date: format %Y%m%d song_extra_info.csv song_id song name - the name of the song. isrc - International Standard Recording Code, theoretically can be used as an identity of a song. However, what worth to note is, ISRCs generated from providers have not been officially verified; therefore the information in ISRC, such as country code and reference year, can be misleading/incorrect. Multiple songs could share one ISRC since a single recording could be re-published several times. In preliminary EDA, we take a look to the features in different data files. 1train_df.info() &lt;class 'pandas.core.frame.DataFrame'&gt; RangeIndex: 7377418 entries, 0 to 7377417 Data columns (total 6 columns): # Column Dtype --- ------ ----- 0 msno object 1 song_id object 2 source_system_tab object 3 source_screen_name object 4 source_type object 5 target int64 dtypes: int64(1), object(5) memory usage: 337.7+ MB 1test_df.info() &lt;class 'pandas.core.frame.DataFrame'&gt; RangeIndex: 2556790 entries, 0 to 2556789 Data columns (total 6 columns): # Column Dtype --- ------ ----- 0 id int64 1 msno object 2 song_id object 3 source_system_tab object 4 source_screen_name object 5 source_type object dtypes: int64(1), object(5) memory usage: 117.0+ MB 1song_df.info() &lt;class 'pandas.core.frame.DataFrame'&gt; RangeIndex: 2296320 entries, 0 to 2296319 Data columns (total 7 columns): # Column Dtype --- ------ ----- 0 song_id object 1 song_length int64 2 genre_ids object 3 artist_name object 4 composer object 5 lyricist object 6 language float64 dtypes: float64(1), int64(1), object(5) memory usage: 122.6+ MB 1members_df.info() &lt;class 'pandas.core.frame.DataFrame'&gt; RangeIndex: 34403 entries, 0 to 34402 Data columns (total 7 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 msno 34403 non-null object 1 city 34403 non-null int64 2 bd 34403 non-null int64 3 gender 14501 non-null object 4 registered_via 34403 non-null int64 5 registration_init_time 34403 non-null int64 6 expiration_date 34403 non-null int64 dtypes: int64(5), object(2) memory usage: 1.8+ MB In the dataset description above, we can obtain the information from both sources of songs, songs, and members. By using the information of sources of songs, we can know where the songs re-played by users are from. The information about songs and members can provide us with details about what kinds of songs are often re-played by users, or what types of members are more likely to repeat playing songs. Such kinds of data can give us insight into users’ behaviors of repeating listening to music. Based on the information from members, songs and sources of songs, we can use them as the input features to machine learning model. The target is the chance of a user listening to a song repetitively after the first observable listening event within a time window was triggered. If there are recurring listening event(s) triggered within a month after the user’s very first observable listening event, its target is marked 1, and 0 otherwise in the training set. Hence, the question can be defined as a binary classification problem and the prediction from machine learning model should be either possibility of the chance, or 0 , 1 binary value indicating if there is a chance. Literature Review/Related WorkAs for the KKBox music recommendation system project, many Kaggle users choose to use a light gradient boosting machine (LGBM) due to its simplicity in preprocessing a large amount of data, and its fast computation speed. Based on the blog [5], the LGBM model is a light gradient boosting machine model, which is based on multiple decision tree models and use gradient boosting ensemble learning method to improve the training accuracy. One work from Kaggle we refer to [3] is to use the LGBM boosting machine as a baseline to fit the dataset and train the model using binary cross-entropy loss since the prediction task in this project is a binary classification task. In addition to the LGBM model, there are some deep learning models used for the recommendation system. One of the neural network models is called the wide and deep model [1] . In this model, first uses an embedding network to transform the sparse categorical data into dense numerical data in one branch. Then it merges the branch of categorical data and the branch of numerical features together to feed the traditional network in the main branch. The main branch of the neural network works as a classifier while the embedding neural network works as a transformation network to preprocess the data. Since in this Kaggle project, no one tries such a neural network model for this task, we are interested in trying this model and see how it works in this problem. Then we compare it with the LGBM models. Modeling ApproachTask - Since this project is to predict the chances of a user listening to a song repetitively after the first observable listening event within a time window was triggered, the output from the machine learning model is the possibility that a user will repeat listening to a song after the first listening event. Hence, this problem can be modeled as a binary classification problem Loss Function - Since this task is a binary classification problem, we simply select binary cross-entropy loss as the loss function used to train parametric machine learning models, like deep learning models, logistic regression, etc. Evaluation Metric - The evaluation metric to measure the performance of the machine learning model is selected to be accuracy and AUC (area under the curve) since we care about how accurate the models could be. LGBM Modeling - Light Gradient Boosting Machine (LGBM) model is a tree-based model，this model merge and train multiple decision tree models to do classification or regression tasks, using the Boosting ensemble learning process. It will automatically encode categorical data into vectors and train models for labels or one-hot encoding. Since it provides a fast way to transform categorical data and train models with good accuracy performance based on the results from leaderboard in Kaggle, we try it here and tune its parameters to fit the data.Here is the reference to use LGBM: https://lightgbm.readthedocs.io/en/latest/Quick-Start.html Wide and Deep Neural Network model - This model using the embedding network in the neural network, this model converts categorical attributes into dense vectors, allowing one to reduce the dimension of categorical data and remove key features such as PCA. Then for feature selection and classification in the main branch, it blends dense embedded vectors with numerical data. The main branch is a typical neural network that acts as a classification function using linear layers, activation functions (relu, softmax, sigmoid). The output is the possibility that the user may repeat listening to the music.The architecture of Wide and Deep Model is referred to https://github.com/zenwan/Wide-and-Deep-PyTorch Baseline model - We choose our baseline model as LGBM boosting machine model with following setings: ‘objective’: ‘binary’, learning_rate = 0.3 , num_leaves = 110, bagging_fraction = 0.95, bagging_freq = 1, bagging_seed = 1, feature_fraction =0.9, feature_fraction_seed =1, max_bin = 256, max_depth = 10, num_rounds = 200.After we choose the baseline model, we tune the LGBM model by changing the max_depth parameter to be [10 , 15, 20 , 25, 30] to see how the depth affects our LGBM performance. After that, we also implement the Wide and Deep Model with learning rate = 0.001, binary cross-entropy loss, and 2 epochs. The reason why we use different settings on the Wide and Deep model is that the training process of the neural network is very slow and the convergence speed of the model is slower than the LGBM model. The training loss of the neural network is easy to increase after decreasing for some iterations. So we need to tune some parameters to train the neural network model. Project Trajectory, Results, and InterpretationOur goal in this project doesn’t change. It is to construct a recommendation system to predict the chances of a user listening to a song repetitively after the first observable listening event within a time window was triggered. If there are recurring listening event(s) triggered within a month after the user’s very first observable listening event, its target is marked 1, and 0 otherwise in the training set. Only one thing we change is that we choose to use neural network model and light gradient boosting machine (LGBM) model for this project, rather than using traditional models like KNN, logistic regression, Here is the trajectory of our project Timeline Date Assignment Wenkang Shule Oct 22 Proposal ✓ ✓ week 1 Set up environment and Exploratory Data Analysis (EDA) ✓ ✓ week 2 Data preprocessing ✓ week 3 Modeling ✓ Nov 12 Interim Report ✓ ✓ week 4 Training model and Evaluation ✓ week 5 Training model and Evaluation ✓ week 6 Wrap up and preparing website presentation ✓ ✓ week 7 Writting Final Report ✓ ✓ Dec 10 Website and Final Report ✓ ✓ Dec 13 Notebooks and other supporting materials ✓ ✓ In this project, we follow these steps to build our data pipeline and recommendation system model:First, in the exploratory data analysis step, we use visualization techniques to visualize and plot the features from the dataset to explore the correlation across features and find which features are most related to the target. Here are partial results from EDA section Genre Counts Artist Count Count of system tabs, screen names, source types In the Exploratory Data Analysis (EDA) part, we first analyze Song information by plotting and visualizing the counts of song genres, composers, artists, and the counts of source types, source screen names, system tabs. In this part, we find that the distributions of the counts of song genres, composers, artists are long-tailed distribution. That is, most users prefer listening to specific genres of music or songs created by specific artists, composers. In this case, we can know that those specific genres of music or songs from specific composers, artists are more likely to be re-played by users. In addition, when visualizing the counts of users vs source screen names, system tabs, and source types, we find that most users are more likely to repeat listening to music from their local sources, local library, rather than from online sources. That is, the features like source types, screen names, and system tabs provide important information for our recommendation system. Hence we choose to keep such kinds of features for modeling later. Later, we analyze Member information, like visualizing the count of bd/age attribute and analyzing the correlation between different attributes using the heatmap plot. In this part, we find that there are many outliers in bd/age features with values outside the range of [0, 100]. After we remove such kind of outliers, we find that most users have ages between 20 to 40. After this, we try to plot bivariate plots to visualize and analyze the relationship between attributes, like city and age/bd, expiration date, and target. what we find is that registration_init_year is negatively correlative to bd/age, city, registered_via. Song_length is also negatively correlative to language. In order to find which features are most related to targets, we plot the correlation matrix using a heatmap to visualize the correlation across features. In this part, since we find almost all other features have similar correlation values to the target, we choose to keep those features for modeling. Then in data preprocessing, we clean and transform the features to a suitable format, like converting String DateTime data to DateTime format and separate year, month, day as new features, removing the outliers in bd/age features, filling missing values, creating new features like count of composers, artist, genres and converting object data type to categorical data type before training model Later, we also construct a data pipeline to extract, transform, load data set by integrating the operations in the data preprocessing step into one single transformation function, which enable us to easily clean and transform dataset directly. After the data preprocessing and transformation step, we split the dataset into a training set (80% of the dataset) and a validation set (20% of the dataset) for training and validating our models. We also determine the Loss function (binary cross-entropy), evaluation metric (accuracy and AUC) to train our Light Gradient Boosting machines (LGBM) models and Wide and Deep Neural network model. During training our LGBM models, we are interested in how the max_depth affect the model performance, so we also try different max_depth parameters to tune our LGBM models. In the Model evaluation step, we simply use the validation dataset to validate the final trained models and then let models make predictions on the test set from Kaggle and submit predictions to Kaggle to see the final evaluation scores. Accuracy Results on our validation on LGBM model to see effect of max_depth on accuracy Index Lgbm with max_depth Validation Accuracy 0 10 0.709764 1 15 0.719106 2 20 0.723689 3 25 0.725822 4 30 0.728842 We can observe that as the value of max_depth of the decision tree in the Boosting machine increases, both validation accuracy, and test accuracy increase gradually. It implies that the performance of our LGBM models may be improved by increasing the max_depth. As increasing max_depth can improve the learning/fitting ability of the LGBM model, it is possible that tuning other parameters like the number of leaves, the number of training epochs may also help improve the accuracy and let models better fit the dataset. Accuracy Results on Kaggle Testset Model name private score public score LGBM Boosting Machine Model 4 0.67423 0.67256 LGBM Boosting Machine Model 3 0.67435 0.67241 LGBM Boosting Machine Model 2 0.67416 0.67208 LGBM Boosting Machine Model 1 0.67416 0.67188 LGBM Boosting Machine Model 0 0.67206 0.66940 Wide and Deep model 0.61628 0.61117 Best Accuracy Results on Kaggle Testset from Kaggle Leaderboard Ranking name Score Entries 1 Bing Bai 0.74787 263 In the final results on test data from Kaggle, we can see that light gradient boosting machines have the accuracy performance better than the Wide and Deep Neural Network model. The best score in LGBM models is 67.256% while the Wide and Deep model has an accuracy of 61.11%. Although the Wide and Deep model performance is not so well, we may improve its performance by tunning the parameters like dropout rate in the neural network, learning rate, training epochs in the future. In addition, in our experiments we try two epochs only, this is because we run the program in Google Colab and also try it in the Kaggle platform, but the hardware is not powerful enough to train the model quickly and there is a time limit in using GPU. Therefore, we can try better hardware to boost the training process in the future. What’s more, the best testing score from the Kaggle leaderboard is only 74.7887%, which means that this project is still challenging. It could be due to the difficulty in parsing and transforming the dataset to extract more meaningful patterns. The limitations of designing and training good models are also some factors since we don’t have enough computation power to train a large model, like Google, OpenAI. Overall, our works explore the effect of max_depth on LGBM models’ performance and also compare the performance of the Wide and Deep model with the performance of the LGBM model. Conclusions and Future WorkIn this project, there are still a few things that perform not very well. One obvious thing is the performance of the Wide and Deep model. We can easily see that the performance from the neural network model is low. This could be due to the small training epochs we use., as we use only 2 epochs to train the model using limited computation resources (Google colab). What’s more, we can also try different neural network architecture to better fit the dataset, or tune the parameters like learning rate, weight decay in the network to increase the learning ability of our network. As for the LGBM model, it seems like the LGBM model can better fit the dataset when using more training epochs and a larger max_depth value. We can also try to tune other parameters like the number of leaves, etc. Overall, in the future, we may do the followings to improve our project:In this project, there are several things we can improve in the future: We can use a better hardware platform for training models, rather than using Google Colab or Kaggle platform, so that we can better train the deep learning model. Tune the LGBM models using a grid search and choose larger max_depth values or tune other parameters Try to create more new features from text attributes like composer, lyricist, artist and use feature importance methods to pick features that most contribute to the prediction In conclusion, we collect a 1.7GB KKBOX music dataset from Kaggle and do exploratory data analysis (EDA) on the data by visualizing the attributes and compute the correlations among features. Then we clean the dataset by removing outliers from age/bd attributes, filling missing categorical data with new labels, and missing numerical data with median value. We also transform the text data and create new features. After that, we use 80% dataset as training set and 20% dataset as a validation set.In Modeling and Evaluation, we use LGBM models and Wide &amp; Deep Neural network models to fit the dataset and also tune the max_depth parameter in LGBM to do binary classification tasks. The best accuracy performance of our models is 67.25% while the best accuracy from the Kaggle leaderboard is about 74%. In the end, we also summarize the future works to improve the project, like using better hardware resources, tunning other parameters of models, and explore more useful features for training. References[1] https://github.com/zenwan/Wide-and-Deep-PyTorch/blob/master/wide_deep/torch_model.py [2] https://www.kaggle.com/c/kkbox-music-recommendation-challenge/submit [3] https://www.kaggle.com/asmitavikas/feature-engineered-0-68310 [4] https://blog.statsbot.co/recommendation-system-algorithms-ba67f39ac9a3 [5] https://medium.com/@pushkarmandot/https-medium-com-pushkarmandot-what-is-lightgbm-how-to-implement-it-how-to-fine-tune-the-parameters-60347819b7fc [6] https://towardsdatascience.com/how-to-build-a-wide-and-deep-model-using-keras-in-tensorflow-2-0-2f7a236b5a4b [7] https://lightgbm.readthedocs.io/en/latest/pythonapi/lightgbm.Dataset.html#lightgbm.Dataset Support Materials Dataset:You can find the dataset from Kaggle: https://www.kaggle.com/c/kkbox-music-recommendation-challenge/data Or you can get the dataset from our Google Drive: https://drive.google.com/file/d/1-WJHZUWFtz9ksfvFoX-dc-ZKjZ6fTk0D/view?usp=sharing Link to Our website/ notebook in databrick platform:https://databricks-prod-cloudfront.cloud.databricks.com/public/4027ec902e239c93eaaa8714f173bcfc/3546981394788271/3905787926422076/7844345949955417/latest.html Link to Our notebook in Google Colab (This notebook is as same as notebook in databrick, but in Colab, you can directly run it after copying it to your drive):https://colab.research.google.com/drive/1dssuTVKvDXM0zULihRt4tJUoOUoCxaFj?usp=sharing Github repository of this project:https://github.com/wenkangwei/cpsc6300-final-project Declaration of academic integrity and responsibilityIn your report, you should include a declaration of academic integrity as follows: 12345With my signature, I certify on my honor that:The submitted work is my and my teammates' original work and not copied from the work of someone else.Each use of existing work of others in the submitted is cited with proper reference.Signature: ___Wenkang Wei_________ Date: ______12/4/2020________ 12345With my signature, I certify on my honor that:The submitted work is my and my teammates' original work and not copied from the work of someone else.Each use of existing work of others in the submitted is cited with proper reference.Signature: ___ShuLe Zhu_________ Date: ______12/4/2020________ CreditThe above project template is based on a template developed by Harvard IACS CS109 staff (see https://github.com/Harvard-IACS/2019-CS109A/tree/master/content/projects).","link":"/2020/12/08/report/"},{"title":"tensorflow-practice","text":"Common packages for data12345import numpy as npimport pandas as pdimport matplotlib.pyplot as plt%matplotlib inlineimport seaborn as sns Tensorflow Practice Data set loading and visualization Data Preprocessing/transforming Modeling using tf.nn (more basic) / tf.keras (advance and convenient API for modeling) Performance Visualization Model saving and reuse, reload 123456import tensorflow as tfimport tensorflow.keras as keras# This one-hot convert words to index number rather than binary formfrom tensorflow.keras.preprocessing.text import one_hot#tf.one_hot import text/ labels into binary 0,1 form without compressionfrom numba import cuda 12# Check if GPU is enabledprint(cuda.gpus) &lt;Managed Device 0&gt;, &lt;Managed Device 1&gt; Basic Practise tf.constant contain types of data. Operation like +, -, *, / , should be applied to data with same types only 12345678910ls = np.random.randint(0,100,50)ls= tf.constant(ls)x1= tf.constant(1)x2= tf.constant(2)x4= tf.constant(5)x3 = x1+x2*x4tf.print(x3,[x1,x2])tf.print(tf.argsort(ls))c = tf.argsort(ls) 11 [1, 2] [32 24 27 ... 20 40 49] Practice with Text+Numeric type data: titanic data12345678TRAIN_DATA_URL = \"https://storage.googleapis.com/tf-datasets/titanic/train.csv\"TEST_DATA_URL = \"https://storage.googleapis.com/tf-datasets/titanic/eval.csv\"# The first param in get_file: the name of dataset to save# The second param: url to the dataset trainset_path = keras.utils.get_file('titanic_train.csv', TRAIN_DATA_URL)testset_path = keras.utils.get_file('titanic_test.csv', TEST_DATA_URL)trainset_path, testset_path Downloading data from https://storage.googleapis.com/tf-datasets/titanic/train.csv 32768/30874 [===============================] - 0s 0us/step Downloading data from https://storage.googleapis.com/tf-datasets/titanic/eval.csv 16384/13049 [=====================================] - 0s 0us/step ('/home/wenkanw/.keras/datasets/titanic_train.csv', '/home/wenkanw/.keras/datasets/titanic_test.csv') 123# Load CSV file with pandastrainset = pd.read_csv(trainset_path)testset = pd.read_csv(testset_path) 1trainset.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } survived sex age n_siblings_spouses parch fare class deck embark_town alone 0 0 male 22.0 1 0 7.2500 Third unknown Southampton n 1 1 female 38.0 1 0 71.2833 First C Cherbourg n 2 1 female 26.0 0 0 7.9250 Third unknown Southampton y 3 1 female 35.0 1 0 53.1000 First C Southampton n 4 0 male 28.0 0 0 8.4583 Third unknown Queenstown y 1trainset.groupby('sex').head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } survived sex age n_siblings_spouses parch fare class deck embark_town alone 0 0 male 22.0 1 0 7.2500 Third unknown Southampton n 1 1 female 38.0 1 0 71.2833 First C Cherbourg n 2 1 female 26.0 0 0 7.9250 Third unknown Southampton y 3 1 female 35.0 1 0 53.1000 First C Southampton n 4 0 male 28.0 0 0 8.4583 Third unknown Queenstown y 5 0 male 2.0 3 1 21.0750 Third unknown Southampton n 6 1 female 27.0 0 2 11.1333 Third unknown Southampton n 7 1 female 14.0 1 0 30.0708 Second unknown Cherbourg n 9 0 male 20.0 0 0 8.0500 Third unknown Southampton y 10 0 male 39.0 1 5 31.2750 Third unknown Southampton n","link":"/2020/07/14/tensorflow-practice/"},{"title":"KKBox Music Recommendation System","text":"12from google.colab import drivedrive.mount('/content/drive') Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(&quot;/content/drive&quot;, force_remount=True). 1!pip install kaggle Requirement already satisfied: kaggle in /usr/local/lib/python3.6/dist-packages (1.5.9) Requirement already satisfied: python-slugify in /usr/local/lib/python3.6/dist-packages (from kaggle) (4.0.1) Requirement already satisfied: python-dateutil in /usr/local/lib/python3.6/dist-packages (from kaggle) (2.8.1) Requirement already satisfied: six&gt;=1.10 in /usr/local/lib/python3.6/dist-packages (from kaggle) (1.15.0) Requirement already satisfied: certifi in /usr/local/lib/python3.6/dist-packages (from kaggle) (2020.11.8) Requirement already satisfied: urllib3 in /usr/local/lib/python3.6/dist-packages (from kaggle) (1.24.3) Requirement already satisfied: slugify in /usr/local/lib/python3.6/dist-packages (from kaggle) (0.0.1) Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from kaggle) (4.41.1) Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from kaggle) (2.23.0) Requirement already satisfied: text-unidecode&gt;=1.3 in /usr/local/lib/python3.6/dist-packages (from python-slugify-&gt;kaggle) (1.3) Requirement already satisfied: idna&lt;3,&gt;=2.5 in /usr/local/lib/python3.6/dist-packages (from requests-&gt;kaggle) (2.10) Requirement already satisfied: chardet&lt;4,&gt;=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests-&gt;kaggle) (3.0.4) 1!cp /content/drive/My\\ Drive/Colab\\ Notebooks/KKBox-MusicRecommendationSystem/kaggle.json . 1234567# !!chmod 600 kaggle.json!mkdir -p ~/.kaggle!cp kaggle.json ~/.kaggle/!ls ~/.kaggle!mkdir -p ./kaggle/!chmod 600 /root/.kaggle/kaggle.json!kaggle competitions download kkbox-music-recommendation-challenge kaggle.json Warning: Looks like you're using an outdated API Version, please consider updating (server 1.5.9 / client 1.5.4) song_extra_info.csv.7z: Skipping, found more recently modified local copy (use --force to force download) train.csv.7z: Skipping, found more recently modified local copy (use --force to force download) test.csv.7z: Skipping, found more recently modified local copy (use --force to force download) songs.csv.7z: Skipping, found more recently modified local copy (use --force to force download) members.csv.7z: Skipping, found more recently modified local copy (use --force to force download) sample_submission.csv.7z: Skipping, found more recently modified local copy (use --force to force download) 123456789101112!mkdir kaggle/working!mkdir kaggle/working/train!mkdir kaggle/working/train/data!apt-get install p7zip!apt-get install p7zip-full !7za e members.csv.7z !7za e songs.csv.7z !7za e song_extra_info.csv.7z !7za e train.csv.7z !7za e sample_submission.csv.7z !7za e test.csv.7z !mv *.csv kaggle/working/train/data mkdir: cannot create directory ‘kaggle/working’: File exists mkdir: cannot create directory ‘kaggle/working/train’: File exists mkdir: cannot create directory ‘kaggle/working/train/data’: File exists Reading package lists... Done Building dependency tree Reading state information... Done p7zip is already the newest version (16.02+dfsg-6). 0 upgraded, 0 newly installed, 0 to remove and 14 not upgraded. Reading package lists... Done Building dependency tree Reading state information... Done p7zip-full is already the newest version (16.02+dfsg-6). 0 upgraded, 0 newly installed, 0 to remove and 14 not upgraded. 7-Zip (a) [64] 16.02 : Copyright (c) 1999-2016 Igor Pavlov : 2016-05-21 p7zip Version 16.02 (locale=en_US.UTF-8,Utf16=on,HugeFiles=on,64 bits,2 CPUs AMD EPYC 7B12 (830F10),ASM,AES-NI) Scanning the drive for archives: 0M Scan\b\b\b\b\b\b\b\b\b \b\b\b\b\b\b\b\b\b1 file, 1349856 bytes (1319 KiB) Extracting archive: members.csv.7z -- Path = members.csv.7z Type = 7z Physical Size = 1349856 Headers Size = 130 Method = LZMA2:3m Solid = - Blocks = 1 0%\b\b\b\b \b\b\b\bEverything is Ok Size: 2503827 Compressed: 1349856 7-Zip (a) [64] 16.02 : Copyright (c) 1999-2016 Igor Pavlov : 2016-05-21 p7zip Version 16.02 (locale=en_US.UTF-8,Utf16=on,HugeFiles=on,64 bits,2 CPUs AMD EPYC 7B12 (830F10),ASM,AES-NI) Scanning the drive for archives: 0M Scan\b\b\b\b\b\b\b\b\b \b\b\b\b\b\b\b\b\b1 file, 105809525 bytes (101 MiB) Extracting archive: songs.csv.7z -- Path = songs.csv.7z Type = 7z Physical Size = 105809525 Headers Size = 122 Method = LZMA2:24 Solid = - Blocks = 1 0%\b\b\b\b \b\b\b\b 2% - songs.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 5% - songs.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 8% - songs.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 11% - songs.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 14% - songs.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 17% - songs.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 20% - songs.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 24% - songs.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 27% - songs.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 30% - songs.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 33% - songs.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 35% - songs.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 39% - songs.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 41% - songs.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 45% - songs.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 49% - songs.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 51% - songs.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 54% - songs.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 57% - songs.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 60% - songs.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 62% - songs.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 65% - songs.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 68% - songs.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 70% - songs.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 73% - songs.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 76% - songs.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 79% - songs.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 82% - songs.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 85% - songs.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 87% - songs.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 90% - songs.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 92% - songs.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 95% - songs.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 98% - songs.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bEverything is Ok Size: 221828666 Compressed: 105809525 7-Zip (a) [64] 16.02 : Copyright (c) 1999-2016 Igor Pavlov : 2016-05-21 p7zip Version 16.02 (locale=en_US.UTF-8,Utf16=on,HugeFiles=on,64 bits,2 CPUs AMD EPYC 7B12 (830F10),ASM,AES-NI) Scanning the drive for archives: 0M Scan\b\b\b\b\b\b\b\b\b \b\b\b\b\b\b\b\b\b1 file, 103608205 bytes (99 MiB) Extracting archive: song_extra_info.csv.7z -- Path = song_extra_info.csv.7z Type = 7z Physical Size = 103608205 Headers Size = 140 Method = LZMA:25 Solid = - Blocks = 1 0%\b\b\b\b \b\b\b\b 1% - song_extra_info.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 4% - song_extra_info.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 6% - song_extra_info.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 9% - song_extra_info.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 11% - song_extra_info.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 13% - song_extra_info.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 16% - song_extra_info.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 18% - song_extra_info.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 20% - song_extra_info.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 23% - song_extra_info.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 25% - song_extra_info.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 27% - song_extra_info.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 30% - song_extra_info.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 32% - song_extra_info.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 34% - song_extra_info.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 37% - song_extra_info.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 39% - song_extra_info.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 41% - song_extra_info.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 44% - song_extra_info.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 46% - song_extra_info.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 48% - song_extra_info.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 50% - song_extra_info.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 53% - song_extra_info.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 55% - song_extra_info.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 57% - song_extra_info.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 60% - song_extra_info.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 62% - song_extra_info.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 65% - song_extra_info.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 68% - song_extra_info.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 71% - song_extra_info.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 74% - song_extra_info.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 76% - song_extra_info.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 78% - song_extra_info.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 81% - song_extra_info.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 83% - song_extra_info.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 85% - song_extra_info.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 88% - song_extra_info.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 92% - song_extra_info.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 95% - song_extra_info.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 99% - song_extra_info.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bEverything is Ok Size: 181010294 Compressed: 103608205 7-Zip (a) [64] 16.02 : Copyright (c) 1999-2016 Igor Pavlov : 2016-05-21 p7zip Version 16.02 (locale=en_US.UTF-8,Utf16=on,HugeFiles=on,64 bits,2 CPUs AMD EPYC 7B12 (830F10),ASM,AES-NI) Scanning the drive for archives: 0M Scan\b\b\b\b\b\b\b\b\b \b\b\b\b\b\b\b\b\b1 file, 106420688 bytes (102 MiB) Extracting archive: train.csv.7z -- Path = train.csv.7z Type = 7z Physical Size = 106420688 Headers Size = 122 Method = LZMA2:24 Solid = - Blocks = 1 0%\b\b\b\b \b\b\b\b 2% - train.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 4% - train.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 6% - train.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 9% - train.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 10% - train.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 13% - train.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 14% - train.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 17% - train.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 19% - train.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 21% - train.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 23% - train.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 25% - train.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 27% - train.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 29% - train.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 31% - train.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 33% - train.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 34% - train.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 36% - train.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 38% - train.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 41% - train.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 43% - train.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 46% - train.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 48% - train.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 50% - train.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 52% - train.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 54% - train.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 56% - train.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 58% - train.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 59% - train.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 62% - train.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 65% - train.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 68% - train.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 70% - train.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 73% - train.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 75% - train.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 76% - train.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 78% - train.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 80% - train.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 82% - train.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 84% - train.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 86% - train.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 88% - train.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 90% - train.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 91% - train.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 93% - train.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 95% - train.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 97% - train.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 99% - train.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bEverything is Ok Size: 971675848 Compressed: 106420688 7-Zip (a) [64] 16.02 : Copyright (c) 1999-2016 Igor Pavlov : 2016-05-21 p7zip Version 16.02 (locale=en_US.UTF-8,Utf16=on,HugeFiles=on,64 bits,2 CPUs AMD EPYC 7B12 (830F10),ASM,AES-NI) Scanning the drive for archives: 0M Scan\b\b\b\b\b\b\b\b\b \b\b\b\b\b\b\b\b\b1 file, 463688 bytes (453 KiB) Extracting archive: sample_submission.csv.7z -- Path = sample_submission.csv.7z Type = 7z Physical Size = 463688 Headers Size = 146 Method = LZMA2:24 Solid = - Blocks = 1 0%\b\b\b\b \b\b\b\bEverything is Ok Size: 29570380 Compressed: 463688 7-Zip (a) [64] 16.02 : Copyright (c) 1999-2016 Igor Pavlov : 2016-05-21 p7zip Version 16.02 (locale=en_US.UTF-8,Utf16=on,HugeFiles=on,64 bits,2 CPUs AMD EPYC 7B12 (830F10),ASM,AES-NI) Scanning the drive for archives: 0M Scan\b\b\b\b\b\b\b\b\b \b\b\b\b\b\b\b\b\b1 file, 43925208 bytes (42 MiB) Extracting archive: test.csv.7z -- Path = test.csv.7z Type = 7z Physical Size = 43925208 Headers Size = 122 Method = LZMA2:24 Solid = - Blocks = 1 0%\b\b\b\b \b\b\b\b 4% - test.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 8% - test.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 13% - test.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 18% - test.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 22% - test.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 28% - test.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 34% - test.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 42% - test.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 49% - test.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 55% - test.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 61% - test.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 67% - test.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 74% - test.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 80% - test.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 86% - test.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 90% - test.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 96% - test.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b \b\b\b\b\b\b\b\b\b\b\b\b\b\b\bEverything is Ok Size: 347789925 Compressed: 43925208 1234567891011121314151617# This Python 3 environment comes with many helpful analytics libraries installed# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python# For example, here's several helpful packages to loadimport numpy as np # linear algebraimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)# Input data files are available in the read-only \"../input/\" directory# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directoryimport osfor dirname, _, filenames in os.walk('./kaggle/input'): for filename in filenames: print(os.path.join(dirname, filename))# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save &amp; Run All\" # You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session Use 7z to uncompress the csv files1!ls ./kaggle/working/train/data/ members.csv song_extra_info.csv test.csv sample_submission.csv songs.csv train.csv 1!du -h ./kaggle/working/train/data/ 1.7G ./kaggle/working/train/data/ 1!cat /proc/cpuinfo processor : 0 vendor_id : AuthenticAMD cpu family : 23 model : 49 model name : AMD EPYC 7B12 stepping : 0 microcode : 0x1000065 cpu MHz : 2250.000 cache size : 512 KB physical id : 0 siblings : 2 core id : 0 cpu cores : 1 apicid : 0 initial apicid : 0 fpu : yes fpu_exception : yes cpuid level : 13 wp : yes flags : fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid extd_apicid tsc_known_freq pni pclmulqdq ssse3 fma cx16 sse4_1 sse4_2 movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm cmp_legacy cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw topoext ssbd ibrs ibpb stibp vmmcall fsgsbase tsc_adjust bmi1 avx2 smep bmi2 rdseed adx smap clflushopt clwb sha_ni xsaveopt xsavec xgetbv1 clzero xsaveerptr arat npt nrip_save umip rdpid bugs : sysret_ss_attrs spectre_v1 spectre_v2 spec_store_bypass bogomips : 4500.00 TLB size : 3072 4K pages clflush size : 64 cache_alignment : 64 address sizes : 48 bits physical, 48 bits virtual power management: processor : 1 vendor_id : AuthenticAMD cpu family : 23 model : 49 model name : AMD EPYC 7B12 stepping : 0 microcode : 0x1000065 cpu MHz : 2250.000 cache size : 512 KB physical id : 0 siblings : 2 core id : 0 cpu cores : 1 apicid : 1 initial apicid : 1 fpu : yes fpu_exception : yes cpuid level : 13 wp : yes flags : fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid extd_apicid tsc_known_freq pni pclmulqdq ssse3 fma cx16 sse4_1 sse4_2 movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm cmp_legacy cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw topoext ssbd ibrs ibpb stibp vmmcall fsgsbase tsc_adjust bmi1 avx2 smep bmi2 rdseed adx smap clflushopt clwb sha_ni xsaveopt xsavec xgetbv1 clzero xsaveerptr arat npt nrip_save umip rdpid bugs : sysret_ss_attrs spectre_v1 spectre_v2 spec_store_bypass bogomips : 4500.00 TLB size : 3072 4K pages clflush size : 64 cache_alignment : 64 address sizes : 48 bits physical, 48 bits virtual power management: KKBox-Music Recommendation SystemGoal of this projectIn this project, we are going to build a recommendation system to predict the chances of a user listening to a song repetitively after the first observable listening event within a time window was triggered. If there are recurring listening event(s) triggered within a month after the user’s very first observable listening event, its target is marked 1, and 0 otherwise in the training set. The same rule applies to the testing set. 0. Data Collection and DescriptionThe KKBox dataset is composed of following files: train.csv msno: user id song_id: song id source_system_tab: the name of the tab where the event was triggered. System tabs are used to categorize KKBOX mobile apps functions. For example, tab my library contains functions to manipulate the local storage, and tab search contains functions relating to search. source_screen_name: name of the layout a user sees. source_type: an entry point a user first plays music on mobile apps. An entry point could be album, online-playlist, song .. etc. target: this is the target variable. target=1 means there are recurring listening event(s) triggered within a month after the user’s very first observable listening event, target=0 otherwise . test.csv id: row id (will be used for submission) msno: user id song_id: song id source_system_tab: the name of the tab where the event was triggered. System tabs are used to categorize KKBOX mobile apps functions. For example, tab my library contains functions to manipulate the local storage, and tab search contains functions relating to search. source_screen_name: name of the layout a user sees. source_type: an entry point a user first plays music on mobile apps. An entry point could be album, online-playlist, song .. etc. sample_submission.csv sample submission file in the format that we expect you to submit id: same as id in test.csv target: this is the target variable. target=1 means there are recurring listening event(s) triggered within a month after the user’s very first observable listening event, target=0 otherwise . songs.csv The songs. Note that data is in unicode. song_id song_length: in ms genre_ids: genre category. Some songs have multiple genres and they are separated by | artist_name composer lyricist language members.csv user information. msno city bd: age. Note: this column has outlier values, please use your judgement. gender registered_via: registration method registration_init_time: format %Y%m%d expiration_date: format %Y%m%d song_extra_info.csv song_id song name - the name of the song. isrc - International Standard Recording Code, theoretically can be used as an identity of a song. However, what worth to note is, ISRCs generated from providers have not been officially verified; therefore the information in ISRC, such as country code and reference year, can be misleading/incorrect. Multiple songs could share one ISRC since a single recording could be re-published several times. 1. Data Cleaning and Exploratory Data Analysis (EDA) Find the Description and summary of each CSV file and Determine Null object, categorical attributes, numerical attributes Convert some attribute types to correct data type, like convert string to float, if necessary Handle Missing values Plot univariate, bivariate plots to visualize and analyze relationship between attributes and target Analysis Summary in this section 2. Data PreprocessingNote that This section is to give some examples to preprocess data like filling missing values and removing outliers. In order to train models, you should start from step 3 ETL to extract and transform data directly using integrated functions 3. Data Pipeline: Extract, Transformation, Load (ETL)4. Machine Learning Modeling LGBM Boosting machineIn the modeling part, we use LGBM model first, which is a light gradient boosting machine model using tree-based basic models for boosting. Since the dataset is large, 1.9 GB and the number of attributes can increase during transformation, LGBM provides a very fast way to train a machine model, so we try it here. In this part, we try LGBM model with different max_depth of tree: [10, 15,20, 25, 30] and see how max_depth affects the accuracy on prediction Wide and Deep Neural network modelIn addition to LGBM model, we are also interested in trying the Wide and Deep Neural network model since it is one of the popular neural network model in recommendation system and we want to see if this can help us improve the accuracy.In wide and deep model, It first uses a technique called embedding, which projects the sparse categorical features into dense features vectors with smaller dimension and extract the main features. Then it concatenates the embedded vectors with the numerical features together to train a traditional neural network classifier. 5. Model Training and validation In model training and validation step, we split the data set into training set(80% of dataset) and validation set (20% of dataset) and then use them to train and keep track of the performance of models. 6. Model EvaluationIn Model evaluation step, we simply use the validation set to validate the final trained models and then let models make predictions on testset from kaggle and submit predictions to kaggle to see the final evaluation scores. 7. Summary123456789101112#import necessary packages hereimport warningswarnings.filterwarnings('ignore')import numpy as np import pandas as pdimport matplotlib.pyplot as pltimport seaborn as snsimport lightgbm as lgbfrom subprocess import check_output# print(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))np.random.seed(2020) 1. Exploratory Data Analysis12345678root = './kaggle/working/train/data/'# !ls ../input/kkbox-music-recommendation-challengetrain_df = pd.read_csv(root+ \"train.csv\")test_df = pd.read_csv(root+ \"test.csv\")song_df = pd.read_csv(root+ \"songs.csv\")song_extra_df = pd.read_csv(root+ \"song_extra_info.csv\")members_df = pd.read_csv(root+ \"members.csv\")# sample_df = pd.read_csv(root+ \"sample_submission.csv\") 1.1 Take a look to the info and format of data1train_df.info() &lt;class 'pandas.core.frame.DataFrame'&gt; RangeIndex: 7377418 entries, 0 to 7377417 Data columns (total 6 columns): # Column Dtype --- ------ ----- 0 msno object 1 song_id object 2 source_system_tab object 3 source_screen_name object 4 source_type object 5 target int64 dtypes: int64(1), object(5) memory usage: 337.7+ MB 1train_df.count() msno 7377418 song_id 7377418 source_system_tab 7352569 source_screen_name 6962614 source_type 7355879 target 7377418 dtype: int64 We can see that attributes: source_system_tab, source_screen_name, source_type contain missing values1train_df.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } msno song_id source_system_tab source_screen_name source_type target 0 FGtllVqz18RPiwJj/edr2gV78zirAiY/9SmYvia+kCg= BBzumQNXUHKdEBOB7mAJuzok+IJA1c2Ryg/yzTF6tik= explore Explore online-playlist 1 1 Xumu+NIjS6QYVxDS4/t3SawvJ7viT9hPKXmf0RtLNx8= bhp/MpSNoqoxOIB+/l8WPqu6jldth4DIpCm3ayXnJqM= my library Local playlist more local-playlist 1 2 Xumu+NIjS6QYVxDS4/t3SawvJ7viT9hPKXmf0RtLNx8= JNWfrrC7zNN7BdMpsISKa4Mw+xVJYNnxXh3/Epw7QgY= my library Local playlist more local-playlist 1 3 Xumu+NIjS6QYVxDS4/t3SawvJ7viT9hPKXmf0RtLNx8= 2A87tzfnJTSWqD7gIZHisolhe4DMdzkbd6LzO1KHjNs= my library Local playlist more local-playlist 1 4 FGtllVqz18RPiwJj/edr2gV78zirAiY/9SmYvia+kCg= 3qm6XTZ6MOCU11x8FIVbAGH5l5uMkT3/ZalWG1oo2Gc= explore Explore online-playlist 1 1test_df.info() &lt;class 'pandas.core.frame.DataFrame'&gt; RangeIndex: 2556790 entries, 0 to 2556789 Data columns (total 6 columns): # Column Dtype --- ------ ----- 0 id int64 1 msno object 2 song_id object 3 source_system_tab object 4 source_screen_name object 5 source_type object dtypes: int64(1), object(5) memory usage: 117.0+ MB 1test_df.count() id 2556790 msno 2556790 song_id 2556790 source_system_tab 2548348 source_screen_name 2393907 source_type 2549493 dtype: int64 In test dataset, We can see that attributes that contain missing values: source_system_tab source_screen_name source_type 1test_df.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } id msno song_id source_system_tab source_screen_name source_type 0 0 V8ruy7SGk7tDm3zA51DPpn6qutt+vmKMBKa21dp54uM= WmHKgKMlp1lQMecNdNvDMkvIycZYHnFwDT72I5sIssc= my library Local playlist more local-library 1 1 V8ruy7SGk7tDm3zA51DPpn6qutt+vmKMBKa21dp54uM= y/rsZ9DC7FwK5F2PK2D5mj+aOBUJAjuu3dZ14NgE0vM= my library Local playlist more local-library 2 2 /uQAlrAkaczV+nWCd2sPF2ekvXPRipV7q0l+gbLuxjw= 8eZLFOdGVdXBSqoAv5nsLigeH2BvKXzTQYtUM53I0k4= discover NaN song-based-playlist 3 3 1a6oo/iXKatxQx4eS9zTVD+KlSVaAFbTIqVvwLC1Y0k= ztCf8thYsS4YN3GcIL/bvoxLm/T5mYBVKOO4C9NiVfQ= radio Radio radio 4 4 1a6oo/iXKatxQx4eS9zTVD+KlSVaAFbTIqVvwLC1Y0k= MKVMpslKcQhMaFEgcEQhEfi5+RZhMYlU3eRDpySrH8Y= radio Radio radio 1song_df.info() &lt;class 'pandas.core.frame.DataFrame'&gt; RangeIndex: 2296320 entries, 0 to 2296319 Data columns (total 7 columns): # Column Dtype --- ------ ----- 0 song_id object 1 song_length int64 2 genre_ids object 3 artist_name object 4 composer object 5 lyricist object 6 language float64 dtypes: float64(1), int64(1), object(5) memory usage: 122.6+ MB 1song_df.count() song_id 2296320 song_length 2296320 genre_ids 2202204 artist_name 2296320 composer 1224966 lyricist 351052 language 2296319 dtype: int64 Attributes that contain missing values are composer lyricist genre_ids language 1song_df.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } song_id song_length genre_ids artist_name composer lyricist language 0 CXoTN1eb7AI+DntdU1vbcwGRV4SCIDxZu+YD8JP8r4E= 247640 465 張信哲 (Jeff Chang) 董貞 何啟弘 3.0 1 o0kFgae9QtnYgRkVPqLJwa05zIhRlUjfF7O1tDw0ZDU= 197328 444 BLACKPINK TEDDY| FUTURE BOUNCE| Bekuh BOOM TEDDY 31.0 2 DwVvVurfpuz+XPuFvucclVQEyPqcpUkHR0ne1RQzPs0= 231781 465 SUPER JUNIOR NaN NaN 31.0 3 dKMBWoZyScdxSkihKG+Vf47nc18N9q4m58+b4e7dSSE= 273554 465 S.H.E 湯小康 徐世珍 3.0 4 W3bqWd3T+VeHFzHAUfARgW9AvVRaF4N5Yzm4Mr6Eo/o= 140329 726 貴族精選 Traditional Traditional 52.0 1song_extra_df.info() &lt;class 'pandas.core.frame.DataFrame'&gt; RangeIndex: 2295971 entries, 0 to 2295970 Data columns (total 3 columns): # Column Dtype --- ------ ----- 0 song_id object 1 name object 2 isrc object dtypes: object(3) memory usage: 52.6+ MB 1song_extra_df.count() song_id 2295971 name 2295969 isrc 2159423 dtype: int64 1song_extra_df.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } song_id name isrc 0 LP7pLJoJFBvyuUwvu+oLzjT+bI+UeBPURCecJsX1jjs= 我們 TWUM71200043 1 ClazTFnk6r0Bnuie44bocdNMM3rdlrq0bCGAsGUWcHE= Let Me Love You QMZSY1600015 2 u2ja/bZE3zhCGxvbbOB3zOoUjx27u40cf5g09UXMoKQ= 原諒我 TWA530887303 3 92Fqsy0+p6+RHe2EoLKjHahORHR1Kq1TBJoClW9v+Ts= Classic USSM11301446 4 0QFmz/+rJy1Q56C1DuYqT9hKKqi5TUqx0sN0IwvoHrw= 愛投羅網 TWA471306001 1members_df.info() &lt;class 'pandas.core.frame.DataFrame'&gt; RangeIndex: 34403 entries, 0 to 34402 Data columns (total 7 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 msno 34403 non-null object 1 city 34403 non-null int64 2 bd 34403 non-null int64 3 gender 14501 non-null object 4 registered_via 34403 non-null int64 5 registration_init_time 34403 non-null int64 6 expiration_date 34403 non-null int64 dtypes: int64(5), object(2) memory usage: 1.8+ MB 1train_df['song_id'].head() 0 BBzumQNXUHKdEBOB7mAJuzok+IJA1c2Ryg/yzTF6tik= 1 bhp/MpSNoqoxOIB+/l8WPqu6jldth4DIpCm3ayXnJqM= 2 JNWfrrC7zNN7BdMpsISKa4Mw+xVJYNnxXh3/Epw7QgY= 3 2A87tzfnJTSWqD7gIZHisolhe4DMdzkbd6LzO1KHjNs= 4 3qm6XTZ6MOCU11x8FIVbAGH5l5uMkT3/ZalWG1oo2Gc= Name: song_id, dtype: object 1song_df['song_id'].head() 0 CXoTN1eb7AI+DntdU1vbcwGRV4SCIDxZu+YD8JP8r4E= 1 o0kFgae9QtnYgRkVPqLJwa05zIhRlUjfF7O1tDw0ZDU= 2 DwVvVurfpuz+XPuFvucclVQEyPqcpUkHR0ne1RQzPs0= 3 dKMBWoZyScdxSkihKG+Vf47nc18N9q4m58+b4e7dSSE= 4 W3bqWd3T+VeHFzHAUfARgW9AvVRaF4N5Yzm4Mr6Eo/o= Name: song_id, dtype: object 123print(\"Unique Song amount in trainset:\",train_df['song_id'].nunique())print(\"Unique Song amount in testset:\", test_df['song_id'].nunique())print(\"Unique Song amount in song list:\",song_df['song_id'].nunique()) Unique Song amount in trainset: 359966 Unique Song amount in testset: 224753 Unique Song amount in song list: 2296320 1.2 Explore Song information12345# Merge two dataframe based on song_id so that we can analyze the song information together with training datauser_music_df = train_df.merge(song_df,on='song_id',how=\"left\", copy =False)user_music_df[\"song_id\"] = user_music_df[\"song_id\"].astype(\"category\")user_music_df.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } msno song_id source_system_tab source_screen_name source_type target song_length genre_ids artist_name composer lyricist language 0 FGtllVqz18RPiwJj/edr2gV78zirAiY/9SmYvia+kCg= BBzumQNXUHKdEBOB7mAJuzok+IJA1c2Ryg/yzTF6tik= explore Explore online-playlist 1 206471.0 359 Bastille Dan Smith| Mark Crew NaN 52.0 1 Xumu+NIjS6QYVxDS4/t3SawvJ7viT9hPKXmf0RtLNx8= bhp/MpSNoqoxOIB+/l8WPqu6jldth4DIpCm3ayXnJqM= my library Local playlist more local-playlist 1 284584.0 1259 Various Artists NaN NaN 52.0 2 Xumu+NIjS6QYVxDS4/t3SawvJ7viT9hPKXmf0RtLNx8= JNWfrrC7zNN7BdMpsISKa4Mw+xVJYNnxXh3/Epw7QgY= my library Local playlist more local-playlist 1 225396.0 1259 Nas N. Jones、W. Adams、J. Lordan、D. Ingle NaN 52.0 3 Xumu+NIjS6QYVxDS4/t3SawvJ7viT9hPKXmf0RtLNx8= 2A87tzfnJTSWqD7gIZHisolhe4DMdzkbd6LzO1KHjNs= my library Local playlist more local-playlist 1 255512.0 1019 Soundway Kwadwo Donkoh NaN -1.0 4 FGtllVqz18RPiwJj/edr2gV78zirAiY/9SmYvia+kCg= 3qm6XTZ6MOCU11x8FIVbAGH5l5uMkT3/ZalWG1oo2Gc= explore Explore online-playlist 1 187802.0 1011 Brett Young Brett Young| Kelly Archer| Justin Ebach NaN 52.0 1user_music_df['song_id'].nunique(), user_music_df['genre_ids'].nunique() (359966, 572) 1user_music_df.count() msno 7377418 song_id 7377418 source_system_tab 7352569 source_screen_name 6962614 source_type 7355879 target 7377418 song_length 7377304 genre_ids 7258963 artist_name 7377304 composer 5701712 lyricist 4198620 language 7377268 dtype: int64 1234567# plot the top-20 frequent genre_ids df_genre = user_music_df.sample(n=5000)df_genre = df_genre[\"genre_ids\"].value_counts().sort_values(ascending=False)[:20]df_genre = df_genre.sort_values(ascending=True)ax = df_genre.plot.barh(figsize=(15,8))ax.set_ylabel(\"song genre ids\")ax.set_xlabel(\"Count\") Text(0.5, 0, 'Count') 12345678910111213#selec the top-20 frequent artist_namedf_artist = user_music_df[\"artist_name\"].value_counts().sort_values(ascending=False)[:20]#plot in descending order in horizonal directiondf_artist = df_artist.sort_values(ascending=True)ax = df_artist.plot.barh(figsize=(15,10))ax.set_ylabel(\"song artist_name\")ax.set_xlabel(\"Count\")# artist_name # composer # lyricist Text(0.5, 0, 'Count') 1df_artist.head(10) The Chainsmokers 44215 梁靜茹 (Fish Leong) 44290 丁噹 (Della) 45762 楊丞琳 (Rainie Yang) 46006 蘇打綠 (Sodagreen) 47177 蔡依林 (Jolin Tsai) 49055 Eric 周興哲 49426 A-Lin 52913 Maroon 5 55151 謝和弦 (R-chord) 57040 Name: artist_name, dtype: int64 12345678fig, ax = plt.subplots(1, figsize=(15,8))df_composer = user_music_df[\"composer\"].value_counts().sort_values(ascending=False)[:20]ax = sns.barplot([i for i in df_composer.index],df_composer,ax= ax)ax.set_xlabel(\"song composer\")ax.set_ylabel(\"Count\") Text(0, 0.5, 'Count') 1df_composer.head(20).index Index(['周杰倫', '阿信', '林俊傑', '陳皓宇', 'JJ Lin', '張簡君偉', 'Eric Chou', '韋禮安', '八三夭 阿璞', 'R-chord', '怪獸', '吳青峰', '周湯豪', 'G.E.M. 鄧紫棋', '陳小霞', 'JerryC', '吳克群', '薛之謙', 'Rocoberry', '李榮浩'], dtype='object') Analyse the relationship between target and song 123456789101112131415161718192021222324252627fig, ax = plt.subplots(3,1,figsize=(15,18))# df = user_music_df[['source_system_tab','source_screen_name','source_type']]# df['source_system_tab'].value_counts().plot.bar(rot=20,ax=ax[0])# ax[0].set_xlabel(\"source_system_tab\")# ax[0].set_ylabel(\"count\")# df['source_screen_name'].value_counts().plot.bar(rot=30,ax=ax[1])# ax[1].set_xlabel(\"source_screen_name\")# ax[1].set_ylabel(\"count\")# df['source_type'].value_counts().plot.bar(rot=20,ax=ax[2])# ax[2].set_xlabel(\"source_type\")# ax[2].set_ylabel(\"count\")sns.countplot(y= 'source_system_tab',hue='target', order = user_music_df['source_system_tab'].value_counts().index, data=user_music_df,dodge=True, ax= ax[0])sns.countplot(y= 'source_screen_name',hue='target', order = user_music_df['source_screen_name'].value_counts().index, data=user_music_df,dodge=True, ax= ax[1])sns.countplot(y= 'source_type',hue='target', order = user_music_df['source_type'].value_counts().index, data=user_music_df,dodge=True, ax= ax[2]) &lt;matplotlib.axes._subplots.AxesSubplot at 0x7f269a90e128&gt; We can see that local library and local playlist are the main sources that users repeat playing music and Most of users more prefer to play music from local library than to play music onlineAnalyze Relationship between Target and members info1members_df.info() &lt;class 'pandas.core.frame.DataFrame'&gt; RangeIndex: 34403 entries, 0 to 34402 Data columns (total 7 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 msno 34403 non-null object 1 city 34403 non-null int64 2 bd 34403 non-null int64 3 gender 14501 non-null object 4 registered_via 34403 non-null int64 5 registration_init_time 34403 non-null int64 6 expiration_date 34403 non-null int64 dtypes: int64(5), object(2) memory usage: 1.8+ MB 12members_df[\"registration_init_time\"] = pd.to_datetime(members_df[\"registration_init_time\"], format=\"%Y%m%d\")members_df[\"expiration_date\"] = pd.to_datetime(members_df[\"expiration_date\"], format=\"%Y%m%d\") Parse the datetime data12345678members_df[\"registration_init_day\"] = members_df[\"registration_init_time\"].dt.daymembers_df[\"registration_init_month\"] = members_df[\"registration_init_time\"].dt.monthmembers_df[\"registration_init_year\"] = members_df[\"registration_init_time\"].dt.yearmembers_df[\"expiration_day\"] = members_df[\"expiration_date\"].dt.daymembers_df[\"expiration_month\"] = members_df[\"expiration_date\"].dt.monthmembers_df[\"expiration_year\"] = members_df[\"expiration_date\"].dt.yearmembers_df = members_df.drop(columns = [\"registration_init_time\", \"expiration_date\"],axis=1)members_df.info() &lt;class 'pandas.core.frame.DataFrame'&gt; RangeIndex: 34403 entries, 0 to 34402 Data columns (total 11 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 msno 34403 non-null object 1 city 34403 non-null int64 2 bd 34403 non-null int64 3 gender 14501 non-null object 4 registered_via 34403 non-null int64 5 registration_init_day 34403 non-null int64 6 registration_init_month 34403 non-null int64 7 registration_init_year 34403 non-null int64 8 expiration_day 34403 non-null int64 9 expiration_month 34403 non-null int64 10 expiration_year 34403 non-null int64 dtypes: int64(9), object(2) memory usage: 2.9+ MB 123456member_music_df = user_music_df.merge(members_df,on='msno',how=\"left\", copy=False)#after merging, the axis used to merge becomes object type,so need to convert it back to category typemember_music_df[\"msno\"] = member_music_df[\"msno\"].astype(\"category\")member_music_df[\"song_id\"] = member_music_df[\"song_id\"].astype(\"category\")member_music_df.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } msno song_id source_system_tab source_screen_name source_type target song_length genre_ids artist_name composer lyricist language city bd gender registered_via registration_init_day registration_init_month registration_init_year expiration_day expiration_month expiration_year 0 FGtllVqz18RPiwJj/edr2gV78zirAiY/9SmYvia+kCg= BBzumQNXUHKdEBOB7mAJuzok+IJA1c2Ryg/yzTF6tik= explore Explore online-playlist 1 206471.0 359 Bastille Dan Smith| Mark Crew NaN 52.0 1 0 NaN 7 2 1 2012 5 10 2017 1 Xumu+NIjS6QYVxDS4/t3SawvJ7viT9hPKXmf0RtLNx8= bhp/MpSNoqoxOIB+/l8WPqu6jldth4DIpCm3ayXnJqM= my library Local playlist more local-playlist 1 284584.0 1259 Various Artists NaN NaN 52.0 13 24 female 9 25 5 2011 11 9 2017 2 Xumu+NIjS6QYVxDS4/t3SawvJ7viT9hPKXmf0RtLNx8= JNWfrrC7zNN7BdMpsISKa4Mw+xVJYNnxXh3/Epw7QgY= my library Local playlist more local-playlist 1 225396.0 1259 Nas N. Jones、W. Adams、J. Lordan、D. Ingle NaN 52.0 13 24 female 9 25 5 2011 11 9 2017 3 Xumu+NIjS6QYVxDS4/t3SawvJ7viT9hPKXmf0RtLNx8= 2A87tzfnJTSWqD7gIZHisolhe4DMdzkbd6LzO1KHjNs= my library Local playlist more local-playlist 1 255512.0 1019 Soundway Kwadwo Donkoh NaN -1.0 13 24 female 9 25 5 2011 11 9 2017 4 FGtllVqz18RPiwJj/edr2gV78zirAiY/9SmYvia+kCg= 3qm6XTZ6MOCU11x8FIVbAGH5l5uMkT3/ZalWG1oo2Gc= explore Explore online-playlist 1 187802.0 1011 Brett Young Brett Young| Kelly Archer| Justin Ebach NaN 52.0 1 0 NaN 7 2 1 2012 5 10 2017 1member_music_df.info() &lt;class 'pandas.core.frame.DataFrame'&gt; Int64Index: 7377418 entries, 0 to 7377417 Data columns (total 22 columns): # Column Dtype --- ------ ----- 0 msno category 1 song_id category 2 source_system_tab object 3 source_screen_name object 4 source_type object 5 target int64 6 song_length float64 7 genre_ids object 8 artist_name object 9 composer object 10 lyricist object 11 language float64 12 city int64 13 bd int64 14 gender object 15 registered_via int64 16 registration_init_day int64 17 registration_init_month int64 18 registration_init_year int64 19 expiration_day int64 20 expiration_month int64 21 expiration_year int64 dtypes: category(2), float64(2), int64(10), object(8) memory usage: 1.2+ GB 1member_music_df.count() msno 7377418 song_id 7377418 source_system_tab 7352569 source_screen_name 6962614 source_type 7355879 target 7377418 song_length 7377304 genre_ids 7258963 artist_name 7377304 composer 5701712 lyricist 4198620 language 7377268 city 7377418 bd 7377418 gender 4415939 registered_via 7377418 registration_init_day 7377418 registration_init_month 7377418 registration_init_year 7377418 expiration_day 7377418 expiration_month 7377418 expiration_year 7377418 dtype: int64 1member_music_df['bd'].describe() count 7.377418e+06 mean 1.753927e+01 std 2.155447e+01 min -4.300000e+01 25% 0.000000e+00 50% 2.100000e+01 75% 2.900000e+01 max 1.051000e+03 Name: bd, dtype: float64 Visualize distribution of age: bd attributionNote: Since this attribute has outliers, I use remove the data that lies outside range [0,100] 123456789fig, ax = plt.subplots(2, figsize= (15,8))age_df = member_music_df['bd'].loc[(member_music_df['bd']&gt;0) &amp; (member_music_df['bd']&lt;100)]age_df.hist(ax = ax[0])ax[0].set_ylabel(\"count\")member_music_df['bd'].loc[(member_music_df['bd']&lt;0) | (member_music_df['bd']&gt;100)].hist(ax = ax[1])ax[1].set_xlabel(\"age\")ax[1].set_ylabel(\"count\") Text(0, 0.5, 'count') We can see that bd/age has outliers outside range [0,100], so we want to replace the incorrect bd with NaN1member_music_df['bd'].loc[(member_music_df['bd']&lt;=0) | (member_music_df['bd']&gt;=100)]= np.nan 12# member_music_df.describe() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } target song_length language city bd registered_via registration_init_day registration_init_month registration_init_year expiration_day expiration_month expiration_year count 7.377418e+06 7.377304e+06 7.377268e+06 7.377418e+06 4.430216e+06 7.377418e+06 7.377418e+06 7.377418e+06 7.377418e+06 7.377418e+06 7.377418e+06 7.377418e+06 mean 5.035171e-01 2.451210e+05 1.860933e+01 7.511399e+00 2.872200e+01 6.794068e+00 1.581532e+01 6.832306e+00 2.012741e+03 1.562338e+01 8.341742e+00 2.017072e+03 std 4.999877e-01 6.734471e+04 2.117681e+01 6.641625e+00 8.634326e+00 2.275774e+00 8.768549e+00 3.700723e+00 3.018861e+00 9.107235e+00 2.511360e+00 3.982536e-01 min 0.000000e+00 1.393000e+03 -1.000000e+00 1.000000e+00 2.000000e+00 3.000000e+00 1.000000e+00 1.000000e+00 2.004000e+03 1.000000e+00 1.000000e+00 1.970000e+03 25% 0.000000e+00 2.147260e+05 3.000000e+00 1.000000e+00 2.300000e+01 4.000000e+00 8.000000e+00 3.000000e+00 2.011000e+03 8.000000e+00 9.000000e+00 2.017000e+03 50% 1.000000e+00 2.418120e+05 3.000000e+00 5.000000e+00 2.700000e+01 7.000000e+00 1.600000e+01 7.000000e+00 2.013000e+03 1.500000e+01 9.000000e+00 2.017000e+03 75% 1.000000e+00 2.721600e+05 5.200000e+01 1.300000e+01 3.300000e+01 9.000000e+00 2.300000e+01 1.000000e+01 2.015000e+03 2.300000e+01 1.000000e+01 2.017000e+03 max 1.000000e+00 1.085171e+07 5.900000e+01 2.200000e+01 9.500000e+01 1.300000e+01 3.100000e+01 1.200000e+01 2.017000e+03 3.100000e+01 1.200000e+01 2.020000e+03 1234#dataframe.corr() is used to find the pairwise correlation of all columns in the dataframe. #Any na values are automatically excluded. For any non-numeric data type columns in the data frame it is ignored.corr_matrix = member_music_df.corr()_ = sns.heatmap(corr_matrix) 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354#print the top threee attributes that have the strongest correlation with \"Target\" and the corresponding correlation coefficients.corr = corr_matrix['target'].sort_values(ascending= False)for x in corr.index[1:4].to_list(): print(\"{} {}\".format(x, corr[x]))print(\"\")#print the top threee attributes that have the strongest correlation with \"song_length\" and the corresponding correlation coefficients.corr = corr_matrix['song_length'].sort_values(ascending= False)for x in corr.index[1:4].to_list(): print(\"{} {}\".format(x, corr[x]))print(\"\")#print the top threee attributes that have the strongest correlation with \"language\" and the corresponding correlation coefficients.corr = corr_matrix['language'].sort_values(ascending= False)for x in corr.index[1:4].to_list(): print(\"{} {}\".format(x, corr[x]))print(\"\")#print the top threee attributes that have the strongest correlation with \"city\" and the corresponding correlation coefficients.corr = corr_matrix['city'].sort_values(ascending= False)for x in corr.index[1:4].to_list(): print(\"{} {}\".format(x, corr[x]))print(\"\")#print the top threee attributes that have the strongest correlation with \"bd\" and the corresponding correlation coefficients.corr = corr_matrix['bd'].sort_values(ascending= False)for x in corr.index[1:4].to_list(): print(\"{} {}\".format(x, corr[x]))print(\"\")#print the top threee attributes that have the strongest correlation with \"registered_via\" and the corresponding correlation coefficients.corr = corr_matrix['registered_via'].sort_values(ascending= False)for x in corr.index[1:4].to_list(): print(\"{} {}\".format(x, corr[x]))print(\"\")corr = corr_matrix['registration_init_day'].sort_values(ascending= False)for x in corr.index[1:4].to_list(): print(\"{} {}\".format(x, corr[x]))print(\"\")corr = corr_matrix['registration_init_month'].sort_values(ascending= False)for x in corr.index[1:4].to_list(): print(\"{} {}\".format(x, corr[x]))print(\"\")corr = corr_matrix['registration_init_year'].sort_values(ascending= False)for x in corr.index[1:4].to_list(): print(\"{} {}\".format(x, corr[x]))print(\"\")corr = corr_matrix['expiration_day'].sort_values(ascending= False)for x in corr.index[1:4].to_list(): print(\"{} {}\".format(x, corr[x]))print(\"\")corr = corr_matrix['expiration_month'].sort_values(ascending= False)for x in corr.index[1:4].to_list(): print(\"{} {}\".format(x, corr[x]))print(\"\")corr = corr_matrix['expiration_year'].sort_values(ascending= False)for x in corr.index[1:4].to_list(): print(\"{} {}\".format(x, corr[x]))print(\"\") expiration_year 0.042248332355979766 city 0.01211438566189457 expiration_month 0.011817072086387569 bd 0.009861302779254176 city 0.005184912771179072 expiration_year 0.00457185870016758 registration_init_year 0.009070490482763404 registration_init_day 0.001510510575428178 bd 0.001107978394135987 expiration_year 0.15014690465127595 registered_via 0.0737556175747622 target 0.01211438566189457 registered_via 0.1753390015877422 expiration_day 0.056335854806629254 expiration_month 0.032935904360496926 bd 0.1753390015877422 expiration_year 0.08413460079453493 city 0.0737556175747622 expiration_day 0.1493505099924221 registration_init_month 0.04443692475737983 registered_via 0.02554331305533987 expiration_month 0.056911114419175665 registration_init_day 0.04443692475737983 bd 0.005399463812914416 language 0.009070490482763404 target -0.00196242388069252 song_length -0.007434856516605977 registration_init_day 0.1493505099924221 registered_via 0.05695618668075027 bd 0.056335854806629254 registered_via 0.0647318000666518 registration_init_month 0.056911114419175665 bd 0.032935904360496926 city 0.15014690465127595 registered_via 0.08413460079453493 target 0.042248332355979766 123456#graphfig, ax = plt.subplots(1,1,figsize=(10,8), sharex=False)plt.scatter(x = member_music_df['city'],y = member_music_df['bd'])ax.set_ylabel(\"bd\")ax.set_xlabel(\"city\")plt.show() 123456#graphfig, ax = plt.subplots(1,1,figsize=(10,8), sharex=False)plt.scatter(x = member_music_df['target'],y = member_music_df['expiration_year'])ax.set_ylabel(\"expiration_year\")ax.set_xlabel(\"target\")plt.show() 12345#user的musiclist里面可能重听的music print(train_df.target.value_counts()*100/train_df.target.value_counts().sum())print('unique songs ',len(train_df.song_id.unique()))#unique() = Return unique values of Series object.#len() to find unqiue sound. 1 50.351708 0 49.648292 Name: target, dtype: float64 unique songs 359966 1234567repeats=train_df[train_df.target==1]song_repeats=repeats.groupby('song_id',as_index=False).msno.count()song_repeats.columns=['song_id','count']##merge together 2 dataframe and create a new dataframesong_repeats=pd.DataFrame(song_repeats).merge(song_df,left_on='song_id',right_on='song_id')print(\"Print top 50 songs repeated\")repeats.song_id.value_counts().head(50) Print top 50 songs repeated reXuGcEWDDCnL0K3Th//3DFG4S1ACSpJMzA+CFipo1g= 10885 T86YHdD4C9JSc274b1IlMkLuNdz4BQRB50fWWE7hx9g= 10556 FynUyq0+drmIARmK1JZ/qcjNZ7DKkqTY6/0O0lTzNUI= 9808 wBTWuHbjdjxnG1lQcbqnK4FddV24rUhuyrYLd9c/hmk= 9411 PgRtmmESVNtWjoZHO5a1r21vIz9sVZmcJJpFCbRa1LI= 9004 U9kojfZSKaiWOW94PKh1Riyv/zUWxmBRmv0XInQWLGw= 8787 YN4T/yvvXtYrBVN8KTnieiQohHL3T9fnzUkbLWcgLro= 8780 M9rAajz4dYuRhZ7jLvf9RRayVA3os61X/XXHEuW4giA= 8403 43Qm2YzsP99P5wm37B1JIhezUcQ/1CDjYlQx6rBbz2U= 8112 J4qKkLIoW7aYACuTupHLAPZYmRp08en1AEux+GSUzdw= 7903 cy10N2j2sdY/X4BDUcMu2Iumfz7pV3tqE5iEaup2yGI= 7725 750RprmFfLV0bymtDH88g24pLZGVi5VpBAI300P6UOA= 7608 IKMFuL0f5Y8c63Hg9BXkeNJjE0z8yf3gMt/tOxF4QNE= 7224 +SstqMwhQPBQFTPBhLKPT642IiBDXzZFwlzsLl4cGXo= 7061 DLBDZhOoW7zd7GBV99bi92ZXYUS26lzV+jJKbHshP5c= 6901 v/3onppBGoSpGsWb8iaCIO8eX5+iacbH5a4ZUhT7N54= 6879 p/yR06j/RQ2J6yGCFL0K+1R06OeG+eXcwxRgOHDo/Tk= 6536 Xpjwi8UAE2Vv9PZ6cZnhc58MCtl3cKZEO1sdAkqJ4mo= 6399 OaEbZ6TJ1NePtNUeEgWsvFLeopkSln9WQu8PBR5B3+A= 6187 BITuBuNyXQydJcjDL2BUnCu4/IXaJg5IPOuycc/4dtY= 6160 BgqjNqzsyCpEGvxyUmktvHC8WO5+FQO/pQTaZ4broMU= 6140 3VkD5ekIf5duJm1hmYTZlXjyl0zqV8wCzuAh3uocfCg= 6012 8Ckw1wek5d6oEsNUoM4P5iag86TaEmyLwdtrckL0Re8= 6003 n+pMhj/jpCnpiUcSDl4k3i9FJODDddEXmpE48/HczTI= 5787 WL4ipO3Mx9pxd4FMs69ha6o9541+fLeOow67Qkrfnro= 5784 /70HjygVDhHsKBoV8mmsBg/WduSgs4+Zg6GfzhUQbdk= 5588 L6w2d0w84FjTvFr+BhMfgu7dZAsGiOqUGmvvxIG3gvQ= 5480 fEAIgFRWmhXmo6m3ukQeqRksZCcO/7CjkqNckRHiVQo= 5460 +Sm75wnBf/sjm/QMUAFx8N+Ae04kWCXGlgH50tTeM6c= 5412 VkDBgh89umc9m6uAEfD6LXngetyGhln4vh/ArCGO0nY= 5361 fCCmIa0Y5m+MCGbQga31MOLTIqi7ddgXvkjFPmfslGw= 5305 +LztcJcPEEwsikk6+K5udm06XJQMzR4+lzavKLUyE0k= 5298 o9HWMBZMeIPnYEpSuscGoORKE44sj3BYOdvGuIi0P68= 5233 QZBm8SOwnEjNfCpgsKBBGPMGET6y6XaQgnJiirspW7I= 5224 ClazTFnk6r0Bnuie44bocdNMM3rdlrq0bCGAsGUWcHE= 5202 wp1gSQ4LlMEF6bzvEaJl8VdHlAj/EJMTJ0ASrXeddbo= 5110 THqGcrzQyUhBn1NI/+Iptc1vKtxBIEg0uA8iaoJnO1Q= 5086 ys+EL8Sok4HC4i7sDY0+slDNGVZ8+uOQi6TQ6g8VSF4= 5012 zHqZ07gn+YvF36FWzv9+y8KiCMhYhdAUS+vSIKY3UZY= 5001 8f/T4ohROj1wa25YHMItOW2/wJhRXZM0+T5/2p86COc= 4982 G/4+VCRLpfjQJ4SAwMDcf+W8PTw0eOBRgFvg4fHUOO8= 4956 KZ5hwP74wRO6kRapVIprwodtNdVD2EVD3hkZmmyXFPk= 4888 MtFK4NN8Kv1k/xPA3wb8SQaP/jWee52FAaC1s9NFsU4= 4813 UQeOwfhcqgEcIwp3cgNiLGW1237Qjpvqzt/asQimVp0= 4778 JA6C0GEK1sSCVbHyqtruH/ARD1NKolYrw7HXy6EVNAc= 4766 8qWeDv6RTv+hYJxW94e7n6HBzHPGPEZW9FuGhj6pPhQ= 4761 35dx60z4m4+Lg+qIS0l2A8vspbthqnpTylWUu51jW+4= 4679 r4lUPUkz3tAgIWaEyrSYVCxX1yz8PnlVuQz+To0Pd+c= 4650 1PR/lVwL4VeYcZjexwBJ2NOSTfgh8JoVxWCunnbJO/8= 4592 7EnDBkQYJpipCyRd9JBsug4iKnfAunUXc14/96cNotg= 4571 Name: song_id, dtype: int64 2. Data PreprocessingNote: This section is to show how to preprocess data. We can also directly start from Step 3 for data extract, transformation and load using integrated transformation function and skip this step if necessary 2.1 Filling missing values12missing_value_cols = [c for c in member_music_df.columns if member_music_df[c].isnull().any()]missing_value_cols ['source_system_tab', 'source_screen_name', 'source_type', 'song_length', 'genre_ids', 'artist_name', 'composer', 'lyricist', 'language', 'bd', 'gender'] 1member_music_df.count() msno 7377418 song_id 7377418 source_system_tab 7352569 source_screen_name 6962614 source_type 7355879 target 7377418 song_length 7377304 genre_ids 7258963 artist_name 7377304 composer 5701712 lyricist 4198620 language 7377268 city 7377418 bd 4430216 gender 4415939 registered_via 7377418 registration_init_day 7377418 registration_init_month 7377418 registration_init_year 7377418 expiration_day 7377418 expiration_month 7377418 expiration_year 7377418 dtype: int64 1234567891011121314151617181920212223242526272829# list of columns with missing values# ['source_system_tab',# 'source_screen_name',# 'source_type',# 'song_length',# 'genre_ids',# 'artist_name',# 'composer',# 'lyricist',# 'language',# 'bd',# 'gender']def fill_missing_value_v1(x): # fill missing values with the most frequent values return x.fillna(x.value_counts().sort_values(ascending=False).index[0]) categorical_ls = ['source_system_tab', 'source_screen_name','source_type','genre_ids','artist_name','composer', 'lyricist','gender']numerical_ls = ['song_length','language','bd']# Fill missing values for index in numerical_ls: member_music_df[index].fillna(member_music_df[index].median(), inplace=True)for index in categorical_ls: member_music_df[index].fillna(\"no_data\", inplace=True) 1member_music_df.count() msno 7377418 song_id 7377418 source_system_tab 7377418 source_screen_name 7377418 source_type 7377418 target 7377418 song_length 7377418 genre_ids 7377418 artist_name 7377418 composer 7377418 lyricist 7377418 language 7377418 city 7377418 bd 7377418 gender 7377418 registered_via 7377418 registration_init_day 7377418 registration_init_month 7377418 registration_init_year 7377418 expiration_day 7377418 expiration_month 7377418 expiration_year 7377418 dtype: int64 12member_music_df[numerical_ls].head(100) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } song_length language bd 0 206471.0 52.0 27.0 1 284584.0 52.0 24.0 2 225396.0 52.0 24.0 3 255512.0 -1.0 24.0 4 187802.0 52.0 27.0 ... ... ... ... 95 333024.0 3.0 27.0 96 288391.0 3.0 46.0 97 279196.0 3.0 46.0 98 240744.0 3.0 46.0 99 221622.0 3.0 46.0 100 rows × 3 columns 12member_music_df[categorical_ls].head(100) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } source_system_tab source_screen_name source_type genre_ids artist_name composer lyricist gender 0 explore Explore online-playlist 359 Bastille Dan Smith| Mark Crew no_data no_data 1 my library Local playlist more local-playlist 1259 Various Artists no_data no_data female 2 my library Local playlist more local-playlist 1259 Nas N. Jones、W. Adams、J. Lordan、D. Ingle no_data female 3 my library Local playlist more local-playlist 1019 Soundway Kwadwo Donkoh no_data female 4 explore Explore online-playlist 1011 Brett Young Brett Young| Kelly Archer| Justin Ebach no_data no_data ... ... ... ... ... ... ... ... ... 95 my library no_data local-library 458 楊乃文 (Naiwen Yang) 黃建為 葛大為 male 96 my library Local playlist more local-library 458 陳奕迅 (Eason Chan) Jun Jie Lin no_data female 97 my library Local playlist more local-library 458 周杰倫 (Jay Chou) 周杰倫 方文山 female 98 my library Local playlist more local-library 465 范瑋琪 (Christine Fan) 非非 非非 female 99 my library Local playlist more local-library 465|1259 玖壹壹 陳皓宇 廖建至|洪瑜鴻 female 100 rows × 8 columns 2.2 Data TransformationWe can see that the columns like genre_ids, composer, lyricist have multiple values in a cell. In this case, the count of genres, composers, lyricist could be useful information as well 1member_music_df.columns Index(['msno', 'song_id', 'source_system_tab', 'source_screen_name', 'source_type', 'target', 'song_length', 'genre_ids', 'artist_name', 'composer', 'lyricist', 'language', 'city', 'bd', 'gender', 'registered_via', 'registration_init_day', 'registration_init_month', 'registration_init_year', 'expiration_day', 'expiration_month', 'expiration_year'], dtype='object') 1member_music_df.genre_ids.nunique(), member_music_df.composer.nunique(), member_music_df.lyricist.nunique() (573, 76065, 33889) 123456789def count_items(x): if x ==\"no_data\": return 0 return sum(map(x.count, ['|', '/', '\\\\', ';',','])) + 1member_music_df['genre_count']= member_music_df['genre_ids'].apply(count_items)member_music_df['composer_count']= member_music_df['composer'].apply(count_items)member_music_df['lyricist_count']= member_music_df['lyricist'].apply(count_items) 1member_music_df.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } msno song_id source_system_tab source_screen_name source_type target song_length genre_ids artist_name composer lyricist language city bd gender registered_via registration_init_day registration_init_month registration_init_year expiration_day expiration_month expiration_year genre_count composer_count lyricist_count 0 FGtllVqz18RPiwJj/edr2gV78zirAiY/9SmYvia+kCg= BBzumQNXUHKdEBOB7mAJuzok+IJA1c2Ryg/yzTF6tik= explore Explore online-playlist 1 206471.0 359 Bastille Dan Smith| Mark Crew no_data 52.0 1 27.0 no_data 7 2 1 2012 5 10 2017 1 2 0 1 Xumu+NIjS6QYVxDS4/t3SawvJ7viT9hPKXmf0RtLNx8= bhp/MpSNoqoxOIB+/l8WPqu6jldth4DIpCm3ayXnJqM= my library Local playlist more local-playlist 1 284584.0 1259 Various Artists no_data no_data 52.0 13 24.0 female 9 25 5 2011 11 9 2017 1 0 0 2 Xumu+NIjS6QYVxDS4/t3SawvJ7viT9hPKXmf0RtLNx8= JNWfrrC7zNN7BdMpsISKa4Mw+xVJYNnxXh3/Epw7QgY= my library Local playlist more local-playlist 1 225396.0 1259 Nas N. Jones、W. Adams、J. Lordan、D. Ingle no_data 52.0 13 24.0 female 9 25 5 2011 11 9 2017 1 1 0 3 Xumu+NIjS6QYVxDS4/t3SawvJ7viT9hPKXmf0RtLNx8= 2A87tzfnJTSWqD7gIZHisolhe4DMdzkbd6LzO1KHjNs= my library Local playlist more local-playlist 1 255512.0 1019 Soundway Kwadwo Donkoh no_data -1.0 13 24.0 female 9 25 5 2011 11 9 2017 1 1 0 4 FGtllVqz18RPiwJj/edr2gV78zirAiY/9SmYvia+kCg= 3qm6XTZ6MOCU11x8FIVbAGH5l5uMkT3/ZalWG1oo2Gc= explore Explore online-playlist 1 187802.0 1011 Brett Young Brett Young| Kelly Archer| Justin Ebach no_data 52.0 1 27.0 no_data 7 2 1 2012 5 10 2017 1 3 0 1member_music_df.info() 3. Data Extract, Transform and Load (ETL)We can skip Step 2 if we just want to transform data directly 3.1 Transformation Function for Data cleaning123456789101112#import necessary packages hereimport warningswarnings.filterwarnings('ignore')import numpy as np import pandas as pdimport matplotlib.pyplot as pltimport seaborn as snsimport lightgbm as lgbfrom subprocess import check_output# print(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))np.random.seed(2020) 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152def transform_data(data, song_df, members_df): # Merge song data with data set data = data.merge(song_df,on='song_id',how=\"left\", copy =False) # preprocess member data members_df[\"registration_init_time\"] = pd.to_datetime(members_df[\"registration_init_time\"], format=\"%Y%m%d\") members_df[\"expiration_date\"] = pd.to_datetime(members_df[\"expiration_date\"], format=\"%Y%m%d\") members_df[\"registration_init_day\"] = members_df[\"registration_init_time\"].dt.day members_df[\"registration_init_month\"] = members_df[\"registration_init_time\"].dt.month members_df[\"registration_init_year\"] = members_df[\"registration_init_time\"].dt.year members_df[\"expiration_day\"] = members_df[\"expiration_date\"].dt.day members_df[\"expiration_month\"] = members_df[\"expiration_date\"].dt.month members_df[\"expiration_year\"] = members_df[\"expiration_date\"].dt.year members_df = members_df.drop(columns = [\"registration_init_time\", \"expiration_date\"],axis=1) # merge member data with dataset data = data.merge(members_df,on='msno',how=\"left\", copy=False) # Remove outliers of bd age data['bd'].loc[(data['bd']&lt;=0) | (data['bd']&gt;=100)]= np.nan categorical_ls = ['source_system_tab', 'source_screen_name','source_type','genre_ids','artist_name','composer', 'lyricist','gender'] numerical_ls = ['song_length','language','bd'] # Fill missing values for index in numerical_ls: data[index].fillna(data[index].median(), inplace=True) for index in categorical_ls: data[index].fillna(\"no_data\", inplace=True) def count_items(x): if x ==\"no_data\": return 0 return sum(map(x.count, ['|', '/', '\\\\', ';',','])) + 1 data['genre_count']= data['genre_ids'].apply(count_items) data['composer_count']= data['composer'].apply(count_items) data['lyricist_count']= data['lyricist'].apply(count_items) # Convert object type to categorical type for c in data.columns: if data[c].dtype=='O': data[c] = data[c].astype(\"category\",copy=False) if 'id' in data.columns: ids = data['id'] data.drop(['id'], inplace=True,axis=1) else: ids =None return ids, data 3.2 Transform the composer, artist, lyricist to counts as new features123456root = './kaggle/working/train/data/'train_df = pd.read_csv(root+ \"train.csv\")test_df = pd.read_csv(root+ \"test.csv\")song_df = pd.read_csv(root+ \"songs.csv\")# song_extra_df = pd.read_csv(root+ \"song_extra_info.csv\")members_df = pd.read_csv(root+ \"members.csv\") 3.2.1 Transform train set1_, train_data = transform_data(train_df, song_df, members_df) 123y_train = train_data['target']train_data.drop(['target'], axis=1,inplace=True)X_train = train_data Transform the name of composer, artist, lyricist to new features like counts, number of intersection of names 12345678910111213141516def transform_names_intersection(data): #This function finds the intersection of names in composer, artist, lyricist def check_name_list(x): #convert string to name list dataframe strings = None strings = x.str.split(r\"//|/|;|、|\\| \") return strings df = data[[\"composer\",\"artist_name\", \"lyricist\"]].apply(check_name_list) data[\"composer_artist_intersect\"] =[len(set(a) &amp; set(b)) for a, b in zip(df.composer, df.artist_name)] data[\"composer_lyricist_intersect\"] =[len(set(a) &amp; set(b)) for a, b in zip(df.composer, df.lyricist)] data[\"artist_lyricist_intersect\"] =[len(set(a) &amp; set(b)) for a, b in zip(df.artist_name, df.lyricist)] return data _ = transform_names_intersection(X_train)X_train.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } msno song_id source_system_tab source_screen_name source_type song_length genre_ids artist_name composer lyricist language city bd gender registered_via registration_init_day registration_init_month registration_init_year expiration_day expiration_month expiration_year genre_count composer_count lyricist_count composer_artist_intersect composer_lyricist_intersect artist_lyricist_intersect 0 FGtllVqz18RPiwJj/edr2gV78zirAiY/9SmYvia+kCg= BBzumQNXUHKdEBOB7mAJuzok+IJA1c2Ryg/yzTF6tik= explore Explore online-playlist 206471.0 359 Bastille Dan Smith| Mark Crew no_data 52.0 1 27.0 no_data 7 2 1 2012 5 10 2017 1 2 0 0 0 0 1 Xumu+NIjS6QYVxDS4/t3SawvJ7viT9hPKXmf0RtLNx8= bhp/MpSNoqoxOIB+/l8WPqu6jldth4DIpCm3ayXnJqM= my library Local playlist more local-playlist 284584.0 1259 Various Artists no_data no_data 52.0 13 24.0 female 9 25 5 2011 11 9 2017 1 0 0 0 1 0 2 Xumu+NIjS6QYVxDS4/t3SawvJ7viT9hPKXmf0RtLNx8= JNWfrrC7zNN7BdMpsISKa4Mw+xVJYNnxXh3/Epw7QgY= my library Local playlist more local-playlist 225396.0 1259 Nas N. Jones、W. Adams、J. Lordan、D. Ingle no_data 52.0 13 24.0 female 9 25 5 2011 11 9 2017 1 1 0 0 0 0 3 Xumu+NIjS6QYVxDS4/t3SawvJ7viT9hPKXmf0RtLNx8= 2A87tzfnJTSWqD7gIZHisolhe4DMdzkbd6LzO1KHjNs= my library Local playlist more local-playlist 255512.0 1019 Soundway Kwadwo Donkoh no_data -1.0 13 24.0 female 9 25 5 2011 11 9 2017 1 1 0 0 0 0 4 FGtllVqz18RPiwJj/edr2gV78zirAiY/9SmYvia+kCg= 3qm6XTZ6MOCU11x8FIVbAGH5l5uMkT3/ZalWG1oo2Gc= explore Explore online-playlist 187802.0 1011 Brett Young Brett Young| Kelly Archer| Justin Ebach no_data 52.0 1 27.0 no_data 7 2 1 2012 5 10 2017 1 3 0 1 0 0 3.2.2 Transform Testset123ids, test_data = transform_data(test_df, song_df, members_df)_ = transform_names_intersection(test_data)test_data.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } msno song_id source_system_tab source_screen_name source_type song_length genre_ids artist_name composer lyricist language city bd gender registered_via registration_init_day registration_init_month registration_init_year expiration_day expiration_month expiration_year genre_count composer_count lyricist_count composer_artist_intersect composer_lyricist_intersect artist_lyricist_intersect 0 V8ruy7SGk7tDm3zA51DPpn6qutt+vmKMBKa21dp54uM= WmHKgKMlp1lQMecNdNvDMkvIycZYHnFwDT72I5sIssc= my library Local playlist more local-library 224130.0 458 梁文音 (Rachel Liang) Qi Zheng Zhang no_data 3.0 1 27.0 no_data 7 19 2 2016 18 9 2017 1 1 0 0 0 0 1 V8ruy7SGk7tDm3zA51DPpn6qutt+vmKMBKa21dp54uM= y/rsZ9DC7FwK5F2PK2D5mj+aOBUJAjuu3dZ14NgE0vM= my library Local playlist more local-library 320470.0 465 林俊傑 (JJ Lin) 林俊傑 孫燕姿/易家揚 3.0 1 27.0 no_data 7 19 2 2016 18 9 2017 1 1 2 0 0 0 2 /uQAlrAkaczV+nWCd2sPF2ekvXPRipV7q0l+gbLuxjw= 8eZLFOdGVdXBSqoAv5nsLigeH2BvKXzTQYtUM53I0k4= discover no_data song-based-playlist 315899.0 2022 Yu Takahashi (高橋優) Yu Takahashi Yu Takahashi 17.0 1 27.0 no_data 4 17 11 2016 24 11 2016 1 1 1 0 1 0 3 1a6oo/iXKatxQx4eS9zTVD+KlSVaAFbTIqVvwLC1Y0k= ztCf8thYsS4YN3GcIL/bvoxLm/T5mYBVKOO4C9NiVfQ= radio Radio radio 285210.0 465 U2 The Edge| Adam Clayton| Larry Mullen| Jr. no_data 52.0 3 30.0 male 9 25 7 2007 30 4 2017 1 4 0 0 0 0 4 1a6oo/iXKatxQx4eS9zTVD+KlSVaAFbTIqVvwLC1Y0k= MKVMpslKcQhMaFEgcEQhEfi5+RZhMYlU3eRDpySrH8Y= radio Radio radio 197590.0 873 Yoga Mr Sound Neuromancer no_data -1.0 3 30.0 male 9 25 7 2007 30 4 2017 1 1 0 0 0 0 1X_train.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } msno song_id source_system_tab source_screen_name source_type song_length genre_ids artist_name composer lyricist language city bd gender registered_via registration_init_day registration_init_month registration_init_year expiration_day expiration_month expiration_year genre_count composer_count lyricist_count composer_artist_intersect composer_lyricist_intersect artist_lyricist_intersect 0 FGtllVqz18RPiwJj/edr2gV78zirAiY/9SmYvia+kCg= BBzumQNXUHKdEBOB7mAJuzok+IJA1c2Ryg/yzTF6tik= explore Explore online-playlist 206471.0 359 Bastille Dan Smith| Mark Crew no_data 52.0 1 27.0 no_data 7 2 1 2012 5 10 2017 1 2 0 0 0 0 1 Xumu+NIjS6QYVxDS4/t3SawvJ7viT9hPKXmf0RtLNx8= bhp/MpSNoqoxOIB+/l8WPqu6jldth4DIpCm3ayXnJqM= my library Local playlist more local-playlist 284584.0 1259 Various Artists no_data no_data 52.0 13 24.0 female 9 25 5 2011 11 9 2017 1 0 0 0 1 0 2 Xumu+NIjS6QYVxDS4/t3SawvJ7viT9hPKXmf0RtLNx8= JNWfrrC7zNN7BdMpsISKa4Mw+xVJYNnxXh3/Epw7QgY= my library Local playlist more local-playlist 225396.0 1259 Nas N. Jones、W. Adams、J. Lordan、D. Ingle no_data 52.0 13 24.0 female 9 25 5 2011 11 9 2017 1 1 0 0 0 0 3 Xumu+NIjS6QYVxDS4/t3SawvJ7viT9hPKXmf0RtLNx8= 2A87tzfnJTSWqD7gIZHisolhe4DMdzkbd6LzO1KHjNs= my library Local playlist more local-playlist 255512.0 1019 Soundway Kwadwo Donkoh no_data -1.0 13 24.0 female 9 25 5 2011 11 9 2017 1 1 0 0 0 0 4 FGtllVqz18RPiwJj/edr2gV78zirAiY/9SmYvia+kCg= 3qm6XTZ6MOCU11x8FIVbAGH5l5uMkT3/ZalWG1oo2Gc= explore Explore online-playlist 187802.0 1011 Brett Young Brett Young| Kelly Archer| Justin Ebach no_data 52.0 1 27.0 no_data 7 2 1 2012 5 10 2017 1 3 0 1 0 0 1test_data.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } msno song_id source_system_tab source_screen_name source_type song_length genre_ids artist_name composer lyricist language city bd gender registered_via registration_init_day registration_init_month registration_init_year expiration_day expiration_month expiration_year genre_count composer_count lyricist_count composer_artist_intersect composer_lyricist_intersect artist_lyricist_intersect 0 V8ruy7SGk7tDm3zA51DPpn6qutt+vmKMBKa21dp54uM= WmHKgKMlp1lQMecNdNvDMkvIycZYHnFwDT72I5sIssc= my library Local playlist more local-library 224130.0 458 梁文音 (Rachel Liang) Qi Zheng Zhang no_data 3.0 1 27.0 no_data 7 19 2 2016 18 9 2017 1 1 0 0 0 0 1 V8ruy7SGk7tDm3zA51DPpn6qutt+vmKMBKa21dp54uM= y/rsZ9DC7FwK5F2PK2D5mj+aOBUJAjuu3dZ14NgE0vM= my library Local playlist more local-library 320470.0 465 林俊傑 (JJ Lin) 林俊傑 孫燕姿/易家揚 3.0 1 27.0 no_data 7 19 2 2016 18 9 2017 1 1 2 0 0 0 2 /uQAlrAkaczV+nWCd2sPF2ekvXPRipV7q0l+gbLuxjw= 8eZLFOdGVdXBSqoAv5nsLigeH2BvKXzTQYtUM53I0k4= discover no_data song-based-playlist 315899.0 2022 Yu Takahashi (高橋優) Yu Takahashi Yu Takahashi 17.0 1 27.0 no_data 4 17 11 2016 24 11 2016 1 1 1 0 1 0 3 1a6oo/iXKatxQx4eS9zTVD+KlSVaAFbTIqVvwLC1Y0k= ztCf8thYsS4YN3GcIL/bvoxLm/T5mYBVKOO4C9NiVfQ= radio Radio radio 285210.0 465 U2 The Edge| Adam Clayton| Larry Mullen| Jr. no_data 52.0 3 30.0 male 9 25 7 2007 30 4 2017 1 4 0 0 0 0 4 1a6oo/iXKatxQx4eS9zTVD+KlSVaAFbTIqVvwLC1Y0k= MKVMpslKcQhMaFEgcEQhEfi5+RZhMYlU3eRDpySrH8Y= radio Radio radio 197590.0 873 Yoga Mr Sound Neuromancer no_data -1.0 3 30.0 male 9 25 7 2007 30 4 2017 1 1 0 0 0 0 3.2.3 Split validation set and trainset123456789from sklearn.model_selection import StratifiedShuffleSplit, StratifiedKFold, train_test_splitss_split = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=2021)# Split training set and testing settrain_index, valid_index ,test_index = None, None, Nonefor train_i, test_i in ss_split.split(np.zeros(y_train.shape) ,y_train): train_index = train_i test_index = test_i print(train_index.shape, test_index.shape) (5901934,) (1475484,) 12345678X_validset = X_train.iloc[test_index]y_validset = y_train.iloc[test_index].valuesX_trainset = X_train.iloc[train_index]y_trainset = y_train.iloc[train_index].values#delete dataframes to save spacedel X_train, y_train 1X_trainset.info() &lt;class 'pandas.core.frame.DataFrame'&gt; Int64Index: 5901934 entries, 7066318 to 5275539 Data columns (total 27 columns): # Column Dtype --- ------ ----- 0 msno category 1 song_id category 2 source_system_tab category 3 source_screen_name category 4 source_type category 5 song_length float64 6 genre_ids category 7 artist_name category 8 composer category 9 lyricist category 10 language float64 11 city int64 12 bd float64 13 gender category 14 registered_via int64 15 registration_init_day int64 16 registration_init_month int64 17 registration_init_year int64 18 expiration_day int64 19 expiration_month int64 20 expiration_year int64 21 genre_count int64 22 composer_count int64 23 lyricist_count int64 24 composer_artist_intersect int64 25 composer_lyricist_intersect int64 26 artist_lyricist_intersect int64 dtypes: category(10), float64(3), int64(14) memory usage: 966.0 MB 4. LGBM Modeling123import lightgbm as lgbtrain_set = lgb.Dataset(X_trainset, y_trainset)valid_set = lgb.Dataset(X_validset, y_validset) 12num_leaves = 110max_depths = [10, 15, 20, 25,30] 5.Model Training and Validation on LGBM models12345678910111213141516171819params = { 'objective': 'binary', 'metric': 'binary_logloss', 'boosting': 'gbdt', 'learning_rate': 0.3 , 'verbose': 0, 'num_leaves': num_leaves, 'bagging_fraction': 0.95, 'bagging_freq': 1, 'bagging_seed': 1, 'feature_fraction': 0.9, 'feature_fraction_seed': 1, 'max_bin': 256, 'max_depth': max_depths[0], 'num_rounds': 200, 'metric' : 'auc' }%time model_f1 = lgb.train(params, train_set=train_set, valid_sets=valid_set, verbose_eval=5) [5] valid_0's auc: 0.710928 [10] valid_0's auc: 0.723954 [15] valid_0's auc: 0.731661 [20] valid_0's auc: 0.736653 [25] valid_0's auc: 0.740424 [30] valid_0's auc: 0.744678 [35] valid_0's auc: 0.749056 [40] valid_0's auc: 0.752277 [45] valid_0's auc: 0.754501 [50] valid_0's auc: 0.756448 [55] valid_0's auc: 0.758097 [60] valid_0's auc: 0.75991 [65] valid_0's auc: 0.761418 [70] valid_0's auc: 0.762683 [75] valid_0's auc: 0.764243 [80] valid_0's auc: 0.765646 [85] valid_0's auc: 0.766883 [90] valid_0's auc: 0.767921 [95] valid_0's auc: 0.769111 [100] valid_0's auc: 0.770006 [105] valid_0's auc: 0.770934 [110] valid_0's auc: 0.772012 [115] valid_0's auc: 0.772747 [120] valid_0's auc: 0.773835 [125] valid_0's auc: 0.774486 [130] valid_0's auc: 0.775258 [135] valid_0's auc: 0.775887 [140] valid_0's auc: 0.776838 [145] valid_0's auc: 0.777587 [150] valid_0's auc: 0.778113 [155] valid_0's auc: 0.778714 [160] valid_0's auc: 0.77929 [165] valid_0's auc: 0.779884 [170] valid_0's auc: 0.780354 [175] valid_0's auc: 0.781586 [180] valid_0's auc: 0.782002 [185] valid_0's auc: 0.782517 [190] valid_0's auc: 0.783075 [195] valid_0's auc: 0.783496 [200] valid_0's auc: 0.784083 CPU times: user 8min 20s, sys: 3 s, total: 8min 23s Wall time: 4min 25s 12345678910111213141516171819params = { 'objective': 'binary', 'metric': 'binary_logloss', 'boosting': 'gbdt', 'learning_rate': 0.3 , 'verbose': 0, 'num_leaves': num_leaves, 'bagging_fraction': 0.95, 'bagging_freq': 1, 'bagging_seed': 1, 'feature_fraction': 0.9, 'feature_fraction_seed': 1, 'max_bin': 256, 'max_depth': max_depths[1], 'num_rounds': 200, 'metric' : 'auc' }%time model_f2 = lgb.train(params, train_set=train_set, valid_sets=valid_set, verbose_eval=5) [5] valid_0's auc: 0.727917 [10] valid_0's auc: 0.742629 [15] valid_0's auc: 0.74811 [20] valid_0's auc: 0.754257 [25] valid_0's auc: 0.758256 [30] valid_0's auc: 0.76119 [35] valid_0's auc: 0.763674 [40] valid_0's auc: 0.76626 [45] valid_0's auc: 0.7681 [50] valid_0's auc: 0.769933 [55] valid_0's auc: 0.771692 [60] valid_0's auc: 0.773121 [65] valid_0's auc: 0.774693 [70] valid_0's auc: 0.776149 [75] valid_0's auc: 0.777157 [80] valid_0's auc: 0.778674 [85] valid_0's auc: 0.780085 [90] valid_0's auc: 0.78098 [95] valid_0's auc: 0.782016 [100] valid_0's auc: 0.783028 [105] valid_0's auc: 0.783782 [110] valid_0's auc: 0.784875 [115] valid_0's auc: 0.785417 [120] valid_0's auc: 0.786042 [125] valid_0's auc: 0.78665 [130] valid_0's auc: 0.787237 [135] valid_0's auc: 0.787897 [140] valid_0's auc: 0.788426 [145] valid_0's auc: 0.788904 [150] valid_0's auc: 0.789517 [155] valid_0's auc: 0.78991 [160] valid_0's auc: 0.790561 [165] valid_0's auc: 0.791319 [170] valid_0's auc: 0.791855 [175] valid_0's auc: 0.792519 [180] valid_0's auc: 0.792922 [185] valid_0's auc: 0.793727 [190] valid_0's auc: 0.794061 [195] valid_0's auc: 0.794584 [200] valid_0's auc: 0.794811 CPU times: user 9min 27s, sys: 1.28 s, total: 9min 28s Wall time: 4min 51s 12345678910111213141516171819params = { 'objective': 'binary', 'metric': 'binary_logloss', 'boosting': 'gbdt', 'learning_rate': 0.3 , 'verbose': 0, 'num_leaves': num_leaves, 'bagging_fraction': 0.95, 'bagging_freq': 1, 'bagging_seed': 1, 'feature_fraction': 0.9, 'feature_fraction_seed': 1, 'max_bin': 256, 'max_depth': max_depths[2], 'num_rounds': 200, 'metric' : 'auc' }%time model_f3 = lgb.train(params, train_set=train_set, valid_sets=valid_set, verbose_eval=5) [5] valid_0's auc: 0.734133 [10] valid_0's auc: 0.749742 [15] valid_0's auc: 0.75615 [20] valid_0's auc: 0.761276 [25] valid_0's auc: 0.766358 [30] valid_0's auc: 0.769127 [35] valid_0's auc: 0.771531 [40] valid_0's auc: 0.773761 [45] valid_0's auc: 0.775287 [50] valid_0's auc: 0.777329 [55] valid_0's auc: 0.779154 [60] valid_0's auc: 0.780391 [65] valid_0's auc: 0.782072 [70] valid_0's auc: 0.783786 [75] valid_0's auc: 0.784989 [80] valid_0's auc: 0.785685 [85] valid_0's auc: 0.786851 [90] valid_0's auc: 0.787643 [95] valid_0's auc: 0.788312 [100] valid_0's auc: 0.789305 [105] valid_0's auc: 0.790256 [110] valid_0's auc: 0.791037 [115] valid_0's auc: 0.79177 [120] valid_0's auc: 0.792466 [125] valid_0's auc: 0.792988 [130] valid_0's auc: 0.793478 [135] valid_0's auc: 0.793961 [140] valid_0's auc: 0.794871 [145] valid_0's auc: 0.795495 [150] valid_0's auc: 0.795952 [155] valid_0's auc: 0.796269 [160] valid_0's auc: 0.796888 [165] valid_0's auc: 0.797808 [170] valid_0's auc: 0.7982 [175] valid_0's auc: 0.798443 [180] valid_0's auc: 0.798959 [185] valid_0's auc: 0.799395 [190] valid_0's auc: 0.799687 [195] valid_0's auc: 0.800153 [200] valid_0's auc: 0.800409 CPU times: user 11min 16s, sys: 1.54 s, total: 11min 17s Wall time: 5min 47s 12345678910111213141516171819params = { 'objective': 'binary', 'metric': 'binary_logloss', 'boosting': 'gbdt', 'learning_rate': 0.3 , 'verbose': 0, 'num_leaves': num_leaves, 'bagging_fraction': 0.95, 'bagging_freq': 1, 'bagging_seed': 1, 'feature_fraction': 0.9, 'feature_fraction_seed': 1, 'max_bin': 256, 'max_depth': max_depths[3], 'num_rounds': 200, 'metric' : 'auc' }%time model_f4 = lgb.train(params, train_set=train_set, valid_sets=valid_set, verbose_eval=5) [5] valid_0's auc: 0.736351 [10] valid_0's auc: 0.754592 [15] valid_0's auc: 0.76195 [20] valid_0's auc: 0.766405 [25] valid_0's auc: 0.770538 [30] valid_0's auc: 0.772566 [35] valid_0's auc: 0.775789 [40] valid_0's auc: 0.777994 [45] valid_0's auc: 0.779658 [50] valid_0's auc: 0.781394 [55] valid_0's auc: 0.783194 [60] valid_0's auc: 0.784808 [65] valid_0's auc: 0.786109 [70] valid_0's auc: 0.787265 [75] valid_0's auc: 0.788079 [80] valid_0's auc: 0.789109 [85] valid_0's auc: 0.78986 [90] valid_0's auc: 0.790613 [95] valid_0's auc: 0.791347 [100] valid_0's auc: 0.79209 [105] valid_0's auc: 0.793348 [110] valid_0's auc: 0.79409 [115] valid_0's auc: 0.794754 [120] valid_0's auc: 0.795411 [125] valid_0's auc: 0.795866 [130] valid_0's auc: 0.796604 [135] valid_0's auc: 0.79781 [140] valid_0's auc: 0.798172 [145] valid_0's auc: 0.798723 [150] valid_0's auc: 0.799132 [155] valid_0's auc: 0.799488 [160] valid_0's auc: 0.800115 [165] valid_0's auc: 0.800509 [170] valid_0's auc: 0.800784 [175] valid_0's auc: 0.801118 [180] valid_0's auc: 0.801448 [185] valid_0's auc: 0.801882 [190] valid_0's auc: 0.8022 [195] valid_0's auc: 0.802578 [200] valid_0's auc: 0.802953 CPU times: user 12min 34s, sys: 1.44 s, total: 12min 36s Wall time: 6min 27s 12345678910111213141516171819params = { 'objective': 'binary', 'metric': 'binary_logloss', 'boosting': 'gbdt', 'learning_rate': 0.3 , 'verbose': 0, 'num_leaves': num_leaves, 'bagging_fraction': 0.95, 'bagging_freq': 1, 'bagging_seed': 1, 'feature_fraction': 0.9, 'feature_fraction_seed': 1, 'max_bin': 256, 'max_depth': max_depths[4], 'num_rounds': 200, 'metric' : 'auc' }%time model_f5 = lgb.train(params, train_set=train_set, valid_sets=valid_set, verbose_eval=5) [5] valid_0's auc: 0.739442 [10] valid_0's auc: 0.757442 [15] valid_0's auc: 0.766439 [20] valid_0's auc: 0.77132 [25] valid_0's auc: 0.774735 [30] valid_0's auc: 0.777071 [35] valid_0's auc: 0.779247 [40] valid_0's auc: 0.781616 [45] valid_0's auc: 0.782953 [50] valid_0's auc: 0.785154 [55] valid_0's auc: 0.786877 [60] valid_0's auc: 0.787993 [65] valid_0's auc: 0.788839 [70] valid_0's auc: 0.790254 [75] valid_0's auc: 0.791088 [80] valid_0's auc: 0.792455 [85] valid_0's auc: 0.79365 [90] valid_0's auc: 0.794445 [95] valid_0's auc: 0.795072 [100] valid_0's auc: 0.796276 [105] valid_0's auc: 0.797737 [110] valid_0's auc: 0.798265 [115] valid_0's auc: 0.799021 [120] valid_0's auc: 0.799964 [125] valid_0's auc: 0.800469 [130] valid_0's auc: 0.801445 [135] valid_0's auc: 0.801851 [140] valid_0's auc: 0.802299 [145] valid_0's auc: 0.802599 [150] valid_0's auc: 0.803381 [155] valid_0's auc: 0.803696 [160] valid_0's auc: 0.803926 [165] valid_0's auc: 0.80443 [170] valid_0's auc: 0.804694 [175] valid_0's auc: 0.804897 [180] valid_0's auc: 0.80524 [185] valid_0's auc: 0.805486 [190] valid_0's auc: 0.805804 [195] valid_0's auc: 0.806059 [200] valid_0's auc: 0.806525 CPU times: user 14min 7s, sys: 1.66 s, total: 14min 9s Wall time: 7min 14s 6.Model Evaluation on LGBM models12345678from sklearn.metrics import accuracy_scoredef evaluation_lgbm(model, X =X_validset , y= y_validset): out = model.predict(X) preds = out&gt;=0.5 acc = accuracy_score(preds, y) print(\"Evaluation acc:\", acc) return acc 1X_validset.shape (1475484, 27) 12345acc_1 = evaluation_lgbm(model_f1)acc_2 = evaluation_lgbm(model_f2)acc_3 = evaluation_lgbm(model_f3)acc_4 = evaluation_lgbm(model_f4)acc_5 = evaluation_lgbm(model_f5) Evaluation acc: 0.709764389176704 Evaluation acc: 0.719106408473423 Evaluation acc: 0.7236893114395005 Evaluation acc: 0.7258221708944319 Evaluation acc: 0.728842196865571 12eval_df = pd.DataFrame({\"Lgbm with max_depth\":max_depths,\"Validation Accuracy\":[acc_1,acc_2,acc_3,acc_4,acc_5]})eval_df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Lgbm with max_depth Validation Accuracy 0 10 0.709764 1 15 0.719106 2 20 0.723689 3 25 0.725822 4 30 0.728842 Create Submission Files12345678models = [model_f1,model_f2,model_f3,model_f4,model_f5]for i in range(len(models)): preds_test = models[i].predict(test_data) submission = pd.DataFrame() submission['id'] = ids submission['target'] = preds_test submission.to_csv(root + 'submission_lgbm_model_'+ str(i)+'.csv.gz', compression = 'gzip', index=False, float_format = '%.5f') print(\"Predictions from model \",i,\": \",preds_test) Predictions from model 0 : [0.47177512 0.48584262 0.19651648 ... 0.39917036 0.30263348 0.36468783] Predictions from model 1 : [0.45280296 0.55415074 0.17824637 ... 0.41500494 0.30757934 0.34520384] Predictions from model 2 : [0.39847416 0.48724786 0.15954141 ... 0.38293317 0.27657349 0.28451098] Predictions from model 3 : [0.3825275 0.39659855 0.15904321 ... 0.3515784 0.21812496 0.28995803] Predictions from model 4 : [0.3951268 0.45704878 0.14609333 ... 0.35033303 0.23065677 0.2885925 ] Scores from kaggle test set Model name private score public score LGBM Boosting Machine Model 4 0.67423 0.67256 LGBM Boosting Machine Model 3 0.67435 0.67241 LGBM Boosting Machine Model 2 0.67416 0.67208 LGBM Boosting Machine Model 1 0.67416 0.67188 LGBM Boosting Machine Model 0 0.67206 0.66940 4. Wide &amp; Depth neural network modelWide and Deep model（2 branches–&gt;merge two branches–&gt;main branch） This model converts categorical attributes into dense vectors using embedding network in neural network, which enable us to reduce the dimension of categorical data and extract main features like PCA. Then it combines dense embedded vectors with numerical data for features selection and classifcation in the main branch. The output is possibility that user may repeat listening to the music Label Encoding for categorical dataConvert categorical data into numerical labels before using embedding 12345678910111213141516171819202122232425262728from sklearn.preprocessing import LabelEncodercategorical_ls1 = ['source_system_tab', 'source_screen_name','source_type','genre_ids','gender']categorical_ls2 = ['artist_name','composer', 'lyricist']numerical_ls = ['song_length','language','bd',\"registration_init_year\", \"expiration_day\",\"expiration_month\",\"expiration_year\", \"genre_count\",\"composer_count\",\"lyricist_count\",\"composer_artist_intersect\", \"composer_lyricist_intersect\",\"artist_lyricist_intersect\"]max_values = {}# labelencoders = {}for col in categorical_ls1: print(col) lbl = LabelEncoder() df = pd.concat([X_trainset[col], X_validset[col],test_data[col]],ignore_index=True) lbl.fit(df) df = lbl.transform(list(df.values.astype('str',copy=False))) X_trainset[col] = lbl.transform(list(X_trainset[col].values.astype('str',copy=False))) X_validset[col] = lbl.transform(list(X_validset[col].values.astype('str',copy=False))) test_data[col] = lbl.transform(list(test_data[col].values.astype('str',copy=False))) max_values[col] = df.max() + 2 #set the range of embedding input larger# Compute embedding dimensionsemb_dims1 = []emb_dims2 = []for i in categorical_ls1: emb_dims1.append((max_values[i], min((max_values[i]+1)//2, 50))) source_system_tab source_screen_name source_type genre_ids gender 12# max_valuesX_trainset.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } msno song_id source_system_tab source_screen_name source_type song_length genre_ids artist_name composer lyricist language city bd gender registered_via registration_init_day registration_init_month registration_init_year expiration_day expiration_month expiration_year genre_count composer_count lyricist_count composer_artist_intersect composer_lyricist_intersect artist_lyricist_intersect 7066318 BQ7nOoOUipsqjOBANK+ilA8F7TVaOHSI8gVPWElXsuI= FaTUlIiCh/6sEOasPm1vgIk9XqavgSGgRGYuOkzTF0o= 0 3 7 203520.0 364 田馥甄 (Hebe) 倪子岡 李格弟 3.0 5 25.0 0 9 2 9 2006 11 10 2017 1 1 1 0 0 0 1471565 Ul+UpO5PxuhCn040AK8gzR1A/mE/k3KbL13gO7Uc4Ts= +SstqMwhQPBQFTPBhLKPT642IiBDXzZFwlzsLl4cGXo= 3 8 3 283846.0 371 陳勢安 (Andrew Tan) 覃嘉健 馬嵩惟 3.0 13 47.0 0 9 1 2 2006 30 9 2017 1 1 1 0 0 0 6176886 sa6oKy94c62R5Eq0YHkNzZrJSo9j5E7JGjTDHnYRKqs= K6fBQxiNhgWazjXrZUGlZIm9ltT4o+Vq19sWmZRdAhg= 2 12 2 296960.0 371 蔡依林 (Jolin Tsai) 郭子 鄔裕康 3.0 5 38.0 1 7 27 11 2011 30 12 2017 1 1 1 0 0 0 3527889 LG/BLgJxw5AvXy0pkgaHYYWeU7jKS+ms/51+7TaBY9Y= O+/KJ5a5GzbgLZrCOw/t/iDOPTrDcrz5ZnOtaK9blA8= 3 8 0 235403.0 200 ONE OK ROCK Toru/Taka Taka 17.0 1 27.0 2 7 1 6 2013 1 10 2017 1 2 1 0 1 0 6073849 KmAJtsNcrofH6qMoHvET89mQAlC1EN3r3r3rkfW2iT4= WogFv1yz1n49l4gNSbf76bWxas8nNvzHntrj4FuzC24= 3 22 7 210604.0 371 Twins no_data no_data 24.0 13 28.0 1 9 2 11 2016 7 10 2017 1 0 0 0 1 0 1X_validset.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } msno song_id source_system_tab source_screen_name source_type song_length genre_ids artist_name composer lyricist language city bd gender registered_via registration_init_day registration_init_month registration_init_year expiration_day expiration_month expiration_year genre_count composer_count lyricist_count composer_artist_intersect composer_lyricist_intersect artist_lyricist_intersect 3400479 RZDFiPWpvwi1RWF5NAPEkvmogqe+7rGys+zoLU9he2M= MvON55vzjT7QW7GSs/UVLZrE/LJpMAVFUjXwZczdw40= 0 11 7 356379.0 97 Nas Amy Winehouse| Salaam Remi| Nasir Jones p/k/a NAS no_data 52.0 1 27.0 2 7 7 12 2010 20 9 2017 1 5 0 0 0 0 2481022 C3EZ5oh7XDt5fP9OY20RPlD8MA+rBknmvmDhA1tHGMU= 5PvPCUIB7vVuCNpQRKXIOcWvh9EerujDAbrjV7G6ZE0= 3 8 3 216920.0 349 貴族精選 no_data Super Market| Microdot 31.0 5 27.0 2 3 11 12 2012 5 1 2018 1 0 2 0 0 0 5808216 O1pwjdTED6P3lKm52VBxVUtaSVc31S9PmIw+07WBNw4= va3+1L2wraJkzDbHjvdo+e+0TTJcLko0k0pqBn09nJE= 3 8 3 268225.0 548 Various Artists no_data no_data 3.0 13 82.0 0 9 29 4 2007 23 1 2018 1 0 0 0 1 0 42686 WFCCMzA4hADGBduTS6X8mXlutyiC0P33QkTG6zr5yCg= U9kojfZSKaiWOW94PKh1Riyv/zUWxmBRmv0XInQWLGw= 7 11 7 290063.0 364 周杰倫 (Jay Chou) 周杰倫 方文山 3.0 13 32.0 0 7 12 12 2010 9 9 2017 1 1 1 0 0 0 1850837 h0fTru8nYMv9bR0j6kBh8kiXDaybzWBYaSHbUIVzeBs= J1sgBEFbcXSK6eiN7CK1WNxsso0/sY6t0BMX+c+iPNw= 0 11 7 220450.0 111 Usher no_data no_data 52.0 22 28.0 0 9 2 1 2008 28 9 2017 3 0 0 0 1 0 123456789101112131415161718192021222324252627282930313233343536373839import torchfrom torch import nnimport torch.nn.functional as Ffrom torch.utils.data import Dataset, DataLoaderclass TabularDataset(Dataset): def __init__(self, x_data, y_data, cat_cols1, cat_cols2, num_cols): \"\"\" data: pandas data frame; cat_cols: list of string, the names of the categorical columns in the data, will be passed through the embedding layers; num_cols: list of string y_data: the target \"\"\" self.n = x_data.shape[0] self.y = y_data.astype(np.float32).reshape(-1, 1)#.values.reshape(-1, 1) self.cat_cols1 = cat_cols1 self.cat_cols2 = cat_cols2 self.num_cols = num_cols self.num_X = x_data[self.num_cols].astype(np.float32).values self.cat_X1 = x_data[self.cat_cols1].astype(np.int64).values self.cat_X2 = x_data[self.cat_cols2].astype(np.int64).values def print_data(self): return self.num_X, self.cat_X1, self.cat_X2, self.y def __len__(self): \"\"\" total number of samples \"\"\" return self.n def __getitem__(self, idx): \"\"\" Generates one sample of data. \"\"\" return [self.y[idx], self.num_X[idx], self.cat_X1[idx], self.cat_X2[idx]] 12345train_dataset = TabularDataset(x_data=X_trainset, y_data=y_trainset, cat_cols1=categorical_ls1, cat_cols2=[], num_cols=numerical_ls)val_dataset = TabularDataset(x_data=X_validset, y_data=y_validset, cat_cols1=categorical_ls1, cat_cols2=[], num_cols=numerical_ls) 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980class FeedForwardNN(nn.Module): def __init__(self, emb_dims1, emb_dims2, no_of_num, lin_layer_sizes, output_size, emb_dropout, lin_layer_dropouts, branch2_enable=0): \"\"\" emb_dims: List of two element tuples; no_of_num: Integer, the number of continuous features in the data; lin_layer_sizes: List of integers. The size of each linear layer; output_size: Integer, the size of the final output; emb_dropout: Float, the dropout to be used after the embedding layers. lin_layer_dropouts: List of floats, the dropouts to be used after each linear layer. \"\"\" super().__init__() self.branch2_enable = branch2_enable # embedding layers self.emb_layers1 = nn.ModuleList([nn.Embedding(x, y) for x, y in emb_dims1]) self.emb_layers2 = nn.ModuleList([nn.Embedding(x, y) for x, y in emb_dims2]) # 计算各个emb参数数量，为后续Linear layer的输入做准备 self.no_of_embs1 = sum([y for x, y in emb_dims1]) self.no_of_embs2 = sum([y for x, y in emb_dims2]) self.no_of_num = no_of_num # 分支1 self.branch1 = nn.Linear(self.no_of_embs1, lin_layer_sizes[0]) self.branch1_2 = nn.Linear(lin_layer_sizes[0], lin_layer_sizes[1]) nn.init.kaiming_normal_(self.branch1.weight.data) nn.init.kaiming_normal_(self.branch1_2.weight.data) # 分支2 if branch2_enable: self.branch2 = nn.Linear(self.no_of_embs2, lin_layer_sizes[0] * 2) self.branch2_2 = nn.Linear(lin_layer_sizes[0] * 2, lin_layer_sizes[1] * 2) nn.init.kaiming_normal_(self.branch2.weight.data) nn.init.kaiming_normal_(self.branch2_2.weight.data) # 主分支# self.main_layer1 = nn.Linear(lin_layer_sizes[1] * 3 + self.no_of_num, lin_layer_sizes[2]) self.main_layer1 = nn.Linear(77, lin_layer_sizes[2]) self.main_layer2 = nn.Linear(lin_layer_sizes[2], lin_layer_sizes[3]) # batch normal self.branch_bn_layers1 = nn.BatchNorm1d(lin_layer_sizes[0]) self.branch_bn_layers2 = nn.BatchNorm1d(lin_layer_sizes[0] * 2) self.main_bn_layer = nn.BatchNorm1d(lin_layer_sizes[2]) # Dropout Layers self.emb_dropout_layer = nn.Dropout(emb_dropout) self.dropout_layers = nn.ModuleList([nn.Dropout(size) for size in lin_layer_dropouts]) # Output layer self.output_layer = nn.Linear(lin_layer_sizes[-1], output_size) nn.init.kaiming_normal_(self.output_layer.weight.data) self.sigmoid = nn.Sigmoid() def forward(self, num_data, cat_data1, cat_data2): # embedding categorical feature and cat them together x1 = [emb_layer(torch.tensor(cat_data1[:, i])) for i, emb_layer in enumerate(self.emb_layers1)] x1 = torch.cat(x1, 1) x1 = self.emb_dropout_layer(F.relu(self.branch1(x1))) x1 = self.branch_bn_layers1(x1) x1 = self.dropout_layers[0](F.relu(self.branch1_2(x1))) if self.branch2_enable: x2 = [emb_layer(torch.tensor(cat_data2[:, i])) for i, emb_layer in enumerate(self.emb_layers2)] x2 = torch.cat(x2, 1) x2 = self.emb_dropout_layer(F.relu(self.branch2(x2))) x2 = self.branch_bn_layers2(x2) x2 = self.dropout_layers[0](F.relu(self.branch2_2(x2))) main = torch.cat([x1, x2, num_data], 1) else: main = torch.cat([x1, num_data], 1)# print(\"Main Shape: \", main.shape) main = self.dropout_layers[1](F.relu(self.main_layer1(main))) main = self.main_bn_layer(main) main = self.dropout_layers[2](F.relu(self.main_layer2(main))) out = self.output_layer(main) out = self.sigmoid(out) return out 123batchsize = 64train_dataloader = DataLoader(train_dataset, batchsize, shuffle=True, num_workers=2)val_dataloader = DataLoader(val_dataset, 64, shuffle=True, num_workers=2) 1# next(iter(train_dataloader))[3] 123456789np.random.seed(2020)device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")model = FeedForwardNN(emb_dims1=emb_dims1, emb_dims2=emb_dims2, no_of_num=len(numerical_ls), lin_layer_sizes=[128,64,32,16], output_size=1, lin_layer_dropouts=[0.1, 0.1, 0.05], emb_dropout=0.05).to(device) 1device,len(train_dataloader) (device(type='cpu'), 92218) 5.Model Training and Validation on Wide and Deep model12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273no_of_epochs = 2batch_num = 4000# criterion = torch.nn.MSELoss()criterion = torch.nn.BCELoss()optimizer = torch.optim.Adam(model.parameters(), lr=0.001)lrscheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.1, patience=2, threshold=0.9 )total_data = train_dataset.__len__()best_val_score = 0.0best_model =Noneprint_every = 500steps = 0running_loss = 0for epoch in range(no_of_epochs): model.train() batch_cnt = 0 for index, datas in enumerate(train_dataloader): if batch_cnt == batch_num: break steps += 1 batch_cnt += 1 y, num_x, cat_x1, cat_x2 = datas cat_x1 = cat_x1.to(device) cat_x2 = cat_x2.to(device) num_x = num_x.to(device) y = y.to(device) # Forward Pass optimizer.zero_grad() preds = model.forward(num_x, cat_x1, cat_x2) loss = criterion(preds, y) loss.backward() optimizer.step() running_loss += loss.item() if steps % print_every == 0: val_loss = 0 model.eval() val_acc = 0. total_len = 0. with torch.no_grad(): for val_index, val_datas in enumerate(val_dataloader): y, num_x, cat_x1, cat_x2 = val_datas cat_x1 = cat_x1.to(device) cat_x2 = cat_x2.to(device) num_x = num_x.to(device) y = y.to(device) out = model.forward(num_x, cat_x1, cat_x2) batch_loss = criterion(out, y) val_acc += ((out&gt;0.5)==y ).sum().detach().to('cpu').numpy() total_len += len(out) val_loss += batch_loss.item() val_acc /= total_len if val_acc&gt; best_val_score: best_val_score = val_acc torch.save(model,\"checkpoint.pt\") # print(\"Checkpoint saved.\") # update scheduler lrscheduler.step(val_loss) print(f\"Epoch {epoch+1}/{no_of_epochs}..\" f\"Train loss:{running_loss/print_every:.4f}..\" f\"Validation loss:{val_loss/len(val_dataloader):.4f}..\" f\"Validation Acc:{val_acc:.4f}..\" f\"Best Validation Acc:{best_val_score:.4f}..\") running_loss = 0 model.train()print(\"Training Completed\")best_model = torch.load(\"checkpoint.pt\") Epoch 1/2..Train loss:0.6945..Validation loss:0.6925..Validation Acc:0.5302..Best Validation Acc:0.5302.. Epoch 1/2..Train loss:0.6742..Validation loss:0.6663..Validation Acc:0.6229..Best Validation Acc:0.6229.. Epoch 1/2..Train loss:0.6640..Validation loss:0.6618..Validation Acc:0.6245..Best Validation Acc:0.6245.. Epoch 1/2..Train loss:0.6623..Validation loss:0.6768..Validation Acc:0.6252..Best Validation Acc:0.6252.. Epoch 1/2..Train loss:0.6628..Validation loss:0.6648..Validation Acc:0.6249..Best Validation Acc:0.6252.. Epoch 1/2..Train loss:0.6636..Validation loss:0.6656..Validation Acc:0.6253..Best Validation Acc:0.6253.. Epoch 1/2..Train loss:0.6623..Validation loss:0.6674..Validation Acc:0.6254..Best Validation Acc:0.6254.. Epoch 1/2..Train loss:0.6640..Validation loss:0.6649..Validation Acc:0.6255..Best Validation Acc:0.6255.. Epoch 2/2..Train loss:0.6597..Validation loss:0.6638..Validation Acc:0.6256..Best Validation Acc:0.6256.. Epoch 2/2..Train loss:0.6605..Validation loss:0.6584..Validation Acc:0.6259..Best Validation Acc:0.6259.. Epoch 2/2..Train loss:0.6609..Validation loss:0.6636..Validation Acc:0.6263..Best Validation Acc:0.6263.. Epoch 2/2..Train loss:0.6602..Validation loss:0.6620..Validation Acc:0.6263..Best Validation Acc:0.6263.. Epoch 2/2..Train loss:0.6647..Validation loss:0.6658..Validation Acc:0.6264..Best Validation Acc:0.6264.. Epoch 2/2..Train loss:0.6615..Validation loss:0.6644..Validation Acc:0.6262..Best Validation Acc:0.6264.. Epoch 2/2..Train loss:0.6641..Validation loss:0.6764..Validation Acc:0.6254..Best Validation Acc:0.6264.. Epoch 2/2..Train loss:0.6600..Validation loss:0.6926..Validation Acc:0.6263..Best Validation Acc:0.6264.. Training Completed 1print(f\"Best Validation Acc:{best_val_score:.4f}..\") Best Validation Acc:0.6264.. 1model = torch.load(\"checkpoint.pt\") 12test_dataset = TabularDataset(x_data=test_data, y_data=np.zeros(len(test_data)), cat_cols1=categorical_ls1, cat_cols2=[], num_cols=numerical_ls) 123456789101112131415161718192021def evaluation(test_dataloder): model.eval() total_cnt = 0. correct_cnt = 0. acc = None with torch.no_grad(): for test_index, test_datas in enumerate(test_dataloder): y, num_x, cat_x1, cat_x2 = test_datas cat_x1 = cat_x1.to(device) cat_x2 = cat_x2.to(device) num_x = num_x.to(device) y = y.to(device) out = model.forward(num_x, cat_x1, cat_x2) correct_cnt += ((out&gt;0.5)==y ).sum().detach().to('cpu').numpy() total_cnt += len(out)# out = out.squeeze().to(\"cpu\").numpy().tolist() acc = 100* correct_cnt / total_cnt print(\"Evaluation Acc: %.4f %%\"%(acc)) return acc 6.Model Evaluation on Wide and Deep model using validation set1acc = evaluation(val_dataloader) Evaluation Acc: 62.6432 % 1234567891011121314151617def predict_test(test_dataset): preds = [] model.eval() test_dataloder = DataLoader(test_dataset, 200, shuffle=False, num_workers=4) with torch.no_grad(): for test_index, test_datas in enumerate(test_dataloder): y, num_x, cat_x1, cat_x2 = test_datas cat_x1 = cat_x1.to(device) cat_x2 = cat_x2.to(device) num_x = num_x.to(device) y = y.to(device) out = model.forward(num_x, cat_x1, cat_x2) out = out.squeeze().to(\"cpu\").numpy().tolist()# print(out) preds.extend(out) return np.array(preds) Make Predictions and submission1234567preds = predict_test(test_dataset)submission = pd.DataFrame()submission['id'] = idssubmission['target'] = predssubmission.to_csv(root + 'submission_WideAndDeep_model.csv.gz', compression = 'gzip', index=False, float_format = '%.5f')print(\"Model Predictions: \",preds)# !kaggle competitions submit -c ./train/data -f submission_lgbm_model.csv.gz -m \"Message\" Model Predictions: [0.60901934 0.60901934 0.35112065 ... 0.54463851 0.48204085 0.54300648] submission_WideAndDeep_model.csv.gzWideAndDeep_model0.616280.61117 12perf_df = pd.DataFrame({\"model name\":['Wide and Deep model'],\"private_score\":[0.61628], \"public score\": [0.61117]})perf_df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } model name private_score public score 0 Wide and Deep model 0.61628 0.61117 12345model_names = [\"lgbm model 0\",\"lgbm model 1\",\"lgbm model 2\",\"lgbm model 3\",\"lgbm model 4\"]private_score = [0.67206,0.67416,0.67416,0.67435,0.67423]public_score = [0.66940,0.67188,0.67208,0.67241,0.67256]perf_df = pd.DataFrame({\"model name\":model_names,\"max_depth:\":max_depths,\"private_score\":private_score, \"public score\": public_score})perf_df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } model name max_depth: private_score public score 0 lgbm model 0 10 0.67206 0.66940 1 lgbm model 1 15 0.67416 0.67188 2 lgbm model 2 20 0.67416 0.67208 3 lgbm model 3 25 0.67435 0.67241 4 lgbm model 4 30 0.67423 0.67256 7. SummaryType answers here","link":"/2020/12/01/kkboxmusicrecommendation-notebook-v4/"}],"tags":[{"name":"DeepFM","slug":"DeepFM","link":"/tags/DeepFM/"},{"name":"C++","slug":"C","link":"/tags/C/"},{"name":"review","slug":"review","link":"/tags/review/"},{"name":"Convolution Neural Network","slug":"Convolution-Neural-Network","link":"/tags/Convolution-Neural-Network/"},{"name":"Backpropagation","slug":"Backpropagation","link":"/tags/Backpropagation/"},{"name":"Dropout","slug":"Dropout","link":"/tags/Dropout/"},{"name":"Binary Tree","slug":"Binary-Tree","link":"/tags/Binary-Tree/"},{"name":"Traversal","slug":"Traversal","link":"/tags/Traversal/"},{"name":"BuckSort","slug":"BuckSort","link":"/tags/BuckSort/"},{"name":"Sorting","slug":"Sorting","link":"/tags/Sorting/"},{"name":"Parallel Computing","slug":"Parallel-Computing","link":"/tags/Parallel-Computing/"},{"name":"binary search","slug":"binary-search","link":"/tags/binary-search/"},{"name":"Amdahl&#39;s Law","slug":"Amdahl-s-Law","link":"/tags/Amdahl-s-Law/"},{"name":"Parallel Speedup","slug":"Parallel-Speedup","link":"/tags/Parallel-Speedup/"},{"name":"JavaScript","slug":"JavaScript","link":"/tags/JavaScript/"},{"name":"D3.js","slug":"D3-js","link":"/tags/D3-js/"},{"name":"commands","slug":"commands","link":"/tags/commands/"},{"name":"K-Mean Clustering","slug":"K-Mean-Clustering","link":"/tags/K-Mean-Clustering/"},{"name":"Unsupervised Learning","slug":"Unsupervised-Learning","link":"/tags/Unsupervised-Learning/"},{"name":"non-parametric learning","slug":"non-parametric-learning","link":"/tags/non-parametric-learning/"},{"name":"Regression","slug":"Regression","link":"/tags/Regression/"},{"name":"Variable Selection","slug":"Variable-Selection","link":"/tags/Variable-Selection/"},{"name":"Markdown","slug":"Markdown","link":"/tags/Markdown/"},{"name":"Bagging","slug":"Bagging","link":"/tags/Bagging/"},{"name":"Boosting","slug":"Boosting","link":"/tags/Boosting/"},{"name":"Stacking","slug":"Stacking","link":"/tags/Stacking/"},{"name":"ROC curve","slug":"ROC-curve","link":"/tags/ROC-curve/"},{"name":"Confusion Metric","slug":"Confusion-Metric","link":"/tags/Confusion-Metric/"},{"name":"Cross Validation","slug":"Cross-Validation","link":"/tags/Cross-Validation/"},{"name":"Holdout","slug":"Holdout","link":"/tags/Holdout/"},{"name":"Ensemble Learning","slug":"Ensemble-Learning","link":"/tags/Ensemble-Learning/"},{"name":"Model Evaluation","slug":"Model-Evaluation","link":"/tags/Model-Evaluation/"},{"name":"Model Selection","slug":"Model-Selection","link":"/tags/Model-Selection/"},{"name":"Nature Language Processing","slug":"Nature-Language-Processing","link":"/tags/Nature-Language-Processing/"},{"name":"Word vector","slug":"Word-vector","link":"/tags/Word-vector/"},{"name":"Natural Language Representations","slug":"Natural-Language-Representations","link":"/tags/Natural-Language-Representations/"},{"name":"Negative Sampling","slug":"Negative-Sampling","link":"/tags/Negative-Sampling/"},{"name":"Word Embedding","slug":"Word-Embedding","link":"/tags/Word-Embedding/"},{"name":"PySpark","slug":"PySpark","link":"/tags/PySpark/"},{"name":"Data Analysis","slug":"Data-Analysis","link":"/tags/Data-Analysis/"},{"name":"KMean Clustering","slug":"KMean-Clustering","link":"/tags/KMean-Clustering/"},{"name":"Machine Learning","slug":"Machine-Learning","link":"/tags/Machine-Learning/"},{"name":"Collaborative Filtering","slug":"Collaborative-Filtering","link":"/tags/Collaborative-Filtering/"},{"name":"Wide&amp;Deep Model","slug":"Wide-Deep-Model","link":"/tags/Wide-Deep-Model/"},{"name":"NeuralFM","slug":"NeuralFM","link":"/tags/NeuralFM/"},{"name":"Hypothesis Test","slug":"Hypothesis-Test","link":"/tags/Hypothesis-Test/"},{"name":"P-value","slug":"P-value","link":"/tags/P-value/"},{"name":"Independence Test","slug":"Independence-Test","link":"/tags/Independence-Test/"},{"name":"Regular Expression","slug":"Regular-Expression","link":"/tags/Regular-Expression/"},{"name":"BeautifulSoup","slug":"BeautifulSoup","link":"/tags/BeautifulSoup/"},{"name":"Web Scrapping","slug":"Web-Scrapping","link":"/tags/Web-Scrapping/"},{"name":"insertion sort","slug":"insertion-sort","link":"/tags/insertion-sort/"},{"name":"bubble sort","slug":"bubble-sort","link":"/tags/bubble-sort/"},{"name":"merge sort","slug":"merge-sort","link":"/tags/merge-sort/"},{"name":"quick sort","slug":"quick-sort","link":"/tags/quick-sort/"},{"name":"Hexo","slug":"Hexo","link":"/tags/Hexo/"},{"name":"Icarus","slug":"Icarus","link":"/tags/Icarus/"},{"name":"LGBM","slug":"LGBM","link":"/tags/LGBM/"},{"name":"Wide and Deep Model","slug":"Wide-and-Deep-Model","link":"/tags/Wide-and-Deep-Model/"},{"name":"Recommendation System","slug":"Recommendation-System","link":"/tags/Recommendation-System/"},{"name":"Boosting Machine","slug":"Boosting-Machine","link":"/tags/Boosting-Machine/"},{"name":"Wide and Deep","slug":"Wide-and-Deep","link":"/tags/Wide-and-Deep/"}],"categories":[{"name":"Machine Learning","slug":"Machine-Learning","link":"/categories/Machine-Learning/"},{"name":"NLP","slug":"NLP","link":"/categories/NLP/"},{"name":"Recommendation System","slug":"Recommendation-System","link":"/categories/Recommendation-System/"},{"name":"Programming","slug":"Programming","link":"/categories/Programming/"},{"name":"Deep Learning","slug":"Deep-Learning","link":"/categories/Deep-Learning/"},{"name":"Data Structure","slug":"Data-Structure","link":"/categories/Data-Structure/"},{"name":"Parallel Computing","slug":"Parallel-Computing","link":"/categories/Parallel-Computing/"},{"name":"Searching","slug":"Searching","link":"/categories/Searching/"},{"name":"Linux","slug":"Linux","link":"/categories/Linux/"},{"name":"Ensemble Method","slug":"Machine-Learning/Ensemble-Method","link":"/categories/Machine-Learning/Ensemble-Method/"},{"name":"Machine Learning","slug":"NLP/Machine-Learning","link":"/categories/NLP/Machine-Learning/"},{"name":"Data Strucure","slug":"Searching/Data-Strucure","link":"/categories/Searching/Data-Strucure/"},{"name":"PySpark","slug":"PySpark","link":"/categories/PySpark/"},{"name":"Statistic","slug":"Statistic","link":"/categories/Statistic/"},{"name":"Accuracy Improvement","slug":"Machine-Learning/Ensemble-Method/Accuracy-Improvement","link":"/categories/Machine-Learning/Ensemble-Method/Accuracy-Improvement/"},{"name":"Data Collection","slug":"Data-Collection","link":"/categories/Data-Collection/"},{"name":"Sorting","slug":"Data-Structure/Sorting","link":"/categories/Data-Structure/Sorting/"},{"name":"Web","slug":"Web","link":"/categories/Web/"},{"name":"Report","slug":"Report","link":"/categories/Report/"}]}