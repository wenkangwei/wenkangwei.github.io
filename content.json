{"pages":[{"title":"","text":"","link":"/index.html"},{"title":"Wenkang Wei","text":"My Websites Coding practice website: https://wenkangwei.gitbook.io/leetcode-notes/It summaries of coding practice and implementation of algorithms Technical blog: https://wenkangwei.github.io/It summaries of algorithms and projects Source code repository: https://github.com/wenkangweiIt stores my open source projects, source codes and some datasets Who am I? My ResumeMachine Learning Fan and Researcher - I’m a second-year master student of computer engineering with minor of Computer Science. So far, I’m doing researches about deep learning and data mining. I’m working with Dr. Adam hoover on Human activity (Eating) pattern recognition research. I’m also a research assistant in Clemson-Clair Lab of Dr. Kai Liu with research topic of machine learning optimization. Job Seeker - I’m actively looking for a full-time job related to Data Scientist or Machine Learning Engineer that starts from Summer 2021 after my graduation in May 2021My Resume can be found Here or LinkedInYou are welcomed to email me if you have any suggestion or reference to me. blogger - I find that writing blogs can help me manage my tehcnical notes and problems I solved in the past. What’s more, it provides a platform for me to communicate techniques with more people. I’m trying to manage my technical notes and my thoughts on some problems I wrote in the past. Geek - I usually find some technical projects to do once I’m leisure. Some projects are inspired by my daily life, my classes on campus and my researches. To know more about it, Click HERE Dreamer - I usually imagine how our brains work and ponder how AI will collaborate with human in the future and how can we borrow human learning behaviors or architectures of human brain to design a new AI algorithm. Hopefully, the secret behine human brain can be revealed one day. Amateur of paintingI’m a fan of japanese animes and hence sometimes do some paintings related to anime characters. Educational BackgroundI’m a master student of computer engineering with the focus area of intelligent system and pattern recogniton. I have received the BS degree with EE major and named to Dean’s List as well in Clemson Univeristy. If you are interested in my academic achievements, Click HERE Work ExperienceMachine Learning Research Assistant - Summer 2020-Current Proof of convergence and convergence rate of Multiple Update Algorithm (MUA) in Non-Negative Matrix Factorization Problem Formulated Matrix Factorization Problem into Constraint Optimization Problem Applied Linear Algebra, Lagrange multiplier to simplify problem and utilized Lipschitz gradient, convex optimization to prove the convergence and convergence rate of MUA algorithm Implemented MUA and ALS (alternative least square) algorithm in Google Colab and Matlab to verify convergence result Collaborated and communicated with CS professor Dr Kai Liu to present mathematic proof process orally Wrote a paper in AAAI format using Latex (unpublished due to copyright reason) Technical Skills Programming: Python/Jupyter Notebook PostgreSQL C/C++ Matlab HTML, Markdown, Latex Tools: Deep Learning Framework: PyTorch / Tensorflow Distributed Machine Learning: PySpark, Hadoop MapReduce,MPI Data Analysis toolkits: sklearn, pandas, seaborn, etc Platform: Raspberry Pi, Linux, Google Colab, Git, AWS (RDS, EC2) Theory and Analysis Techniques: Feature Engineering, Data Visualization and Preprocessing Techniques, PCA, NLP text processing: Word Embedding, TF-IDF, etc Machine Learning Modeling: Collaborate Filtering, Matrix Factorization, SVM, Decision Tree, Clustering, Convolution Neural Network etc Techniques for Model Evaluation and Improvement: Cross-Validation, Ensemble Learning, ROC, AUC, Feature Importance, etc. Selected ProjectsImage Classification Car Classification using Transfer Learning - Fall 2020 Constructed data pipeline by PyTorch to extracted and transformed Stanford car images dataset (1.96GB dataset with 196 classes) Modified and tuned pre-trained models Google-Net, VGG-16 , Res-Net 50 to fit car dataset using early stopping, weight decay techniques Improved test accuracy of the best model to 85% using cross-validation model selection techniques Recommendation System Recommendation System based on MovieLens 25M dataset ((PySpark, Hadoop, HDFS, SQL) Utilized PySpark to load movielens 25M dataset (25 million ratings) and used SQL to query and analyze data in databrick cluster platform Implemented and applied Mapper, Reducer functions in Hadoop File system to analyze contribution of different movie genres to ratings Applied Collaborative Filtering and Matrix Factorization methods to construct a recommendation system with PySpark Achieved 0.67 mean square error score and deployed recommendation system using IPython widget KKBox Music Recommendation System Utilized Exploratory Data Analysis (EDA) techniques and data visualization to analyze relationship between features Constructed Data pipeline to clean data by filling missing values, converting data type and transform data using OneHot encoding, Embedding, etc. Implemented and applied Light gradient boosting machine and wide and deep neural network for recommendation system Data Science and NLP Bank Churn Prediction Visualized and analyzed data related to customer churn by using visualization toolkits: seaborn, matplotlib Preprocessed and transforms categorical data for Machine Learning model training using pandas toolkit and normalization techniques Established Data Pipeline and ML Models: Random Forest, Logistic Regression, SVM, etc. and Evaluated Models using ROC,AUC Improved Models Accuracy from 80% to 86% by using Model Selection, Cross Validation and Feature Selection, L1 Regularization techniques Youtube Comments Analysis and Pet Owners Classification (PySpark, SQL, Databrick Cluster) Utilized PySpark and PostgreSQL to load, query and explore Youtube comment text data (about 1GB after decompression) Built data pipeline and applied Term-Frequency-Inverse Document-Frequency(TF-IDF) to transform text data into numerical data Applied Logistic Regression, Random Forest, Gradient Boosting machine in PySpark to classify cat or dog owners from comments Achieved 92% prediction accuracy on test set using grid search and cross validation Software Development Real-time Signal Visualization System (C++, Qt, GDB) Designed a visualization software system based on Qt toolkit, Arduino using C/C++ to solve the problem of visualizing voltage signal data in real time with self-motivation and initiative Designed GUI components and class modules for software interface in Qt and software framework to control data visualization behaviors in C++ using data structure (queue) and Object-Oriented Programming (OOP) techniques Integrated, tested and debugged GUI components with software framework using GDB toolkit and Qt IDE Wrote technical document for software system in Github with video demo. Link to demo: https://github.com/wenkangwei/SerialPlot More projects will be uploaded soon. They are mentioned in my resume or LinkedIn. Please feel free to contact me via email: wenkanw@g.clemson.edu or LinkedIn if you have any questions or any job opportunities for me. Thanks! My InterestsResearch Interests:Data mining, Recommendation System, NLP, machine learning,deep learning and their applications. Other Interests:Anime, painting, music, badminton,swimming… My Framework for solving problems / researching You are welcomed to contact me by wenkangwei917@gmail.com or by https://github.com/wenkangwei if you have any suggestion on my projects or my blog.","link":"/about/index.html"},{"title":"images","text":"","link":"/images/index.html"},{"title":"src","text":"","link":"/src/index.html"},{"title":"categories","text":"","link":"/categories/index.html"},{"title":"tags","text":"","link":"/tags/index.html"}],"posts":[{"title":"DeepLearning-1 ConvolutionNetwork","text":"IntroductionConvolution neural network has been applied to different domains widely for feature extractor / filter. One of the most successful area is computer vision, in which convolution network is trained and used to extract the general feature we are interested in. This article summarizes how forward passing and backpropagation works in CNN to train the CNN filters. Then some properties of CNN are mentioned. Terms in CNN N: number of data points/ samples in dataset C: number of channel in one sample. Example: in RGB image, it has 3 channels: Red, Green, Blue H: height of one data point matrix / amount of rows in one sample W: width / amount of columns in one sample x: input with shape (N, C, H, W) y: output of convolution network kernel/filter/weight: filter with trainable weights and shape of (KH, KW), where KH: height / rows of kernel, KW: width/ columns of kernel b: bias term. One kernel correponds to one bias stride: The number of pixel between adjacent receptive fields in horizonal, vertical directions. padding: the number of rows, columns added to the boundaries/edges of a sample matrix. Usually, we set the padding row/column values to zeros. We usually set padding = (1,1) or (2,2) to add 1 or 2 row(s)/column(s) to each edge of sample matrix Ho: height/ amount of rows of output fromm convolution Wo: width/ amount of columns of output from convolution Output shape from convolution network: Ho = 1 + (H + 2*padding - KH )/stride Wo = 1 + (W + 2*padding - KW )/stride if Ho, Wo are not integer, that means when moving the kernel along input, index out of range occurs. In this case, we can simply drop the last column/row that make index out of range. Or, we can add zero columns/rows to fill the out of range pieces in matrix. How does CNN workForward pass in CNN Convolution with 1-D Input: without padding, stride =1, channel =1After we define some terms above, let consider there is 1-D input X with shape: C=1, H=1, W=4 and 1-D kernel K with shape KH=1, KW = 3.$$X = [x1, x2, x3, x4], K = [w1, w2,w3]$$ Then the output of convolution is$$Y = \\begin{bmatrix}y_1 \\\\y_2\\end{bmatrix}$$ where $y_1 = w_1 * x_1 + w_2 * x_2 + w_3 * x_3 +b$ and $y_2 = w_1 * x_2 + w_2 * x_3 + w_3 * x_4 +b$ where b is the bias term in convolution Convolution with 2-D Input: stride =1, no padding, channel =1$$X = \\begin{bmatrix}x1 &amp; x2 &amp; x3 \\\\x4 &amp; x5 &amp; x6 \\\\x7 &amp; x8 &amp; x9\\end{bmatrix} ,K = \\begin{bmatrix}w1 &amp; w2 \\\\w3 &amp; w4\\end{bmatrix}$$ Based on these equations, Ho = 1 + (H + 2*padding - KH )/stride Wo = 1 + (W + 2*padding - KW )/stride we know that the output Y has shape H =2, W= 2: $$Y = \\begin{bmatrix}y1 &amp; y2 \\\\y3 &amp; y4\\end{bmatrix}$$ $$y1 = sum( \\begin{bmatrix}x1 &amp; x2 \\\\x4 &amp; x5\\end{bmatrix} * \\begin{bmatrix}w1 &amp; w2 \\\\w3 &amp; w4\\end{bmatrix} ) +b = w1x1 + w2x2 + w3x4 + w4x5 + b$$ where * is element-wise multiplicationSimilarly, we have $$\\begin{matrix}y2 = w1x2 + w2x3 + w3x5 + w4x6 + b, \\\\y3 = w1x4 + w2x5 + w3x7 + w4x8 + b, \\\\y4 = w1x5 + w2x6 + w3x8 + w4x9 + b \\\\\\end{matrix}$$ Convolution with 2-D Input: with padding = (1,1) stride =1, channel =1Here I add 1 row, 1 column zeros pad to four edges of 2-D matrix. Then the sample X becomes 4 by 4 matrix. After padding, we will do the same forward pass process as step 2. $$X = \\begin{bmatrix}0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\\\0 &amp; x1 &amp; x2 &amp; x3 &amp; 0 \\\\0 &amp; x4 &amp; x5 &amp; x6 &amp; 0 \\\\0 &amp; x7 &amp; x8 &amp; x9 &amp; 0 \\\\0 &amp; 0 &amp; 0 &amp; 0 &amp; 0\\end{bmatrix}$$ and$$K = \\begin{bmatrix}w1 &amp; w2 &amp; w3 \\\\w4 &amp; w5 &amp; w6 \\\\w7 &amp; w8 &amp; w9 \\\\\\end{bmatrix}$$ The output Y becomes:$$Y = \\begin{bmatrix}y1 &amp; y2 &amp; y3 \\\\y4 &amp; y5 &amp; y6 \\\\y7 &amp; y8 &amp; y9 \\\\\\end{bmatrix}$$ where$$y1 = w_1 * 0 + w_2 * 0 + w_3 * 0 + w_4 * 0 + w_5 * x_1 + w_6 * x_2 + w_7 * 0 +w_8 * x_4 + w_9 * x_5$$ Let denote the $i^{th}$ row and $j^{th}$ column entry in output Y as $y_{i,j}$, the $h^{th}$ row and $w^{th}$ column entry in kernel K as $w_{h,w}$, the number of stride as $s$. We can write down the formula to compute every entry in output Y:$$y_{i,j} = \\sum_{h=1}^{KH}\\sum_{w = 1}^{KW} w_{h,w} * x_{s * i+h-1, s * j+w -1}$$ Convolution with 2-D Input: with channel =3, padding = (1,1) stride =1when channel of sample X is more than 1, we will need the amount of kernels K equal to the number of input channel $C_{in}$.Example: Input X with Channel C=3$$X_R = \\begin{bmatrix}xr1 &amp; xr2 &amp; xr3 \\\\xr4 &amp; xr5 &amp; xr6 \\\\xr7 &amp; xr8 &amp; xr9\\end{bmatrix}$$ $$X_G = \\begin{bmatrix}xg1 &amp; xg2 &amp; xg3 \\\\xg4 &amp; xg5 &amp; xg6 \\\\xg7 &amp; xg8 &amp; xg9\\end{bmatrix}$$ $$X_B = \\begin{bmatrix}xb1 &amp; xb2 &amp; xb3 \\\\xb4 &amp; xb5 &amp; xb6 \\\\xb7 &amp; xb8 &amp; xb9\\end{bmatrix}$$ Then we will need one kernel for each input channel. Hence we will have 3 kernels $K_R, K_G,K_B$. The output of this convolution network is the sum of all convolution output$$Y = conv(X_R, K_R) + conv(X_G, K_G) + conv(X_B, K_B)$$ In this cases, the output channel is still C= 1. If we want the output channel to be more than 1. Let say the output channel $C_{out}$ =3, then we have $C_{out} * C_{in}$ kernels in total and each kernel has shape of $W_o * H_o$. Hence we will have parameters with amount of $C_{out} * C_{in} * W_o * H_o$ or $C_{out} * C_{in} * (1 + (H + 2 * pad - KH) / stride ) * (1 + (W + 2 * pad - KW) / stride )$ Chain RuleBefore talking about how to update the weight in kernel, let talk about the chain rule first.Denote loss function as $L(y_p, y_t)$, where $y_p$ is prediction / output from estimator and $y_p =f_w(x)$ is a function of input x, $y_t$ is target, where x, y,w are all scalar values Let: $y_p = f_w(x) =wx$$$L(w) = L(y_p, y_{t}) = (y_p - y_{t})^2 = (f_w(x) - y_{t})^2 = (wx - y_{t})^2$$ Then in chain rule to find the gradient of weight w, we have$$\\nabla_w L(w) = \\frac{dL(w)}{dw} = \\frac{dL}{dy_p} * \\frac{dy_p}{dw} = [2 *(y_p - y_t)] * [x] = 2x(wx - y_t)$$ By chain rule, we can extend $ \\frac{dL}{dy_p} * \\frac{dy_p}{dw}$ to $\\frac{dL}{dy_p} * \\frac{dy_p}{dy_1} * \\frac{dy_1}{dy_2} *…\\frac{dy_i}{dw}$ in many terms Backpropagation in CNNLet’s go back the feed forward step in CNN, we have equation$$y_{i,j} =f_w(x) = \\sum_{h=1}^{KH}\\sum_{w = 1}^{KW} w_{h,w} * x_{s * i+h-1, s * j+w -1}$$ Where the output vector of convolution is $Y$ and $y_{i,j}$ is the $i^{th}$ row and $j^{th}$ column entry of Y. $Y^t$ is the target. $x_{i,j}$ is the $i^{th}$ row and $j^{th}$ column entry of input X. Define loss function $L(w)$ . Then gradient of weight $w_{h,w}$ in kernel is obtained by$$\\frac{dL(w)}{dw_{h,w}} = \\sum_{i=1}^{H}\\sum_{j=1}^{W}\\frac{dL}{dy_{i,j}} * \\frac{dy_{i,j}}{dw_{h,w}}$$ Example: $$X = \\begin{bmatrix}x_{11} &amp; x_{12} &amp; x_{13} \\\\x_{21} &amp; x_{22} &amp; x_{23} \\\\x_{31} &amp; x_{32} &amp; x_{33}\\end{bmatrix} , K = \\begin{bmatrix}w_{11} &amp; w_{12} \\\\w_{21} &amp; w_{22}\\end{bmatrix}$$ $$Y = \\begin{bmatrix}y_{11} &amp; y_{12} \\\\y_{21} &amp; y_{22}\\end{bmatrix}$$ Let Loss function be $L(w) = 0.5* \\sum_i\\sum_j (y_{i,j} - y^{t}_{i,j})^2$and we have $$\\frac{dL}{dy_{i,j}} = (y_{i,j} - y^{t}_{i,j})$$ $$\\frac{dL}{dy} = \\begin{bmatrix}\\frac{dL}{dy_{11}} &amp; \\frac{dL}{dy_{12}} \\\\\\frac{dL}{dy_{21}} &amp; \\frac{dL}{dy_{22}}\\end{bmatrix}$$ $$\\frac{dy_{11}}{dw_{11}} = \\frac{d(w_{11} * x_{11} + w_{12} * x_{12} + w_{21} * x_{21} + w_{22} * x_{22} +b)}{dw_{11}}$$ $$\\frac{dy_{11}}{dw_{11}} = x_{11}$$ Similarly, we have $$\\frac{dy}{dw_{11}} = \\begin{bmatrix}\\frac{dy_{11}}{dw_{11}} &amp; \\frac{ddy_{12}}{dw_{11}} \\\\\\frac{ddy_{21}}{dw_{11}} &amp; \\frac{ddy_{22}}{dw_{11}}\\end{bmatrix} = \\begin{bmatrix}x_{11} &amp; x_{12} \\\\x_{21} &amp; x_{22}\\end{bmatrix}$$ Finally, gradient of weight $w_{11}$ becomes$$\\frac{dL}{dw_{11}} = sum(\\frac{dL}{dy} * \\frac{dy}{dw_{11}} ) = \\sum_{i=1}^{2}\\sum_{j=1}^{2}\\frac{dL}{dy_{i,j}} * \\frac{dy_{i,j}}{dw_{1,1}}$$ $$\\frac{dL}{dw_{11}} = \\sum_{i=1}^{2}\\sum_{j=1}^{2} (y_{i,j}-y^t_{i,j})*x_{i,j}$$ where * is element-wise multiplication Repeat doing this, we can find the gradient for all weights in all kernels. Properties in CNN the weight matrix in CNN is small and save memory compared with traditional dense network.For example, if we have an 1818 input matrix we want to output a 3x3 matrix, we can either use a 16x16 convolution kernel with stride =1, or 9 3x3 kernels to do this.In this case, 933 is smaller than 1616. Hence CNN with deeper convolution network and small filter can save memory Updating weights in CNN is fast, as the weight / kernel is small CNN can be used for transfer learning by transferring learned kernels CNN can be used for down-sampling data, reducing data size.For example, in ResNet, it uses pixel convolution: kernel size =1 * 1 and stride =2 to down sample data by half of original size in each channel of matrix Reference[1] https://towardsdatascience.com/backpropagation-in-a-convolutional-layer-24c8d64d8509 [2] https://arxiv.org/pdf/1512.03385.pdf","link":"/2020/11/03/DL-ConvolutionNetwork/"},{"title":"Cpp review 1","text":"Important Keywords const Type Meaning Explanation const int x constant variable Value of x must be initialized first const int *x constant content; mutable pointer; d constant content pointer x points to. address of pointer x is mutable. x can be initialized after defintion int * const x constant pointer; mutable content; pointer x must be initialized first; const int* const x Constant pointer; Constant content x must be initialized first const Class a constant class var content of member var of a can not be changed; can not call not-constant functions;(avoid changing member values) void func(const int x) const var as input function can not change the local variable; const int&amp; funct(x) return constant reference pointer using this reference can not change its content const int funct(x) return constant var return a constant variable int func() const constant member function Function is unable to change member values in the class const MyClass c constant Class variable variable c can not call non-const member function and c can not change member variable values define Macro preprocesing keyword. Different from const, #define doesn’t have data type Compiler replaces the words only without checking the variable and its memory. E.g #define A B. Replace A with B only, compiler doesn’t check the type of A. In C++ program, usually use const rather than define for security. sizeof(x) / sizeof x check number of bytes of variable based on data type sizeof class_variable = sum of bytes of all member variables in class in 64-bit machine, sizeof pointer = 8 bytes = 64 bits static static global var only visiable to current file data is stored in global memory, whereas local var is stored in stack and heap. Automatically initialized as 0 Eg: 12static int a;int main(){...} static local var It is initialized once stored in global memory without releasing memory until program terminates can be used in local code block only. static function visible to current file only can be defined in public, protected, private region. static class member: used for private class info, rather than instance info Different from common private member (which can be called and modified via this pointer), static class member DO NOT have this pointer Only private and protected member can define static. Public member can not define static.However, static function can be defined in public region as well. Can be called only by private/protected class function Initialization for static member inside class requires const definition Initialization for static member outside class requires that it can be initialized once. Initialization must be outside code block, like global variable Example code: 12345678910111213class myclass{ public: void func(){cout&lt;&lt;b;} // call private var inside member function private: static int a; //or static const int a =0; static int a=0;//wrong, a must be const static int b;}int myclass::b = 10; // way to initialize staic class member, like global var, but won't conflict global var.int main(){...} external Tell the compiler that there is an function, whose implement of function is in other file. inline inline function tell the compiler to embed the code of the function into every place where the function is called. So, inline function reduce the time used to expand the function by using more memory. Normal function in C/C++ is expanded by compiler only when they are call. This requires longer time to run program Inline function has higher memory complexity and lower time complexity than normal function Not suitable for construtor function (since it will generate lots of codes ) Not suitable for virtual function inline function is unable to use render/loop operation Note: Inline function doesn’t support loop and switch Usually, inline function is used when function has a few codes. (usually less than 10 lines) typedef and #define It is a sentence not command for compiler, so it requires “;” compared with #define Compiler check the grammar and type of typedef, but doesn’t check type of #define #define simply replaces the notation with new one, it doesn’t check the grammar or operation in definition Example: 123typdef struct A{...} B; //define A as type B#define A B // define / replace B with A#define func(x) x*x explicit Require constructor to run explictlyEg:1234567c = myclass(1);//orc= (myclass) 12; // convert type explictlymyclass c;c = 1; // Not allowed, need (myclass) 1 Using Declaration of “using” keyword: It indicates the class we use is from std library. Example: 1using std::vector; It indicates the class vector is from std Compile command: It tells the compiler to use the whole namespace (every element from this namespace) Example: 1using namespace std; Variable type global local static Operators Operator computes from right to left: &lt;condition-expression&gt; ? &lt;return if condition=__True__&gt; : &lt;return if condition =__False__&gt;It returns value from right to left = Operator Priority arithmetic operator: *, % , / are higher than +, - Relationship operator: &gt;, &lt; , &gt;=，&lt;= are higher than !=, == logic operator: ! higher than &amp;&amp;, &amp;&amp; higher than || logic bit operator: ~ higher than &amp;, &amp; higher than | Grammar a++ and ++a a++: return another local variable contain value of a. Then a=a+1 ++a: a=a+1 and then return a. Speed: a++ &gt; a+=1 &gt; a=a+1. Switch input must be one of char,short, int 123456switch(a):{ case x1: ....; break; case x2:....; break; default: ....; break} Pointer and reference Pointer Reference Pointer is a variable Reference is an essentially implicit pointer or the representation of the address of a memory piece Pointer can be empty or uninitialized; Reference requires variable to be initialized before using reference. nullptr virtual pointer Pointer and Array Array Pointer: the pointer pointing to the address of an array Array Name: array name is not a pointer, but just return the address of the array (address of the first element) Pointer for array element: 123int arr[] = {1,2,3,4,5};int *p = arrcout&lt;&lt;*p++; *p++ : return element *p, then p++ ++*p: (*p) element +1 and then return. Without changing the address pointer p pointing to Array Pointer 123int arr[] = {1,2,3,4,5};size =5;int (*p)[size] = &amp;arr; (*p)[size] : array pointer, pointing to array NOT element size of (*p)[size] must be number of element in arr p++: address skip the whole array size (5 * 4 byteshere), not element size (*p)[0]: return arr[0] Wild Pointer(野指针) pointer that has not been initialized before using Dangling Pointer(悬空指针) pointer that point to invalid memory address (after memory released) Macro and PreprocesssorDifference between Class and StructureObject-Oriented DesignBasic Features of OOD in C++ Three Traits Encapsulation(封装性)A mechanism of bundling the data, and the functions that use them and data abstraction is a mechanism of exposing only the interfaces and hiding the implementation details from the user Polymorphism(多态)Different classes inherent from the same parent class can have different methods, class members. It diversifies features of classes. Polymorphims in C++ is achieved by using virtual keyword. Inheritance(继承)The members of the base class become members of thederived class. It helps save our time and avoid re-writing the same codes. Publich, Protected, private Properties Private Protected Public Accessibility only Base Class member/method only Base Class and Child Class member/method Any instance and Class member Inheritance Properties: Private Inheritance: Base Class’s private members won’t be inherited by Child Class Base Class’s public and protected members will be converted to be private in Child Class Protected Inheritance: Base Class’s private members won’t be inherited by Child Class Base Class’s protected, public members wil become protected in Child Class, Base Class’s members can not be accessed by Child Class of Base Class’s Child Class. Public Inheritance: Base Class’s private members won’t be inherited by Child Class Base Class’s protected, public members don’t change (Still protected, or public) Types of overloading overloading the amount of parameter Example 12345Class A{ publich: void func(int a) {} void func(int a, int b) {}} overloading the data types of parameter Example 12345Class A{ public: void func(int a) {} char func(char a) {}} overloading of const and non-const methods 123456Class A{ public: void func(int a) {} // the const method can not change class member values void func(int a) const {}} overloading of Operators Example: 1234Class A{publich: int operator + (const int a, const int b) {}} Operators that can not be overloaded: :: , pointer operator . *, condition operator ?: , sizeof, typeof const_cast static_cast dynamic_cast reinterpret_cast Methods that can not be inherent Constructor Deconstructor Friend method Operators Methods that can be inherited static functions non-static functions non-static members (Note: static members must be private or protected that are usually unable to be inherent) Virtual function (used for polymorphism) Interface and Virtual class, method and pointer Pure virtual functionit is applied by using virtual keyword and =0 behind the function. 1234class A{ public: virtual int func(int x) =0;} Abstract ClassAbstract class provide an appropriate base class from which other classes can inheritAn abstract class must contain at least one pure virtual function InterfaceInterface describes the behavior a class without a particular implementation of that class.Interface in C++ is implemented by using Abstract class. Virtual Function and PolymorphismA Child of base class can overload the virtual function and give particular implement of the function to achieve polymorphism Virtual Function mechanism: using virtual pointer and virtual table In base class with virtual function, Compiler creates an virtual pointer vptr and a virtual table vtbl vptr points to the address of vtbl to write and store the addresses of all virtual functions in this base class. The pointer vptr is stored at the beginning of the memory of the base class. (the first element of the Class is vptr ) Example: For class A and Class B, each of them have virtual pointer __vptr__ pointing to a virtual table, which stores addresses of virtual functions. In addition, the virtual pointer of each class is stored at the beginning of the class. 4. In __single inheritance__ (a class is allowed to inherit only one class), child class will inherit the vptr from base class, but __create a new vtbl table to store addresses of new virtual functions and inherited/overrided virtual functions__ 5. In __multiple inheritance__: + child class inherits multiple vptr, then creates multiple vtbls, in which each vtbl corresponds to each base class (Hence __the number of vtbl = number of base class inherited__). + __All addresses of new virtual functions will be stored at the end of the first vtbls__ Template and Generic Function A generic function defines a general set of operations that will be applied to various types of data. Memory allocation and Types of memory new and malloc new malloc A keyword a function it calls the operator operator new() -&gt; malloc()-&gt; then call constructor in class directly allocate memory without using constructor return class pointer return void pointer can be overloading can not be overloading No need to tell size of memory Need to tell size of memory delete and free delete free A keyword a function it calls the operator operator delete() -&gt; free()-&gt; then call deconstructor in class directly free memory without using constructor return class pointer return void pointer can be overloading can not be overloading No need to check if memory exists Need to check memory first create and delete Class array 123456int size=10;ClassA* a = new ClassA[size]delete[] a; // input is an array of int class int ClassA::func(int[] a){....} malloc, calloc, realloc and alloca malloc: allocate memory without initialization calloc: allocate memory and initialize them to zero realloc: extend the memory with bigger size than before. alloca: allocate tempory memory on stack on local scope. Memory will be released outside the scope. Hence no need to use free() link and compile in C++ Dynamic Link Static Link Regular Expression (re)STL containerReference[1] https://www.tutorialspoint.com/cplusplus/cpp_overloadinging.htm [2] https://blog.csdn.net/csdn_chai/article/details/78041050","link":"/2021/03/08/Cpp-review/"},{"title":"Data Structure - BuckSort with Parallel Computing","text":"IntroductionThis article introduces the basic bucket sort algorithm and the parallel version of bucket sort Simple Bucket SortStep of Bucket Sort Create n buckets and each bucket has a range, such as [0,4) Assign every element of unsorted array to the corresponding bucket based on the range of bucket. Ex: element 3 should be assigned to the bucket with range [0,4) Sort every bucket using insertion sort or other sorting method Merge all buckets together based on range to get the overall sorted arrayExampleUsing two buckets with range: [0,5), [5, 10]12345678unsorted array: [10, 2 , 5, 9, 4, 6, 1] / \\assign element to buckets: [2,4,1] [10,5,9,6] | | Sort every bucket [1,2,4] [5,6,9,10]with insertion sortMerge buckets: [1,2,4,5,6,9,10] Analysis Time complexity: O(n) for assigning element to buckets. O(n^2) for insertion sort in insertion. When merging buckets to a new buckets: O(n). Depending on the insertion operation, the time complexity can be different. Average case: O(n) if we think insertion time is O(1), else O(n)+O(n^2) = O(n^2) when using insertion sortif using quicksort, mergesort, it becomes O(nlog(m))+O(n) = O(nlogm) Memory complexity: O(n), since we use buckets to store element, where n = number of bucket * size of bucket Bucket Sort with Parallel Computing methodStep of Bucket Sort of parallel version There are p computing nodes/processors For every computing node, it has 1 large bucket and m small bucket and p=m In processor 0, it divides unsorted array into pieces evenly to every processors Every small bucket / large bucket has its range Every processor breaks its own piece of array into its small buckets based on the range of small bucket Sort every small bucket using some sorting (quicksort, insertion sort, etc) Gather all small buckets that have the same range into the large bucket that have that range Sort every large bucket Merge all large buckets into a sorted array ExampleUsing two small buckets: b1, b2 and two large buckets: lb1, lb2, with range: [0,5), [5, 10].There are two computing nodes/processor 1234567891011121314unsorted array: [10, 2 , 5, 9, 4, 6, 1, 7] / \\2 nodes: node1: [10,2,5,9] node2: [4,6,1,7] | | Divide them into b1: [2] b2:[10,5,9] b1:[4,1] b2:[6,7]Small buckets | |Sort: b1:[2], b2:[5,9,10] b1:[1,4] b2:[6,7] | |Send to large buckets: node1: [2,1,4] node2: [5,9,10,6,7] | |Sort: node1: [1,2,4] node2: [5,6,7,9,10]Merge buckets: [1,2,4,5,6,7, 9,10] Analysis Time complexity: Average Case O(n) if we consider sorting time as O(1). Or O(nlogm) if we use quicksort/mergesort Memory Complexity: O(n) since we use p computing nodes , large buckets and m smaller buckets to store data Reference[1] https://media.geeksforgeeks.org/wp-content/uploads/BucketSort.png","link":"/2020/09/23/Data-Structure-BuckSort-Parallel-Computing/"},{"title":"Data Structure 3 - BinaryTree","text":"Binary TreeDefinition of Binary TreeThe settings of binary tree are following: Each node in the tree contains no more than 2 children nodes (left node, right node) Leaf nodes of the tree are the nodes that contain no children nodes Traversal of Binary TreeType of traversals of binary treeConsider this example of binary tree Pre-order traversalThe order of visiting nodes: current node -&gt; left children node -&gt; right children node.In the binary tree above, we start from the root node (current node) and follow the rule to visit each node. Then have cur_node= 10, left_node =5. when we goes to left_node 5, 5 becomes the new current node and hence print 5 and then its left_node 2. When it finds there is no left node after 2, it goes back to 5 and visit its right node.Repeat doing this, we have Pre-Order traversal of this binary tree: 10-&gt; 5-&gt;2-&gt; 7-&gt; 15-&gt;20 In-order traversalThe order of visiting nodes: left children node -&gt; current node -&gt; right children nodeSimilar to Pre-Order traversal, except the traversal order, In-order requires the current node is the second node to be visited. Hence, In-Order traversal of this example is: 2-&gt;5-&gt;7-&gt;10-&gt;15-&gt;20 Post-order traversalThe order of visiting nodes: left children node -&gt; right children node -&gt; current nodeSimilarly, Post-Order traversal of the example is: 2-&gt;7-&gt;5-&gt;15-&gt;20-&gt;10 Level-order traversalIt traverses the binary tree from top level to lower level. In each level of tree, it iterates the nodes from left to right. Level-Order traversal of this example: 10-&gt;5-&gt;15-&gt;2-&gt;7-&gt;20 Implementation of Traversal of Binary Tree Recursive Method Pre-OrderTime Complexity: O(n)Memory Complexity: O(h), h is the height of the tree. It is used by recursion to store address of function in stack 12345678910111213141516class TreeNode(): def __init__(self): self.left = None self.right = None self.val = Noneclass Solution(): def Pre_Order(self, root): if not root: return [] result = [] result.append(root.val) left_list = self.Pre_Order(root.left) right_list = self.Pre_Order(root.right) result.extend(left_list) result.extend(right_list) return result In-OrderTime Complexity: O(n)Memory Complexity: O(h), h is the height of the tree. It is used by recursion to store address of function in stack 1234567891011class Solution(): def In_Order(self, root): if not root: return [] result = [] left_list = self.In_Order(root.left) result.extend(left_list) result.append(root.val) right_list = self.In_Order(root.right) result.extend(right_list) return result Post-OrderTime Complexity: O(n)Memory Complexity: O(h), h is the height of the tree. It is used by recursion to store address of function in stack 1234567891011class Solution(): def Post_Order(self, root): if not root: return [] result = [] left_list = self.Post_Order(root.left) result.extend(left_list) right_list = self.Post_Order(root.right) result.extend(right_list) result.append(root.val) return result Iterative Method Pre-OrderTime Complexity: O(n)Memory Complexity: O(h), h is the height of the tree123456789101112131415161718192021222324class Solution(object): def PreOrder(self, root): \"\"\" input: TreeNode root return: Integer[] \"\"\" # write your solution here if not root: return [] stack = [(root, 1)] result = [] while len(stack)&gt;0: node, count = stack.pop() if count ==1: result.append(node.val) stack.append((node,2)) if node.left: node = node.left stack.append((node, 1)) if count == 2: if node.right: node= node.right stack.append((node,1)) return result In-OrderTime Complexity: O(n)Memory Complexity: O(h), h is the height of the tree123456789101112131415161718192021222324class Solution(object): def InOrder(self, root): \"\"\" input: TreeNode root return: Integer[] \"\"\" # write your solution here if not root: return [] stack = [(root, 1)] result = [] while len(stack)&gt;0: node, count = stack.pop() if count ==1: stack.append((node,2)) if node.left: node = node.left stack.append((node, 1)) if count == 2: result.append(node.val) if node.right: node= node.right stack.append((node,1)) return result Post-OrderTime Complexity: O(n)Memory Complexity: O(h)1234567891011121314151617181920212223242526class Solution(object): def PostOrder(self, root): \"\"\" input: TreeNode root return: Integer[] \"\"\" # write your solution here if not root: return [] stack = [(root, 1)] result = [] while len(stack)&gt;0: node, count = stack.pop() if count ==1: stack.append((node,2)) if node.left: node = node.left stack.append((node, 1)) if count == 2: stack.append((node, 3)) if node.right: node= node.right stack.append((node,1)) if count==3: result.append(node.val) return result Level-OrderTime Complexity: O(n)Memory Complexity: O(h)1234567891011121314151617class Solution(object): def LevelOrder(self,root): if not root: return [] queue = [root] result = [] while len(queue) &gt;0: # dequeue node = queue[0] del queue[0] result.append(node.val) # enqueue if node.left: queue.append(node.left) if node.right: queue.append(node.right) return result Special Binary Tree1. Balanced Binary TreeBalanced Binary Tree is a tree that the depth of left and right subtrees of every node differ by 1 or less.Hence, for each node in the tree, we need to check the heights of left, right subtrees.In the examples below: Example 1 is a balanced tree, but Example 2 is not, since left and right subtrees of the node of 20 in example 2 has the height difference 2-0 = 2 &gt;1. 2. Complete Binary TreeA Binary Tree is a complete Binary Tree if all the levels are completely filled except possibly the last level and the last level has all keys as left as possibleConsider the following example: 12345 10 / \\ 5 1 / \\ / 2 4 2 This is an complete as well as balanced tree.However, the following one is balanced but not complete tree, since in the last level, all keys are not as left as possible as the node 2 should be in the left node, but it doesn’t. 12345 10 / \\ 5 1 / \\ \\2 4 2 4. Perfect Binary TreePerfect Binary Tree A Binary tree is a Perfect Binary Tree in which all the internal nodes have two children and all leaf nodes are at the same level. Example of a perfect tree 12345 10 / \\ 5 1 / \\ / \\2 4 3 2 Example of not a perfect tree 12345 10 / \\ 5 1 / \\ /2 4 3 This is a complete, balanced binary tree, but not a perfect tree 5. Binary Search TreeBinary search tree is a tree that for every node in the tree, the values in left subtree are smaller than its value, the values in right subtree are greater than its.if we consider the duplicated values in the tree, the values in right subtree can be greater than or equal to the node value. This case should be discussed if duplicated values exist. This difference should be determined when discussing with the hiring manager Example of Binary Search Tree 12345 10 / \\ 5 11 / \\ \\2 7 21 Example of Not a Binary Search Tree 12345 10 / \\ 5 1 / \\ \\2 7 21 6. AVL TreeAVL tree is a self-balancing Binary Search Tree (BST) where the difference between heights of left and right subtrees cannot be more than one for all nodes.When inserting each element in the AVL tree, it will re-balance the tree automatically.The operations of this data structure will be demonstrated in future 7. Red-Black TreeRed-Black tree is also a self-balancing tree. It has the following constraints: Each node has a color either red or black Root of tree is always black No adjacent red nodes. The parent/ children of a red node could not be red. But there could be adjacent black nodes Every path from a node (including root) to any of its descendant NULL node has the same number of black nodes.Example: Every path starting from node 18 to NiL/ None, have the same amount of black node 1. SummaryIn future, I may write the notes about more operations about binary tree, AVL tree, Red-Black Tree, just like computing height of tree, isBalanced, isSymmetric, insertion and deletion of element in AVL tree/Red-Black tree Reference[1] https://www.geeksforgeeks.org/red-black-tree-set-1-introduction-2/[2] https://www.geeksforgeeks.org/avl-tree-set-1-insertion/[3] https://zh.wikipedia.org/wiki/%E4%BA%8C%E5%85%83%E6%90%9C%E5%B0%8B%E6%A8%B9","link":"/2020/09/06/Data-Structure-BinaryTree/"},{"title":"DeepLearning -2 DropOut","text":"Introduction and Problem of OverfittingIn deep learning, Overfitting is a common problem, in which model fits the training data very well, but perform worse in test data / unseen data. This is due to that when the model learns general features of the dataset, it also learns some specific features in some specific samples well. It makes generalization error increase as well. Dropout is a regularization method that approximates training a large number of neural networks with different architectures in parallel. Or we can regard DropOut as a method of sampling sub-neural network within a full neural network by randomly selecting some neurons to feedforward and update weight. How DropOut WorkDropOut can be applied to any hidden layer. It is to randomly select some neurons in the hidden layer to output to the next layer. As the image shows here, in the second hidden layer, we randomly select some neurons to output and disable other neurons during feedforward process. Then in back propagation process, we only update the weights connected to those selected neurons. We can also regard DropOut as Sampling training technique in training weight. In back propagation, since we randomly sample neurons to output, it can be regarded as a 0,1 mask to multiply the output in hidden layer. So in back propagation, the gradient to the weights of ignored neurons will be zeros. Note that DropOut is applied in training step only. We DO NOT use dropout in prediction step as it can make the prediction unstable when randomly choosing different neurons for output. DropOut RateDropOut rate is the possibility of training a given node in a hidden layer. If dropout rate is large, then it is more likely to select and train the node in hidden layer. For example, if dropout rate = 0.1, then each node in a hidden layer has only 0.1 possibility of being trained (enabled to feedforward and back propagation) in training step. If dropout rate = 1, then all neurons in network will be trained. Code of dropout12345678910111213141516171819202122def forward(X): p = 0.5 # p is dropout rate out1 = np.maximum(0, np.dot(w1,X)+b1) mask1 = np.random.rand(*out1.shape) &lt; p # create dropout mask out1 *= mask # since we select parts of neurons for output, # the scale of out1 has changed, we need to use /p to # re-scale the output out1 /= p out2 = np.maximum(0, np.dot(w2,out1)+b2) mask2 = (np.random.rand(*out2.shape) &lt; p)/p out2 *= mask2 out3 = np.maximum(0, np.dot(w3,out2)+b3) return out3def predict(x): # In prediction, we don't need dropout as dropout make the output # difference and unstable out1 = np.maximum(0, np.dot(w1,X)+b1) out2 = np.maximum(0, np.dot(w2,out1)+b2) out3 = np.maximum(0, np.dot(w3,out2)+b3) return out3 Properties of DropOut dropout is one way to regularize neural network and avoid overfitting dropout is extremely effective and simple It can be applied to any hidden layer output Reference[1] https://machinelearningmastery.com dropout-for-regularizing-deep-neural-networks/ [2] https://blog.csdn.net/qq_28888837/article/details/84673884","link":"/2020/11/13/DL-DropOut/"},{"title":"Data Structure 1 - Binary Search","text":"Summary of Binary Search (also called half-interval search)Given an array A = [a0, a1,….an], where elements are in increasing order, that is, a1&lt; a2 &lt;…&lt; an. We are going to find the index of an element ak inside the array.Let consider the array below, where a0=1, a1=3… index 0 1 … k … n a[i] a[0] = 1 a[1] =3 … a[k] = ak =10 … a[n]=an= 20 let set left variable L store index of a1 and right variable R stores index of an. Then Let L=0, R=n Check if a[L]=ak or a[R]=ak. If yes, return index of ak. Otherwise, step3 Find middle index M =$\\frac{(L+R)}{2}$ and check if a[M]=ak. If no, continue if a[M]&gt;ak, let R=M and search subarray between index L~ MReason:we know array is in increasing order and elements between M and R must be larger than a[M] and ak. Hence we only need to search elements between L and M if a[M]&lt;ak, let L=M search subarray between index M~ RThe logic is similar to step 4 Back to Step 3 until ak is found, Or R = L+1, that is, ak is not found. Analysis Computational Complexity: O($log_2(n)$) since it makes $log_2 n$ iterations. Space complexity of binary search is O(1) My Thoughts Its main idea is to reduce the searching space by half each time by using mid-intervel Assumption It requires we know L and R It assumes the array has been sorted. If the array has not been sorted, we can use Binary-Search-Tree to sort and store data and then use in-order traversal to obtain index of each element When to Apply It is good to search specific values that can be found by comparison of values However, in my opinion, It is not good enough to explore the combination of elements. For example, Finding the max sum of N numbers in an array. Then the searching space would be too large to find when using binary search. Extension Extensive Problem 1:Given a very very large (may be infinite) array and we don’t know how large it is. Use binary search to find the index of a given element ak. Example: Given an array like this, a1 a2 … ak …(we don’t know the end of the array) 1 3 … a[k] … Analysis: since we don’t know the size of array, we are hardly to find the right boundary R we only know the left boundary L=0 Idea: Jump Out to find R: We can first find the smallest integer X, such that a[$z^X$]$\\geq$ ak, where z is positive integer, maybe 2, 10… Set R= $z^X$ Jump in to find ak: Apply binary search on the region between $z^{X-1}$ and $z^X$. Evaluate Time Complexity since jump out step is $z^X$, then the time required to find the region is O($log_Z$(n)). Time required to search ak insdie $z^{X-1}$ and $z^X$ is O($log_2$(n)) Total complexity is O($log_Z$(n) + $log_2$(n)) Compare the value of Z to find the best performance. __Draw the picture of $log_z(X)$ !__ Reference[1] “https://www.cdn.geeksforgeeks.org/wp-content/uploads/binary-tree-to-DLL.png&quot;","link":"/2020/07/20/Data-Structure-binary-search/"},{"title":"Distributed Computing Basic Note - 1","text":"IntroductionThis blog introduces basic types of parallel computing architecture, memory, and basic evaluation metrics of parallel computing. Parallel Computing Features of Parallel computing Features of Problem: problem task can be broken into discrete pieces of work Problem can be solved faster with multiple computing resource than single resource Features of Computing resource Single computer with multiple processors Multiple computers connected in network Combine 1,2 Special computing component (GPU) Execution Execute multiple instructions concurrently in time Relationship between Parallel, Distributed, Cluster, Concurrency Computing:$ Cluster Computing \\subset Distributed Computing \\subset Parallel computing \\subset Concurrencty Computing$ Evaluation of Computing performanceAmdahl’s LawAssume:$f_s$: fraction that the program is not parallelizable (the ratio of series part to whole program)Or called portion of series in the programp: Number of processor$t_p$: runtime of parallelizable operation$t_s$: runtime of series operationS(p): Parallel SpeedupThenAmdahl’s Law: $t_p = f_s S(p) + \\frac{(1-p)f_s}{p} $ Parallel SpeedupParallel Speedup: $S(p) = \\frac{t_s}{t_p} = \\frac{p}{(p-1)*f+1}$. Usually Parallel Speedup $S(p) \\leq p$ Limiting Factors affecting Parallel Speedup: Not parallelizable code (or Series operation runtime): $t_s$. Longer runtime series code takes, larger $f_s$ is and Smaller the Parallel Efficiency is Communication Overhead: more time is spent on communication, then it also increases $t_s$ Parallel EfficiencyParallel Efficiency: $E = \\frac{S(p)}{p} = \\frac{1}{(p-1)*f + 1}$Parallel Efficiency measures how efficient the parallelization is. Higher Efficiency is, more busy each processor is and Faster the operations are executed. So we want to increase parallel Efficiency and maximize the ratio between S(p) and p. Type of Parallel Speedup Linear Speedup (normal case): $S(p) &lt; p$ Sublinear Speedup: $S(p) = p$ Superlinear Speedup: $S(p) &gt; p$ limiting Factors lead to superlilnear Speedup: Poor sequential reference implementation, leading to $t_s$ very large and series runtime very large 2. Memory caching: when memory cache is small, it will increase $t_s$ and lead to lower processing speed such that $t_s &gt;t_p$ and S(p) &gt;p3. I/O blocking : block I/O leads to delay of runtime, $t_s$ will increase Types of ComputersFlynne’s TaxonomyThis taxonomy classifies the computer architecture into four types: SISD: Single Instruction Single Datastream SIMD: Single Instruction Multiple Datastreams MISD: Multiple Instructions Single Datastream MIMD: Multiple Instructions Multiple Datastreams Note: SIMD, MISD, MIMD architecture belongs to parallel computer since they satisfy the requirements of feature in parallel computing ( break task into piece, execute multiple(same or different )instrutions concurrently, etc) Type of Memory Shared MemoryMultiple processors share the same memory Distributed Memory and Message PassingMultiple processors have its own memory but use message passing method to communicate with each other distributedly across network. Hyper-ModelCombining Shared memory and distributed Memory together. Heterogeneous computing (accelerators) GPU FPGA Benchmarking and Ranking supercomputers LINPACK (Linear Algebra Package): Dense Matrix Solver HPCC: High-Performance Computing Challenge SHOC: Scalable Heterogeneous Computing - Non-traditional systems (GPU) TestDFSIO - I/O Performance of MapReduce/Hadoop Distributed File System","link":"/2020/09/21/Distributed-Computing-note-1/"},{"title":"Recommendation-System-GATE","text":"Background这篇文章介绍了来自麦吉尔大学(McGrill University) Chen Ma 等人在 2019 WSDM发表的文章: Gated-Attentive Autoencoder for Content-Aware Recommendation (GATE). 文章链接看这里: Link.我的Google Slide PPT介绍可以看这里 Slide Motivation在推荐系统里面随着用户和商品的急速增长， 个性化推荐系统这篇文章考虑了以下两个问题: 一些稀疏的隐性反馈信息利用困难。 比如一个用户喜欢一个商品， 而这个商品在内容上面(比如文字描述信息等)和这个被买的商品很相似，那么这个商品就也很有可能会被买。另外，像用1来标记用户喜欢，0来标记用户不喜欢或者没评分这种稀疏的评分方式也可能有一些隐性反馈可以挖掘。 不同类型的数据的结合使用困难。比如像是文本描述数据和0或者1的评分标签两种不同类型的数据要同时结合起来让模型学习，需要把各自的隐性特征(hidden representation)通过一定方式结合起来使用。 而这篇文章为了解决这两个问题，主要从模型的结构入手，并提出创新的结构。这篇文章的主要贡献有以下: 提出一种word-attention module 去捕捉item的文本特征的各个单词的重要性。把重要的单词特征进行提取和融合 提出一种 neural gating layer去自动把rating的hidden representation和 word attention module学到的embedding进行自动融合得到当前item的hidden representation。 它也假设了用户如果对一个item的neighbor 的item内容感兴趣，也会对这个item内容感兴趣。所以也提出一种基于item-item 关系的neighbor-attention layer把相邻的item的信息也考虑进来。 这个模型的word-attention的layer是可以可视化并且解释的。 Gated-Attentive Autoencoder for Content-Aware RecommendationFormulation这里我们先来定义一些参数和输入: R: m-by-n binary user preferences matrix. Rij = 1 -&gt; user prefers. ri : binary rating of the ith item in matrix R D: The entire collection of n items is represented by a list of documents D. Di is the ith item content, represented by a sequence of words. N: n-by-n item binary adjacency matrix. Nij = 1 means item i and j are connected (neighbor) Loss首先这个模型是要把稀疏的rating matrix R进行预测把为0的entry的进行rating的预测和填充，这个就像是矩阵分解里面把稀疏矩阵变成dense matrix一样。而使用的loss 就是基于Auto-encoder 的loss改进得到的: 加了Regularization term后， loss变成: Model ArchitectureGATE的架构如下， 他有4种不同模块组成(分别对应不同的颜色)。黄色代表encoder-decoder模型， 绿色代表word-attention 模块， 蓝色代表Neural Gating layer, 红色代表neighbor-attention layer Encoder and Decoder首先encoder和decoder的模块里面， encoder 模块先通过两层的MLP对binary rating进行encoding得到隐性信息。这个有点像矩阵分解(Matrix Factorization)里面从稀疏的binary rating得到item的embedding一样。 其中ri代表第i个item的rating，每个值代表一个user的评价。1代表user 喜欢这个item。而0代表没有评分。所以这个ri是一个multi-onehot vector Word Attention在word attention 模块， 它的目标是要把item的文本描述里面的多个重要的单词进行提取和学习，因此为了提取分散在不同位置的重要的单词，这里用了attention机制进行学习。它的步骤如下:先计算Attention matrix：$$A_ {i} = \\text{softmax}(\\mathbf{W}_ {a1} tanh(\\mathbf{W}_ {a2} \\mathbf{D}_ i + \\mathbf{b}_ {a2}) + b_ {a1} )$$ 然后计算hidden representation matrix $\\mathbf{Z}_ i ^c$, 这个矩阵的每一列代表一个做了attention之后的单词的embedding: $$\\mathbf{Z}_ i ^c = \\mathbf{A}_ i D_ i ^T$$ 最后我们将每个word embedding通过乘上一个learnable的weight 系数并将所有word embedding相加进行学习得到最后content的embedding， 即$$\\mathbf{z}_ i^c = a_ t (\\mathbf{Z}_ i^{c^T} w_t)$$ 这里的$w_t$ 就是一个weight column vector对每个word embedding权重相加。 Neural Gating Layer在GATE里面的Neural Gating layer 参考了LSTM里面的相加的layer通过一个自适应的系数G对模型的rating embedding和 content embedding权重进行相加和信息融合。公式如下:$$\\mathbf{G} = \\text{sigmoid}(\\mathbf{W}_ {g1}z_ {i}^{r} + \\mathbf{W}_ {g2}z_ {i}^{c} + b_ g) \\\\z_ i^g = G \\cdot z_ i^r + (1-G) \\cdot z_ i ^c$$ 其中上标为c的代表item content的embedding对应的参数 而上标为r的代表item binary rating的embedding对应的参数. 这里虽然说是把不同的embedding的信息进行融合，但是因为信息的来源不一样，有可能每个值所代表的隐性信息代表的含义也不一样，这样直接相加的话还是感觉有些牵强。比起这个方式的信息融合， 我更加倾向于把embedding拼接然后做linear projection的方法. 而G是一个scalar value系数对两边的embedding的信息进行权衡。 Neighbor Attention layerNeighbor Attention layer 是设计是基于一个假设: 用户对一个item的喜欢可以从这个用户对这个item的相关临近的item的喜欢里面看出。如果这个用户喜欢这个item的neighborhood里面的item，那么用户也有可能喜欢这个item。这一层的layer结构如下： 所以在做完这个item的hidden representation计算得到$z_ i^g$ 后我们可以根据之前的adjacency matrix N 找到这个item的neighborhood里面的item。而这个adjacency matrix的搭建是根据rating matrix R计算item和item之间的相似度，再通过threshold取值得到， 原文是这么说的:For items that do not inherently have item-item relations, we can compute the item-itemsimilarity from the binary rating matrix R and set a threshold to select neighbors. 而这一层的计算公式如下: Visualization of important word这篇文章最后的创新点就是它可以通过学习到的 word attention的attention matrix $A_ i$ 对重要单词进行可视化。首先$A_ i$ matrix每一列代表了一个单词的重要性系数， 然后把 $A_ i \\in R^{da \\times li}$ 通过把每一个row 进行相加得到$a_ i \\in R^{li}$的 vector，而这个vector每个值代表那个单词的重要性。 ExperimentsMetrics这里paper用的metrics有两个: Recall@k (R@k)$Recall @ k = \\frac{\\text{num of relevant recommended top K}}{ \\text{ num of user prefers}}$.For each user, Recall@k (R@k) indicates what percentage of her rateditems can emerge in the top k recommended items.top-K的Recall随着k越大 对用户喜欢的item的命中率越高，recall越大 NDCG@k (N@k) is the normalized discounted cumulative gain at k. It measures of ranking quality. (how relevant the recommended top k elements are)NDCG 测量推荐的商品和用户的相关度。top-K的NDCG随着k越大 越有可能推和用户无关的东西，那么NDCG就越小。计算公式如下: 下面 是其中一部分用于不同数据集的对比实验。 Comparison下面是对不同模型在同一个k设定下的实验: Hyper-parameter Tuning下面是对loss里面的confidence 系数$p$和 word attention embedding的大小 da的效果实验. 原文里面$p$ 选取了20， da选取50.不同不同数据集有不同的设定，这个可以看看原文。 Source CodeGithub source code: https://github.com/allenjack/GATE/blob/master/run.py 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131import torchfrom torch.autograd import Variableimport torch.nn.functional as Fif torch.cuda.is_available(): import torch.cuda as Telse: import torch as Tclass GATE(torch.nn.Module): def __init__(self, num_users, num_items, num_words, H, drop_rate=0.5, att_dim=20): \"\"\" In the constructor we instantiate two nn.Linear modules and assign them as member variables. \"\"\" super(GATE, self).__init__() self.H = H[1] self.drop_rate = drop_rate if torch.cuda.is_available(): self.item_gated_embedding = torch.nn.Embedding( num_embeddings=num_items + 1, embedding_dim=self.H, padding_idx=num_items # the last item index is the padded index ).cuda() self.item_gated_embedding.weight.requires_grad = False self.word_embedding = torch.nn.Embedding( num_embeddings=num_words + 1, embedding_dim=self.H, padding_idx=num_words # the last word index is the padded index ).cuda() self.aggregation = torch.nn.Linear(att_dim, 1).cuda() self.att1 = torch.nn.Linear(self.H, self.H).cuda() self.att2 = torch.nn.Linear(self.H, att_dim).cuda() self.linear1 = torch.nn.Linear(num_users, H[0]).cuda() self.linear2 = torch.nn.Linear(H[0], H[1]).cuda() self.linear3 = torch.nn.Linear(H[1], H[2]).cuda() self.linear4 = torch.nn.Linear(H[2], num_users).cuda() else: self.item_gated_embedding = torch.nn.Embedding( num_embeddings=num_items + 1, embedding_dim=self.H, padding_idx=num_items # the last item index is the padded index ) self.item_gated_embedding.weight.requires_grad = False self.word_embedding = torch.nn.Embedding( num_embeddings=num_words + 1, embedding_dim=self.H, padding_idx=num_words # the last word index is the padded index ) self.aggregation = torch.nn.Linear(att_dim, 1) self.att1 = torch.nn.Linear(self.H, self.H) self.att2 = torch.nn.Linear(self.H, att_dim) self.linear1 = torch.nn.Linear(num_users, H[0]) self.linear2 = torch.nn.Linear(H[0], H[1]) self.linear3 = torch.nn.Linear(H[1], H[2]) self.linear4 = torch.nn.Linear(H[2], num_users) self.neighbor_attention = Variable(torch.zeros(self.H, self.H).type(T.FloatTensor), requires_grad=True) self.neighbor_attention = torch.nn.init.xavier_uniform_(self.neighbor_attention) # G = sigmoid(gate_matrix1 \\dot item_embedding + gate_matrix2 \\dot item_context_embedding + bias) self.gate_matrix1 = Variable(torch.zeros(self.H, self.H).type(T.FloatTensor), requires_grad=True) self.gate_matrix2 = Variable(torch.zeros(self.H, self.H).type(T.FloatTensor), requires_grad=True) self.gate_matrix1 = torch.nn.init.xavier_uniform_(self.gate_matrix1) self.gate_matrix2 = torch.nn.init.xavier_uniform_(self.gate_matrix2) self.gate_bias = Variable(torch.zeros(1, self.H).type(T.FloatTensor), requires_grad=True) self.gate_bias = torch.nn.init.xavier_uniform_(self.gate_bias) def forward(self, batch_item_index, batch_x, batch_word_seq, batch_neighbor_index): z_1 = F.tanh(self.linear1(batch_x)) # z_1 = F.dropout(z_1, self.drop_rate) z_rating = F.tanh(self.linear2(z_1)) z_content = self.get_content_z(batch_word_seq) gate = F.sigmoid(z_rating.mm(self.gate_matrix1) + z_content.mm(self.gate_matrix2) + self.gate_bias) gated_embedding = gate * z_rating + (1 - gate) * z_content # save the embedding for direct lookup self.item_gated_embedding.weight[batch_item_index] = gated_embedding.data gated_neighbor_embedding = self.item_gated_embedding(batch_neighbor_index) # aug_gated_embedding: [256, 1, 50] aug_gated_embedding = torch.unsqueeze(gated_embedding, 1) score = torch.matmul(aug_gated_embedding, torch.unsqueeze(self.neighbor_attention, 0)) # score: [256, 1, 480] score = torch.bmm(score, gated_neighbor_embedding.permute(0, 2, 1)) # make the 0 in score, which will make a difference in softmax score = torch.where(score == 0, T.FloatTensor([float('-inf')]), score) score = F.softmax(score, dim=2) # if the vectors all are '-inf', softmax will generate 'nan', so replace with 0 score = torch.where(score != score, T.FloatTensor([0]), score) gated_neighbor_embedding = torch.bmm(score, gated_neighbor_embedding) gated_neighbor_embedding = torch.squeeze(gated_neighbor_embedding, 1) # gated_embedding = F.dropout(gated_embedding, self.drop_rate) # gated_neighbor_embedding = F.dropout(gated_neighbor_embedding, self.drop_rate) z_3 = F.tanh(self.linear3(gated_embedding)) # z_3 = F.dropout(z_3, self.drop_rate) z_3_neighbor = F.tanh(self.linear3(gated_neighbor_embedding)) # z_3_neighbor = F.dropout(z_3_neighbor, self.drop_rate) y_pred = F.sigmoid(self.linear4(z_3) + z_3_neighbor.mm(self.linear4.weight.t())) return y_pred def get_content_z(self, batch_word_seq): # [batch_size, num_word, hidden_dim], e.g., [256, 300, 100] batch_word_embedding = self.word_embedding(batch_word_seq) score = F.tanh(self.att1(batch_word_embedding)) score = F.tanh(self.att2(score)) # score dimension: [256, 300, 20] score = F.softmax(score, dim=1) # permute to make the matrix as [256, 50, 176] matrix_z = torch.bmm(batch_word_embedding.permute(0, 2, 1), score) linear_z = self.aggregation(matrix_z) linear_z = torch.squeeze(linear_z, 2) z = F.tanh(linear_z) return z Reference[1] Paper： https://arxiv.org/abs/1812.02869[2] Github: https://github.com/allenjack/GATE[3] 我知乎的博客: https://zhuanlan.zhihu.com/p/390923011","link":"/2021/07/18/GATE-paper/"},{"title":"GNN-3-NodeEmbedding","text":"Node Embedding and Node Prediction1. Introduction首先回顾一下上一篇文章关于MessagePassing的GNN的内容。在GNN的MessagePassing里面，每个node代表一个subject，然后edge代表subject之间的关系。而MessagePassing一句话总结就是让每个node的特征(node representation 或node embedding)通过对应的neighbor nodes的信息进行聚集并用于更新每个node的信息，从而学习到更好的node embedding表达。之后我们可以对学习到的node embedding里面每个node的特征向量输入到classifier里面进行node classification识别subject的类别。除了node classification外，node embedding还能用来做edge classification， graph classification 等任务。 而这次项目的目的是要实现GNN图神经网络并进行实践应用到Cora (scientific publications)科学文献dataset里面，并对每篇文章进行分类和预测文章的类别。这次的项目的大概流程是: Introduction to Cora Dataset: 简单介绍一下Cora论文数据集的构成 Data Visualization: 对Cora数据集的node representation 和class 进行可视化 Modeling and training: 搭建和训练MLP, GCN, GAT等模型对node 进行classification以及测试 Visualize learned node representation: 将学到的node representation进行可视化分析每个类分布的不同 Assignment: 尝试其他不同的Dataset看一下不同GNN的效果 Conclusion: 总结一下学到什么 2. Data Description这里先来介绍Coras Dataset的内容，Coras Dataset 的解释可以从官网找到: https://linqs.soe.ucsc.edu/dataCoras dataset content: Number of nodes: 2708 nodes. 每个node代表一篇论文 Number of edges/links: 5429条无向的边，如果用有向的边表示就是10556条 Number of class: 8. 总共有7个类别的论文 Dimension of node representation: 1433. 字典的大小为1433每个词用0,1 表示文章有没有那个词. 每篇文章的node representation就有1433 123456789101112131415161718192021222324252627from torch_geometric.datasets import Planetoidfrom torch_geometric.transforms import NormalizeFeaturesdataset = Planetoid(root='./dataset', name='Cora', transform=NormalizeFeatures())print()print(f'Dataset: {dataset}:')print('======================')print(f'Number of graphs: {len(dataset)}')print(f'Number of features: {dataset.num_features}')print(f'Number of classes: {dataset.num_classes}')data = dataset[0] # Get the first graph object.print()print(data)print('======================')# Gather some statistics about the graph.print(f'Number of nodes: {data.num_nodes}')print(f'Number of edges: {data.num_edges}')print(f'Average node degree: {data.num_edges / data.num_nodes:.2f}')print(f'Number of training nodes: {data.train_mask.sum()}')print(f'Training node label rate: {int(data.train_mask.sum()) / data.num_nodes:.2f}')print(f'Contains isolated nodes: {data.contains_isolated_nodes()}')print(f'Contains self-loops: {data.contains_self_loops()}')print(f'Is undirected: {data.is_undirected()}') Dataset: Cora(): ====================== Number of graphs: 1 Number of features: 1433 Number of classes: 7 Data(edge_index=[2, 10556], test_mask=[2708], train_mask=[2708], val_mask=[2708], x=[2708, 1433], y=[2708]) ====================== Number of nodes: 2708 Number of edges: 10556 Average node degree: 3.90 Number of training nodes: 140 Training node label rate: 0.05 Contains isolated nodes: False Contains self-loops: False Is undirected: True 1data.train_mask,data.y.unique() (tensor([ True, True, True, ..., False, False, False]), tensor([0, 1, 2, 3, 4, 5, 6])) 2. Data Visualization这里简单用TSNE的降维算法把1433 维的node representation降到2维从而来显示每个class的数据的分布, 每种颜色代表一个class。从下面每种颜色的点的分布来看，在学习之前的不同类别的node representation是很难区分开来的，所以很多节点的特征都混在一起 123456789101112import matplotlib.pyplot as pltfrom sklearn.manifold import TSNEdef visualize(h, color,title=\"\"): # convert node data x to TSNE embedding data z = TSNE(n_components=2).fit_transform(h.detach().cpu().numpy()) plt.figure(figsize=(10,10)) plt.xticks([]) plt.yticks([]) plt.title(title) plt.scatter(z[:, 0], z[:, 1], s=70, c=color, cmap=\"Set2\") plt.show() 1data.x.shape, data.y.shape (torch.Size([2708, 1433]), torch.Size([2708])) 1visualize(data.x,data.y,\"Node Data Clusters\") 3. 用不同GNN对node embedding进行学习这里先简单来设计training, testing 的通用函数, 之后尝试用以下不同的模型进行学习和对比: MLP GNN GAT GraphSAGE 1234567891011121314151617181920212223def train(model, criterion, optimizer,data, use_mask=True): model.train() optimizer.zero_grad() # Clear gradients. out = model(data.x, data.edge_index) # Perform a single forward pass. if use_mask: loss = criterion(out[data.train_mask], data.y[data.train_mask]) # Compute the loss solely based on the training nodes. else: loss = criterion(out, data.y) # Compute the loss solely based on the training nodes. loss.backward() # Derive gradients. optimizer.step() # Update parameters based on gradients. return lossdef test(model, data, use_mask=True): model.eval() out = model(data.x, data.edge_index) pred = out.argmax(dim=1) # Use the class with highest probability. if use_mask: test_correct = pred[data.test_mask] == data.y[data.test_mask] # Check against ground-truth labels. test_acc = int(test_correct.sum()) / int(data.test_mask.sum()) # Derive ratio of correct predictions. else: test_correct = pred == data.y # Check against ground-truth labels. test_acc = int(test_correct.sum()) / len(data.y) # Derive ratio of correct predictions. return test_acc 3.1 MLP12345678910111213141516171819202122232425262728293031import torchfrom torch.nn import Linearimport torch.nn.functional as Fclass MLP(torch.nn.Module): def __init__(self, in_channel, classes, hidden_channels,random_seed=12345): super(MLP, self).__init__() torch.manual_seed(random_seed) self.lin1 = Linear(in_channel, hidden_channels) self.lin2 = Linear(hidden_channels, classes) def forward(self, x, index): x = self.lin1(x) x = x.relu() x = F.dropout(x, p=0.5, training=self.training) x = self.lin2(x) return xmodel_mlp = MLP(in_channel = dataset.num_features, classes = dataset.num_classes, hidden_channels=16)optimizer = torch.optim.Adam(model_mlp.parameters(), lr=0.01, weight_decay=5e-4)criterion = torch.nn.CrossEntropyLoss()print(\"Model Strucutre:\")print(model_mlp)for epoch in range(1, 201): loss = train(model_mlp, criterion, optimizer,data) if epoch%20==0: print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}')print()test_acc = test(model_mlp,data)print(f'Test Accuracy: {test_acc:.4f}') Model Strucutre: MLP( (lin1): Linear(in_features=1433, out_features=16, bias=True) (lin2): Linear(in_features=16, out_features=7, bias=True) ) Epoch: 020, Loss: 1.7441 Epoch: 040, Loss: 1.2543 Epoch: 060, Loss: 0.8578 Epoch: 080, Loss: 0.6368 Epoch: 100, Loss: 0.5350 Epoch: 120, Loss: 0.4745 Epoch: 140, Loss: 0.4031 Epoch: 160, Loss: 0.3782 Epoch: 180, Loss: 0.4203 Epoch: 200, Loss: 0.3810 Test Accuracy: 0.5900 12 3.2 GCNGCN Layer 公式如下: $$\\mathbf{x}_ i^{(k)} = \\sum_{j \\in \\mathcal{N}(i) \\cup { i }} \\frac{1}{\\sqrt{\\deg(i)} \\cdot \\sqrt{\\deg(j)}} \\cdot ( \\mathbf{\\Theta} \\cdot \\mathbf{x}_ j^{(k-1)} ),$$ 这里一些函数定义如下： $\\phi(..)$: message函数GCN一样都是linear projection之后用degree进行normalization $\\square(..)$ : aggregate 函数用 add $\\gamma(..)$: update 函数是直接将aggregate后的结果输出 这里把MLP里面的linear layer换成是GCN layer 123456789101112131415161718192021222324252627282930313233import torchfrom torch_geometric.nn import GCNConvfrom torch.nn import functional as Fclass GCN(torch.nn.Module): def __init__(self, in_channel,classes, hidden_channels): super(GCN, self).__init__() torch.manual_seed(12345) self.conv1 = GCNConv(in_channel, hidden_channels) self.conv2 = GCNConv(hidden_channels, classes) def forward(self, x, edge_index): x = self.conv1(x, edge_index) x = x.relu() x = F.dropout(x, p=0.5, training=self.training) x = self.conv2(x, edge_index) return xmodel_gcn = GCN(in_channel= dataset.num_features, classes= dataset.num_classes, hidden_channels=16)print(\"Model Architecture: \")print(model_gcn)optimizer = torch.optim.Adam(model_gcn.parameters(), lr=0.01, weight_decay=5e-4)criterion = torch.nn.CrossEntropyLoss()for epoch in range(1, 201): loss = train(model_gcn, criterion, optimizer,data) if epoch%20==0: print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}')print()test_acc = test(model_gcn,data)print(f'Test Accuracy: {test_acc:.4f}') Model Architecture: GCN( (conv1): GCNConv(1433, 16) (conv2): GCNConv(16, 7) ) Epoch: 020, Loss: 1.7184 Epoch: 040, Loss: 1.3363 Epoch: 060, Loss: 1.0066 Epoch: 080, Loss: 0.7248 Epoch: 100, Loss: 0.5833 Epoch: 120, Loss: 0.5064 Epoch: 140, Loss: 0.4131 Epoch: 160, Loss: 0.3799 Epoch: 180, Loss: 0.3186 Epoch: 200, Loss: 0.3006 Test Accuracy: 0.8140 12 3.3 GAT (Graph Attention Network) paper link: https://arxiv.org/pdf/1710.10903.pdf Graph Attention Network 的attention公式如下: $$\\alpha_ {i,j} = \\frac{ \\exp(\\mathrm{LeakyReLU}(\\mathbf{a}^{\\top}[\\mathbf{W}\\mathbf{h}_ i , \\Vert , \\mathbf{W}\\mathbf{h}_ j]))}{\\sum_ {k \\in \\mathcal{N}(i) \\cup { i }}\\exp(\\mathrm{LeakyReLU}(\\mathbf{a}^{\\top}[\\mathbf{W} \\mathbf{h}_ i , \\Vert , \\mathbf{W}\\mathbf{h}_ k]))}.$$ 节点信息更新$$\\mathbf{h}_ i^{‘} = \\sigma(\\frac{1}{K} \\sum_ {k=1}^K\\sum_ {j \\in N(i)} a_{ij}^{k}\\mathbf{W}^k\\mathbf{h}_ {i})$$ 实际上GAT就是在每个节点把邻居的信息聚合时根据邻居节点的node representation和这个节点的node representation的相似度对聚合的信息有侧重地聚合其中每个参数的代表: $\\mathbf{h}_i$: 节点 i的node representation。这个node representation可以是GNN的某一层的输出 $\\mathbf{W}$: shared linear transformation. 用于每个节点的共享的线性投映矩阵，所有节点都用相同的W进行投映 $k \\in \\mathcal{N}(i) \\cup { i }$: 第i个节点的邻居节点(包括第i个节点本身)。注意因为这里涉及两个sum，两个loop所以计算有点慢 $\\Vert$: 把两个向量拼接 12345678910111213141516171819202122232425262728293031323334353637383940import torchfrom torch.nn import Linearimport torch.nn.functional as Ffrom torch_geometric.nn import GATConvclass GAT(torch.nn.Module): def __init__(self, in_channel, classes, hidden_channels, dropout_r = 0.2): super(GAT, self).__init__() torch.manual_seed(12345) self.dropout_r = dropout_r self.conv1 = GATConv(in_channel, hidden_channels) self.conv2 = GATConv(hidden_channels, hidden_channels) self.linear = torch.nn.Linear(hidden_channels,classes) def forward(self, x, edge_index): x = self.conv1(x, edge_index) x = x.relu() x = F.dropout(x, p=self.dropout_r, training=self.training) x = self.conv2(x, edge_index) x = self.linear(x) return x model_gat = GAT(in_channel = dataset.num_features, classes = dataset.num_classes, hidden_channels = 16)optimizer = torch.optim.Adam(model_gat.parameters(), lr=0.01, weight_decay=5e-4)criterion = torch.nn.CrossEntropyLoss()print(\"Model Strucutre:\")print(model_gat)for epoch in range(1, 201): loss = train(model_gat, criterion, optimizer,data) if epoch%20==0: print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}')print()test_acc = test(model_gat,data)print(f'Test Accuracy: {test_acc:.4f}') Model Strucutre: GAT( (conv1): GATConv(1433, 16, heads=1) (conv2): GATConv(16, 16, heads=1) (linear): Linear(in_features=16, out_features=7, bias=True) ) Epoch: 020, Loss: 1.5780 Epoch: 040, Loss: 0.5588 Epoch: 060, Loss: 0.1466 Epoch: 080, Loss: 0.0755 Epoch: 100, Loss: 0.0585 Epoch: 120, Loss: 0.0351 Epoch: 140, Loss: 0.0406 Epoch: 160, Loss: 0.0292 Epoch: 180, Loss: 0.0285 Epoch: 200, Loss: 0.0287 Test Accuracy: 0.7230 对GAT做一点调参，提一下性能 hidden_channels 用24时比小于16和大于32的时候好 dropout=0.8时效果也更好，可能GAT里面的attention的机制容易对一部分特征overfitting epoch设置300更加长些也效果好点 这里调了下参数有了 6% 的提升 12345678910111213model_gat = GAT(in_channel = dataset.num_features, classes = dataset.num_classes, hidden_channels = 24, dropout_r= 0.8)optimizer = torch.optim.Adam(model_gat.parameters(), lr=0.01, weight_decay=5e-4)criterion = torch.nn.CrossEntropyLoss()print(\"Model Strucutre:\")print(model_gat)for epoch in range(1, 301): loss = train(model_gat, criterion, optimizer,data) if epoch%20==0: print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}')print()test_acc = test(model_gat,data)print(f'Test Accuracy: {test_acc:.4f}') Model Strucutre: GAT( (conv1): GATConv(1433, 24, heads=1) (conv2): GATConv(24, 24, heads=1) (linear): Linear(in_features=24, out_features=7, bias=True) ) Epoch: 020, Loss: 1.6420 Epoch: 040, Loss: 0.7042 Epoch: 060, Loss: 0.4498 Epoch: 080, Loss: 0.2709 Epoch: 100, Loss: 0.2429 Epoch: 120, Loss: 0.1849 Epoch: 140, Loss: 0.2643 Epoch: 160, Loss: 0.1832 Epoch: 180, Loss: 0.2135 Epoch: 200, Loss: 0.1697 Epoch: 220, Loss: 0.1485 Epoch: 240, Loss: 0.1359 Epoch: 260, Loss: 0.1606 Epoch: 280, Loss: 0.1778 Epoch: 300, Loss: 0.1555 Test Accuracy: 0.7810 12 3.4. GraphSAGE (Sample and Aggregate Graph Embedding SAGE) Paper Link: https://arxiv.org/pdf/1706.02216.pdf 其他GNN的node embedding的学习方法都是假设了图里面所有node都是在训练时已经见到的并且有自己的特征数据作为训练集。 而在训练之后，当这些已经见过的node的特征值改变时，可以用GNN对它进行预测。但是实际问题里面，有可能有些node在训练时完全没有见过的(但是出现时会和其他已经见过的node存在link)，因此不能在训练时用这些node的数据进行训练(这个有点像推荐系统的Embedding里面没有见过的userid或itemid的冷启动情况)。GraphSAGE就是用来解决这个问题 GraphSAGE是一种 inductive的representation learning的方法，就是归纳法。它是用于预测之前没有见过的node的embed的ing的特征。它的主要思想是通过学习多个aggregate函数(paper里面提出来mean, LSTM, pooling 三个)，然后这些aggregate函数用neighbor的信息来生成之前没有见过的node的embedding之后再做预测。下面是GraphSAGE的流程图： GraphSAGE 的node embedding的其中一个生成公式为(还有其他用于生成embedding的aggregate函数公式可以参考原文):$$\\mathbf{x}_ {i}^{‘} = \\mathbf{W}_ {1}x_{i} + \\textbf{mean}_ {j \\in N(i)}(\\mathbf{x}_{j})$$ GraphSAGE 的graph-based unsupervised loss function 定义为 $$\\mathbf{J}_ {G}(z_{u}) = -log(\\sigma(\\mathbf{z}_ {u}^{T}\\mathbf{z}_ {v})) - \\mathbf{Q} \\cdot \\mathbf{E}_ {v_ {n} \\in P_ {n}(v)}log(\\sigma(-\\mathbf{z}_ {u}^{T} \\mathbf{z}_ {v_{n}}))$$ 其中: $j \\in N(i)$ 为第i个节点的第j个neighbor节点 $v$ 是和 $u$ 在定长的random walk采样路径出现的节点 $Q$ 是负样本的个数， $P_{n}(v)$ 是负采样的分布 $z_{u}$是node representation特征 这里$\\sigma()$里面计算的是节点和random walk采样时同时出现的其他节点的相似度。相似度越大，loss越小 GraphSAGE 的计算embedding算法流程如下: 这里GraphSAGE的基本思路就是 先设定好K (iteration的次数又或者叫search depth搜索的深度)以及初始化没有见过的节点$v$的初始的node embedding 每次遍历时都找到节点 $v$ 的neighbor的nodes并把他们的信息aggregate得到neighbor信息的聚合的embedding 把上一层的$v$ 的embedding和新得到的聚合的neighbor node embedding进行拼接和linear transform得到下一层 node $v$的输出，最后生成之前没见过的node $v$ 的embedding 12345678910111213141516171819202122232425262728293031323334353637383940import torchfrom torch.nn import Linearimport torch.nn.functional as Ffrom torch_geometric.nn import SAGEConvclass SAGE(torch.nn.Module): def __init__(self, in_channel, classes, hidden_channels, dropout_r = 0.2): super(SAGE, self).__init__() torch.manual_seed(12345) self.dropout_r = dropout_r self.conv1 = SAGEConv(in_channel, hidden_channels,normalize=True) self.conv2 = SAGEConv(hidden_channels, hidden_channels,normalize=True) self.linear = torch.nn.Linear(hidden_channels,classes) def forward(self, x, edge_index): x = self.conv1(x, edge_index) x = x.relu() x = F.dropout(x, p=self.dropout_r, training=self.training) x = self.conv2(x, edge_index) x = self.linear(x) return x model_sage = SAGE(in_channel = dataset.num_features, classes = dataset.num_classes, hidden_channels = 24,dropout_r= 0.5)optimizer = torch.optim.Adam(model_sage.parameters(), lr=0.03, weight_decay=5e-4)criterion = torch.nn.CrossEntropyLoss()print(\"Model Strucutre:\")print(model_sage)for epoch in range(1, 201): loss = train(model_sage, criterion, optimizer,data) if epoch%20==0: print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}')print()test_acc = test(model_sage,data)print(f'Test Accuracy: {test_acc:.4f}') Model Strucutre: SAGE( (conv1): SAGEConv(1433, 24) (conv2): SAGEConv(24, 24) (linear): Linear(in_features=24, out_features=7, bias=True) ) Epoch: 020, Loss: 0.3678 Epoch: 040, Loss: 0.0956 Epoch: 060, Loss: 0.0435 Epoch: 080, Loss: 0.0424 Epoch: 100, Loss: 0.1066 Epoch: 120, Loss: 0.0316 Epoch: 140, Loss: 0.0474 Epoch: 160, Loss: 0.0640 Epoch: 180, Loss: 0.1417 Epoch: 200, Loss: 0.0442 Test Accuracy: 0.7800 12 3.5 Node Representation Cluster Visualization123456models = {\"MLP\":model_mlp, \"GCN\":model_gcn,\"GAT\":model_gat, \"GraphSAGE\":model_sage}for k in models.keys(): model = models[k] out = model(data.x, data.edge_index) title = f\"Node Representation learned by {k}\" visualize(out, data.y, title) 从降维后的 node embedding的cluster的分布来分析不同模型的性能: 可以看到GCN, GAT, GraphSAGE 的node embedding的7个clusters都比MLP要区分得清楚，即每个类的特征差异较大容易被识别出来所以GNN都比MLP要好 GAT和GraphSAGE的clusters之间的距离都比GCN的clusters之间的距离要远，特别是GAT。GAT的每个cluster都收缩成一束一束的聚合起来。而因为这里显示的cluster是data.x样本，包括了训练集在内，所以cluster区分得越明显很有可能是数据拟合得很好甚至是有overfitting的可能。 12 4. Assignment 此篇文章涉及的代码可见于codes/learn_node_representation.ipynb，请参照这份代码使用PyG中不同的图卷积模块在PyG的不同数据集上实现节点分类或回归任务。 4.1 Dataset选择 Pubmed: https://arxiv.org/abs/1603.08861这个数据集合Cora一样: Nodes represent documents and edges represent citation links Citeseer: https://arxiv.org/abs/1603.08861这个数据集合Cora一样: Nodes represent documents and edges represent citation links CitationFull： https://arxiv.org/abs/1707.03815Nodes represent documents and edges represent citation links. 本来想尝试其他数据，但是其他数据集太大训练起来很慢，后期有时间再试试看 这里写个函数一次性打印所有要用的dataset的信息，看一下不同dataset的node， edge信息，并用table打印出来 1234567891011121314# dataset used by GATfrom torch_geometric.datasets import PPI, Reddit,NELL,QM7b,CitationFull,AMinerpubmed_dataset = Planetoid(root='./dataset', name='Pubmed', transform=NormalizeFeatures())Citeseer_dataset = Planetoid(root='./dataset', name='CITESEER', transform=NormalizeFeatures())CitationFull_dataset =CitationFull(root='./dataset',name=\"DBLP\",transform=NormalizeFeatures())# PPI_train = PPI(root='./dataset', split='train',transform=NormalizeFeatures())# PPI_test = PPI(root='./dataset', split='test',transform=NormalizeFeatures())# PPI_dataset = {'train':PPI_train,\"test\":PPI_test}# AMiner_dataset = AMiner(root='./dataset',transform=NormalizeFeatures())#QM7b_dataset = QM7b(root='./dataset', transform=NormalizeFeatures())# Reddit_dataset = Reddit(root='./dataset', transform=NormalizeFeatures())# NELL_dataset = NELL(root='./dataset', transform=NormalizeFeatures()) Downloading https://data.dgl.ai/dataset/ppi.zip Extracting dataset/ppi.zip 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748import pandas as pd#mydatasets = {\"Pubmed\":pubmed_dataset, \"Citeseer\":Citeseer_dataset, \"Reddit\":Reddit_dataset,\"NELL\":NELL_dataset}mydatasets = {\"CitationFull\":CitationFull_dataset,\"Pubmed\":pubmed_dataset, \"Citeseer\":Citeseer_dataset}df = pd.DataFrame(columns={\"dataset\",\"#graphs\",\"#features\",\"#classes\"})dic = {\"dataset\":[],\"#graphs\":[],\"#features\":[],\"#classes\":[],\"#nodes\":[],\"#edges\":[],\"Has_isolated_nodes\":[],\"undirected\":[]}for k in mydatasets.keys(): tmp = {k:mydatasets[k]} if k in [\"PPI\"]: tmp = mydatasets[k] for key in tmp.keys(): dataset = tmp[key] print(\"dataset: \",key) dic['dataset'].append(key) dic['#graphs'].append(len(dataset)) dic['#features'].append(dataset.num_features) dic['#classes'].append(dataset.num_classes) data = dataset[0] dic['#nodes'].append(data.num_nodes) dic['#edges'].append(data.num_edges) dic['Has_isolated_nodes'].append(data.contains_isolated_nodes()) dic['undirected'].append(data.is_undirected())data_stat = pd.DataFrame(dic)data_stat.T# print()# print(f'Dataset: {dataset}:')# print('======================')# print(f'Number of graphs: {len(dataset)}')# print(f'Number of features: {dataset.num_features}')# print(f'Number of classes: {dataset.num_classes}')# data = dataset[0] # Get the first graph object.# print()# print(data)# print('======================')# # Gather some statistics about the graph.# print(f'Number of nodes: {data.num_nodes}')# print(f'Number of edges: {data.num_edges}')# print(f'Average node degree: {data.num_edges / data.num_nodes:.2f}')# print(f'Number of training nodes: {data.train_mask.sum()}')# print(f'Training node label rate: {int(data.train_mask.sum()) / data.num_nodes:.2f}')# print(f'Contains isolated nodes: {data.contains_isolated_nodes()}')# print(f'Contains self-loops: {data.contains_self_loops()}')# print(f'Is undirected: {data.is_undirected()}') dataset: CitationFull dataset: Pubmed dataset: Citeseer .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 0 1 2 dataset CitationFull Pubmed Citeseer #graphs 1 1 1 #features 1639 500 3703 #classes 4 3 6 #nodes 17716 19717 3327 #edges 105734 88648 9104 Has_isolated_nodes False False True undirected True True True 12# KarateClub_dataset[0]# ! du -h ./dataset/ 4.2 GNN Model Training这里写个一次性训练所有模型和训练集的函数，并把所有模型的结果打印 12345678910111213141516171819202122232425262728293031323334353637383940414243models = {}def train_models(models, datasets): res = {} mymodels = {} for key in datasets.keys(): if key in [\"CitationFull\"]: dataset = datasets[key] testset = datasets[key] else: dataset = datasets[key] testset= None data = dataset[0] if key not in res.keys(): res[key] = {} if key not in models.keys(): mymodels[key] = {} model_mlp2 = MLP(in_channel = dataset.num_features, classes = dataset.num_classes,hidden_channels=16) model_gcn2 = GCN(in_channel = dataset.num_features, classes = dataset.num_classes,hidden_channels=16) model_sage2 = SAGE(in_channel = dataset.num_features, classes = dataset.num_classes,hidden_channels = 24,dropout_r= 0.5) model_gat2 = GAT(in_channel = dataset.num_features, classes = dataset.num_classes,hidden_channels = 24, dropout_r= 0.8) models = {\"MLP\":model_mlp2, \"GCN\":model_gcn2,\"GAT\":model_gat2, \"GraphSAGE\":model_sage2} mymodels[key] = models for name, model in models.items(): if name not in res[key].keys(): res[key][name] =None print(f\"Dataset: {key}, model: {name}\") optimizer = torch.optim.Adam(model.parameters(), lr=0.03, weight_decay=5e-4) criterion = torch.nn.CrossEntropyLoss() #print(\"Model Strucutre:\") #print(model) for epoch in range(1, 201): use_test_mask = True if testset== None else False loss = train(model, criterion, optimizer,data,use_test_mask) if epoch%20==0: print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}') print() use_test_mask = True if testset== None else False test_acc = test(model,data,use_test_mask) print(f'Test Accuracy: {test_acc:.4f}') res[key][name] = test_acc return pd.DataFrame(res), mymodels 12test_results, trained_models = train_models(models, mydatasets)test_results Dataset: CitationFull, model: MLP Epoch: 020, Loss: 0.9961 Epoch: 040, Loss: 0.8182 Epoch: 060, Loss: 0.7515 Epoch: 080, Loss: 0.7192 Epoch: 100, Loss: 0.6986 Epoch: 120, Loss: 0.6905 Epoch: 140, Loss: 0.6799 Epoch: 160, Loss: 0.6746 Epoch: 180, Loss: 0.6669 Epoch: 200, Loss: 0.6673 Test Accuracy: 0.8128 Dataset: CitationFull, model: GCN Epoch: 020, Loss: 0.7686 Epoch: 040, Loss: 0.5725 Epoch: 060, Loss: 0.5072 Epoch: 080, Loss: 0.4807 Epoch: 100, Loss: 0.4712 Epoch: 120, Loss: 0.4601 Epoch: 140, Loss: 0.4548 Epoch: 160, Loss: 0.4475 Epoch: 180, Loss: 0.4489 Epoch: 200, Loss: 0.4439 Test Accuracy: 0.8606 Dataset: CitationFull, model: GAT Epoch: 020, Loss: 0.7240 Epoch: 040, Loss: 0.5830 Epoch: 060, Loss: 0.4896 Epoch: 080, Loss: 0.4724 Epoch: 100, Loss: 0.4547 Epoch: 120, Loss: 0.4440 Epoch: 140, Loss: 0.4313 Epoch: 160, Loss: 0.4150 Epoch: 180, Loss: 0.4025 Epoch: 200, Loss: 0.3877 Test Accuracy: 0.8818 Dataset: CitationFull, model: GraphSAGE Epoch: 020, Loss: 0.4138 Epoch: 040, Loss: 0.2719 Epoch: 060, Loss: 0.2323 Epoch: 080, Loss: 0.2151 Epoch: 100, Loss: 0.2272 Epoch: 120, Loss: 0.1943 Epoch: 140, Loss: 0.2020 Epoch: 160, Loss: 0.1900 Epoch: 180, Loss: 0.1968 Epoch: 200, Loss: 0.2002 Test Accuracy: 0.9690 Dataset: Pubmed, model: MLP Epoch: 020, Loss: 0.4589 Epoch: 040, Loss: 0.1964 Epoch: 060, Loss: 0.1649 Epoch: 080, Loss: 0.1044 Epoch: 100, Loss: 0.0887 Epoch: 120, Loss: 0.1578 Epoch: 140, Loss: 0.1371 Epoch: 160, Loss: 0.1269 Epoch: 180, Loss: 0.1296 Epoch: 200, Loss: 0.1131 Test Accuracy: 0.7280 Dataset: Pubmed, model: GCN Epoch: 020, Loss: 0.5874 Epoch: 040, Loss: 0.2958 Epoch: 060, Loss: 0.2396 Epoch: 080, Loss: 0.1658 Epoch: 100, Loss: 0.1735 Epoch: 120, Loss: 0.1848 Epoch: 140, Loss: 0.1198 Epoch: 160, Loss: 0.1091 Epoch: 180, Loss: 0.1549 Epoch: 200, Loss: 0.1069 Test Accuracy: 0.7930 Dataset: Pubmed, model: GAT Epoch: 020, Loss: 0.3768 Epoch: 040, Loss: 0.1317 Epoch: 060, Loss: 0.1555 Epoch: 080, Loss: 0.2786 Epoch: 100, Loss: 0.1570 Epoch: 120, Loss: 0.1774 Epoch: 140, Loss: 0.0932 Epoch: 160, Loss: 0.1104 Epoch: 180, Loss: 0.0623 Epoch: 200, Loss: 0.0201 Test Accuracy: 0.7140 Dataset: Pubmed, model: GraphSAGE Epoch: 020, Loss: 0.0753 Epoch: 040, Loss: 0.0226 Epoch: 060, Loss: 0.0557 Epoch: 080, Loss: 0.0116 Epoch: 100, Loss: 0.0086 Epoch: 120, Loss: 0.0297 Epoch: 140, Loss: 0.0106 Epoch: 160, Loss: 0.0835 Epoch: 180, Loss: 0.0064 Epoch: 200, Loss: 0.0906 Test Accuracy: 0.7700 Dataset: Citeseer, model: MLP Epoch: 020, Loss: 1.1613 Epoch: 040, Loss: 0.5621 Epoch: 060, Loss: 0.4475 Epoch: 080, Loss: 0.4112 Epoch: 100, Loss: 0.3795 Epoch: 120, Loss: 0.3416 Epoch: 140, Loss: 0.3785 Epoch: 160, Loss: 0.3341 Epoch: 180, Loss: 0.3596 Epoch: 200, Loss: 0.4045 Test Accuracy: 0.5910 Dataset: Citeseer, model: GCN Epoch: 020, Loss: 1.3142 Epoch: 040, Loss: 0.8198 Epoch: 060, Loss: 0.5696 Epoch: 080, Loss: 0.4871 Epoch: 100, Loss: 0.4850 Epoch: 120, Loss: 0.4268 Epoch: 140, Loss: 0.3961 Epoch: 160, Loss: 0.3845 Epoch: 180, Loss: 0.3508 Epoch: 200, Loss: 0.3517 Test Accuracy: 0.7010 Dataset: Citeseer, model: GAT Epoch: 020, Loss: 0.7527 Epoch: 040, Loss: 0.4082 Epoch: 060, Loss: 0.4495 Epoch: 080, Loss: 0.4963 Epoch: 100, Loss: 0.3861 Epoch: 120, Loss: 0.2898 Epoch: 140, Loss: 0.3495 Epoch: 160, Loss: 0.2082 Epoch: 180, Loss: 0.2877 Epoch: 200, Loss: 0.3382 Test Accuracy: 0.6530 Dataset: Citeseer, model: GraphSAGE Epoch: 020, Loss: 0.3369 Epoch: 040, Loss: 0.1300 Epoch: 060, Loss: 0.0721 Epoch: 080, Loss: 0.0956 Epoch: 100, Loss: 0.1785 Epoch: 120, Loss: 0.0476 Epoch: 140, Loss: 0.0424 Epoch: 160, Loss: 0.0556 Epoch: 180, Loss: 0.1063 Epoch: 200, Loss: 0.0645 Test Accuracy: 0.6720 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } CitationFull Pubmed Citeseer MLP 0.812768 0.728 0.591 GCN 0.860634 0.793 0.701 GAT 0.881802 0.714 0.653 GraphSAGE 0.969011 0.770 0.672 Conclusion 和以往的像图片文本之类的数据不同，图的数据的训练的不同点有下面几个 除了要输入节点的数据特征外，还要输入edge作为关联的数据。 另外GNN在训练时是可以同时更新多个不同node的embedding。 GNN训练时更加像NLP的训练方法，都是把一部分的node（在NLP里面是word token）mask掉并对这部分内容(node或者link)预测 另外在node classification里面因为GNN输入是图，输出也是转换后学习后的图(图里的每个node的值代表这个node所属的class)，可以一下子把所有要预测的node进行预测。不过也可以通过mask的形式在图里面采样进行分batch训练 另外在上面实验中可以看到，在相同配置中GCN要把GAT和GraphSAGE好，GAT比较容易overfit，而GAT和GraphSAGE训练相对于GCN比较慢因为在训练node embedding时设计到两个循环。不过GAT，GraphSAGE相对于GCN训练时的loss收敛得很快。就目前的任务来看感觉GCN比GAT，GraphSAGE要好，但是可能不同的任务模型用起来效果也不一样 当node embedding训练得好的时候，不同的类的node embedding特征很容易被区别开来，相同类的node的特征会内聚而不同类的特征会远离，这个其实和普通的NN分类器里面提取的特征一样，比较正常。 Reference Datawhale:https://github.com/datawhalechina/team-learning-nlp/blob/master/GNN/Markdown%E7%89%88%E6%9C%AC/5-%E5%9F%BA%E4%BA%8E%E5%9B%BE%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E8%8A%82%E7%82%B9%E8%A1%A8%E5%BE%81%E5%AD%A6%E4%B9%A0.md 知乎 https://zhuanlan.zhihu.com/p/106706203 PyG中内置的数据转换方法：torch-geometric-transforms 一个可视化高纬数据的工具：t-distributed Stochastic Neighbor Embedding 提出GCN的论文：Semi-supervised Classification with Graph Convolutional Network GCNConv官方文档：torch_geometric.nn.conv.GCNConv 提出GAT的论文： Graph Attention Networks","link":"/2021/06/23/GNN-3-NodeEmbedding/"},{"title":"GNN-2-MessagePassing","text":"GNN-2-Message Passing 消息传递神经网络1. Introduction在图神经网络里面，在对数据和样本之间的关系进行建模得到图的edge， node之后，我们需要在图里面把每个节点的信息根据它的neighbor的信息进行更新，从而达到node的信息更新和节点特征(Node Representation)的特征表达。而这个把node节点信息相互传递从而更新节点表征的方法也叫Message Passing。MessagePassing是一种聚合邻接节点信息来更新中心节点信息的范式，它将卷积算子推广到了不规则数据领域，实现了图与神经网络的连接。消息传递范式因为简单、强大的特性，于是被人们广泛地使用。遵循消息传递范式的图神经网络被称为消息传递图神经网络。 这一节里面我们讨论和实践 图神经网络一下几点: Message Passing 的原理 PyG (PyTorch Geometric)里面的MessagePassing类的理解和改写 通过MessagePassing, GCNConv 搭建Graph Convolution Neural network (GCN) 并通过实际的数据进行训练 对MessagePassing的基类函数如 aggregation， update， 的method进行理解和使用 Jupyter Notebook source code 可以看这里: https://github.com/wenkangwei/Datawhale-Team-Learning/blob/main/GNN/Task-2-MessagePassing/GNN-Task-2-MessagePassing.ipynb 注：这篇文章参考了datawhale教学文档, Torch Geometric 官方文档， Deep Learning on Graph, 并添加了自己一些想法。 2.How Message Passing works Message Passing的基本思路 以图片为例，如果我们的任务是node prediction去预测node A的特征值/node representation，那么图片里node A就是target node。然后 MessagePassing的过程如下 图中黄色方框部分内容的是一次邻居节点信息传递到中心节点的过程：B节点的邻接节点（A,C）的信息经过变换后聚合到B节点，接着B节点信息与邻居节点聚合信息一起经过变换得到B节点的新的节点信息。同时，分别如红色和绿色方框部分所示，同样的过程，C、D节点的信息也被更新。实际上，同样的过程在所有节点上都进行了一遍，所有节点的信息都更新了一遍。 每个node的值是同时更新的 把步骤1 的“邻居节点信息传递到中心节点的过程”进行多次。如图中蓝色方框部分所示，A节点的邻接节点（B,C,D）的已经发生过一次更新的节点信息，经过变换、聚合、再变换产生了A节点第二次更新的节点信息。多次更新后的节点信息就作为节点表征。 一句话总结就是每次都把图里面的node的信息根据邻居节点进行更新，并多次把图的信息不断刷新得到Node representation。 Message Passing GNN 的泛式 MessagePassing图神经网络遵循上述的“聚合邻接节点信息来更新中心节点信息的过程”，来生成节点表征。Message Passing GNN的通用公式可以描述为 $$\\mathbf{x}_ i^{(k)} = \\gamma^{(k)} ( \\mathbf{x}_ i^{(k-1)}, \\square_{j \\in \\mathcal{N}(i)} , \\phi^{(k)}(\\mathbf{x}_ i^{(k-1)}, \\mathbf{x}_ j^{(k-1)},\\mathbf{e}_{j,i}) ),$$ 根据官方文档 以及CREATING MESSAGE PASSING NETWORKS, 我们定义 $\\mathbf{x}^{(k-1)}_i\\in\\mathbb{R}^F$表示神经网络的$(k-1)$层中节点$i$的节点表征 $\\mathbf{e}_{j,i} \\in \\mathbb{R}^D$ 表示从节点$j$到节点$i$的边的属性信息。 $\\square$表示可微分的、具有排列不变性（函数输出结果与输入参数的排列无关）的函数, 比如aggregation 函数。比如sum， mean, min等函数和输入的参数顺序无关的函数。 $\\gamma$ : 可微分可导的update 函数，比如MLPs（多层感知器） $\\phi$: 可微分可导的message 函数，比如MLPs（多层感知器）和 linear Projection等 Note: 神经网络的生成节点表征的操作称为节点嵌入（Node Embedding），节点表征也可以称为节点嵌入。这里考虑节点嵌入只代指神经网络生成节点表征的操作。 未经过训练的图神经网络生成的节点表征还不是好的节点表征，好的节点表征可用于衡量节点之间的相似性。通过监督学习对图神经网络做很好的训练，图神经网络才可以生成好的节点表征。我们将在第5节介绍此部分内容。 节点表征与节点属性的区分：遵循被广泛使用的约定，此次组队学习我们也约定，节点属性data.x是节点的第0层(GNN输入层)节点表征，第$h$层的节点表征经过一次的节点间信息传递产生第$h+1$层的节点表征。不过，节点属性不单指data.x，广义上它就指节点的属性，如节点的度(in-degree, out-degree)等。 3. MessagePassing Class in PyTorch Geometric3.1 MessagePassing 的Base Class 函数Pytorch Geometric(PyG)提供了MessagePassing基类，它封装了“消息传递”的运行流程。通过继承MessagePassing基类，可以方便地构造消息传递图神经网络。构造一个最简单的消息传递图神经网络类，我们只需定义message()方法（ 𝜙(..) ）、update()方法（ 𝛾(..) ），以及使用的消息聚合方案（aggr=”add”、aggr=”mean”或aggr=”max”。MessagePassing Base Class中这里最重要的3个函数是： MessagePassing.aggregate(...)：用于处理聚集到节点的信息的函数 MessagePassing.message(...)：用于搭建传送到 node i的节点消息，相对于𝜙(..)函数 MessagePassing.update(aggr_out, ...): 用于更新节点的信息，相对于𝛾(..) 以下是一些常用函数的解释: MessagePassing(aggr=&quot;add&quot;, flow=&quot;source_to_target&quot;, node_dim=-2): aggr: aggregation function聚合函数的选项, 可以用 (“add”, “mean” or “max”) flow: 信息传递方向 (either “source_to_target” or “target_to_source”) node_dim：定义沿着哪个维度传播，默认值为-2，也就是节点表征张量（data.x, Tensor）的哪一个维度是节点维度。节点表征张量x形状为[num_nodes, num_features]，其第0维度/columns（也是第-2维度）是节点维度(节点的个数)，其第1维度（也是第-1维度）是节点表征维度，所以我们可以设置node_dim=-2。 MessagePassing.propagate(edge_index, size=None, **kwargs): edge_index: 一个matrix存放每条edge 的索引信息(起始和终止的node的index) size: 基于非对称的邻接矩阵进行消息传递（当图为二部图时），需要传递参数size=(N, M)。如果size=None, 默认邻接矩阵是对称的 **kwargs：图的其他特征 MessagePassing.message(...)： 首先确定要给节点$i$传递消息的边的集合： 如果flow=&quot;source_to_target&quot;，则是$(j,i) \\in \\mathcal{E}$的边的集合； 如果flow=&quot;target_to_source&quot;，则是$(i,j) \\in \\mathcal{E}$的边的集合。 接着为各条边创建要传递给节点$i$的消息，即实现$\\phi$函数。 MessagePassing.message(...)方法可以接收传递给MessagePassing.propagate(edge_index, size=None, **kwargs)方法的所有参数，我们在message()方法的参数列表里定义要接收的参数，例如我们要接收x,y,z参数，则我们应定义message(x,y,z)方法。 传递给propagate()方法的参数，如果是节点的属性的话，可以被拆分成属于中心节点的部分和属于邻接节点的部分，只需在变量名后面加上_i或_j。例如，我们自己定义的meassage方法包含参数x_i，那么首先propagate()方法将节点表征拆分成中心节点表征和邻接节点表征，接着propagate()方法调用message方法并传递中心节点表征给参数x_i。而如果我们自己定义的meassage方法包含参数x_j，那么propagate()方法会传递邻接节点表征给参数x_j。 我们用$i$表示“消息传递”中的中心节点，用$j$表示“消息传递”中的邻接节点。 MessagePassing.aggregate(...)： 将从源节点传递过来的消息聚合在目标节点上，一般可选的聚合方式有sum, mean和max。 MessagePassing.message_and_aggregate(...)： 在一些场景里，邻接节点信息变换和邻接节点信息聚合这两项操作可以融合在一起，那么我们可以在此方法里定义这两项操作，从而让程序运行更加高效。 MessagePassing.update(aggr_out, ...): 为每个节点$i \\in \\mathcal{V}$更新节点表征，即实现$\\gamma$函数。此方法以aggregate方法的输出为第一个参数，并接收所有传递给propagate()方法的参数。 3.2 MessagePassing 的Base Class 函数3.2.1 propagate 函数的输入propagate 函数的输入 有edge_index, x (node embedding matrix), 以及其他自定义的输入参数(degree, norm之类的)。其中edge_index的储存形式如下$$\\mathbf{Edge index}=[\\begin{array}{lllll} [0 &amp; 0&amp; 1&amp; 4&amp;..8] \\\\ [0&amp; 1&amp; 4&amp; 1&amp; ..9] \\\\ \\end{array}]$$其中Edge_index的shape = [2, amount of edge]. Edge_index[0]第一行是source node的index， Edge_index[1]第二行是target node的index. Note 如果edge_index 用 torch tensor来储存，那么propagate函数会分别调用message, aggregate的函数 如果edge_index 用 torch_sparse的SparseTensor类来储存，那么propagate函数会调用message_and_aggregate的函数而不是两个单独的函数 当edge_index, x(node embedding)输入到propagate后，它会自动通过 __collect__()函数 把输入解析得到以下参数: 如果self.flow=”source_to_target”: x_i: edge_index的target node的index列表(edge_index[1])对应的node embedding向量列表。比如 edge_index的target node列表是 edge_index[1], length = E, 而node embedding的维度为dim, 那么 x_i =x[edge_index[1]]是edge_index[1]所对应的embedding列表， x_i的shape= [E, dim]。举个例子就是 target node 的索引列表是 edge_index[1] = [0, 1, 2]而 E=3, dim=2, 那么 x_i = [[0.5,0.6],[0.1,0.22],[0.2,0.3]]。x_i里面的每一行分别对应target node 0, 1,2的node embedding向量 deg_i: edge_index的target node的index列表对应的degree列表。这个和x_i同理 x_j：edge_index的source node的edge_index[0]列表对应的node embedding向量列表。 deg_j: edge_index的source node的edge_index[0]列表对应的degree列表。这个和x_j同理 如果flow=”target_to_source” 那么有_ i后缀代表source, _ j后缀代表target node 在得到target node的edge_index和 对应的source node的node embedding vectors之后，我们就可以把每个target node对应的所有node embedding向量聚合一起得到target node的信息集合用于搭建 message了3.2.2 message 函数的输入message 函数输入一般包括: x_i, x_j, deg_i, deg_j, edge_index以及其他自定义的参数输入 3.2.3 aggregate 函数的输入aggregate 函数输入除了有 inputs (来自message函数的输入) 外 一般还包括: inputs, x_i, x_j, deg_i, deg_j, edge_index以及其他自定义的参数输入。 3.2.4 message_and_aggregate 函数的输入message_and_aggregate 函数输入 一般还包括: x_i, x_j, deg_i, deg_j, edge_index以及其他自定义的参数输入。 3.2.5 update 函数的输入update 函数输入包括inputs以及其他自定义的参数输入。 12 4. Coding Practice4.1 基于 Message Passing的泛式(框架)搭建Graph Convolution Network (GCN)根据PyG的官方文档，**GCNConv** 的公式是$$\\mathbf{x}_ i^{(k)} = \\sum_{j \\in \\mathcal{N}(i) \\cup { i }} \\frac{1}{\\sqrt{\\deg(i)} \\cdot \\sqrt{\\deg(j)}} \\cdot ( \\mathbf{\\Theta} \\cdot \\mathbf{x}_ j^{(k-1)} ),$$ 矩阵的形式是$$\\mathbf{X}^{(k)} = \\mathbf{D}^{-0.5}\\mathbf{A}\\mathbf{D}^{-0.5}\\mathbf{X}^{(k-1)}\\mathbf{\\Theta}$$ 其中，$\\mathbf{x}_i$ 的节点的特征是由它的近邻的node的信息(包括node i自己)进行更新，所以计算时j是节点i的邻居(包括节点i本身)的子集里面的node。 邻接节点的表征$\\mathbf{x}_j^{(k-1)}$首先通过与权重矩阵$\\mathbf{\\Theta}$相乘进行变换，然后按端点的度$\\deg(i), \\deg(j)$进行归一化处理，最后进行求和。这个公式可以分为以下几个步骤： 向邻接矩阵添加自环边。 对节点表征做线性转换。 计算归一化系数。 归一化邻接节点的节点表征。 将相邻节点表征相加（”求和 “聚合）。 步骤1-3通常是在消息传递发生之前计算的。步骤4-5可以使用MessagePassing基类轻松处理。该层的全部实现如下所示。 12345678910111213141516171819202122232425262728293031323334353637from torch_geometric.nn import MessagePassingfrom torch_geometric.utils import add_self_loops, degreeimport torchclass GCNConv(MessagePassing): def __init__(self, in_channels, out_channels): super(GCNConv, self).__init__(aggr='add') # \"Add\" aggregation (Step 5). self.lin = torch.nn.Linear(in_channels, out_channels) def forward(self, x, edge_index): # x has shape [N, in_channels] # edge_index has shape [2, E] # Step 1: Add self-loops to the adjacency matrix. # Adds a self-loop (i,i)∈E to every node i∈V in the graph given by edge_index. # In case the graph is weighted, self-loops will be added with edge weights denoted by fill_value. edge_index, _ = add_self_loops(edge_index, num_nodes=x.size(0)) # Step 2: Linearly transform node feature matrix. x = self.lin(x) # Step 3: Compute normalization: 1/sqrt(degree(i)) * 1/sqrt(degree(j)) row, col = edge_index deg = degree(col, x.size(0), dtype=x.dtype) deg_inv_sqrt = deg.pow(-0.5) deg_inv_sqrt[deg_inv_sqrt == float('inf')] = 0 norm = deg_inv_sqrt[row] * deg_inv_sqrt[col] # Step 4-5: Start propagating messages. return self.propagate(edge_index, x=x, norm=norm) def message(self, x_j, norm): # x_j has shape [E, out_channels] # Step 4: Normalize node features. return norm.view(-1, 1) * x_j 12## download data to current directory#! wget https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.x 123456789101112131415from torch_geometric.datasets import Planetoiddataset = Planetoid(root='./dataset/Cora', name='Cora')data = dataset[0]# GCNConv: #in_channels: dimension of input vector of linear layer# out_channels: dimension of output vector of linear layer#Note: the linear transform is performed before message passing to reduce the dimension of node representation# After message passing, the amount of nodes doesn't changenet = GCNConv(data.num_features, 64)# data.x: a matrix with each row representing the data in a node# data.edge_index: matrix with shape [2, number of edges], each column representing edge from node to another node, value=index of nodeh_nodes = net(data.x, data.edge_index)print(h_nodes.shape) torch.Size([2708, 64]) 1data.x.shape torch.Size([2708, 1433]) 12 4.2 Overwrite methods: messsage, aggregate, update1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465from torch_geometric.datasets import Planetoidimport torchfrom torch_geometric.nn import MessagePassingfrom torch_geometric.utils import add_self_loops, degreefrom torch_sparse import SparseTensorclass GCNConv(MessagePassing): def __init__(self, in_channels, out_channels): super(GCNConv, self).__init__(aggr='add', flow='source_to_target') # \"Add\" aggregation (Step 5). # flow='source_to_target' 表示消息从源节点传播到目标节点 self.lin = torch.nn.Linear(in_channels, out_channels) def forward(self, x, edge_index): # x has shape [N, in_channels] # edge_index has shape [2, E] # Step 1: Add self-loops to the adjacency matrix. print(\"Before self-loop:\",edge_index.shape) edge_index, _ = add_self_loops(edge_index, num_nodes=x.size(0)) print(\"After self-loop:\",edge_index.shape) # Step 2: Linearly transform node feature matrix. x = self.lin(x) # Step 3: Compute normalization. row, col = edge_index deg = degree(col, x.size(0), dtype=x.dtype) deg_inv_sqrt = deg.pow(-0.5) norm = deg_inv_sqrt[row] * deg_inv_sqrt[col] # Step 4-5: Start propagating messages. # Convert edge index to a sparse adjacency matrix representation, with row = from nodes, col = to nodes, value = 0 or 1 indicating if # two nodes are adjacent. adjmat = SparseTensor(row=edge_index[0], col=edge_index[1], value=torch.ones(edge_index.shape[1])) #print(\"Adjacency matrix:\",adjmat) # 此处传的不再是edge_idex，而是SparseTensor类型的Adjancency Matrix return self.propagate(adjmat, x=x, norm=norm, deg=deg.view((-1, 1))) def message(self, x_j, norm, deg_i): # x_j has shape [E, out_channels] # deg_i has shape [E, 1] # Step 4: Normalize node features. return norm.view(-1, 1) * x_j * deg_i def aggregate(self, inputs, index, ptr, dim_size): print('self.aggr:', self.aggr) print(\"`aggregate` is called\") return super().aggregate(inputs, index, ptr=ptr, dim_size=dim_size) def message_and_aggregate(self, adj_t, x, norm): print('`message_and_aggregate` is called') # 没有实现真实的消息传递与消息聚合的操作 def update(self, inputs, deg): print(deg) return inputsdataset = Planetoid(root='dataset/Cora', name='Cora')data = dataset[0]net = GCNConv(data.num_features, 64)h_nodes = net(data.x, data.edge_index)# print(h_nodes.shape) Before self-loop: torch.Size([2, 10556]) After self-loop: torch.Size([2, 13264]) Adjacency matrix: SparseTensor(row=tensor([ 0, 0, 0, ..., 2707, 2707, 2707]), col=tensor([ 0, 633, 1862, ..., 1473, 2706, 2707]), val=tensor([1., 1., 1., ..., 1., 1., 1.]), size=(2708, 2708), nnz=13264, density=0.18%) `message_and_aggregate` is called tensor([[4.], [4.], [6.], ..., [2.], [5.], [5.]]) 5. Assignment5.1 Message Passing 机制总结Message Passing 根据上面讨论的的框架公式，在设计Message Passing 的流程可以归纳为以下几点: 定义和选取 message 函数，𝜙(..)，并根据图的节点信息的输入($x_i^{k-1}, x_j^{k-1}, e_{i,j}$) 对输入进行变换(可导的，比如线性投映进行降维或乘上系数之类的) 定义和选取 aggregation 函数 $\\square(..)$, 对转换后的信息进行邻居节点的信息聚合处理， 常用的有sum, mean, max之类的 定义和选取update()函数（ 𝛾(..) ），把原本的节点信息$x_i^{k-1}$ 和 聚合后的邻居节点信息($\\square(..)$ 函数的输出)的信息进行整合，更新当前的节点信息得到$x_j^{k}$。 用GCN的公式举个栗子，就是$$\\mathbf{x}_ i^{(k)} = \\sum_{j \\in \\mathcal{N}(i) \\cup { i }} \\frac{1}{\\sqrt{\\deg(i)} \\cdot \\sqrt{\\deg(j)}} \\cdot ( \\mathbf{\\Theta} \\cdot \\mathbf{x}_ j^{(k-1)} ),$$ GCN里面的 $\\frac{1}{\\sqrt{\\deg(i)} \\cdot \\sqrt{\\deg(j)}} \\cdot ( \\mathbf{\\Theta} \\cdot \\mathbf{x}_j^{(k-1)} )$ 的操作，里面的$\\mathbf{\\Theta}$ 线性投映和用degree做normalization相对于是 𝜙(..)函数的message的搭建 而 $\\sum_{j \\in \\mathcal{N}(i) \\cup { i }}$ 这一步相对于把邻居节点(包括节点自己)的信息进行聚合, 相对于aggregation 函数 $\\square(..)$ GCN这里因为在做了aggregation后没有用到 $x_i^{k-1}$信息，所以update()函数, 𝛾($x_i^{k-1}, \\square(..)$) 可以看成直接输出(或者是$\\square()$信息聚合后乘上1就输出)。𝛾(..)其实也可以替换为其他可导的的非线性函数比如 logistics， relu之类的。 至于MessagePassing 的Base Class里面的message_and_aggregate()可以看成是 $\\square(\\phi(x_i^{k-1}, x_j^{k-1}, e_{i,j}))$ MessagePassing 的Base Class里面的propagate()函数可以看成是对 $\\gamma(x_i^{k-1}, \\square(\\phi(…)))$ 更新函数的封装。 这一点可以看看官方文档的源码 12 5.2 用MessagePassing 这个BaseClass去实现一个GCN layer这里逐步实现实现一个GCN， 公式如下: $$\\mathbf{x}_ i^{(k)} = \\sum_{j \\in \\mathcal{N}(i) \\cup { i }} \\frac{1}{\\sqrt{\\deg(i)} \\cdot \\sqrt{\\deg(j)}} \\cdot ( \\mathbf{\\Theta} \\cdot \\mathbf{x}_ j^{(k-1)} ),$$ 这里一些函数定义如下： $\\phi(..)$: message函数GCN一样都是linear projection之后用degree进行normalization $\\square(..)$ : aggregate 函数用 add $\\gamma(..)$: update 函数是直接将aggregate后的结果输出 5.2.1 覆写message函数要求该函数接收消息传递源节点属性x、目标节点度d 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144from torch_geometric.datasets import Planetoidimport torchfrom torch import nn, Tensorfrom torch_geometric.nn import MessagePassingfrom torch_geometric.utils import add_self_loops, degreefrom torch_sparse import SparseTensor, matmulclass GCNConv(MessagePassing): def __init__(self, in_channels, out_channels): super(GCNConv, self).__init__(aggr='add', flow='source_to_target') # \"Add\" aggregation (Step 5). # flow='source_to_target' 表示消息从源节点传播到目标节点 self.lin = torch.nn.Linear(in_channels, out_channels) self.lin2 = torch.nn.Linear(out_channels, out_channels) self.relu = torch.nn.ReLU() def propagate(self, edge_index, size=None, **kwargs): # I just copy the source copy from PyG website r\"\"\"The initial call to start propagating messages. Args: edge_index (Tensor or SparseTensor): A :obj:`torch.LongTensor` or a :obj:`torch_sparse.SparseTensor` that defines the underlying graph connectivity/message passing flow. :obj:`edge_index` holds the indices of a general (sparse) assignment matrix of shape :obj:`[N, M]`. If :obj:`edge_index` is of type :obj:`torch.LongTensor`, its shape must be defined as :obj:`[2, num_messages]`, where messages from nodes in :obj:`edge_index[0]` are sent to nodes in :obj:`edge_index[1]` (in case :obj:`flow=\"source_to_target\"`). If :obj:`edge_index` is of type :obj:`torch_sparse.SparseTensor`, its sparse indices :obj:`(row, col)` should relate to :obj:`row = edge_index[1]` and :obj:`col = edge_index[0]`. The major difference between both formats is that we need to input the *transposed* sparse adjacency matrix into :func:`propagate`. size (tuple, optional): The size :obj:`(N, M)` of the assignment matrix in case :obj:`edge_index` is a :obj:`LongTensor`. If set to :obj:`None`, the size will be automatically inferred and assumed to be quadratic. This argument is ignored in case :obj:`edge_index` is a :obj:`torch_sparse.SparseTensor`. (default: :obj:`None`) **kwargs: Any additional data which is needed to construct and aggregate messages, and to update node embeddings. \"\"\" size = self.__check_input__(edge_index, size) # Run \"fused\" message and aggregation (if applicable). if (isinstance(edge_index, SparseTensor) and self.fuse and not self.__explain__): coll_dict = self.__collect__(self.__fused_user_args__, edge_index, size, kwargs) print(\"Using self-defined message-passing\") msg_aggr_kwargs = self.inspector.distribute( 'message_and_aggregate', coll_dict) out = self.message_and_aggregate(edge_index, **msg_aggr_kwargs) update_kwargs = self.inspector.distribute('update', coll_dict) return self.update(out, **update_kwargs) # Otherwise, run both functions in separation. elif isinstance(edge_index, Tensor) or not self.fuse: coll_dict = self.__collect__(self.__user_args__, edge_index, size, kwargs) msg_kwargs = self.inspector.distribute('message', coll_dict) #print(\"Message kwargs: \",msg_kwargs) out = self.message(**msg_kwargs) # For `GNNExplainer`, we require a separate message and aggregate # procedure since this allows us to inject the `edge_mask` into the # message passing computation scheme. if self.__explain__: edge_mask = self.__edge_mask__.sigmoid() # Some ops add self-loops to `edge_index`. We need to do the # same for `edge_mask` (but do not train those). if out.size(self.node_dim) != edge_mask.size(0): loop = edge_mask.new_ones(size[0]) edge_mask = torch.cat([edge_mask, loop], dim=0) assert out.size(self.node_dim) == edge_mask.size(0) out = out * edge_mask.view([-1] + [1] * (out.dim() - 1)) aggr_kwargs = self.inspector.distribute('aggregate', coll_dict) out = self.aggregate(out, **aggr_kwargs) update_kwargs = self.inspector.distribute('update', coll_dict) return self.update(out, **update_kwargs) def forward(self, x, edge_index): # x has shape [N, in_channels] # edge_index has shape [2, E] # Step 1: Add self-loops to the adjacency matrix. edge_index, _ = add_self_loops(edge_index, num_nodes=x.size(0)) # Step 2: Linearly transform node feature matrix. x = self.lin(x) # Compute degree. row, col = edge_index deg = degree(col, x.size(0), dtype=x.dtype) return self.propagate(edge_index, x=x, deg=deg.view((-1, 1))) def message(self, x_j, deg_i,deg_j): # Accoding to __collect__ function # in https://github.com/rusty1s/pytorch_geometric/blob/master/torch_geometric/nn/conv/message_passing.py # when flow = source_to_target # i= 1, j=0, edge_index_i = edge_index[1] = target, so # deg_i is degree of target node, and x_i is target node data # deg_j is degree of source node and x_j is source # x_j has shape [E, out_channels] # deg_i has shape [E, 1] # Step 3: Normalize node features. print(\"--message is called--\") print(\"x_j: \",x_j.shape) print(\"degree: \", deg_i.shape) print(\"degree: \",deg_j.shape) print() # check if degrees of source nodes and degrees of target nodes are equal print(torch.eq(deg_i, deg_j).all()) # compute normalization deg_i = deg_i.pow(-0.5) deg_j = deg_j.pow(-0.5) norm = deg_i * deg_j return norm.view(-1, 1) * x_jdataset = Planetoid(root='dataset/Cora', name='Cora')data = dataset[0]net = GCNConv(data.num_features, 64)h_nodes = net(data.x, data.edge_index)print(\"H_nodes: \", h_nodes.shape)h_nodes --message is called-- x_j: torch.Size([13264, 64]) degree: torch.Size([13264, 1]) degree: torch.Size([13264, 1]) tensor(False) H_nodes: torch.Size([2708, 64]) tensor([[-0.0336, -0.0263, -0.0141, ..., -0.0157, -0.0207, 0.0233], [-0.0204, -0.0698, -0.0737, ..., -0.0233, 0.0268, -0.0347], [-0.0437, -0.0602, -0.0162, ..., 0.0243, 0.0348, -0.0054], ..., [-0.0067, -0.0016, -0.0004, ..., 0.0237, -0.0289, 0.0044], [ 0.0061, 0.0198, -0.0076, ..., 0.0065, 0.0373, -0.0187], [ 0.0080, 0.0146, -0.0173, ..., -0.0250, 0.0205, 0.0163]], grad_fn=&lt;ScatterAddBackward&gt;) 12 5.2.2 在第一个类的基础上，再覆写aggregate函数要求不能调用super类的aggregate函数，并且不能直接复制super类的aggregate函数内容。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101from torch_geometric.datasets import Planetoidimport torchfrom torch import nn, Tensorfrom torch_geometric.nn import MessagePassingfrom torch_geometric.utils import add_self_loops, degreefrom torch_sparse import SparseTensor, matmulclass GCNConv(MessagePassing): def __init__(self, in_channels, out_channels): super(GCNConv, self).__init__(aggr='add', flow='source_to_target') # \"Add\" aggregation (Step 5). # flow='source_to_target' 表示消息从源节点传播到目标节点 self.lin = torch.nn.Linear(in_channels, out_channels) self.lin2 = torch.nn.Linear(out_channels, out_channels) self.relu = torch.nn.ReLU() def forward(self, x, edge_index): # x has shape [N, in_channels] # edge_index has shape [2, E] # Step 1: Add self-loops to the adjacency matrix. edge_index, _ = add_self_loops(edge_index, num_nodes=x.size(0)) # Step 2: Linearly transform node feature matrix. x = self.lin(x) # Compute degree. row, col = edge_index deg = degree(col, x.size(0), dtype=x.dtype) return self.propagate(edge_index, x=x, deg=deg.view((-1, 1))) def message(self, x_j, deg_i,deg_j): # Accoding to __collect__ function # in https://github.com/rusty1s/pytorch_geometric/blob/master/torch_geometric/nn/conv/message_passing.py # when flow = source_to_target # i= 1, j=0, edge_index_i = edge_index[1] = target, so # deg_i is degree of target node, and x_i is target node data # deg_j is degree of source node and x_j is source # x_j has shape [E, out_channels] # deg_i has shape [E, 1] # Step 3: Normalize node features. print(\"--message is called--\") print(\"x_j: \",x_j.shape) print(\"degree: \", deg_i.shape) print(\"degree: \",deg_j.shape) print() # check if degrees of source nodes and degrees of target nodes are equal print(torch.eq(deg_i, deg_j).all()) # compute normalization deg_i = deg_i.pow(-0.5) deg_j = deg_j.pow(-0.5) norm = deg_i * deg_j return norm.view(-1, 1) * x_j def aggregate(self, inputs, index, ptr, dim_size): #from __collect__() function we know that # when flow = source_to_target # out['index'] = out['edge_index_i'] -&gt; input index = edge_index[i] = edge_index[1] = index of target node # inputs: embedding vectors of source nodes # inputs: the outputs from message function, the normalized source node embeding with shape [E, dim of embedding] print(\"--aggregate` is called--\") print('self.aggr:', self.aggr) print('ptr: ', ptr) print('dim_size: ',dim_size) print(\"inputs: \", inputs.shape) print(\"index: \",index.shape, len(index.unique())) print() uni_idx = index.unique() uni_idx.sort() res= [] # find all unique target node index # for each target node, aggregate(sum or mean ) the information from source node to the target node # and obtain target node embedding for i in uni_idx: # i is the index of target node neighbors = inputs[index == i] # aggregate along different vectors of different nodes if self.aggr==\"mean\": agg_res = neighbors.mean(axis=0) else: agg_res = neighbors.sum(axis=0) res.append(agg_res) res = torch.stack(res) return res dataset = Planetoid(root='dataset/Cora', name='Cora')data = dataset[0]net = GCNConv(data.num_features, 64)h_nodes = net(data.x, data.edge_index)print(\"H_nodes: \", h_nodes.shape)h_nodes --message is called-- x_j: torch.Size([13264, 64]) degree: torch.Size([13264, 1]) degree: torch.Size([13264, 1]) tensor(False) --aggregate` is called-- self.aggr: add ptr: None dim_size: 2708 inputs: torch.Size([13264, 64]) index: torch.Size([13264]) 2708 H_nodes: torch.Size([2708, 64]) tensor([[-0.0141, 0.0188, 0.0067, ..., -0.0314, 0.0296, -0.0301], [ 0.0056, -0.0510, 0.0796, ..., -0.0591, 0.0362, 0.0113], [-0.0034, 0.0314, 0.0107, ..., -0.0433, 0.0407, 0.0185], ..., [ 0.0280, 0.0239, 0.0307, ..., -0.0530, -0.0522, 0.0293], [-0.0094, 0.0380, -0.0108, ..., -0.0115, 0.0182, -0.0060], [-0.0058, -0.0127, -0.0221, ..., -0.0027, 0.0008, -0.0052]], grad_fn=&lt;StackBackward&gt;) 12 5.2.3 在第二个类的基础上，再覆写update函数要求对节点信息做一层线性变换。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181from torch_geometric.datasets import Planetoidimport torchfrom torch import nn, Tensorfrom torch_geometric.nn import MessagePassingfrom torch_geometric.utils import add_self_loops, degreefrom torch_sparse import SparseTensor, matmulclass GCNConv(MessagePassing): def __init__(self, in_channels, out_channels): super(GCNConv, self).__init__(aggr='add', flow='source_to_target') # \"Add\" aggregation (Step 5). # flow='source_to_target' 表示消息从源节点传播到目标节点 self.lin = torch.nn.Linear(in_channels, out_channels) self.lin2 = torch.nn.Linear(out_channels, out_channels) self.relu = torch.nn.ReLU() def propagate(self, edge_index, size=None, **kwargs): # I just copy the source copy from PyG website r\"\"\"The initial call to start propagating messages. Args: edge_index (Tensor or SparseTensor): A :obj:`torch.LongTensor` or a :obj:`torch_sparse.SparseTensor` that defines the underlying graph connectivity/message passing flow. :obj:`edge_index` holds the indices of a general (sparse) assignment matrix of shape :obj:`[N, M]`. If :obj:`edge_index` is of type :obj:`torch.LongTensor`, its shape must be defined as :obj:`[2, num_messages]`, where messages from nodes in :obj:`edge_index[0]` are sent to nodes in :obj:`edge_index[1]` (in case :obj:`flow=\"source_to_target\"`). If :obj:`edge_index` is of type :obj:`torch_sparse.SparseTensor`, its sparse indices :obj:`(row, col)` should relate to :obj:`row = edge_index[1]` and :obj:`col = edge_index[0]`. The major difference between both formats is that we need to input the *transposed* sparse adjacency matrix into :func:`propagate`. size (tuple, optional): The size :obj:`(N, M)` of the assignment matrix in case :obj:`edge_index` is a :obj:`LongTensor`. If set to :obj:`None`, the size will be automatically inferred and assumed to be quadratic. This argument is ignored in case :obj:`edge_index` is a :obj:`torch_sparse.SparseTensor`. (default: :obj:`None`) **kwargs: Any additional data which is needed to construct and aggregate messages, and to update node embeddings. \"\"\" size = self.__check_input__(edge_index, size) # Run \"fused\" message and aggregation (if applicable). if (isinstance(edge_index, SparseTensor) and self.fuse and not self.__explain__): coll_dict = self.__collect__(self.__fused_user_args__, edge_index, size, kwargs) print(\"Using self-defined message-passing\") msg_aggr_kwargs = self.inspector.distribute( 'message_and_aggregate', coll_dict) out = self.message_and_aggregate(edge_index, **msg_aggr_kwargs) update_kwargs = self.inspector.distribute('update', coll_dict) return self.update(out, **update_kwargs) # Otherwise, run both functions in separation. elif isinstance(edge_index, Tensor) or not self.fuse: coll_dict = self.__collect__(self.__user_args__, edge_index, size, kwargs) msg_kwargs = self.inspector.distribute('message', coll_dict) #print(\"Message kwargs: \",msg_kwargs) out = self.message(**msg_kwargs) # For `GNNExplainer`, we require a separate message and aggregate # procedure since this allows us to inject the `edge_mask` into the # message passing computation scheme. if self.__explain__: edge_mask = self.__edge_mask__.sigmoid() # Some ops add self-loops to `edge_index`. We need to do the # same for `edge_mask` (but do not train those). if out.size(self.node_dim) != edge_mask.size(0): loop = edge_mask.new_ones(size[0]) edge_mask = torch.cat([edge_mask, loop], dim=0) assert out.size(self.node_dim) == edge_mask.size(0) out = out * edge_mask.view([-1] + [1] * (out.dim() - 1)) aggr_kwargs = self.inspector.distribute('aggregate', coll_dict) out = self.aggregate(out, **aggr_kwargs) update_kwargs = self.inspector.distribute('update', coll_dict) return self.update(out, **update_kwargs) def forward(self, x, edge_index): # x has shape [N, in_channels] # edge_index has shape [2, E] # Step 1: Add self-loops to the adjacency matrix. edge_index, _ = add_self_loops(edge_index, num_nodes=x.size(0)) # Step 2: Linearly transform node feature matrix. x = self.lin(x) # Compute degree. row, col = edge_index deg = degree(col, x.size(0), dtype=x.dtype) return self.propagate(edge_index, x=x, deg=deg.view((-1, 1))) def message(self, x_j, deg_i,deg_j): # Accoding to __collect__ function # in https://github.com/rusty1s/pytorch_geometric/blob/master/torch_geometric/nn/conv/message_passing.py # when flow = source_to_target # i= 1, j=0, edge_index_i = edge_index[1] = target, so # deg_i is degree of target node, and x_i is target node data # deg_j is degree of source node and x_j is source # x_j has shape [E, out_channels] # deg_i has shape [E, 1] # Step 3: Normalize node features. print(\"--message is called--\") print(\"x_j: \",x_j.shape) print(\"degree: \", deg_i.shape) print(\"degree: \",deg_j.shape) print() # check if degrees of source nodes and degrees of target nodes are equal print(torch.eq(deg_i, deg_j).all()) # compute normalization deg_i = deg_i.pow(-0.5) deg_j = deg_j.pow(-0.5) norm = deg_i * deg_j return norm.view(-1, 1) * x_j def aggregate(self, inputs, index, ptr, dim_size): #from __collect__() function we know that # when flow = source_to_target # out['index'] = out['edge_index_i'] -&gt; input index = edge_index[i] = edge_index[1] = index of target node # inputs: embedding vectors of source nodes # inputs: the outputs from message function, the normalized source node embeding with shape [E, dim of embedding] print(\"--aggregate` is called--\") print('self.aggr:', self.aggr) print('ptr: ', ptr) print('dim_size: ',dim_size) print(\"inputs: \", inputs.shape) print(\"index: \",index.shape, len(index.unique())) print() uni_idx = index.unique() uni_idx.sort() res= [] # find all unique target node index # for each target node, aggregate(sum or mean ) the information from source node to the target node # and obtain target node embedding for i in uni_idx: # i is the index of target node neighbors = inputs[index == i] # aggregate along different vectors of different nodes if self.aggr==\"mean\": agg_res = neighbors.mean(axis=0) else: agg_res = neighbors.sum(axis=0) res.append(agg_res) res = torch.stack(res) return res def update(self,inputs, deg ): print(\"--update func is called--\") return self.lin2(inputs)dataset = Planetoid(root='dataset/Cora', name='Cora')data = dataset[0]net = GCNConv(data.num_features, 64)h_nodes = net(data.x, data.edge_index)print(\"H_nodes: \", h_nodes.shape)h_nodes --message is called-- x_j: torch.Size([13264, 64]) degree: torch.Size([13264, 1]) degree: torch.Size([13264, 1]) tensor(False) --aggregate` is called-- self.aggr: add ptr: None dim_size: 2708 inputs: torch.Size([13264, 64]) index: torch.Size([13264]) 2708 --update func is called-- H_nodes: torch.Size([2708, 64]) tensor([[-0.0139, -0.0065, 0.1316, ..., 0.0401, -0.1439, -0.0718], [-0.0333, -0.0545, 0.1637, ..., -0.0098, -0.1503, -0.0837], [-0.0245, -0.0277, 0.1248, ..., 0.0264, -0.1423, -0.0829], ..., [-0.0678, -0.0061, 0.1510, ..., 0.0332, -0.1420, -0.0876], [-0.0289, -0.0100, 0.1211, ..., 0.0339, -0.1905, -0.0764], [-0.0255, -0.0036, 0.1290, ..., 0.0366, -0.1623, -0.0631]], grad_fn=&lt;AddmmBackward&gt;) 12 5.2.4 在第三个类的基础上，再覆写message_and_aggregate函数要求在这一个函数中实现前面message函数和aggregate函数的功能。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224from torch_geometric.datasets import Planetoidimport torchfrom torch import nn, Tensorfrom torch_geometric.nn import MessagePassingfrom torch_geometric.utils import add_self_loops, degreefrom torch_sparse import SparseTensor, matmulclass GCNConv(MessagePassing): def __init__(self, in_channels, out_channels): super(GCNConv, self).__init__(aggr='add', flow='source_to_target') # \"Add\" aggregation (Step 5). # flow='source_to_target' 表示消息从源节点传播到目标节点 self.lin = torch.nn.Linear(in_channels, out_channels) self.lin2 = torch.nn.Linear(out_channels, out_channels) self.relu = torch.nn.ReLU() def propagate(self, edge_index, size=None, **kwargs): # I just copy the source copy from PyG website r\"\"\"The initial call to start propagating messages. Args: edge_index (Tensor or SparseTensor): A :obj:`torch.LongTensor` or a :obj:`torch_sparse.SparseTensor` that defines the underlying graph connectivity/message passing flow. :obj:`edge_index` holds the indices of a general (sparse) assignment matrix of shape :obj:`[N, M]`. If :obj:`edge_index` is of type :obj:`torch.LongTensor`, its shape must be defined as :obj:`[2, num_messages]`, where messages from nodes in :obj:`edge_index[0]` are sent to nodes in :obj:`edge_index[1]` (in case :obj:`flow=\"source_to_target\"`). If :obj:`edge_index` is of type :obj:`torch_sparse.SparseTensor`, its sparse indices :obj:`(row, col)` should relate to :obj:`row = edge_index[1]` and :obj:`col = edge_index[0]`. The major difference between both formats is that we need to input the *transposed* sparse adjacency matrix into :func:`propagate`. size (tuple, optional): The size :obj:`(N, M)` of the assignment matrix in case :obj:`edge_index` is a :obj:`LongTensor`. If set to :obj:`None`, the size will be automatically inferred and assumed to be quadratic. This argument is ignored in case :obj:`edge_index` is a :obj:`torch_sparse.SparseTensor`. (default: :obj:`None`) **kwargs: Any additional data which is needed to construct and aggregate messages, and to update node embeddings. \"\"\" size = self.__check_input__(edge_index, size) # Run \"fused\" message and aggregation (if applicable). if (isinstance(edge_index, SparseTensor) and self.fuse and not self.__explain__): coll_dict = self.__collect__(self.__fused_user_args__, edge_index, size, kwargs) #print(\"Using self-defined message-passing\") msg_aggr_kwargs = self.inspector.distribute( 'message_and_aggregate', coll_dict) out = self.message_and_aggregate(edge_index, **msg_aggr_kwargs) update_kwargs = self.inspector.distribute('update', coll_dict) return self.update(out, **update_kwargs) # Otherwise, run both functions in separation. elif isinstance(edge_index, Tensor) or not self.fuse: coll_dict = self.__collect__(self.__user_args__, edge_index, size, kwargs) msg_kwargs = self.inspector.distribute('message', coll_dict) #print(\"Message kwargs: \",msg_kwargs) out = self.message(**msg_kwargs) # For `GNNExplainer`, we require a separate message and aggregate # procedure since this allows us to inject the `edge_mask` into the # message passing computation scheme. if self.__explain__: edge_mask = self.__edge_mask__.sigmoid() # Some ops add self-loops to `edge_index`. We need to do the # same for `edge_mask` (but do not train those). if out.size(self.node_dim) != edge_mask.size(0): loop = edge_mask.new_ones(size[0]) edge_mask = torch.cat([edge_mask, loop], dim=0) assert out.size(self.node_dim) == edge_mask.size(0) out = out * edge_mask.view([-1] + [1] * (out.dim() - 1)) aggr_kwargs = self.inspector.distribute('aggregate', coll_dict) out = self.aggregate(out, **aggr_kwargs) update_kwargs = self.inspector.distribute('update', coll_dict) return self.update(out, **update_kwargs) def forward(self, x, edge_index): # x has shape [N, in_channels] # edge_index has shape [2, E] # Step 1: Add self-loops to the adjacency matrix. edge_index, _ = add_self_loops(edge_index, num_nodes=x.size(0)) # Step 2: Linearly transform node feature matrix. x = self.lin(x) # Compute degree. row, col = edge_index deg = degree(col, x.size(0), dtype=x.dtype) adjmat = SparseTensor(row=edge_index[0], col=edge_index[1], value=torch.ones(edge_index.shape[1])) return self.propagate(adjmat, x=x, deg=deg.view((-1, 1))) def message(self, x_j, deg_i,deg_j): # Accoding to __collect__ function # in https://github.com/rusty1s/pytorch_geometric/blob/master/torch_geometric/nn/conv/message_passing.py # when flow = source_to_target # i= 1, j=0, edge_index_i = edge_index[1] = target, so # deg_i is degree of target node, and x_i is target node data # deg_j is degree of source node and x_j is source # x_j has shape [E, out_channels] # deg_i has shape [E, 1] # Step 3: Normalize node features. print(\"--message is called--\") print(\"x_j: \",x_j.shape) print(\"degree: \", deg_i.shape) print(\"degree: \",deg_j.shape) print() # check if degrees of source nodes and degrees of target nodes are equal print(torch.eq(deg_i, deg_j).all()) # compute normalization deg_i = deg_i.pow(-0.5) deg_j = deg_j.pow(-0.5) norm = deg_i * deg_j return norm.view(-1, 1) * x_j def aggregate(self, inputs, index, ptr, dim_size): #from __collect__() function we know that # when flow = source_to_target # out['index'] = out['edge_index_i'] -&gt; input index = edge_index[i] = edge_index[1] = index of target node # inputs: embedding vectors of source nodes # inputs: the outputs from message function, the normalized source node embeding with shape [E, dim of embedding] print(\"--aggregate` is called--\") print('self.aggr:', self.aggr) print('ptr: ', ptr) print('dim_size: ',dim_size) print(\"inputs: \", inputs.shape) print(\"index: \",index.shape, len(index.unique())) print() uni_idx = index.unique() uni_idx.sort() res= [] # find all unique target node index # for each target node, aggregate(sum or mean ) the information from source node to the target node # and obtain target node embedding for i in uni_idx: # i is the index of target node neighbors = inputs[index == i] # aggregate along different vectors of different nodes if self.aggr==\"mean\": agg_res = neighbors.mean(axis=0) else: agg_res = neighbors.sum(axis=0) res.append(agg_res) res = torch.stack(res) return res def message_and_aggregate(self, adj_t, x_j, index,deg_i, deg_j): # note: # adj_t: adjacency matrix # norm: normalization coefficient 1/sqrt(deg_i)*sqrt(deg_j) # number of '1' in adj_t = length of norm ## Print something to debug #print('`message_and_aggregate` is called') #print(\"adj_t: \",adj_t) #print(\"deg:\", deg) print(\"--message_and_aggregate is called --\") # Step3: compute normalization deg_i = deg_i.pow(-0.5) deg_j = deg_j.pow(-0.5) norm = deg_i * deg_j # Step4: compute normalized message inputs = norm.view(-1, 1) * x_j # Step5: aggregate function sum uni_idx = index.unique() uni_idx.sort() res= [] # find all unique target node index # for each target node, aggregate(sum or mean ) the information from source node to the target node # and obtain target node embedding for i in uni_idx: # i is the index of target node neighbors = inputs[index == i] # aggregate along different vectors of different nodes if self.aggr==\"mean\": agg_res = neighbors.mean(axis=0) else: agg_res = neighbors.sum(axis=0) res.append(agg_res) res = torch.stack(res) return res def update(self,inputs, deg ): print(\"--update func is called--\") return self.lin2(inputs)dataset = Planetoid(root='dataset/Cora', name='Cora')data = dataset[0]net = GCNConv(data.num_features, 64)h_nodes = net(data.x, data.edge_index)print(\"H_nodes: \", h_nodes.shape)h_nodes --message_and_aggregate is called -- --update func is called-- H_nodes: torch.Size([2708, 64]) tensor([[-0.0301, -0.0607, -0.0843, ..., -0.0092, 0.0735, 0.1196], [-0.0287, -0.0805, -0.0924, ..., -0.0665, 0.0596, 0.0680], [-0.0236, -0.0952, -0.1220, ..., -0.0735, 0.0296, 0.0909], ..., [-0.0257, -0.0769, -0.0840, ..., -0.0068, 0.0807, 0.1330], [-0.0402, -0.0765, -0.1098, ..., -0.0396, 0.0407, 0.1058], [-0.0421, -0.0787, -0.1024, ..., -0.0455, 0.0361, 0.1054]], grad_fn=&lt;AddmmBackward&gt;) 12 5.3 设计自定义一个GCN layer这里我自定义的GCN layer公式如下：$$\\mathbf{x}_ i^{(k)} = \\sigma(\\frac{1}{|\\mathcal{N}(i)|+1} \\times \\sum_{j \\in \\mathcal{N}(i) \\cup { i }} \\frac{1}{\\sqrt{\\deg(i)} \\cdot \\sqrt{\\deg(j)}} \\cdot ( \\mathbf{\\Theta} \\cdot \\mathbf{x}_ j^{(k-1)} ) ) + \\mathbf{\\Theta} \\cdot \\mathbf{x}_ i^{(k-1)} ,$$ 这里一些函数定义如下： $\\phi(..)$: message函数和之前的GCN一样都是linear projection之后用degree进行normalization $\\square(..)$ : aggregate 函数 用来mean $\\gamma(..)$: update 函数是先用了ReLu activation函数, 在加上shortcut把之前投映之后的输入加上来，模拟了resnet的结构 这里只用了 message_and_aggregate 函数，所以没有实现message， aggregate的单独的函数 propagate 函数是直接从官方文档copy过来，方便理解GNN的propagate的流程的。 从中可以看到，如果输入到propagate的tensor是SparseTensor, 那么会直接调用message_and_aggregate函数，而不是单独调用两个函数，所以只要实现这个合并的函数就行了 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171from torch_geometric.datasets import Planetoidimport torchfrom torch import nn, Tensorfrom torch_geometric.nn import MessagePassingfrom torch_geometric.utils import add_self_loops, degreefrom torch_sparse import SparseTensor, matmulclass MyGCNConv(MessagePassing): def __init__(self, in_channels, out_channels): super(MyGCNConv, self).__init__(aggr='mean', flow='source_to_target') # \"Add\" aggregation (Step 5). # flow='source_to_target' 表示消息从源节点传播到目标节点 self.lin = torch.nn.Linear(in_channels, out_channels) self.relu = torch.nn.ReLU() def propagate(self, edge_index, size=None, **kwargs): # I just copy the source copy from PyG website r\"\"\"The initial call to start propagating messages. Args: edge_index (Tensor or SparseTensor): A :obj:`torch.LongTensor` or a :obj:`torch_sparse.SparseTensor` that defines the underlying graph connectivity/message passing flow. :obj:`edge_index` holds the indices of a general (sparse) assignment matrix of shape :obj:`[N, M]`. If :obj:`edge_index` is of type :obj:`torch.LongTensor`, its shape must be defined as :obj:`[2, num_messages]`, where messages from nodes in :obj:`edge_index[0]` are sent to nodes in :obj:`edge_index[1]` (in case :obj:`flow=\"source_to_target\"`). If :obj:`edge_index` is of type :obj:`torch_sparse.SparseTensor`, its sparse indices :obj:`(row, col)` should relate to :obj:`row = edge_index[1]` and :obj:`col = edge_index[0]`. The major difference between both formats is that we need to input the *transposed* sparse adjacency matrix into :func:`propagate`. size (tuple, optional): The size :obj:`(N, M)` of the assignment matrix in case :obj:`edge_index` is a :obj:`LongTensor`. If set to :obj:`None`, the size will be automatically inferred and assumed to be quadratic. This argument is ignored in case :obj:`edge_index` is a :obj:`torch_sparse.SparseTensor`. (default: :obj:`None`) **kwargs: Any additional data which is needed to construct and aggregate messages, and to update node embeddings. \"\"\" size = self.__check_input__(edge_index, size) # Run \"fused\" message and aggregation (if applicable). if (isinstance(edge_index, SparseTensor) and self.fuse and not self.__explain__): coll_dict = self.__collect__(self.__fused_user_args__, edge_index, size, kwargs) print(\"Using self-defined message-passing\") msg_aggr_kwargs = self.inspector.distribute( 'message_and_aggregate', coll_dict) out = self.message_and_aggregate(edge_index, **msg_aggr_kwargs) update_kwargs = self.inspector.distribute('update', coll_dict) return self.update(out, **update_kwargs) # Otherwise, run both functions in separation. elif isinstance(edge_index, Tensor) or not self.fuse: coll_dict = self.__collect__(self.__user_args__, edge_index, size, kwargs) msg_kwargs = self.inspector.distribute('message', coll_dict) out = self.message(**msg_kwargs) # For `GNNExplainer`, we require a separate message and aggregate # procedure since this allows us to inject the `edge_mask` into the # message passing computation scheme. if self.__explain__: edge_mask = self.__edge_mask__.sigmoid() # Some ops add self-loops to `edge_index`. We need to do the # same for `edge_mask` (but do not train those). if out.size(self.node_dim) != edge_mask.size(0): loop = edge_mask.new_ones(size[0]) edge_mask = torch.cat([edge_mask, loop], dim=0) assert out.size(self.node_dim) == edge_mask.size(0) out = out * edge_mask.view([-1] + [1] * (out.dim() - 1)) aggr_kwargs = self.inspector.distribute('aggregate', coll_dict) out = self.aggregate(out, **aggr_kwargs) update_kwargs = self.inspector.distribute('update', coll_dict) return self.update(out, **update_kwargs) def forward(self, x, edge_index): # x has shape [N, in_channels] # edge_index has shape [2, E] # Step 1: Add self-loops to the adjacency matrix. edge_index, _ = add_self_loops(edge_index, num_nodes=x.size(0)) # Step 2: Linearly transform node feature matrix. x = self.lin(x) # Step 3: Compute normalization. row, col = edge_index deg = degree(col, x.size(0), dtype=x.dtype) deg_inv_sqrt = deg.pow(-0.5) # note: norm is in shape of (number of edge, ) norm = deg_inv_sqrt[row] * deg_inv_sqrt[col] print(\"Get degree Shape: \", edge_index.shape) print(\"Norm Shape: \",norm.shape) # Step 4-5: Start propagating messages. # Convert edge index to a sparse adjacency matrix representation, with row = from nodes, col = to nodes. # When value = 1 in adjacency matrix, it indicates two nodes are adjacent. # adjmat = SparseTensor(row=edge_index[0], col=edge_index[1], value=torch.ones(edge_index.shape[1])) # 这里 adjacency matrix 的值从1 变成 normalization 的值，方便乘法计算 adjmat = SparseTensor(row=edge_index[0], col=edge_index[1], value=norm) # 此处传的不再是edge_idex，而是SparseTensor类型的Adjancency Matrix return self.propagate(adjmat, x=x, norm=norm, deg=deg.view((-1, 1))) def message(self, x_j, norm, deg_i): # x_j has shape [E, out_channels] # deg_i has shape [E, 1] # Step 4: Normalize node features. return norm.view(-1, 1) * x_j * deg_i def aggregate(self, inputs, index, ptr, dim_size): print('self.aggr:', self.aggr) print(\"`aggregate` is called\") return super().aggregate(inputs, index, ptr=ptr, dim_size=dim_size) def message_and_aggregate(self, adj_t, x, norm,deg): # note: # adj_t: adjacency matrix # norm: normalization coefficient 1/sqrt(deg_i)*sqrt(deg_j) # number of '1' in adj_t = length of norm ## Print something to debug #print('`message_and_aggregate` is called') #print(\"adj_t: \",adj_t) #print(\"deg:\", deg) adj_t = adj_t.to_dense() N = len(adj_t) out = [] x0 = x[:] for i in range(N): # 计算每个 xi 的neighbor传过来的信息的平均值 x_sum = torch.matmul(x.T,adj_t[i]) x_avg = x_sum/deg[i] out.append(x_avg) out = torch.stack(out) return [out, x0] def update(self, inputs, deg): print(\"Update result\") print(\"Degree\",deg) # resnet的结构 x0 = inputs[1] output = self.relu(inputs[0]) + x0 return outputdataset = Planetoid(root='dataset/Cora', name='Cora')data = dataset[0]net = MyGCNConv(data.num_features, 64)h_nodes = net(data.x, data.edge_index) Get degree Shape: torch.Size([2, 13264]) Norm Shape: torch.Size([13264]) Using self-defined message-passing Update result Degree tensor([[4.], [4.], [6.], ..., [2.], [5.], [5.]]) 1h_nodes tensor([[-2.4017e-02, 4.7570e-02, 1.1954e-02, ..., 1.3043e-02, 2.0967e-02, -8.4416e-02], [-8.5681e-02, 1.2029e-01, 1.0756e-01, ..., 5.4046e-02, -8.9611e-02, -1.9092e-01], [ 6.2691e-02, -2.7604e-02, -6.0106e-02, ..., -3.0790e-05, 7.8295e-03, -7.2708e-02], ..., [ 2.0562e-02, 6.4994e-02, 1.0240e-01, ..., -3.2108e-03, 6.4759e-02, 1.3680e-02], [-1.9234e-02, -2.0179e-02, 3.0165e-02, ..., -1.4412e-01, -4.2793e-02, -5.4195e-02], [-2.6318e-02, -2.6606e-02, 9.8404e-02, ..., -5.1031e-02, -2.9973e-02, 1.8722e-02]], grad_fn=&lt;AddBackward0&gt;) 12 Reference[1] Datawhale 参考资料: https://github.com/datawhalechina/team-learning-nlp/blob/master/GNN/Markdown%E7%89%88%E6%9C%AC/4-%E6%B6%88%E6%81%AF%E4%BC%A0%E9%80%92%E5%9B%BE%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C.md [2] PyG官方文档：https://pytorch-geometric.readthedocs.io/en/latest/index.html[3] paper: https://arxiv.org/pdf/2007.02133.pdf[4] paper: https://arxiv.org/pdf/1609.02907.pdf[5] Deep Learning on Graph: https://github.com/datawhalechina/team-learning-nlp/blob/master/GNN/Markdown%E7%89%88%E6%9C%AC/4-%E6%B6%88%E6%81%AF%E4%BC%A0%E9%80%92%E5%9B%BE%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C.md[6] PyG MessagePassing 函数解释: https://blog.csdn.net/qq_41987033/article/details/103377561","link":"/2021/06/19/GNN-2-MessagePassing/"},{"title":"GNN-1-Basic","text":"1. Graph Basic参考资料: “Chapter 2 - Foundations of Graphs, Deep Learning on Graphs” 图的表示定义一（图）： 一个图被记为 $\\mathcal{G}={\\mathcal{V}, \\mathcal{E}}$，其中 $\\mathcal{V}={v_{1}, \\ldots, v_{N}}$ 是数量为 $N=|\\mathcal{V}|$ 的结点的集合， $\\mathcal{E}={e_{1}, \\ldots, e_{M}}$ 是数量为 $M$ 的边的集合。 图用节点表示实体（entities ），用边表示实体间的关系（relations）。 节点和边的信息可以是类别型的（categorical），类别型数据的取值只能是哪一类别。一般称类别型的信息为标签（label）。 节点和边的信息可以是数值型的（numeric），类别型数据的取值范围为实数。一般称类别型的信息为属性（attribute）。 大部分情况中，节点含有信息，边可能含有信息。 定义二（图的邻接矩阵）： 给定一个图$\\mathcal{G}={\\mathcal{V}, \\mathcal{E}}$，其对应的邻接矩阵被记为$\\mathbf{A} \\in{0,1}^{N \\times N}$。$\\mathbf{A}_{i, j}=1$表示存在从结点$v_i$到$v_j$的边，反之表示不存在从结点$v_i$到$v_j$的边。 在无向图中，从结点$v_i$到$v_j$的边存在，意味着从结点$v_j$到$v_i$的边也存在。因而无向图的邻接矩阵是对称的。 在无权图中，各条边的权重被认为是等价的，即认为**各条边的权重为$1$**。 对于有权图，其对应的邻接矩阵通常被记为$\\mathbf{W} \\in{0,1}^{N \\times N}$，其中$\\mathbf{W}{i, j}=w{ij}$表示从结点$v_i$到$v_j$的边的权重。若边不存在时，边的权重为$0$。 一个无向无权图的例子： 其邻接矩阵(Adjacency matrix)为： $$\\mathbf{A}=(\\begin{array}{lllll} 0 &amp; 1 &amp; 0 &amp; 1 &amp; 1 \\\\ 1 &amp; 0 &amp; 1 &amp; 0 &amp; 0 \\\\ 0 &amp; 1 &amp; 0 &amp; 0 &amp; 1 \\\\ 1 &amp; 0 &amp; 0 &amp; 0 &amp; 1 \\\\ 1 &amp; 0 &amp; 1 &amp; 1 &amp; 0 \\end{array})$$ 二、图的属性定义三（结点的度，degree）： 对于有向有权图，结点$v_i$的出度（out degree）等于从$v_i$出发的边的权重之和，结点$v_i$的入度（in degree）等于从连向$v_i$的边的权重之和。 无向图是有向图的特殊情况，结点的出度与入度相等。 无权图是有权图的特殊情况，各边的权重为$1$，那么结点$v_i$的出度（out degree）等于从$v_i$出发的边的数量，结点$v_i$的入度（in degree）等于从连向$v_i$的边的数量。 结点$v_i$的度记为$d(v_i)$，入度记为$d_{in}(v_i)$，出度记为$d_{out}(v_i)$。 定义四（邻接结点，neighbors）： 结点$v_i$的邻接结点为与结点$v_i$直接相连的结点，其被记为**$\\mathcal{N(v_i)}$**。 结点$v_i$的$k$跳远的邻接节点（neighbors with $k$-hop）指的是到结点$v_i$要走$k$步的节点（一个节点的$2$跳远的邻接节点包含了自身）。 定义五（行走，walk）： $walk(v_1, v_2) = (v_1, e_6,e_5,e_4,e_1,v_2)$，这是一次“行走”，它是一次从节点$v_1$出发，依次经过边$e_6,e_5,e_4,e_1$，最终到达节点$v_2$的“行走”。 下图所示为$walk(v_1, v_2) = (v_1, e_6,e_5,e_4,e_1,v_2)$，其中红色数字标识了边的访问序号。 在“行走”中，节点是运行重复的。 定理六： 有一图，其邻接矩阵为 $\\mathbf{A}$, $\\mathbf{A}^{n}$为邻接矩阵的$n$次方，那么$\\mathbf{A}^{n}[i,j]$等于从结点$v_i$到结点$v_j$的长度为$n$的行走的个数。 定义七（路径，path）： “路径”是结点不可重复的“行走”。 定义八（子图，subgraph）： 有一图$\\mathcal{G}={\\mathcal{V}, \\mathcal{E}}$，另有一图$\\mathcal{G}^{\\prime}={\\mathcal{V}^{\\prime}, \\mathcal{E}^{\\prime}}$，其中$\\mathcal{V}^{\\prime} \\in \\mathcal{V}$，$\\mathcal{E}^{\\prime} \\in \\mathcal{E}$并且$\\mathcal{V}^{\\prime}$不包含$\\mathcal{E}^{\\prime}$中未出现过的结点，那么$\\mathcal{G}^{\\prime}$是$\\mathcal{G}$的子图。 定义九（连通分量，connected component）： 给定图$\\mathcal{G}^{\\prime}={\\mathcal{V}^{\\prime}, \\mathcal{E}^{\\prime}}$是图$\\mathcal{G}={\\mathcal{V}, \\mathcal{E}}$的子图。记属于图$\\mathcal{G}$但不属于$\\mathcal{G}^{\\prime}$图的结点集合记为$\\mathcal{V}/\\mathcal{V}^{\\prime}$ 。如果属于$\\mathcal{V}^{\\prime}$的任意结点对之间存在至少一条路径，但不存在一条边连接属于$\\mathcal{V}^{\\prime}$的结点与属于$\\mathcal{V}/\\mathcal{V}^{\\prime}$的结点，那么图$\\mathcal{G}^{\\prime}$是图$\\mathcal{G}$的连通分量。 定义十（连通图，connected graph）： 当一个图只包含一个连通分量，即其自身，那么该图是一个连通图。 定义十一（最短路径，shortest path）： $v_{s}, v_{t} \\in \\mathcal{V}$ 是图$\\mathcal{G}={\\mathcal{V}, \\mathcal{E}}$上的一对结点，结点对 $v_{s}, v_{t} \\in \\mathcal{V}$ 之间所有路径的集合记为 $P_{\\mathrm{st}}$ 。结点对$v_{s}, v_{t}$之间的最短路径 $p_{\\mathrm{s} t}^{\\mathrm{sp}}$为$\\mathcal{P}_{\\mathrm{st}}$ 中长度最短的一条路径，其形式化定义为$$p_{\\mathrm{s} t}^{\\mathrm{sp}}=\\arg \\min {p \\in \\mathcal{P}{\\mathrm{st}}}|p|$$其中，$p$表示$\\mathcal{P}_{\\mathrm{st}}$中的一条路径，$|p|$是路径$p$的长度。 定义十二（直径，diameter）： 给定一个连通图$\\mathcal{G}={\\mathcal{V}, \\mathcal{E}}$，其直径为其所有结点对之间的最短路径的最小值，形式化定义为 $$diameter(\\mathcal{G})=\\max_{v_{s}, v_{t} \\in \\mathcal{V}} \\min_{p \\in \\mathcal{P}_{s t}}|p|$$ 定义十三（拉普拉斯矩阵，Laplacian Matrix）： 给定一个图$\\mathcal{G}={\\mathcal{V}, \\mathcal{E}}$，其邻接矩阵为$A$，其拉普拉斯矩阵定义为$\\mathbf{L=D-A}$，其中$\\mathbf{D=diag(d(v_1), \\cdots, d(v_N))}$。 定义十四（对称归一化的拉普拉斯矩阵，Symmetric normalized Laplacian）： 给定一个图$\\mathcal{G}={\\mathcal{V}, \\mathcal{E}}$，其邻接矩阵为$A$，其规范化的拉普拉斯矩阵定义为 $$\\mathbf{L=D^{-\\frac{1}{2}}(D-A)D^{-\\frac{1}{2}}=I-D^{-\\frac{1}{2}}AD^{-\\frac{1}{2}}}$$ 三、图的种类 同质图（Homogeneous Graph）：只有一种类型的节点和一种类型的边的图。 异质图（Heterogeneous Graph）：存在多种类型的节点和多种类型的边的图。 二部图（Bipartite Graphs）：节点分为两类，只有不同类的节点之间存在边。 四、图结构数据上的机器学习 节点预测：预测节点的类别或某类属性的取值 例子：对是否是潜在客户分类、对游戏玩家的消费能力做预测 边预测：预测两个节点间是否存在链接 例子：Knowledge graph completion、好友推荐、商品推荐 图的预测：对不同的图进行分类或预测图的属性 例子：分子属性预测 节点聚类：检测节点是否形成一个社区 例子：社交圈检测 其他任务 图生成：例如药物发现 图演变：例如物理模拟 …… 五、应用神经网络于图面临的挑战在学习了简单的图论知识，我们再来回顾应用神经网络于图面临的挑战。 过去的深度学习应用中，我们主要接触的数据形式主要是这四种：矩阵、张量、序列（sequence）和时间序列（time series），它们都是规则的结构化的数据。然而图数据是非规则的非结构化的，它具有以下的特点： 任意的大小和复杂的拓扑结构； 没有固定的结点排序或参考点； 通常是动态的，并具有多模态的特征； 图的信息并非只蕴含在节点信息和边的信息中，图的信息还包括了图的拓扑结构。 以往的深度学习技术是为规则且结构化的数据设计的，无法直接用于图数据。应用于图数据的神经网络，要求 适用于不同度的节点； 节点表征的计算与邻接节点的排序无关； 不但能够根据节点信息、邻接节点的信息和边的信息计算节点表征，还能根据图拓扑结构计算节点表征。下面的图片展示了一个需要根据图拓扑结构计算节点表征的例子。图片中展示了两个图，它们同样有俩黄、俩蓝、俩绿，共6个节点，因此它们的节点信息相同；假设边两端节点的信息为边的信息，那么这两个图有一样的边，即它们的边信息相同。但这两个图是不一样的图，它们的拓扑结构不一样。 六、结语在此篇文章中，我们学习了简单的图论知识。对于学习此次组队学习后续的内容，掌握这些图论知识已经足够。如果有小伙伴希望掌握更多的图论知识可以参阅参考文献“Chapter 2 - Foundations of Graphs, Deep Learning on Graphs”。 参考资料 Chapter 2 - Foundations of Graphs, Deep Learning on Graphs 2. Practice先看看torch的version 以及torch使用的cuda version 12import torchtorch.version.cuda, torch.__version__ ('11.1', '1.8.1') 根据它们的版本对 下面的html的名字调整， 这里我用的是torch cuda 是 11.1，torch 是1.8.1所以用torch-1.8.1+cu111.html 12345# pip install torch-scatter -f https://pytorch-geometric.com/whl/torch-1.8.1+cu111.html --no-cache# pip install torch-sparse -f https://pytorch-geometric.com/whl/torch-1.8.1+cu111.html --no-cache# pip install torch-cluster -f https://pytorch-geometric.com/whl/torch-1.8.1+cu111.html --no-cache# pip install torch-spline-conv -f https://pytorch-geometric.com/whl/torch-1.8.1+cu111.html --no-cache# pip install torch-geometric 测试安装好的torch_geometric,另外也下载cora数据进行测试 1234567891011121314151617from torch_geometric.datasets import KarateClubdataset = KarateClub()data = dataset[0] # Get the first graph object.print(data)print('==============================================================')print(f'Number of node features:{data.num_node_features}') # 节点属性的维度print(f'Number of node features: {data.num_features}')# 同样是节点属性的维度print(f'Number of edge features: {data.num_edge_features}') # 边属性的维度print(f'Average node degree: {data.num_edges /data.num_nodes:.2f}') # 平均节点度print(f'if edge indices are ordered and do not contain duplicate entries.: {data.is_coalesced()}') # 是否边是有序的同时不含有重复的边print(f'Number of training nodes: {data.train_mask.sum()}') # 用作训练集的节点print(f'Training node label rate: {int(data.train_mask.sum()) / data.num_nodes:.2f}') #用作训练集的节点的数量print(f'Contains isolated nodes: {data.contains_isolated_nodes()}') # 此图是否包含孤立的节点print(f'Contains self-loops: {data.contains_self_loops()}') # 此图是否包含自环的边print(f'Is undirected: {data.is_undirected()}') # 此图 是否是无向图 Data(edge_index=[2, 156], train_mask=[34], x=[34, 34], y=[34]) ============================================================== Number of node features:34 Number of node features: 34 Number of edge features: 0 Average node degree: 4.59 if edge indices are ordered and do not contain duplicate entries.: True Number of training nodes: 4 Training node label rate: 0.12 Contains isolated nodes: False Contains self-loops: False Is undirected: True 1! wget https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.x --2021-06-14 13:56:45-- https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.x Resolving github.com (github.com)... 140.82.112.4 Connecting to github.com (github.com)|140.82.112.4|:443... connected. HTTP request sent, awaiting response... 302 Found Location: https://raw.githubusercontent.com/kimiyoung/planetoid/master/data/ind.cora.x [following] --2021-06-14 13:56:46-- https://raw.githubusercontent.com/kimiyoung/planetoid/master/data/ind.cora.x Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.111.133, 185.199.108.133, ... Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected. HTTP request sent, awaiting response... 200 OK Length: 22119 (22K) [application/octet-stream] Saving to: ‘ind.cora.x’ ind.cora.x 100%[===================&gt;] 21.60K --.-KB/s in 0s 2021-06-14 13:56:46 (69.8 MB/s) - ‘ind.cora.x’ saved [22119/22119] 123456789from torch_geometric.datasets import Planetoiddataset = Planetoid(root='./dataset/Cora', name='Cora')# Cora()len(dataset)# 1dataset.num_classes# 7dataset.num_node_features# 1433 Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.x Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.tx Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.allx Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.y Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.ty Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.ally Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.graph Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.test.index Processing... Done! 1433 12345678910data = dataset[0]# Data(edge_index=[2, 10556], test_mask=[2708],# train_mask=[2708], val_mask=[2708], x=[2708,1433], y=[2708])data.is_undirected()# Truedata.train_mask.sum().item()# 140data.val_mask.sum().item()# 500data.test_mask.sum().item() 1000 3. Torch_geometric Data set 使用在 from torch_geometric.data import Data调用 Data class之后， Data创建的实例时的输入有以下: data.x:节点的数据矩阵，这个矩阵的大小为 [num_nodes, num_node_features] data.edge_index: 图的连接信息，存放在Coordinate format (COO format) 它的矩阵大小为[2, num_edges] data type是torch.long data.edge_attr: 边的特征矩阵它的shape是 [num_edges, num_edge_features] data.y: 用于训练图神经网络的target，它能够有不同的形状，可以是对应节点特征的target，也可以是对应整个图的target, e.g., node-level targets of shape [num_nodes, *] or graph-level targets of shape [1, *] data.pos: 节点的位置信息，它的shape是 [num_nodes, num_dimensions]， 每一行代表有一个node的位置 详情可以参考官方文档: https://pytorch-geometric.readthedocs.io/en/latest/notes/introduction.html 4. Assignment这里实践一遍， 通过把Data class继承自定义自己的graph dataset， 这个和torch的自定义dataset的创建很相似。 题目: 请通过继承Data 类实现一个类，专门用于表示“机构-作者-论文”的网络。该网络包含“机构“、”作者“和”论文”三类节点，以及“作者-机构“和“作者-论文“两类边。对要实现的类的要求：1）用不同的属性存储不同节点的属性；2）用不同的属性存储不同的边（边没有属性）；3）逐一实现获取不同节点数量的方法。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192from torch_geometric.data import Dataimport numpy as npclass MyGraphData(Data): def __init__(self, data, labels, **args ): \"\"\" data: structural data, a table. Each column of table is one type of node so in this case: we have author, department, paper 3 different types of node. each value in the table represent a data value/ID value in a data node in graph Example: author depart paper 1 1 1 2 1 2 Then author #1 in department 1 writes 1 paper author #2 in department 2 writes 2 papers This is a heterogenuous graph I consider the missing value and isolated nodes in the input data as well. **args: other args passed to the Graph Data \"\"\" super(MyGraphData, self).__init__() self.data = data self.edge_index = None self.x = None self.y = None self.node_list = [] self.labels = labels self.create_graph(self.data) pass def create_graph(self,data): self.edge_index = [] for row in self.data.values: # we get 3 nodes here, each has type / target: department, author, paper respectively dept = row[0] author = row[1] paper = row[2] print(dept, author, paper) dept_node_index = self.__add_node(self.node_list, dept, self.labels['dept']) #if dept !=None else None author_node_index = self.__add_node(self.node_list, author, self.labels['author']) #if author !=None else None paper_node_index = self.__add_node(self.node_list, paper, self.labels['paper']) #if paper !=None else None # add undirected author-department edge if dept_node_index!=None and author_node_index!=None: self.edge_index.append([dept_node_index, author_node_index]) self.edge_index.append([ author_node_index,dept_node_index]) # add undirected author-paper edge if author_node_index!=None and paper_node_index!=None: self.edge_index.append([ author_node_index,paper_node_index]) self.edge_index.append([ paper_node_index,author_node_index]) # first row = from node , second row = to node self.edge_index = torch.tensor(np.array(self.edge_index).T, dtype= torch.long) # gather value of each node into a feature matrix x self.x = torch.tensor([node[0] for node in self.node_list], dtype= torch.float) # gather target for each node self.y = torch.tensor([node[1] for node in self.node_list], dtype = torch.float) return self.x, self.edge_index, self.y def __add_node(self, node_ls, node, target): if node == None or np.isnan(node): return None if node_ls.count([node, target]) ==0: # check if node exists node_ls.append([node, target]) # return the index of the unique node node_idx = node_ls.index([node, target]) return node_idx @property def dept_nums(self): return self.data['dept'].nunique() @property def author_nums(self): return self.data['author'].nunique() @property def paper_nums(self): return self.data['paper'].nunique() @property def isolated_nodes(self): iso_nodes = [] for n in range(len(self.node_list)): if n not in self.edge_index: iso_nodes.append(self.node_list[n]) return iso_nodes 1234# Data(edge_index=[2, 4], x=[3, 1])import pandas as pdx = pd.DataFrame(data= [[1,1,1],[2,2,1],[None, None, 3]], columns=[\"dept\",\"author\",\"paper\"])data = MyGraphData(data = x, labels = {\"dept\":0,\"author\":1,\"paper\":2}) 1.0 1.0 1.0 2.0 2.0 1.0 nan nan 3.0 12345678print(\"Test Results:\")print(f\"Number of authors: {data.author_nums}\")print(f\"Number of papers: {data.paper_nums}\")print(f\"Number of departments: {data.dept_nums}\")print(f\"Number of isolated nodes: {len(data.isolated_nodes)}\")print(f\"Edge index: {data.edge_index}\")print(f\"x representation matrix: {data.x}\")print(f\"Node index [value, label]: {data.node_list}\") Test Results: Number of authors: 2 Number of papers: 2 Number of departments: 2 Number of isolated nodes: 1 Edge index: tensor([[0, 1, 1, 2, 3, 4, 4, 2], [1, 0, 2, 1, 4, 3, 2, 4]]) x representation matrix: tensor([1., 1., 1., 2., 2., 3.]) Node index [value, label]: [[1.0, 0], [1.0, 1], [1.0, 2], [2.0, 0], [2.0, 1], [3.0, 2]] 12","link":"/2021/06/16/GNN-1-Basic/"},{"title":"GNN-5-ClusterGCN","text":"GCN-Task-5-Cluster GCN1. Introduction这篇文章的目的主要是理解 Cluster-GCN这篇文章的内容(要解决的问题，思路，等)和通过代码实现一下Cluster-GCN网络。 Cluster-GCN的文章可以查看: https://arxiv.org/pdf/1905.07953.pdf这篇blog的结构大概如下: 解释Cluster-GCN的内容 Cluster-GCN要解决的问题 基本思路 Cluster-GCN的特点和优缺点 Coding实现 数据集 Cluster-GCN模型 Training and Testing Assignment from datawhale 总结文章的重点 Reference 参考文献 2. Cluster-GCNCluster GCN是由国立台湾大学Wei-Lin Chiang，Google research 的Yang Li和 Samy Bengio， Cho-Jui Hsieh 等人提出的GCN网络(看到Bengio等Google大牛的名字就知道这篇文章很值得一读)。文章的全称是Cluster-GCN: An Efficient Algorithm for Training Deep and Large Graph Convolutional Networks。 从名字就可以知道这个Cluster-GCN的目的是要优化深度学习和超大图网络的效率而提出来的(一般都会涉及时间和空间的复杂度分析)。这里先来看看这篇文章要解决的问题 2.1 Problem to solve首先这篇文章提出和分析了在GNN学习里面的几个问题: Full-batch gradient descent: 这里先做以下定义 node的个数为N Embedding dimension = F GCN的layer层数为L. 那么在用Full batch GD 全梯度下降方法时所需的Space Complexity = O(NFL)并且计算梯度时如果node个数很多有上百万个，每个epoch里面梯度计算也是很慢的。因此这种方法不考虑 Mini-batch SGD Mini-batch SGD算是对Full batch GD的方法通过随机采样降低要计算的梯度的存储空间以及加快了计算的速度。 但是mini-batch SGD会使得时间复杂度随着GCN的层数增加而指数级增长。paper原话是这么解释的: mini-batch SGD introduces a significant computational overhead due to the neighborhood expansion problem—to compute the loss on a single node at layer L, it requires that node’s neighbor nodes’ embeddings at layer L − 1, which again requires their neighbors’ embeddings at layer L − 2 and recursive ones in the downstream layers. This leads to time complexity exponential to the GCN depth. 用我自己的话来理解就是，以下图为例 如果在GCN第i层的第j个节点的embedding vector用$Z_ {i}^{j}$，而第i个节点的neighbor用$N(i)$表示， 那么在第0层输入层节点A的embedding就是$Z_ {A}^{0}$, $N(A)$ = {B,D}。那么我们就有以下的公式: 在第1层 L1, $Z_ {A}^{1} = f(Z_ {B}^{0}, Z_ {D}^{0}, Z_ {A}^{0})$ 在第2层 L2, $Z_ {A}^{2} = f(Z_ {B}^{1}, Z_ {D}^{1}, Z_ {A}^{1})$, 而其中又可以把$Z_ {A}^{1}$ 展开成L1层里面的式子，$Z_ {B}^{1},Z_ {D}^{1}$ 同理 第3层L3, $Z_ {A}^{3} = f(Z_ {B}^{2}, Z_ {D}^{2}, Z_ {A}^{2})$, 计算时和第二步同理可以多次展开成用L1层的输入表示的形式，这么一来可以看到，随着GCN层数增加，neighborhood expansion problem 邻居展开问题就会使得梯度计算更加复杂 这个节点i在第j层梯度计算都取决于第j-1层前面一层的计算就是neighborhood expansion problem 如果GCN层数为L， 节点的平均degree为d，那么我们计算一个节点的梯度就需要$O(d ^L)$个节点embedding信息， 而由于要乘上权重矩阵W, 计算每个node的embedding需要O($F^2$) 的时间。 那么计算一个node的梯度为$O(d ^LF^2)$ VR-GCN(variance reduction GCN), it uses variance reduction technique to reduce the size of neighborhood sampling nodes 2.2 How Cluster-GCN works2.2.1 Cluster-GCN的思想ClusterGCN的想法是我们能不能找到一把种将节点分成多个batch的方式，并将图划分成多个子图，使得表征利用率最大？我们通过将表征利用率的概念与图节点聚类的目标联系起来来回答这个问题。原文:can we design a batch and the corresponding computation subgraph to maximize the embedding utilization? 节点表征的利用率可以反映出计算的效率。考虑到一个batch有多个节点，时间与空间复杂度的计算就不是上面那样简单了，因为不同的节点同样距离远的邻接节点可以是重叠的，于是计算表征的次数可以小于最坏的情况$O(b d^{L})$。为了反映mini-batch SGD的计算效率，Cluster-GCN论文提出了**”表征利用率”的概念来描述计算效率。在训练过程中，如果节点$i$在$l$层的表征$z_{i}^{(l)}$被计算并在$l+1$层的表征计算中被重复使用$u$次，那么我们说$z_{i}^{(l)}$的表征利用率为$u$。对于随机抽样的mini-batch SGD，$u$非常小，因为图通常是大且稀疏的。假设$u$是一个小常数（节点间同样距离的邻接节点重叠率小），那么mini-batch SGD的训练方式对每个batch需要计算$O(b d^{L})$的表征，于是每次参数更新需要$O(b d^{L} F^{2})$的时间，每个epoch需要$O(N d^{L} F^{2})$的时间，这被称为邻域扩展问题**。 相反的是，全梯度下降训练具有最大的表征利用率——每个节点表征将在上一层被重复使用平均节点度次。因此，全梯度下降法在每个epoch中只需要计算$O(N L)$的表征，这意味着平均下来只需要$O(L)$的表征计算就可以获得一个节点的梯度。 2.2.2 简单的Cluster-GCN方法考虑到在每个batch中，我们计算一组节点（记为$\\mathcal{B}$）从第$1$层到第$L$层的表征。由于图神经网络每一层的计算都使用相同的子图$A_{\\mathcal{B}, \\mathcal{B}}$（$\\mathcal{B}$内部的边），所以表征利用率就是这个batch内边的数量，记为$|A_{\\mathcal{B}, \\mathcal{B}}|_{0}$。因此，为了最大限度地提高表征利用率，理想的划分batch的结果是，batch内的边尽可能多，batch之间的边尽可能少。基于这一点，我们将SGD图神经网络训练的效率与图聚类算法联系起来。 现在我们正式学习Cluster-GCN方法。对于一个图$G$，我们将其节点划分为$c$个簇：$\\mathcal{V}=[\\mathcal{V}_ {1}, \\cdots \\mathcal{V}_ {c}]$，其中$\\mathcal{V}{t}$由第$t$个簇中的节点组成，对应的我们有$c$个子图：$$\\bar{G}=[G{1}, \\cdots, G_{c}]=[{\\mathcal{V}_ {1}, \\mathcal{E}_ {1}}, \\cdots,{\\mathcal{V}_ {c}, \\mathcal{E}_ {c}}]\\notag$$其中$\\mathcal{E}{t}$只由$\\mathcal{V}{t}$中的节点之间的边组成。经过节点重组，邻接矩阵被划分为大小为$c^{2}$的块矩阵，如下所示 $$A=\\bar{A}+\\Delta=[\\begin{array}{ccc}A_{11} &amp; \\cdots &amp; A_{1 c} \\\\\\vdots &amp; \\ddots &amp; \\vdots \\\\A_{c 1} &amp; \\cdots &amp; A_{c c}\\end{array}]\\tag{4}$$ 其中 $$\\bar{A}=[\\begin{array}{ccc}A_{11} &amp; \\cdots &amp; 0 \\\\\\vdots &amp; \\ddots &amp; \\vdots \\\\0 &amp; \\cdots &amp; A_{c c}\\end{array}], \\Delta=[\\begin{array}{ccc}0 &amp; \\cdots &amp; A_ {1 c} \\\\\\vdots &amp; \\ddots &amp; \\vdots \\\\A_{c 1} &amp; \\cdots &amp; 0\\end{array}]\\tag{5}$$ 其中，对角线上的块$A_{t t}$是大小为$|\\mathcal{V}{t}| \\times|\\mathcal{V}{t}|$的邻接矩阵，它由$G_{t}$内部的边构成。$\\bar{A}$是图$\\bar{G}$的邻接矩阵。$A_{s t}$由两个簇$\\mathcal{V}_ {s}$和$\\mathcal{V}_ {t}$之间的边构成。$\\Delta$是由$A$的所有非对角线块组成的矩阵。同样，我们可以根据$[\\mathcal{V}_ {1}, \\cdots, \\mathcal{V}_ {c}]$ 划分节点表征矩阵$X$和类别向量 $Y$，得到$[X_{1}, \\cdots, X_{c}]$和$[Y_{1}, \\cdots, Y_{c}]$，其中$X_{t}$和$Y_{t}$分别由$V_{t}$中节点的表征和类别组成。 接下来我们用块对角线邻接矩阵 $\\bar{A}$ 去近似邻接矩阵$A$，这样做的好处是，完整的损失函数（公示(2）)可以根据batch分解成多个部分之和。以$\\bar{A}^{\\prime}$表示归一化后的$\\bar{A}$，最后一层节点表征矩阵可以做如下的分解： $$\\begin{aligned}Z^{(L)} &amp;=\\bar{A}^{\\prime} \\sigma(\\bar{A}^{\\prime} \\sigma(\\cdots \\sigma(\\bar{A}^{\\prime} X W^{(0)}) W^{(1)}) \\cdots) W^{(L-1)} \\\\&amp;=[\\begin{array}{c}\\bar{A}_ {11}^{\\prime} \\sigma(\\bar{A}_ {11}^{\\prime} \\sigma(\\cdots \\sigma(\\bar{A}_ {11}^{\\prime} X_{1} W^{(0)}) W^{(1)}) \\cdots) W^{(L-1)} \\\\\\vdots \\\\\\bar{A}_ {c c}^{\\prime} \\sigma(\\bar{A}_ {c c}^{\\prime} \\sigma(\\cdots \\sigma(\\bar{A}_ {c c}^{\\prime} X_{c} W^{(0)}) W^{(1)}) \\cdots) W^{(L-1)}\\end{array}]\\end{aligned}\\tag{6}$$ 由于$\\bar{A}$是块对角形式（$\\bar{A}_ {t t}^{\\prime}$是$\\bar{A}^{\\prime}$的对角线上的块），于是损失函数可以分解为$$\\mathcal{L}_ {\\bar{A}^{\\prime}}=\\sum_{t} \\frac{|\\mathcal{V}_ {t}|}{N} \\mathcal{L}_ {\\bar{A}_ {t t}^{\\prime}} \\text { and } \\mathcal{L}_ {\\bar{A}_ {t t}^{\\prime}}=\\frac{1}{|\\mathcal{V}_ {t}|} \\sum_{i \\in \\mathcal{V}_ {t}} \\operatorname{loss}(y_{i}, z_{i}^{(L)})\\tag{7}$$ 基于公式(6)和公式(7)，在训练的每一步中，Cluster-GCN首先采样一个簇 $\\mathcal{V}_ {t}$，然后根据$\\mathcal{L}_ {\\bar{A}^{\\prime}_ {tt}}$ 的梯度进行参数更新。这种训练方式，只需要用到子图 $A_{t t}$, $X_{t}$, $Y_{t}$ 以及神经网络权重矩阵 ${W^{(l)}}_{l=1}^{L}$。 实际中，主要的计算开销在神经网络前向过程中的矩阵乘法运算（公式(6)的一个行）和梯度反向传播。 我们使用图节点聚类算法来划分图。图节点聚类算法将图节点分成多个簇，划分结果是簇内边的数量远多于簇间边的数量。如前所述，每个batch的表征利用率相当于簇内边的数量。直观地说，每个节点和它的邻接节点大部分情况下都位于同一个簇中，因此 $L$ 跳（L-hop）远的邻接节点大概率仍然在同一个簇中。由于我们用块对角线近似邻接矩阵$\\bar{A}$代替邻接矩阵$A$，产生的误差与簇间的边的数量$\\Delta$成正比，所以簇间的边越少越好。综上所述，使用图节点聚类算法对图节点划分多个簇的结果，正是我们希望得到的。 在下图，我们可以看到，Cluster-GCN方法可以避免巨大范围的邻域扩展（图右），因为Cluster-GCN方法将邻域扩展限制在簇内。 2.2.3 Cluster-GCN实现过程 从上图可以看到, Cluster-GCN的实现流程基本是 用METIS partition算法对图的节点进行分解成c个cluster 不断迭代， 每次迭代都随机选取q个cluster进行无放回采样node，links并形成subgraph 对subgraph进行预测和gradient计算 用adam进行node的更新和学习 另外 Cluster-GCN方法提出了一个修改版的公式(9)，以更好地保持邻接节点信息和数值范围。首先给原始的$A$添加一个单位矩阵$I$，并进行归一化处理$$\\tilde{A}=(D+I)^{-1}(A+I)\\tag{10}$$然后考虑，$$X^{(l+1)}=\\sigma((\\tilde{A}+\\lambda \\operatorname{diag}(\\tilde{A})) X^{(l)} W^{(l)})\\tag{11}$$ 以上就是Cluster-GCN的每层layer的更新输出公式 2.3 Properties Advantage 先来看看时间和空间复杂度。 在时间上它只和layer层数 L, embedding feature的大小F以及邻接矩阵的非零的行数||A||和节点个数有关， 而空间上和batch的大小相关，相对于传统的GCN，它把指数次降到1次 除了Time, Space complexity外, paper里面提及在大型的图数据里面如PPI, Reddit是最好的(这个可能有调参的因素在里面) Shortage 在收敛性上面, Cluster-GCN在layer数量超过3层之后Accuracy性能没有明显变大，反而layer到了6层之后，开始收敛不好性能变差,如下图所示 Other properties ClusterGCN在做clustering对节点进行cluster partition时特意对比了 random partition和METIS clustering partition两种方法, 以及batch设计时是用多个cluster还是一个cluster作为一个batch。它表明了用METIS和 multiple clusters as a batch 更能使性能提升，loss降低更多。 12 12 12 12 3.1 官方测试源码这里用了Reddit的dataset进行测试 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108import torchimport torch.nn.functional as Ffrom torch.nn import ModuleListfrom tqdm import tqdmfrom torch_geometric.datasets import Redditfrom torch_geometric.data import ClusterData, ClusterLoader, NeighborSamplerfrom torch_geometric.nn import SAGEConvdataset = Reddit('./data/Reddit')data = dataset[0]cluster_data = ClusterData(data, num_parts=1500, recursive=False, save_dir=dataset.processed_dir)train_loader = ClusterLoader(cluster_data, batch_size=20, shuffle=True, num_workers=12)subgraph_loader = NeighborSampler(data.edge_index, sizes=[-1], batch_size=1024, shuffle=False, num_workers=12)class Net(torch.nn.Module): def __init__(self, in_channels, out_channels): super(Net, self).__init__() self.convs = ModuleList( [SAGEConv(in_channels, 128), SAGEConv(128, out_channels)]) def forward(self, x, edge_index): for i, conv in enumerate(self.convs): x = conv(x, edge_index) if i != len(self.convs) - 1: x = F.relu(x) x = F.dropout(x, p=0.5, training=self.training) return F.log_softmax(x, dim=-1) def inference(self, x_all): pbar = tqdm(total=x_all.size(0) * len(self.convs)) pbar.set_description('Evaluating') # Compute representations of nodes layer by layer, using *all* # available edges. This leads to faster computation in contrast to # immediately computing the final representations of each batch. for i, conv in enumerate(self.convs): xs = [] for batch_size, n_id, adj in subgraph_loader: edge_index, _, size = adj.to(device) x = x_all[n_id].to(device) x_target = x[:size[1]] x = conv((x, x_target), edge_index) if i != len(self.convs) - 1: x = F.relu(x) xs.append(x.cpu()) pbar.update(batch_size) x_all = torch.cat(xs, dim=0) pbar.close() return x_alldevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')model = Net(dataset.num_features, dataset.num_classes).to(device)optimizer = torch.optim.Adam(model.parameters(), lr=0.005)def train(): model.train() total_loss = total_nodes = 0 for batch in train_loader: batch = batch.to(device) optimizer.zero_grad() out = model(batch.x, batch.edge_index) loss = F.nll_loss(out[batch.train_mask], batch.y[batch.train_mask]) loss.backward() optimizer.step() nodes = batch.train_mask.sum().item() total_loss += loss.item() * nodes total_nodes += nodes return total_loss / total_nodes@torch.no_grad()def test(): # Inference should be performed on the full graph. model.eval() out = model.inference(data.x) y_pred = out.argmax(dim=-1) accs = [] for mask in [data.train_mask, data.val_mask, data.test_mask]: correct = y_pred[mask].eq(data.y[mask]).sum().item() accs.append(correct / mask.sum().item()) return accsfor epoch in range(1, 31): loss = train() if epoch % 5 == 0: train_acc, val_acc, test_acc = test() print(f'Epoch: {epoch:02d}, Loss: {loss:.4f}, Train: {train_acc:.4f}, ' f'Val: {val_acc:.4f}, test: {test_acc:.4f}') else: print(f'Epoch: {epoch:02d}, Loss: {loss:.4f}') Downloading https://data.dgl.ai/dataset/reddit.zip Extracting data/Reddit/raw/reddit.zip Processing... Done! Computing METIS partitioning... Done! /home/wenkanw/.conda/envs/mlenv/lib/python3.8/site-packages/torch/utils/data/dataloader.py:474: UserWarning: This DataLoader will create 12 worker processes in total. Our suggested max number of worker in current system is 8, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary. warnings.warn(_create_warning_msg( Epoch: 01, Loss: 1.0684 Epoch: 02, Loss: 0.4532 Epoch: 03, Loss: 0.3874 Epoch: 04, Loss: 0.3552 Evaluating: 100%|██████████| 465930/465930 [00:42&lt;00:00, 10886.52it/s] Epoch: 05, Loss: 0.3361, Train: 0.9568, Val: 0.9523, test: 0.9509 Epoch: 06, Loss: 0.3220 Epoch: 07, Loss: 0.3259 Epoch: 08, Loss: 0.3068 Epoch: 09, Loss: 0.2899 Evaluating: 100%|██████████| 465930/465930 [00:43&lt;00:00, 10825.21it/s] Epoch: 10, Loss: 0.2844, Train: 0.9639, Val: 0.9517, test: 0.9523 Epoch: 11, Loss: 0.2850 Epoch: 12, Loss: 0.2700 Epoch: 13, Loss: 0.2705 Epoch: 14, Loss: 0.2696 Evaluating: 100%|██████████| 465930/465930 [00:43&lt;00:00, 10803.24it/s] Epoch: 15, Loss: 0.2793, Train: 0.9637, Val: 0.9524, test: 0.9506 Epoch: 16, Loss: 0.2699 Epoch: 17, Loss: 0.2556 Epoch: 18, Loss: 0.2656 Epoch: 19, Loss: 0.2642 Evaluating: 100%|██████████| 465930/465930 [00:43&lt;00:00, 10797.29it/s] Epoch: 20, Loss: 0.2491, Train: 0.9686, Val: 0.9551, test: 0.9537 Epoch: 21, Loss: 0.2450 Epoch: 22, Loss: 0.2449 Epoch: 23, Loss: 0.2456 Epoch: 24, Loss: 0.2491 Evaluating: 100%|██████████| 465930/465930 [00:43&lt;00:00, 10737.69it/s] Epoch: 25, Loss: 0.2518, Train: 0.9572, Val: 0.9433, test: 0.9389 Epoch: 26, Loss: 0.2430 Epoch: 27, Loss: 0.2342 Epoch: 28, Loss: 0.2297 Epoch: 29, Loss: 0.2270 Evaluating: 100%|██████████| 465930/465930 [00:43&lt;00:00, 10824.00it/s] Epoch: 30, Loss: 0.2319, Train: 0.9716, Val: 0.9514, test: 0.9517 3.2 自己调整的代码1234567891011121314151617import osos.environ[\"WITH_METIS\"] =\"1\"print(os.getenv('WITH_METIS'))import torchfrom tqdm import tqdmfrom torch.nn import ModuleList, functional as Ffrom torch_geometric.nn import SAGEConvfrom torch_geometric.datasets import Redditfrom torch_geometric.data import ClusterData, ClusterLoader, NeighborSamplerdataset = Reddit('./data/Reddit')data = dataset[0]print(dataset.num_classes)print(data.num_nodes)print(data.num_edges)print(data.num_features) 1 41 232965 114615892 602 12345678910111213141516171819202122232425262728293031323334353637383940414243444546class ClusterGCNNet(torch.nn.Module): def __init__(self, in_channels, out_channels,hidden_dim= 128,num_layers=4): super(ClusterGCNNet, self).__init__() # GraphSAGE layer # 这里参考了 原paper里面的4-layer的设定 + 128 hidden units layer_ls=[SAGEConv(in_channels, hidden_dim)] if num_layers &lt;=2: layer_ls += [SAGEConv(hidden_dim, hidden_dim) for i in range(num_layers-2)] layer_ls.append(SAGEConv(hidden_dim, out_channels)) self.convs = ModuleList(layer_ls) def forward(self, x, edge_index): for i, conv in enumerate(self.convs): x = conv(x, edge_index) if i != len(self.convs) - 1: x = F.relu(x) x = F.dropout(x, p=0.5, training=self.training) return F.log_softmax(x, dim=-1) def inference(self, x_all, subgraph_loader,device): pbar = tqdm(total=x_all.size(0) * len(self.convs)) pbar.set_description('Evaluating') # Compute representations of nodes layer by layer, using *all* # available edges. This leads to faster computation in contrast to # immediately computing the final representations of each batch. for i, conv in enumerate(self.convs): xs = [] for batch_size, n_id, adj in subgraph_loader: edge_index, _, size = adj.to(device) x = x_all[n_id].to(device) x_target = x[:size[1]] x = conv((x, x_target), edge_index) if i != len(self.convs) - 1: x = F.relu(x) xs.append(x.cpu()) pbar.update(batch_size) x_all = torch.cat(xs, dim=0) pbar.close() return x_all 1234567891011121314151617181920212223242526272829303132def train(model,optimizer): model.train() total_loss = total_nodes = 0 for batch in train_loader: batch = batch.to(device) optimizer.zero_grad() out = model(batch.x, batch.edge_index) loss = F.nll_loss(out[batch.train_mask], batch.y[batch.train_mask]) loss.backward() optimizer.step() nodes = batch.train_mask.sum().item() total_loss += loss.item() * nodes total_nodes += nodes return total_loss / total_nodes@torch.no_grad()def test(data, model): # Inference should be performed on the full graph. model.eval() out = model.inference(data.x,subgraph_loader,device) y_pred = out.argmax(dim=-1) accs = [] for mask in [data.train_mask, data.val_mask, data.test_mask]: correct = y_pred[mask].eq(data.y[mask]).sum().item() accs.append(correct / mask.sum().item()) return accs 12 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183import torchimport torch.nn.functional as Ffrom torch.nn import ModuleListfrom tqdm import tqdmfrom torch_geometric.datasets import Redditfrom torch_geometric.data import ClusterData, ClusterLoader, NeighborSamplerfrom torch_geometric.nn import SAGEConvimport timeimport datetimeimport pandas as pdclass Net(torch.nn.Module): def __init__(self, in_channels, out_channels): super(Net, self).__init__() self.convs = ModuleList( [SAGEConv(in_channels, 128), SAGEConv(128, out_channels)]) def forward(self, x, edge_index): for i, conv in enumerate(self.convs): x = conv(x, edge_index) if i != len(self.convs) - 1: x = F.relu(x) x = F.dropout(x, p=0.5, training=self.training) return F.log_softmax(x, dim=-1) def inference(self, x_all): pbar = tqdm(total=x_all.size(0) * len(self.convs)) pbar.set_description('Evaluating') # Compute representations of nodes layer by layer, using *all* # available edges. This leads to faster computation in contrast to # immediately computing the final representations of each batch. for i, conv in enumerate(self.convs): xs = [] for batch_size, n_id, adj in subgraph_loader: edge_index, _, size = adj.to(device) x = x_all[n_id].to(device) x_target = x[:size[1]] x = conv((x, x_target), edge_index) if i != len(self.convs) - 1: x = F.relu(x) xs.append(x.cpu()) pbar.update(batch_size) x_all = torch.cat(xs, dim=0) pbar.close() return x_all class ClusterGCNNet(torch.nn.Module): def __init__(self, in_channels, out_channels,hidden_dim= 128,num_layers=4): super(ClusterGCNNet, self).__init__() # GraphSAGE layer # 这里参考了 原paper里面的4-layer的设定 + 128 hidden units layer_ls=[SAGEConv(in_channels, hidden_dim)] if num_layers &lt;=2: layer_ls += [SAGEConv(hidden_dim, hidden_dim) for i in range(num_layers-2)] layer_ls.append(SAGEConv(hidden_dim, out_channels)) self.convs = ModuleList(layer_ls) def forward(self, x, edge_index): for i, conv in enumerate(self.convs): x = conv(x, edge_index) if i != len(self.convs) - 1: x = F.relu(x) x = F.dropout(x, p=0.5, training=self.training) return F.log_softmax(x, dim=-1) def inference(self, x_all): pbar = tqdm(total=x_all.size(0) * len(self.convs)) pbar.set_description('Evaluating') # Compute representations of nodes layer by layer, using *all* # available edges. This leads to faster computation in contrast to # immediately computing the final representations of each batch. for i, conv in enumerate(self.convs): xs = [] for batch_size, n_id, adj in subgraph_loader: edge_index, _, size = adj.to(device) x = x_all[n_id].to(device) x_target = x[:size[1]] x = conv((x, x_target), edge_index) if i != len(self.convs) - 1: x = F.relu(x) xs.append(x.cpu()) pbar.update(batch_size) x_all = torch.cat(xs, dim=0) pbar.close() return x_alldef train(): model.train() total_loss = total_nodes = 0 for batch in train_loader: batch = batch.to(device) optimizer.zero_grad() out = model(batch.x, batch.edge_index) loss = F.nll_loss(out[batch.train_mask], batch.y[batch.train_mask]) loss.backward() optimizer.step() nodes = batch.train_mask.sum().item() total_loss += loss.item() * nodes total_nodes += nodes return total_loss / total_nodes@torch.no_grad()def test(): # Inference should be performed on the full graph. model.eval() out = model.inference(data.x) y_pred = out.argmax(dim=-1) accs = [] for mask in [data.train_mask, data.val_mask, data.test_mask]: correct = y_pred[mask].eq(data.y[mask]).sum().item() accs.append(correct / mask.sum().item()) return accsdataset = Reddit('./data/Reddit')data = dataset[0]num_clusters = [500, 1000,1500, 2000]result = {\"num_cluster\":[],\"partition_t\":[],\"train_t\":[],\"train_acc\":[] ,\"val_acc\":[],\"test_acc\":[]}for num_part in num_clusters: print(f\"Using num cluster: {num_part}\") start_t = time.time() cluster_data = ClusterData(data, num_parts=num_part, recursive=False, save_dir=dataset.processed_dir) end_t = time.time() partition_t =end_t - start_t print(f\"Partition Time: {partition_t} s\") train_loader = ClusterLoader(cluster_data, batch_size=20, shuffle=True, num_workers=12) subgraph_loader = NeighborSampler(data.edge_index, sizes=[-1], batch_size=1024, shuffle=False, num_workers=12) device = torch.device('cuda' if torch.cuda.is_available() else 'cpu') model = ClusterGCNNet(dataset.num_features, dataset.num_classes,hidden_dim= 128,num_layers=2).to(device) optimizer = torch.optim.Adam(model.parameters(), lr=0.005) start_t = time.time() for epoch in range(1, 31): loss = train() if epoch % 5 == 0: train_acc, val_acc, test_acc = test() print(f'Epoch: {epoch:02d}, Loss: {loss:.4f}, Train: {train_acc:.4f}, ' f'Val: {val_acc:.4f}, test: {test_acc:.4f}') else: print(f'Epoch: {epoch:02d}, Loss: {loss:.4f}') end_t = time.time() train_t =end_t - start_t train_t = datetime.timedelta(seconds=train_t) print(f\"Training Time: {train_t} s\") result[\"num_cluster\"].append(num_part) result['partition_t'].append(partition_t) result['train_t'].append(train_t) result[\"train_acc\"].append(train_acc) result[\"val_acc\"].append(val_acc) result[\"test_acc\"].append(test_acc) df_result= pd.DataFrame(result)df_result Using num cluster: 500 Partition Time: 1.7125461101531982 s Epoch: 01, Loss: 1.8346 Epoch: 02, Loss: 0.6928 Epoch: 03, Loss: 0.4838 Epoch: 04, Loss: 0.4009 Evaluating: 100%|██████████| 465930/465930 [00:57&lt;00:00, 8127.96it/s] Epoch: 05, Loss: 0.3632, Train: 0.9541, Val: 0.9542, test: 0.9531 Epoch: 06, Loss: 0.3425 Epoch: 07, Loss: 0.3161 Epoch: 08, Loss: 0.3140 Epoch: 09, Loss: 0.3006 Evaluating: 100%|██████████| 465930/465930 [00:56&lt;00:00, 8212.24it/s] Epoch: 10, Loss: 0.2787, Train: 0.9637, Val: 0.9586, test: 0.9571 Epoch: 11, Loss: 0.2684 Epoch: 12, Loss: 0.2558 Epoch: 13, Loss: 0.2522 Epoch: 14, Loss: 0.2455 Evaluating: 100%|██████████| 465930/465930 [01:00&lt;00:00, 7730.12it/s] Epoch: 15, Loss: 0.2421, Train: 0.9667, Val: 0.9577, test: 0.9565 Epoch: 16, Loss: 0.2649 Epoch: 17, Loss: 0.2377 Epoch: 18, Loss: 0.2277 Epoch: 19, Loss: 0.2190 Evaluating: 100%|██████████| 465930/465930 [00:54&lt;00:00, 8495.90it/s] Epoch: 20, Loss: 0.2151, Train: 0.9698, Val: 0.9568, test: 0.9559 Epoch: 21, Loss: 0.2138 Epoch: 22, Loss: 0.2101 Epoch: 23, Loss: 0.2084 Epoch: 24, Loss: 0.2062 Evaluating: 100%|██████████| 465930/465930 [00:58&lt;00:00, 7914.75it/s] Epoch: 25, Loss: 0.2057, Train: 0.9713, Val: 0.9558, test: 0.9545 Epoch: 26, Loss: 0.2061 Epoch: 27, Loss: 0.2097 Epoch: 28, Loss: 0.2099 Epoch: 29, Loss: 0.2035 Evaluating: 100%|██████████| 465930/465930 [00:59&lt;00:00, 7891.56it/s] Epoch: 30, Loss: 0.1938, Train: 0.9736, Val: 0.9567, test: 0.9560 Training Time: 0:09:43.498993 s Using num cluster: 1000 Partition Time: 3.144443988800049 s Epoch: 01, Loss: 1.4359 Epoch: 02, Loss: 0.5340 Epoch: 03, Loss: 0.4172 Epoch: 04, Loss: 0.3630 Evaluating: 100%|██████████| 465930/465930 [00:58&lt;00:00, 7984.94it/s] Epoch: 05, Loss: 0.3458, Train: 0.9358, Val: 0.9350, test: 0.9327 Epoch: 06, Loss: 0.3326 Epoch: 07, Loss: 0.3068 Epoch: 08, Loss: 0.2879 Epoch: 09, Loss: 0.2861 Evaluating: 100%|██████████| 465930/465930 [00:59&lt;00:00, 7885.83it/s] Epoch: 10, Loss: 0.2757, Train: 0.9618, Val: 0.9536, test: 0.9506 Epoch: 11, Loss: 0.2700 Epoch: 12, Loss: 0.2535 Epoch: 13, Loss: 0.2523 Epoch: 14, Loss: 0.2461 Evaluating: 100%|██████████| 465930/465930 [00:59&lt;00:00, 7813.02it/s] Epoch: 15, Loss: 0.2412, Train: 0.9688, Val: 0.9568, test: 0.9548 Epoch: 16, Loss: 0.2470 Epoch: 17, Loss: 0.2456 Epoch: 18, Loss: 0.2403 Epoch: 19, Loss: 0.2296 Evaluating: 100%|██████████| 465930/465930 [00:54&lt;00:00, 8482.81it/s] Epoch: 20, Loss: 0.2284, Train: 0.9696, Val: 0.9546, test: 0.9545 Epoch: 21, Loss: 0.2276 Epoch: 22, Loss: 0.2219 Epoch: 23, Loss: 0.2224 Epoch: 24, Loss: 0.2233 Evaluating: 100%|██████████| 465930/465930 [00:57&lt;00:00, 8037.32it/s] Epoch: 25, Loss: 0.2241, Train: 0.9698, Val: 0.9533, test: 0.9520 Epoch: 26, Loss: 0.2226 Epoch: 27, Loss: 0.2129 Epoch: 28, Loss: 0.2171 Epoch: 29, Loss: 0.2312 Evaluating: 100%|██████████| 465930/465930 [00:58&lt;00:00, 7972.01it/s] Epoch: 30, Loss: 0.2149, Train: 0.9739, Val: 0.9559, test: 0.9539 Training Time: 0:09:46.712217 s Using num cluster: 1500 Partition Time: 1.5299558639526367 s Epoch: 01, Loss: 1.1529 Epoch: 02, Loss: 0.4863 Epoch: 03, Loss: 0.3942 Epoch: 04, Loss: 0.3567 Evaluating: 100%|██████████| 465930/465930 [01:00&lt;00:00, 7716.31it/s] Epoch: 05, Loss: 0.3439, Train: 0.9559, Val: 0.9524, test: 0.9513 Epoch: 06, Loss: 0.3230 Epoch: 07, Loss: 0.3062 Epoch: 08, Loss: 0.3013 Epoch: 09, Loss: 0.3049 Evaluating: 100%|██████████| 465930/465930 [01:00&lt;00:00, 7741.65it/s] Epoch: 10, Loss: 0.2984, Train: 0.9609, Val: 0.9518, test: 0.9501 Epoch: 11, Loss: 0.2839 Epoch: 12, Loss: 0.2775 Epoch: 13, Loss: 0.2720 Epoch: 14, Loss: 0.2701 Evaluating: 100%|██████████| 465930/465930 [01:01&lt;00:00, 7567.86it/s] Epoch: 15, Loss: 0.2634, Train: 0.9633, Val: 0.9513, test: 0.9495 Epoch: 16, Loss: 0.2851 Epoch: 17, Loss: 0.2721 Epoch: 18, Loss: 0.2635 Epoch: 19, Loss: 0.2489 Evaluating: 100%|██████████| 465930/465930 [01:00&lt;00:00, 7680.40it/s] Epoch: 20, Loss: 0.2617, Train: 0.9645, Val: 0.9495, test: 0.9494 Epoch: 21, Loss: 0.2517 Epoch: 22, Loss: 0.2424 Epoch: 23, Loss: 0.2411 Epoch: 24, Loss: 0.2370 Evaluating: 100%|██████████| 465930/465930 [01:00&lt;00:00, 7762.74it/s] Epoch: 25, Loss: 0.2379, Train: 0.9702, Val: 0.9521, test: 0.9517 Epoch: 26, Loss: 0.2414 Epoch: 27, Loss: 0.2358 Epoch: 28, Loss: 0.2325 Epoch: 29, Loss: 0.2406 Evaluating: 100%|██████████| 465930/465930 [01:00&lt;00:00, 7753.34it/s] Epoch: 30, Loss: 0.2327, Train: 0.9633, Val: 0.9450, test: 0.9433 Training Time: 0:10:01.358439 s Using num cluster: 2000 Computing METIS partitioning... Done! Partition Time: 298.0074031352997 s 3.3 Result这里因为训练时我尝试了不同的cluster的数目，但是都是试了3种不同cluster数目之后就内存溢出，所以这里我尝试跑了2次，分别对比500,1000,1500以及1000,1500,2000两种情况。可以看到随着cluster数目的增多 test accuracy的的变化是先大后小，而且变化的幅度不大一般都在1%左右 12df_result = pd.DataFrame(result)df_result .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } num_cluster partition_t train_t train_acc val_acc test_acc 0 500 1.712546 0 days 00:09:43.498993 0.973591 0.956695 0.955999 1 1000 3.144444 0 days 00:09:46.712217 0.973949 0.955898 0.953916 2 1500 1.529956 0 days 00:10:01.358439 0.963299 0.945030 0.943306 1pd.DataFrame(result) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } num_cluster partition_t train_t train_acc val_acc test_acc 0 2000 2.519914 0 days 00:04:56.476637 0.966519 0.948219 0.947866 1 1500 3.753822 0 days 00:06:28.267962 0.971231 0.953674 0.952803 2 1000 2.336999 0 days 00:07:26.611871 0.971479 0.951534 0.950075 4. Conclusion and Take-away ClusterGCN的成果 对不同的batch，graph partition的方法进行研究 通过batch的设计和Clustering partition(METIS) 对GCN算法的Time, Space Complexity 在大型图数据里面有很大的提升(解决了neighborhood expansion problem问题) 相对于VR-GCN， 训练时间随着DNN 层数变多而增加的幅度不大。 Cluster-GCN的训练时间随着层数增多几乎是线性的。 能够用于训练大型embedding的特征 Note Vanilla 在CS里面的含义 vanilla is the term used to refer when computer software and sometimes also other computing-related systems like computer hardware or algorithms are not customized from their original form 5. Reference[1] Datawhale: https://github.com/datawhalechina/team-learning-nlp/blob/master/GNN/Markdown%E7%89%88%E6%9C%AC/7-%E8%B6%85%E5%A4%A7%E5%9B%BE%E4%B8%8A%E7%9A%84%E8%8A%82%E7%82%B9%E8%A1%A8%E5%BE%81%E5%AD%A6%E4%B9%A0.md[2] Cluster-GCN 原文: https://arxiv.org/pdf/1905.07953.pdf [3] torch_geometric source code 参考: https://github.com/rusty1s/pytorch_geometric/blob/master/examples/cluster_gcn_reddit.py","link":"/2021/07/01/GNN-5-ClusterGCN/"},{"title":"GNN-6-GIN-GraphRepresentation","text":"GNN-Task-6-GIN1. Introduction在前5篇博客里面，考虑的都是node representation节点的表征，并且每个节点都有自己的特征向量代表这个节点对象的信息。而这次我们考虑的是图的表征，而不是节点的表征。我们要用GNN来学习图的特征(包括节点信息和图的结构)，要如何利用节点的特征来计算图的特征。这里我们首先考虑同构图的特征表达和学习。既然是同构图，那么就是涉及一下几个问题： 什么是同构图(isomorphisc graph)? 如何判断两个图是否同构？ 如何衡量两个图的相似度？ 怎么通过GIN(Graph Isomorphism Network)计算Graph Embedding? 对于这几个问题,这篇博客会先回答什么是通构图，然后提及过去测试同构图和图的相似度的方法(Weisfeiler-Lehman Test and subtree kernels)，之后会回答怎么用图神经网络GIN对graph representation进行学习。 2. What is Isomorphic Graph首先什么是同构图？根据WolframMathWorld的解释: Two graphs which contain the same number of graph vertices connected in the same way are said to be isomorphic. 如果两个图是同构就会满足以下特点(其中第2~4点意味两个图的连接方式一样): 两个图的node个数相同 两个图的edge边数相同 两个图的node的degree序列都是一样的(两个图的node degree一一对应) 如果一个图有环，那么总能在另外一个graph找到长度相同的对应的环 以下图为例，下面两个图里面的节点数(4个)和边的连接方式都是一样的(4条边，1个环)，所以下面两个图是同构图 那么问题来了，如果图十分复杂没法用眼来观察时，怎么知道他们是同构图呢? 这就先涉及到一个叫 WL test 和 graph kernel( Weisfeiler-Lehman Test and graph kernel)的测量方法。下面一节解释这个方法 3. Weisfeiler-Lehman Test and subtree kernelWeisfeiler-Lehman Test 的paper:https://www.jmlr.org/papers/volume12/shervashidze11a/shervashidze11a.pdf 3.1 WL testWL test(Weisfeiler-Lehman Test) 是一个用来判断两个图是否同构的方法。 WL Test 的一维形式，类似于图神经网络中的邻接节点聚合。WL Test步骤: 对两个图的节点进行label(一般可以把相同degree的node打上标志) 对每个node进行neighbor节点的label收集，并且排序(排序是为了确保节点表示的单射性，去除顺序带来的影响) 对每个node的节点的序列通过hashing映射到新的label。将聚合的标签散列（hash）成新标签，该过程形式化为下方的公式， 不断重复迭代地聚合节点及其邻接节点的标签 $$L^{h}_ {u} arrow \\operatorname{hash}(L^{h-1}_ {u} + \\sum_{v \\in \\mathcal{N}(U)} L^{h-1}_ {v})$$在上方的公式中，$L^{h}_{u}$表示节点$u$的第$h$次迭代的标签，第$0$次迭代的标签为节点原始标签。 在迭代过程中，发现两个图之间的节点的标签不同时，就可以确定这两个图是非同构的。需要注意的是节点标签可能的取值只能是有限个数。WL测试不能保证对所有图都有效，特别是对于具有高度对称性的图，如链式图、完全图、环图和星图，它会判断错误。 下面c从图a到图d是WL-test的流程图: 3.2 WL subtree kernelWL-test 虽然能判断两个图是否同构但是不能测量两个图的相似度，并且有时候对高度对称的图容易判断错误。这种情况下，我们可以用WL subtree kernel方法对两个图的相似度。它的步骤是 先对两个图做迭代多次的WL-test label，即按照上面一小节的图a~d不断对图进行relabel 把两个图的多次迭代生成的所有label进行个数的统计，并将他们拼接成一个向量 把两个图的向量做inner product内积进行相似度计算，从而得到kernel值 这个kernel值越大代表两个图越相似，但是不一定是同构图。下图是subtree kernel的计算例子$\\phi$代表图的feature vector， $k_ {wlsubtree}$代表kernel值或两个图的相似度 4. Graph Isomorphism Network（GIN）paper: https://arxiv.org/pdf/1810.00826.pdf 4.1 Motivaition根据原文, GNN的设计目前都是根据以往经验以及启发式方法和通过实验试错得到的，但是对GNN的表达能力缺乏了研究分析也缺少理论证明。而这片文章主要描述GNN的表达能力以及对其分析，另外也通过把它和WL-test 结合设计了简单的同构图网络(GIN)。 这篇文章的重点贡献在 理论上说明了GNN和WL-test 在图结构的识别上有同样的能力 搭建了neighbor aggregation 和 readout 函数使GNN有和WL-test相同的识别网络结构的能力 分析了那些GNN （像GCN，GraphSAGE等）识别不好的图结构 设计了和WL-test有同样的网络结构识别能力的GIN网络 4.2 How does GIN work简单的Readout 函数这篇paper也提出了readout函数用于把GNN最后一层的node representation通过把所有node的信息进行聚合从而得到一个graph的embedding。而这个readout函数一般是简单的排列不变性，和节点的特征排序无关, 比如summation， graph-level pooling。$$\\mathbf{h_ {G}} = \\textbf{READOUT}({\\mathbf{h^{k}_ {v} | v \\in \\mathbf{G} }})$$ 图同构网络GIN的构建能实现判断图同构性的图神经网络需要满足，只在两个节点自身标签一样且它们的邻接节点一样时，图神经网络将这两个节点映射到相同的表征，即映射是单射性的。可重复集合（Multisets）指的是元素可重复的集合，元素在集合中没有顺序关系。 一个节点的所有邻接节点是一个可重复集合，一个节点可以有重复的邻接节点，邻接节点没有顺序关系。因此GIN模型中生成节点表征的方法遵循WL Test算法更新节点标签的过程。 在GIN里面node representation的update公式是$$h_ {v}^{k} = \\text{MLP}^{k}((1+ \\epsilon^{k})h_ {v}^{(k-1)} + \\sum_ {u \\in \\mathbf{N}(v)} h_ {u}^{(k-1)})$$ 在生成节点的表征后仍需要执行图池化（或称为图读出）操作得到图表征，最简单的图读出操作是做求和。由于每一层的节点表征都可能是重要的，因此在图同构网络中，不同层的节点表征在求和后被拼接，其数学定义如下，$$h_ {G} = \\text{CONCAT}(\\text{READOUT}({h_{v}^{(k)}|v\\in G})|k=0,1,\\cdots, K)$$采用拼接而不是相加的原因在于不同层节点的表征属于不同的特征空间。未做严格的证明，这样得到的图的表示与WL Subtree Kernel得到的图的表征是等价的。 12 5. Coding我的代码在这里: https://github.com/wenkangwei/Datawhale-Team-Learning/blob/main/GNN/Task-6-GIN/GNN-Task-6-GIN.ipynb 5.1 GIN 数据特征的表示以下代码参考Stanford SNAP的 molecular的例子。 从官方文档，我们可以找到一下的GIN的node embedding, graph representation以及GINConv layer的代码.这里先以stanford的Open Graph Benchmark (OGB) 的原子结构图的为例。OBG library的AtomEncoder和BondEncoder为例。 由于在当前的例子中，节点（原子）和边（化学键）的属性都为离散值，它们属于不同的空间，无法直接将它们融合在一起。通过嵌入（Embedding），我们可以将节点属性和边属性分别映射到一个新的空间，在这个新的空间中，我们就可以对节点和边进行信息融合。在GINConv中，message()函数中的x_j + edge_attr 操作执行了节点信息和边信息的融合。 接下来，我们通过下方的代码中的AtomEncoder类，来分析将节点属性映射到一个新的空间是如何实现的： full_atom_feature_dims 是一个链表list，存储了节点属性向量每一维可能取值的数量，即X[i] 可能的取值一共有full_atom_feature_dims[i]种情况，X为节点属性； 节点属性有多少维，那么就需要有多少个嵌入函数，通过调用torch.nn.Embedding(dim, emb_dim)可以实例化一个嵌入函数； torch.nn.Embedding(dim, emb_dim)，第一个参数dim为被嵌入数据可能取值的数量，第一个参数emb_dim为要映射到的空间的维度。得到的嵌入函数接受一个大于0小于dim的数，输出一个维度为emb_dim的向量。嵌入函数也包含可训练参数，通过对神经网络的训练，嵌入函数的输出值能够表达不同输入值之间的相似性。 在forward()函数中，我们对不同属性值得到的不同嵌入向量进行了相加操作，实现了将节点的的不同属性融合在一起。 BondEncoder类与AtomEncoder类是类似的。 1234567891011121314151617181920212223242526272829303132333435363738394041424344import torchfrom ogb.utils.features import get_atom_feature_dims, get_bond_feature_dims full_atom_feature_dims = get_atom_feature_dims()full_bond_feature_dims = get_bond_feature_dims()class AtomEncoder(torch.nn.Module): def __init__(self, emb_dim): super(AtomEncoder, self).__init__() self.atom_embedding_list = torch.nn.ModuleList() for i, dim in enumerate(full_atom_feature_dims): emb = torch.nn.Embedding(dim, emb_dim) torch.nn.init.xavier_uniform_(emb.weight.data) self.atom_embedding_list.append(emb) def forward(self, x): x_embedding = 0 for i in range(x.shape[1]): x_embedding += self.atom_embedding_list[i](x[:,i]) return x_embeddingclass BondEncoder(torch.nn.Module): def __init__(self, emb_dim): super(BondEncoder, self).__init__() self.bond_embedding_list = torch.nn.ModuleList() for i, dim in enumerate(full_bond_feature_dims): emb = torch.nn.Embedding(dim, emb_dim) torch.nn.init.xavier_uniform_(emb.weight.data) self.bond_embedding_list.append(emb) def forward(self, edge_attr): bond_embedding = 0 for i in range(edge_attr.shape[1]): bond_embedding += self.bond_embedding_list[i](edge_attr[:,i]) return bond_embedding 5.2 GIN 代码下面的GIN的架构根据ogb里面的molecular的例子的AtomEncoder, BondEncoder先对node， edge进行embedding得到节点和边的特征向量。之后基于这些特征向量的encoder搭建了GINConv卷积layer， 基于GINConv layer而搭建的GINNodeEmbedding节点信息更新的网络,以及基于GINNodeEmbedding的节点信息而计算的GINGraphEmbedding的图向量表达。 这些模块可以参考stanford的ogb的mol的源码，不过下面的代码把源码的class的名字更改了一下 AtomEncoder and BondEncoder for mol example: https://github.com/snap-stanford/ogb/blob/master/ogb/graphproppred/mol_encoder.py GINConv layer: https://github.com/snap-stanford/ogb/blob/master/examples/graphproppred/mol/conv.py 基于GINConv layer而搭建的GINNodeEmbedding： https://github.com/snap-stanford/ogb/blob/955f22515dc0e6a8231c0118f3c8760aa26c45a6/examples/graphproppred/mol/conv.py#L68 基于GINNodeEmbedding 而搭建的GINGraphPooling网络，输出是graph embedding： https://github.com/snap-stanford/ogb/blob/master/examples/graphproppred/mol/gnn.py 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164import torchfrom torch import nnfrom torch_geometric.nn import global_add_pool, global_mean_pool, global_max_pool, GlobalAttention, Set2Setimport torchimport torch.nn.functional as Fimport torchfrom torch import nnfrom torch_geometric.nn import MessagePassingimport torch.nn.functional as Ffrom ogb.graphproppred.mol_encoder import AtomEncoder, BondEncoder### GIN convolution along the graph structureclass GINConv(MessagePassing): def __init__(self, emb_dim): ''' emb_dim (int): node embedding dimensionality ''' super(GINConv, self).__init__(aggr = \"add\") self.mlp = nn.Sequential(nn.Linear(emb_dim, emb_dim), nn.BatchNorm1d(emb_dim), nn.ReLU(), nn.Linear(emb_dim, emb_dim)) self.eps = nn.Parameter(torch.Tensor([0])) self.bond_encoder = BondEncoder(emb_dim = emb_dim) def forward(self, x, edge_index, edge_attr): edge_embedding = self.bond_encoder(edge_attr) # 先将类别型边属性转换为边表征 out = self.mlp((1 + self.eps) *x + self.propagate(edge_index, x=x, edge_attr=edge_embedding)) return out def message(self, x_j, edge_attr): return F.relu(x_j + edge_attr) def update(self, aggr_out): return aggr_out# GNN to generate node embeddingclass GINNodeEmbedding(torch.nn.Module): \"\"\" Output: node representations \"\"\" def __init__(self, num_layers, emb_dim, drop_ratio=0.5, JK=\"last\", residual=False): \"\"\"GIN Node Embedding Module\"\"\" super(GINNodeEmbedding, self).__init__() self.num_layers = num_layers self.drop_ratio = drop_ratio self.JK = JK # add residual connection or not self.residual = residual if self.num_layers &lt; 2: raise ValueError(\"Number of GNN layers must be greater than 1.\") self.atom_encoder = AtomEncoder(emb_dim) # List of GNNs self.convs = torch.nn.ModuleList() self.batch_norms = torch.nn.ModuleList() for layer in range(num_layers): self.convs.append(GINConv(emb_dim)) self.batch_norms.append(torch.nn.BatchNorm1d(emb_dim)) def forward(self, batched_data): x, edge_index, edge_attr = batched_data.x, batched_data.edge_index, batched_data.edge_attr # computing input node embedding h_list = [self.atom_encoder(x)] # 先将类别型原子属性转化为原子表征 for layer in range(self.num_layers): h = self.convs[layer](h_list[layer], edge_index, edge_attr) h = self.batch_norms[layer](h) if layer == self.num_layers - 1: # remove relu for the last layer h = F.dropout(h, self.drop_ratio, training=self.training) else: h = F.dropout(F.relu(h), self.drop_ratio, training=self.training) if self.residual: h += h_list[layer] h_list.append(h) # Different implementations of Jk-concat if self.JK == \"last\": node_representation = h_list[-1] elif self.JK == \"sum\": node_representation = 0 for layer in range(self.num_layers + 1): node_representation += h_list[layer] return node_representationclass GINGraphPooling(nn.Module): def __init__(self, num_tasks=1, num_layers=5, emb_dim=300, residual=False, drop_ratio=0, JK=\"last\", graph_pooling=\"sum\"): \"\"\"GIN Graph Pooling Module Args: num_tasks (int, optional): number of labels to be predicted. Defaults to 1 (控制了图表征的维度，dimension of graph representation). num_layers (int, optional): number of GINConv layers. Defaults to 5. emb_dim (int, optional): dimension of node embedding. Defaults to 300. residual (bool, optional): adding residual connection or not. Defaults to False. drop_ratio (float, optional): dropout rate. Defaults to 0. JK (str, optional): 可选的值为\"last\"和\"sum\"。选\"last\"，只取最后一层的结点的嵌入，选\"sum\"对各层的结点的嵌入求和。Defaults to \"last\". graph_pooling (str, optional): pooling method of node embedding. 可选的值为\"sum\"，\"mean\"，\"max\"，\"attention\"和\"set2set\"。 Defaults to \"sum\". Out: graph representation \"\"\" super(GINGraphPooling, self).__init__() self.num_layers = num_layers self.drop_ratio = drop_ratio self.JK = JK self.emb_dim = emb_dim self.num_tasks = num_tasks if self.num_layers &lt; 2: raise ValueError(\"Number of GNN layers must be greater than 1.\") self.gnn_node = GINNodeEmbedding(num_layers, emb_dim, JK=JK, drop_ratio=drop_ratio, residual=residual) # Pooling function to generate whole-graph embeddings if graph_pooling == \"sum\": self.pool = global_add_pool elif graph_pooling == \"mean\": self.pool = global_mean_pool elif graph_pooling == \"max\": self.pool = global_max_pool elif graph_pooling == \"attention\": self.pool = GlobalAttention(gate_nn=nn.Sequential( nn.Linear(emb_dim, emb_dim), nn.BatchNorm1d(emb_dim), nn.ReLU(), nn.Linear(emb_dim, 1))) elif graph_pooling == \"set2set\": self.pool = Set2Set(emb_dim, processing_steps=2) else: raise ValueError(\"Invalid graph pooling type.\") if graph_pooling == \"set2set\": self.graph_pred_linear = nn.Linear(2*self.emb_dim, self.num_tasks) else: self.graph_pred_linear = nn.Linear(self.emb_dim, self.num_tasks) def forward(self, batched_data): h_node = self.gnn_node(batched_data) h_graph = self.pool(h_node, batched_data.batch) output = self.graph_pred_linear(h_graph) if self.training: return output else: # At inference time, relu is applied to output to ensure positivity # 因为预测目标的取值范围就在 (0, 50] 内 return torch.clamp(output, min=0, max=50) 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181import torchfrom torch_geometric.data import DataLoaderimport torch.optim as optimimport torch.nn.functional as Ffrom tqdm import tqdmimport argparseimport timeimport numpy as np### importing OGBfrom ogb.graphproppred import PygGraphPropPredDataset, Evaluatorcls_criterion = torch.nn.BCEWithLogitsLoss()reg_criterion = torch.nn.MSELoss()def train(model, device, loader, optimizer, task_type): model.train() for step, batch in enumerate(tqdm(loader, desc=\"Iteration\")): batch = batch.to(device) if batch.x.shape[0] == 1 or batch.batch[-1] == 0: pass else: pred = model(batch) optimizer.zero_grad() ## ignore nan targets (unlabeled) when computing training loss. is_labeled = batch.y == batch.y if \"classification\" in task_type: loss = cls_criterion(pred.to(torch.float32)[is_labeled], batch.y.to(torch.float32)[is_labeled]) else: loss = reg_criterion(pred.to(torch.float32)[is_labeled], batch.y.to(torch.float32)[is_labeled]) loss.backward() optimizer.step()def eval(model, device, loader, evaluator): model.eval() y_true = [] y_pred = [] for step, batch in enumerate(tqdm(loader, desc=\"Iteration\")): batch = batch.to(device) if batch.x.shape[0] == 1: pass else: with torch.no_grad(): pred = model(batch) y_true.append(batch.y.view(pred.shape).detach().cpu()) y_pred.append(pred.detach().cpu()) y_true = torch.cat(y_true, dim = 0).numpy() y_pred = torch.cat(y_pred, dim = 0).numpy() input_dict = {\"y_true\": y_true, \"y_pred\": y_pred} return evaluator.eval(input_dict)class Args(): def __init__(self): self.device = 0 self.gnn='gin' self.drop_ratio = 0.5 self.num_layers=5 self.emb_dim = 300 self.batch_size = 32 self.epochs = 100 self.num_workers=0 self.dataset= \"ogbg-molhiv\" self.feature=\"full\" self.filename=\"\" def get_terminal_args(): parser = argparse.ArgumentParser(description='GNN baselines on ogbgmol* data with Pytorch Geometrics') parser.add_argument('--device', type=int, default=0, help='which gpu to use if any (default: 0)') parser.add_argument('--gnn', type=str, default='gin-virtual', help='GNN gin, gin-virtual, or gcn, or gcn-virtual (default: gin-virtual)') parser.add_argument('--drop_ratio', type=float, default=0.5, help='dropout ratio (default: 0.5)') parser.add_argument('--num_layer', type=int, default=5, help='number of GNN message passing layers (default: 5)') parser.add_argument('--emb_dim', type=int, default=300, help='dimensionality of hidden units in GNNs (default: 300)') parser.add_argument('--batch_size', type=int, default=32, help='input batch size for training (default: 32)') parser.add_argument('--epochs', type=int, default=100, help='number of epochs to train (default: 100)') parser.add_argument('--num_workers', type=int, default=0, help='number of workers (default: 0)') parser.add_argument('--dataset', type=str, default=\"ogbg-molhiv\", help='dataset name (default: ogbg-molhiv)') parser.add_argument('--feature', type=str, default=\"full\", help='full feature or simple feature') parser.add_argument('--filename', type=str, default=\"\", help='filename to output result (default: )') args = parser.parse_args() return argsdef main(): # Training settings ## if obtain settings from terminal #args = get_terminal_args() args = Args() args.epochs = 5 device = torch.device(\"cuda:\" + str(args.device)) if torch.cuda.is_available() else torch.device(\"cpu\") ### automatic dataloading and splitting dataset = PygGraphPropPredDataset(name = args.dataset) if args.feature == 'full': pass elif args.feature == 'simple': print('using simple feature') # only retain the top two node/edge features dataset.data.x = dataset.data.x[:,:2] dataset.data.edge_attr = dataset.data.edge_attr[:,:2] split_idx = dataset.get_idx_split() ### automatic evaluator. takes dataset name as input evaluator = Evaluator(args.dataset) train_loader = DataLoader(dataset[split_idx[\"train\"]], batch_size=args.batch_size, shuffle=True, num_workers = args.num_workers) valid_loader = DataLoader(dataset[split_idx[\"valid\"]], batch_size=args.batch_size, shuffle=False, num_workers = args.num_workers) test_loader = DataLoader(dataset[split_idx[\"test\"]], batch_size=args.batch_size, shuffle=False, num_workers = args.num_workers) if args.gnn == 'gin': model = GINGraphPooling( num_tasks = dataset.num_tasks, num_layers = args.num_layers, emb_dim = args.emb_dim, drop_ratio = args.drop_ratio,).to(device) else: raise ValueError('Invalid GNN type') optimizer = optim.Adam(model.parameters(), lr=0.001) valid_curve = [] test_curve = [] train_curve = [] for epoch in range(1, args.epochs + 1): print(\"=====Epoch {}\".format(epoch)) print('Training...') train(model, device, train_loader, optimizer, dataset.task_type) print('Evaluating...') train_perf = eval(model, device, train_loader, evaluator) valid_perf = eval(model, device, valid_loader, evaluator) test_perf = eval(model, device, test_loader, evaluator) print({'Train': train_perf, 'Validation': valid_perf, 'Test': test_perf}) train_curve.append(train_perf[dataset.eval_metric]) valid_curve.append(valid_perf[dataset.eval_metric]) test_curve.append(test_perf[dataset.eval_metric]) if 'classification' in dataset.task_type: best_val_epoch = np.argmax(np.array(valid_curve)) best_train = max(train_curve) else: best_val_epoch = np.argmin(np.array(valid_curve)) best_train = min(train_curve) print('Finished training!') print('Best validation score: {}'.format(valid_curve[best_val_epoch])) print('Test score: {}'.format(test_curve[best_val_epoch])) if not args.filename == '': torch.save({'Val': valid_curve[best_val_epoch], 'Test': test_curve[best_val_epoch], 'Train': train_curve[best_val_epoch], 'BestTrain': best_train}, args.filename)if __name__ == \"__main__\": torch.manual_seed(2021) main() Iteration: 0%| | 4/1029 [00:00&lt;00:29, 34.25it/s] =====Epoch 1 Training... Iteration: 100%|██████████| 1029/1029 [00:25&lt;00:00, 40.35it/s] Iteration: 2%|▏ | 17/1029 [00:00&lt;00:06, 160.89it/s] Evaluating... Iteration: 100%|██████████| 1029/1029 [00:06&lt;00:00, 164.41it/s] Iteration: 100%|██████████| 129/129 [00:00&lt;00:00, 131.22it/s] Iteration: 100%|██████████| 129/129 [00:01&lt;00:00, 95.27it/s] Iteration: 0%| | 4/1029 [00:00&lt;00:27, 37.52it/s] {'Train': {'rocauc': 0.6604943642908611}, 'Validation': {'rocauc': 0.682172251616696}, 'Test': {'rocauc': 0.6643677938932772}} =====Epoch 2 Training... Iteration: 100%|██████████| 1029/1029 [00:23&lt;00:00, 43.43it/s] Iteration: 1%|▏ | 14/1029 [00:00&lt;00:07, 136.74it/s] Evaluating... Iteration: 100%|██████████| 1029/1029 [00:06&lt;00:00, 163.67it/s] Iteration: 100%|██████████| 129/129 [00:00&lt;00:00, 165.12it/s] Iteration: 100%|██████████| 129/129 [00:00&lt;00:00, 165.38it/s] Iteration: 0%| | 4/1029 [00:00&lt;00:30, 34.14it/s] {'Train': {'rocauc': 0.4996526571726294}, 'Validation': {'rocauc': 0.498015873015873}, 'Test': {'rocauc': 0.496861662063771}} =====Epoch 3 Training... Iteration: 100%|██████████| 1029/1029 [00:25&lt;00:00, 40.39it/s] Iteration: 2%|▏ | 16/1029 [00:00&lt;00:06, 159.16it/s] Evaluating... Iteration: 100%|██████████| 1029/1029 [00:06&lt;00:00, 163.72it/s] Iteration: 100%|██████████| 129/129 [00:00&lt;00:00, 165.47it/s] Iteration: 100%|██████████| 129/129 [00:00&lt;00:00, 166.95it/s] Iteration: 0%| | 4/1029 [00:00&lt;00:26, 39.08it/s] {'Train': {'rocauc': 0.5092237564450139}, 'Validation': {'rocauc': 0.5308641975308642}, 'Test': {'rocauc': 0.5}} =====Epoch 4 Training... Iteration: 100%|██████████| 1029/1029 [00:25&lt;00:00, 40.43it/s] Iteration: 1%|▏ | 15/1029 [00:00&lt;00:06, 147.41it/s] Evaluating... Iteration: 100%|██████████| 1029/1029 [00:06&lt;00:00, 163.42it/s] Iteration: 100%|██████████| 129/129 [00:00&lt;00:00, 163.16it/s] Iteration: 100%|██████████| 129/129 [00:00&lt;00:00, 164.34it/s] Iteration: 0%| | 4/1029 [00:00&lt;00:27, 37.85it/s] {'Train': {'rocauc': 0.5711309771569805}, 'Validation': {'rocauc': 0.6154636365863217}, 'Test': {'rocauc': 0.5926070414646865}} =====Epoch 5 Training... Iteration: 100%|██████████| 1029/1029 [00:23&lt;00:00, 43.58it/s] Iteration: 1%|▏ | 14/1029 [00:00&lt;00:07, 140.00it/s] Evaluating... Iteration: 100%|██████████| 1029/1029 [00:06&lt;00:00, 163.92it/s] Iteration: 100%|██████████| 129/129 [00:00&lt;00:00, 166.01it/s] Iteration: 100%|██████████| 129/129 [00:00&lt;00:00, 165.55it/s] {'Train': {'rocauc': 0.5522212973644183}, 'Validation': {'rocauc': 0.5579178301979228}, 'Test': {'rocauc': 0.6050927982386682}} Finished training! Best validation score: 0.682172251616696 Test score: 0.6643677938932772 我们可以看到 GIN的训练的收敛性很快基本上3个epochs就稳定下来，然后在AUC=60%左右 抖动。当然我这里没怎么调参和对GNN的Conv卷积成或者pooling层进行替换测试，可能替换之后以及把learning rate调节小一些会效果更好更加明显。 6. Assignment请画出下方图片中的6号、3号和5号节点的从1层到3层的WL子树。 6号、3号和5号节点的从1层到3层的WL子树： 7. Reference[1] https://calcworkshop.com/trees-graphs/isomorphic-graph/ [2] https://mathworld.wolfram.com/IsomorphicGraphs.html [3] Stanford OGB source code: https://github.com/snap-stanford/ogb [4] Datawhale: https://github.com/datawhalechina/team-learning-nlp/blob/6f8cd26d2cff4f791bab7d553b06ed652b75b854/GNN/Markdown%E7%89%88%E6%9C%AC/8-%E5%9F%BA%E4%BA%8E%E5%9B%BE%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E5%9B%BE%E8%A1%A8%E7%A4%BA%E5%AD%A6%E4%B9%A0.md [5] Pytorch_geometric: https://pytorch-geometric.readthedocs.io/en/latest/modules/nn.html#global-pooling-layers [6] Image: https://miro.medium.com/max/1838/1*CWUg-DZBQNONJXuxQdLrFQ.png","link":"/2021/07/05/GNN-6-GIN-GraphRepresentation/"},{"title":"GNN-4-EdgePrediction","text":"GNN-Task-4 GraphDataset and LinkPrediction1. Introduction在GNN图神经网络的使用中，我们经常要根据实际问题和数据去自定义自己的数据集。另外，图神经网络的任务里面也会经常涉及到各种不同的预测任务(node classification, regression, link prediction等)。 所以这次的内容主要介绍一下内容 怎么用torch geometric的InMemoeryDataset 去搭建自定义的数据集来加载基于内存(数据完全放在内存里面)的数据集，并且自己尝试搭建一个来实践 搭建自己的GNN和图数据集进行link的预测 另外鉴于前几次Datawhale对GNN博客作业评分标准不明确(一些优秀作业简单写几段自己的总结就确实实验或者是大部分是代码基本没有理论，他们的分数都比我理论+实践+自己读完paper后的理解总结的博客高分)，我这里就不参考他们提供的内容做了，直接根据自己的理解写自己的东西 2. self-defined PyG Dataset首先这一节主要介绍怎么使用 torch_geometric 里面的InMemoryDataset,之后自己写一个自定义的InMemoryDataset的数据并测试不同的数据集 2.1 Introduction to InMemoryDataset先来看看 InMemoryDataset的官方解释:class InMemoryDataset(root: Optional[str] = None, transform: Optional[Callable] = None, pre_transform: Optional[Callable] = None, pre_filter: Optional[Callable] = None)Dataset base class for creating graph datasets which fit completely into CPU memory. See here for the accompanying tutorial.PARAMETERS root (string, optional) – 用于存放数据集的根目录路径(默认为None) transform (callable, optional) – A function/transform that takes in an torch_geometric.data.Data object and returns a transformed version. pre_transform (callable, optional) – A function/transform that takes in an torch_geometric.data.Data object and returns a transformed version. The data object will be transformed before being saved to disk. (default: None) pre_filter (callable, optional) – A function that takes in an torch_geometric.data.Data object and returns a boolean value, indicating whether the data object should be included in the final dataset. (default: None) 由于InMemoryDataset 继承了torch_geometric.data.Dataset 基类，而这个基类调用init__函数会调用download, process等函数， 以下是是Dataset 和InMemoryDataset 的构建函数__init()的源码， 以及我自己画的执行的流程图 123456789101112131415161718192021class Dataset(torch.utils.data.Dataset): def __init__(self, root: Optional[str] = None, transform: Optional[Callable] = None, pre_transform: Optional[Callable] = None, pre_filter: Optional[Callable] = None): super().__init__() if isinstance(root, str): root = osp.expanduser(osp.normpath(root)) self.root = root self.transform = transform self.pre_transform = pre_transform self.pre_filter = pre_filter self._indices: Optional[Sequence] = None if 'download' in self.__class__.__dict__.keys(): self._download() if 'process' in self.__class__.__dict__.keys(): self._process() 2.2 Practice: InMemoryDataset Class由于datawhale里面的参考代码不通用，这里我重新把这个自定义的InMemoryDataset重新写了使它能够加载不同的dataset比如Cora, Citeseer, PudMed等以及 transductive和inductive的版本, 这个版本的InMemoryDataset 支持： 下载不同的dataset 下载transductive或者inductive版本的dataset 可以更改下载数据集源的url 可以自定义要下载哪些文件,直接把要下载的文件名以list的形式输入到raw_file_names即可 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111import torchfrom torch_geometric.data import InMemoryDataset, download_urlfrom torch_geometric.io import read_planetoid_dataclass MyDatasetMem(InMemoryDataset): def __init__(self, root, url =None, raw_file_names= None, dataset_name=\"citeseer\", version=\"ind\", transform=None, pre_transform=None): r\"\"\"Dataset base class for creating graph datasets which fit completely into CPU memory. See `here &lt;https://pytorch-geometric.readthedocs.io/en/latest/notes/ Args: root (string, optional): Root directory where the dataset should be saved. (default: :obj:`None`) transform (callable, optional): A function/transform that takes in an :obj:`torch_geometric.data.Data` object and returns a transformed version. The data object will be transformed before every access. (default: :obj:`None`) pre_transform (callable, optional): A function/transform that takes in an :obj:`torch_geometric.data.Data` object and returns a transformed version. The data object will be transformed before being saved to disk. (default: :obj:`None`) pre_filter (callable, optional): A function that takes in an :obj:`torch_geometric.data.Data` object and returns a boolean value, indicating whether the data object should be included in the final dataset. (default: :obj:`None`) 自定义参数: url: url to data source website to download dataset raw_file_names: raw_file_name to download dataset_name: name of dataset to download, like PudMed, Cora, Citeseer version: \"ind\": inductive version of dataset (predict on unseen node) \"trans\": transductive version of dataset (train and predict on all visited node) \"\"\" self.dataset_name = dataset_name self.root = root if url != None: self.url = url else: self.url = \"https://github.com/kimiyoung/planetoid/raw/master/data\" if version == \"ind\" or version == \"trans\": self.version = version else: print(f\"No such version: {version}\") self.version = \"\" assert False if raw_file_names!=None: self.raw_files = raw_file_names else: names = ['x', 'tx', 'allx', 'y', 'ty', 'ally', 'graph', 'test.index'] self.raw_files = [f'{self.version}.{self.dataset_name.lower()}.{name}' for name in names] # call the initialize function from InMemoryDataset to # download and process the dataset and save the processed dataset to processed_data.pt file super(MyDatasetMem, self).__init__(root, transform, pre_transform) print(\"Processed data path: \",self.processed_paths[0]) #load the saved processed dataset self.data, self.slices = torch.load(self.processed_paths[0]) @property def raw_dir(self): \"\"\" return directory that stores the raw data files \"\"\" return osp.join(self.root, 'raw') @property def raw_file_names(self): r\"\"\"The name of the files to find in the :obj:`self.raw_dir` folder in order to skip the download.\"\"\" return self.raw_files @property def processed_file_names(self): r\"\"\"The name of the files to find in the :obj:`self.processed_dir` folder in order to skip the processing.\"\"\" # Note: processed data is stored into torch.pt file return f'{self.dataset_name}_processed_data.pt' def download(self): \"\"\" download data from url based on file names to raw directory \"\"\" for file in self.raw_file_names: download_url('{}/{}'.format(self.url, file), self.raw_dir) def process(self): \"\"\" Load dataset from local directory \"\"\" data = read_planetoid_data(self.raw_dir, self.dataset_name.lower()) data = data if self.pre_transform is None else self.pre_transform(data) torch.save(self.collate([data]), self.processed_paths[0]) def info(data,): print(\"Number of classes: \",data.num_classes) print(\"Number of nodes: \",data[0].num_nodes) print(\"Number of edges: \",data[0].num_edges) print(\"Number of features: \",data[0].num_features) mydataset =MyDatasetMem( root= \"./dataset/MyDatasetMed\", url =None, raw_file_names= None, dataset_name=\"citeseer\", version=\"ind\", transform=None, pre_transform=None) info(mydataset) Processing... Done! Processed data path: dataset/MyDatasetMed/processed/citeseer_processed_data.pt Number of classes: 6 Number of nodes: 3327 Number of edges: 9104 Number of features: 3703 12345mydataset_Cora =MyDatasetMem( root= \"./dataset/MyDatasetMed\", url =None, raw_file_names= None, dataset_name=\"Cora\", version=\"ind\", transform=None, pre_transform=None) info(mydataset_Cora) Processing... Done! Processed data path: dataset/MyDatasetMed/processed/Cora_processed_data.pt Number of classes: 7 Number of nodes: 2708 Number of edges: 10556 Number of features: 1433 12345mydataset_PubMed =MyDatasetMem( root= \"./dataset/MyDatasetMed\", url =None, raw_file_names= None, dataset_name=\"pubmed\", version=\"ind\", transform=None, pre_transform=None) info(mydataset_PubMed) Processing... Done! Processed data path: dataset/MyDatasetMed/processed/pubmed_processed_data.pt Number of classes: 3 Number of nodes: 19717 Number of edges: 88648 Number of features: 500 3. Link Prediction图数据里面常见的任务包括了node prediction， edge prediction (classification 或者 regression)以及graph prediction等。这里介绍link prediction。 linke prediction 的目标是要判断在图里面的一对节点在未来会不会形成新的连接关系(有向或者无向)。在实际应用场景， link prediction有大量的重要的应用比如： 预测用户会不会买商品，买和被卖的关系就是一个有向的link 推荐不同机构里面员工相互合作，这个合作关系也是一个连接edge prediction的任务 提取社交网络里面人与人之间的关系，预测哪些人能够相互喜欢，从而相互推荐也是个link prediction的任务 Link prediction在GNN里面的预测原理参考下面一副图， 这个图里面有7个节点, 从A到E。而实线是已经知道的link，而虚线是要预测的link 以GNN表示Node Embedding时，假设每个node有自己的embedding， dimension= 3， 那么我们有 12345678910node A: [a1, a2, a3]node B: [b1, b2, b3]node C: [c1, c2, c3]node D: [d1, d2, d3]node E: [e1, e2, e3]node F: [f1, f2, f3]已经知道的edge:source: [A,B,C,C,A,D]target: [B,C,E,G,D,F] 那么在GNN里面计算一条undirected link的存在的possibility就是直接计算两个nodes之间的相似度之后把它通过logistics变成概率就可以变成这条link出现的可能性。比如计算节点A和节点B的link的出现的概率: $$\\mathbf{P}(AB) = \\sigma(\\mathbf{v}_ a \\cdot \\mathbf{v}_ b)$$ 其中 $\\mathbf{v}_ a \\cdot \\mathbf{v}_ b= a1\\times b1 + a2\\times b2 + a3\\times b3$ 是两个节点的embedding的点乘，即相乘后再用sum来reduce，$\\sigma$是sigmoid 函数把相似度投映到[0, 1]概率区间，代表这条无向边出现的概率。 除了用GNN这种计算相似度的方法来计算link的概率外，也有像matrix factorization矩阵分解来预测边的值以及计算节点的出现frequency来计算节点与节点之间的关系(Common Neighbors之类的算法)。 可以参考以下的博客: https://medium.com/neo4j/link-prediction-with-neo4j-part-1-an-introduction-713aa779fd9 https://www.analyticsvidhya.com/blog/2020/01/link-prediction-how-to-predict-your-future-connections-on-facebook/ 4. Assignment 尝试使用PyG中的不同的网络层去代替GCNConv，以及不同的层数和不同的out_channels，来实现节点分类任务。 在边预测任务中，尝试用torch_geometric.nn.Sequential容器构造图神经网络。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169import torchfrom torch.nn import functional as Ffrom torch.nn import LeakyReLU, Dropout, BCEWithLogitsLoss, Linearfrom torch_geometric.datasets import Planetoidfrom torch_geometric import transforms as T from torch_geometric.utils import train_test_split_edges, negative_samplingfrom torch_geometric.nn import GCNConv, GATConv, Sequentialfrom sklearn.metrics import roc_auc_scorefrom matplotlib import pyplot as pltimport seaborn as snsimport numpy as np# Define Mynetclass MyNet(torch.nn.Module): def __init__(self, in_channels, out_channels,hidden_channels_ls= [128,128], dropout_rate=0.5,random_seed=None): super(MyNet, self).__init__() torch.manual_seed(random_seed) hidden_channels_ls = [in_channels] + hidden_channels_ls self.conv_ls =[] # Task 1: 使用PyG中的不同的网络层去代替GCNConv，以及不同的层数和不同的out_channels，来实现节点分类任务。 for i in range(len(hidden_channels_ls)-1): hd_in_channel = hidden_channels_ls[i] hd_out_channel = hidden_channels_ls[i+1] # input: x and edge_index output: x self.conv_ls.append( (GATConv(hd_in_channel, hd_out_channel), 'x, edge_index -&gt; x') ) self.conv_ls.append( (LeakyReLU(negative_slope=0.5)) ) self.conv_ls.append( (Dropout(dropout_rate)) ) # Task 2: 在边预测任务中，尝试用torch_geometric.nn.Sequential容器构造图神经网络。 self.conv_seq = Sequential('x, edge_index', self.conv_ls) # classifier self.dropout = Dropout( dropout_rate) self.linear = Linear(hidden_channels_ls[-1], out_channels) def encode(self, x, edge_index): x = self.conv_seq(x, edge_index) x = self.dropout(x) x = self.linear(x) return x def decode(self, z, pos_edge_index, neg_edge_index): edge_index = torch.cat([pos_edge_index, neg_edge_index], dim=-1) # multiply source and target node embedding and sum up all embedding feature values along column # It does dot product operation here to compute similarity between source and target nodes return (z[edge_index[0]] * z[edge_index[1]]).sum(dim=-1) def decode_all(self, z): prob_adj = z @ z.t() return (prob_adj &gt; 0).nonzero(as_tuple=False).t() def get_link_labels(pos_edge_index, neg_edge_index): \"\"\" Concatenate positive edge and sampled negative edge into a tensor and mark them as 1 or 0 label \"\"\" num_links = pos_edge_index.size(1) + neg_edge_index.size(1) link_labels = torch.zeros(num_links, dtype=torch.float) link_labels[:pos_edge_index.size(1)] = 1. return link_labelsdef train(data, model, optimizer, loss_fn,device): model.train() neg_edge_index = negative_sampling( edge_index=data.train_pos_edge_index, num_nodes=data.num_nodes, num_neg_samples=data.train_pos_edge_index.size(1)) optimizer.zero_grad() z = model.encode(data.x.to(device), data.train_pos_edge_index.to(device)) link_logits = model.decode(z, data.train_pos_edge_index, neg_edge_index) #print(\"link shape: \",link_logits.shape) link_labels = get_link_labels(data.train_pos_edge_index, neg_edge_index).to(data.x.device) loss = loss_fn(link_logits, link_labels) loss.backward() optimizer.step() return loss@torch.no_grad()def test(data, model,device): model.eval() z = model.encode(data.x, data.train_pos_edge_index) results = [] for prefix in ['val', 'test']: pos_edge_index = data[f'{prefix}_pos_edge_index'].to(device) neg_edge_index = data[f'{prefix}_neg_edge_index'].to(device) # make prediction on link from model link_logits = model.decode(z, pos_edge_index, neg_edge_index).to(device) # convert prediction values to possibility link_probs = link_logits.sigmoid() # compute loss for link link_labels = get_link_labels(pos_edge_index, neg_edge_index) results.append(roc_auc_score(link_labels.cpu(), link_probs.cpu())) return resultsdef main(dataset): # load self-defined dataset: 这里调用之前学的 自定义的 InMemoryDataset epochs = 300 random_seed = 2021 np.random.RandomState(random_seed) torch.manual_seed(random_seed) data = dataset[0] data.train_mask = data.val_mask = data.test_mask = data.y = None device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") print(f\"Using device: {device}\") print() data = train_test_split_edges(data,val_ratio = 0.05, test_ratio = 0.1) print(\"Data info:\") print(data) data.to(device) # define loss, optimizer and model: 这里调用选取要用的loss，模型，优化器等配置 model = MyNet( in_channels = data.num_features, out_channels = 64, hidden_channels_ls= [128,128], dropout_rate=0.5, random_seed= random_seed).to(device) loss_fn = BCEWithLogitsLoss() optimizer = torch.optim.Adam(model.parameters(), lr = 1e-2) # train model： 模型训练 best_val_auc = test_auc = 0 loss_ls = [0] val_auc_ls = [0] for epoch in range(1, epochs+1): loss = train(data, model, optimizer, loss_fn, device) loss_ls.append(loss.detach().item()) # validation and test val_auc, tmp_test_auc = test(data, model,device) #val_auc =val_auc.detach() #tmp_test_auc = tmp_test_auc.detach() val_auc_ls.append(val_auc) if best_val_auc &lt; val_auc: best_val_auc = val_auc test_auc = tmp_test_auc if epoch%20 ==0: print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}, Val: {val_auc:.4f}, ' f'Test: {test_auc:.4f}') # Make final prediction on all links z = model.encode(data.x, data.train_pos_edge_index) final_edge_index = model.decode_all(z) torch.cuda.empty_cache() t =[0] + [i+1 for i in range(epochs)] fig, ax = plt.subplots(2, figsize=(10,10)) sns.lineplot(t, loss_ls, ax= ax[0]) sns.lineplot(t, val_auc_ls, ax= ax[1]) ax[0].set_xlabel(\"epochs\") ax[1].set_xlabel(\"epochs\") ax[0].set_ylabel(\"loss\") ax[1].set_ylabel(\"AUC\") ax[0].set_title(\"BCELoss\") ax[1].set_title(\"Validation AUC\") plt.show() 12mydataset =MyDatasetMem( root= \"./dataset/MyDatasetMed\", url =None, raw_file_names= None, dataset_name=\"citeseer\", version=\"ind\", transform=T.NormalizeFeatures()) Processed data path: dataset/MyDatasetMed/processed/processed_data.pt 1main(mydataset) Using device: cuda Data info: Data(test_neg_edge_index=[2, 455], test_pos_edge_index=[2, 455], train_neg_adj_mask=[3327, 3327], train_pos_edge_index=[2, 7740], val_neg_edge_index=[2, 227], val_pos_edge_index=[2, 227], x=[3327, 3703]) Epoch: 020, Loss: 0.6077, Val: 0.7825, Test: 0.7407 Epoch: 040, Loss: 0.5646, Val: 0.7975, Test: 0.7761 Epoch: 060, Loss: 0.5200, Val: 0.8195, Test: 0.8342 Epoch: 080, Loss: 0.5082, Val: 0.8164, Test: 0.8379 Epoch: 100, Loss: 0.5093, Val: 0.8038, Test: 0.8379 Epoch: 120, Loss: 0.5089, Val: 0.8023, Test: 0.8379 Epoch: 140, Loss: 0.5087, Val: 0.7941, Test: 0.8379 Epoch: 160, Loss: 0.4914, Val: 0.8296, Test: 0.8425 Epoch: 180, Loss: 0.4879, Val: 0.8310, Test: 0.8361 Epoch: 200, Loss: 0.4980, Val: 0.8348, Test: 0.8297 Epoch: 220, Loss: 0.4865, Val: 0.8287, Test: 0.8265 Epoch: 240, Loss: 0.4870, Val: 0.8295, Test: 0.8265 Epoch: 260, Loss: 0.4869, Val: 0.8362, Test: 0.8459 Epoch: 280, Loss: 0.4825, Val: 0.8473, Test: 0.8488 Epoch: 300, Loss: 0.4821, Val: 0.8434, Test: 0.8483 如下方代码所示，我们以data.train_pos_edge_index为实际参数来进行训练集负样本采样，但这样采样得到的负样本可能包含一些验证集的正样本与测试集的正样本，即可能将真实的正样本标记为负样本，由此会产生冲突。但我们还是这么做，这是为什么？ 1234neg_edge_index = negative_sampling( edge_index=data.train_pos_edge_index, num_nodes=data.num_nodes, num_neg_samples=data.train_pos_edge_index.size(1)) 这里的原因有可能是因为在negative_sampling里面设置的force_undirect 默认是False,即采样的边是有向的，但是我们输入的采样的边是无向的，这样在negative sampling时把正样本(已经知道的边)也采样进去可以考虑一条边的不同方向。 5. Reference[1] Datawhale https://github.com/datawhalechina/team-learning-nlp/blob/master/GNN/Markdown%E7%89%88%E6%9C%AC/6-2-%E8%8A%82%E7%82%B9%E9%A2%84%E6%B5%8B%E4%B8%8E%E8%BE%B9%E9%A2%84%E6%B5%8B%E4%BB%BB%E5%8A%A1%E5%AE%9E%E8%B7%B5.md[2] PyG 源码: https://github.com/rusty1s/pytorch_geometric/blob/master/examples/link_pred.py [3] https://pytorch-geometric.readthedocs.io/en/latest/modules/nn.html[4] https://medium.com/neo4j/link-prediction-with-neo4j-part-1-an-introduction-713aa779fd9[5] https://www.analyticsvidhya.com/blog/2020/01/link-prediction-how-to-predict-your-future-connections-on-facebook/","link":"/2021/06/27/GNN-4-EdgePrediction/"},{"title":"ICarus-Statistic-Plugin","text":"ICarus 网络流量统计工具在Icarus的插件里面，有可以用来统计浏览次数的统计工具，这里介绍4种。这些工具的配置的都是在 theme-&gt;icarus -&gt; _config.yml的文件里面配置. 这次的文章参考了这篇文章 https://ji2xpro.github.io/88bd6454/ 百度统计安装指南 登录百度统计。在“管理 &gt; 网站列表”页面上点击“新增网站”按钮并填写“网站域名”，“网站首页”等站点信息。点击“确定”完成站点创建。 在下一页面上找到hm.baidu.com/hm.js?后的ID并填写到主题配置的plugins -&gt;baidu_analytics -&gt; tracking_id值中。 不蒜子统计安装指南 在theme/icarus/_config.yml文件 将plugins -&gt; busuanzi设置为true来开启不蒜子访客计数器并在网页尾部和每篇博文的头部展示访问次数。 12plugins: busuanzi: true CNZZ统计器安装指南 登录友盟+在友盟+工作台首页点击“创建新应用” &gt; “Web应用”。然后输入“网站名称”，“网站域名”，“网站首页”等站点信息信息。 完成后点击“确认添加站点。 在获取统计代码界面找到“文字样式”一栏下的HTML代码。分别将其中id与web_id的值复制到主题配置的plugins &gt; cnzz &gt; id和web_id中。比如如下的统计安装代码： 1&lt;script type=\"text/javascript\" src=\"https://s9.cnzz.com/z_stat.php?id=123456789000&amp;web_id=123456789000\"&gt;&lt;/script&gt; 在_config.yml文件里面填写 1234plugins: cnzz: id: 123456789000 web_id: 123456789000 Google Analytics安装指南 登录Google Analytics并点击左侧的”管理“(Admin)进入管理界面。在管理界面上点击”创建资产“(Create Property)按钮，选择“测量的应用类型”(What do you want to measure?)为Web。然后点击”继续“(Continue)按钮。 然后，填写“网站名称”(Website Name)，“URL地址”(Website URL)，“行业分类”(Industry Category)，以及“报告时区”(Reporting Time Zone)等信息。点击”创建“(Create)按钮完成资产创建。 在”追踪代码“(Tracking Code)界面上找到”Tracking ID”的值，例如”UA-12345678-0”. 将其填写到主题配置的plugins &gt; google_analytics &gt; tracking_id即可开启Google Analytics插件。 再把id 复杂到theme/icarus/_config.yml里面 123plugins: google_analytics: tracking_id: UA-12345678-0 参考文章 Reference[1] https://ji2xpro.github.io/88bd6454/","link":"/2021/07/02/ICarus-Statistic-Plugin/"},{"title":"GNN-7-Mini-Batching-Practice","text":"1. Introduction在前面的学习中我们只接触了数据可全部储存于内存的数据集，这些数据集对应的数据集类在创建对象时就将所有数据都加载到内存。然而在一些应用场景中，数据集规模超级大，我们很难有足够大的内存完全存下所有数据。因此需要一个按需加载样本到内存的数据集类。在此上半节内容中，我们将学习为一个包含上千万个图样本的数据集构建一个数据集类。 2. Dataset Base Class在PyG中，我们通过继承torch_geometric.data.Dataset基类来自定义一个按需加载样本到内存的数据集类。此基类与Torchvision的Dataset 类的概念密切相关，这与第6节中介绍的torch_geometric.data.InMemoryDataset基类是一样的。 继承torch_geometric.data.InMemoryDataset基类要实现的方法，继承此基类同样要实现，此外还需要实现以下方法： len()：返回数据集中的样本的数量。 get()：实现加载单个图的操作。注意：在内部，__getitem__()返回通过调用get()来获取Data对象，并根据transform参数对它们进行选择性转换。 下面让我们通过一个简化的例子看继承torch_geometric.data.Dataset基类的规范： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546import os.path as ospimport torchfrom torch_geometric.data import Dataset, download_urlclass MyOwnDataset(Dataset): def __init__(self, root, transform=None, pre_transform=None): super(MyOwnDataset, self).__init__(root, transform, pre_transform) @property def raw_file_names(self): # list of file names of raw partial graphs return ['some_file_1', 'some_file_2', ...] @property def processed_file_names(self): # list of file names of processed partial graphs return ['data_1.pt', 'data_2.pt', ...] def download(self): # Download to `self.raw_dir`. path = download_url(url, self.raw_dir) ... def process(self): i = 0 # process each raw file to get corresponding processed file for raw_path in self.raw_paths: # Read data from `raw_path`. data = Data(...) if self.pre_filter is not None and not self.pre_filter(data): continue if self.pre_transform is not None: data = self.pre_transform(data) torch.save(data, osp.join(self.processed_dir, 'data_{}.pt'.format(i))) i += 1 def len(self): return len(self.processed_file_names) def get(self, idx): data = torch.load(osp.join(self.processed_dir, 'data_{}.pt'.format(idx))) return data 其中，每个Data对象在process()方法中单独被保存，并在get()中通过指定索引进行加载。 3. 合并小图组成大图图可以有任意数量的节点和边，它不是规整的数据结构，因此对图数据封装成批的操作与对图像和序列等数据封装成批的操作不同。PyTorch Geometric中采用的将多个图封装成批的方式是，将小图作为连通组件（connected component）的形式合并，构建一个大图。于是小图的邻接矩阵存储在大图邻接矩阵的对角线上。大图的邻接矩阵、属性矩阵、预测目标矩阵分别为：$$\\begin{split}\\mathbf{A} = \\begin{bmatrix} \\mathbf{A}_1 &amp; &amp; \\\\ &amp; \\ddots &amp; \\\\ &amp; &amp; \\mathbf{A}_n \\end{bmatrix}, \\qquad \\mathbf{X} = \\begin{bmatrix} \\mathbf{X}_1 \\\\ \\vdots \\\\ \\mathbf{X}_n \\end{bmatrix}, \\qquad \\mathbf{Y} = \\begin{bmatrix} \\mathbf{Y}_1 \\\\ \\vdots \\\\ \\mathbf{Y}_n \\end{bmatrix}.\\end{split}$$ 此方法有以下关键的优势： 依靠消息传递方案的GNN运算不需要被修改，因为消息仍然不能在属于不同图的两个节点之间交换。 没有额外的计算或内存的开销。例如，这个批处理程序的工作完全不需要对节点或边缘特征进行任何填充。请注意，邻接矩阵没有额外的内存开销，因为它们是以稀疏的方式保存的，只保留非零项，即边。 通过torch_geometric.data.DataLoader类，多个小图被封装成一个大图。torch_geometric.data.DataLoader是PyTorch的DataLoader的子类，它覆盖了collate()函数，该函数定义了一列表的样本是如何封装成批的。因此，所有可以传递给PyTorch DataLoader的参数也可以传递给PyTorch Geometric的 DataLoader，例如，num_workers。 4. Pairs of Graphs如果你想在一个Data对象中存储多个图，例如用于图对等应用，我们需要确保所有这些图的正确封装成批行为。例如，考虑将两个图，一个源图$G_s$和一个目标图$G_t$，存储在一个Data类中，即 12345678910111213141516171819202122232425262728293031323334353637383940414243444546import torchfrom torch_geometric.data import Data, Datasetfrom torch_geometric.data import DataLoaderclass PairData(Data): def __init__(self, edge_index_s, x_s, edge_index_t, x_t): super(PairData, self).__init__() # source graph self.edge_index_s = edge_index_s self.x_s = x_s # target graph self.edge_index_t = edge_index_t self.x_t = x_t def __inc__(self, key, value): if key == 'edge_index_s': return self.x_s.size(0) if key == 'edge_index_t': return self.x_t.size(0) else: return super().__inc__(key, value)edge_index_s = torch.tensor([ [0, 0, 0, 0], [1, 2, 3, 4],])x_s = torch.randn(5, 16) # 5 nodes.edge_index_t = torch.tensor([ [0, 0, 0], [1, 2, 3],])x_t = torch.randn(4, 16) # 4 nodes.data = PairData(edge_index_s, x_s, edge_index_t, x_t)data_list = [data, data]loader = DataLoader(data_list, batch_size=2)batch = next(iter(loader))print(batch)print(batch.edge_index_s)print(batch.edge_index_t) Batch(edge_index_s=[2, 8], edge_index_t=[2, 6], x_s=[10, 16], x_t=[8, 16]) tensor([[0, 0, 0, 0, 5, 5, 5, 5], [1, 2, 3, 4, 6, 7, 8, 9]]) tensor([[0, 0, 0, 4, 4, 4], [1, 2, 3, 5, 6, 7]]) 在新的维度上做拼接有时，Data对象的属性需要在一个新的维度上做拼接（如经典的封装成批），例如，图级别属性或预测目标。具体来说，形状为[num_features]的属性列表应该被返回为[num_examples, num_features]，而不是[num_examples * num_features]。PyTorch Geometric通过在__cat_dim__()中返回一个None的连接维度来实现这一点。 12345678910111213141516171819class MyData(Data): def __cat_dim__(self, key, item): if key == 'foo': return None else: return super().__cat_dim__(key, item)edge_index = torch.tensor([ [0, 1, 1, 2], [1, 0, 2, 1],])foo = torch.randn(16)data = MyData(edge_index=edge_index, foo=foo)data_list = [data, data]loader = DataLoader(data_list, batch_size=2)batch = next(iter(loader))print(batch) WARNING:root:The number of nodes in your data object can only be inferred by its edge indices, and hence may result in unexpected batch-wise behavior, e.g., in case there exists isolated nodes. Please consider explicitly setting the number of nodes for this data object by assigning it to data.num_nodes. WARNING:root:The number of nodes in your data object can only be inferred by its edge indices, and hence may result in unexpected batch-wise behavior, e.g., in case there exists isolated nodes. Please consider explicitly setting the number of nodes for this data object by assigning it to data.num_nodes. WARNING:root:The number of nodes in your data object can only be inferred by its edge indices, and hence may result in unexpected batch-wise behavior, e.g., in case there exists isolated nodes. Please consider explicitly setting the number of nodes for this data object by assigning it to data.num_nodes. WARNING:root:The number of nodes in your data object can only be inferred by its edge indices, and hence may result in unexpected batch-wise behavior, e.g., in case there exists isolated nodes. Please consider explicitly setting the number of nodes for this data object by assigning it to data.num_nodes. Batch(batch=[6], edge_index=[2, 8], foo=[2, 16], ptr=[3]) 5. Advanced Mini-Batching for large-scale graphPCQM4M-LSC是一个分子图的量子特性回归数据集，它包含了3,803,453个图。 注意以下代码依赖于ogb包，通过pip install ogb命令可安装此包。ogb文档可见于Get Started | Open Graph Benchmark (stanford.edu)。 在生成一个该数据集类的对象时，程序首先会检查指定的文件夹下是否存在data.csv.gz文件，如果不在，则会执行download方法，这一过程是在运行super类的__init__方法中发生的。然后程序继续执行__init__方法的剩余部分，读取data.csv.gz文件，获取存储图信息的smiles格式的字符串，以及回归预测的目标homolumogap。我们将由smiles格式的字符串转成图的过程在get()方法中实现，这样我们在生成一个DataLoader变量时，通过指定num_workers可以实现并行执行生成多个图。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164import torchfrom torch import nnfrom torch_geometric.nn import global_add_pool, global_mean_pool, global_max_pool, GlobalAttention, Set2Setimport torchimport torch.nn.functional as Fimport torchfrom torch import nnfrom torch_geometric.nn import MessagePassingimport torch.nn.functional as Ffrom ogb.graphproppred.mol_encoder import AtomEncoder, BondEncoder### GIN convolution along the graph structureclass GINConv(MessagePassing): def __init__(self, emb_dim): ''' emb_dim (int): node embedding dimensionality ''' super(GINConv, self).__init__(aggr = \"add\") self.mlp = nn.Sequential(nn.Linear(emb_dim, emb_dim), nn.BatchNorm1d(emb_dim), nn.ReLU(), nn.Linear(emb_dim, emb_dim)) self.eps = nn.Parameter(torch.Tensor([0])) self.bond_encoder = BondEncoder(emb_dim = emb_dim) def forward(self, x, edge_index, edge_attr): edge_embedding = self.bond_encoder(edge_attr) # 先将类别型边属性转换为边表征 out = self.mlp((1 + self.eps) *x + self.propagate(edge_index, x=x, edge_attr=edge_embedding)) return out def message(self, x_j, edge_attr): return F.relu(x_j + edge_attr) def update(self, aggr_out): return aggr_out# GNN to generate node embeddingclass GINNodeEmbedding(torch.nn.Module): \"\"\" Output: node representations \"\"\" def __init__(self, num_layers, emb_dim, drop_ratio=0.5, JK=\"last\", residual=False): \"\"\"GIN Node Embedding Module\"\"\" super(GINNodeEmbedding, self).__init__() self.num_layers = num_layers self.drop_ratio = drop_ratio self.JK = JK # add residual connection or not self.residual = residual if self.num_layers &lt; 2: raise ValueError(\"Number of GNN layers must be greater than 1.\") self.atom_encoder = AtomEncoder(emb_dim) # List of GNNs self.convs = torch.nn.ModuleList() self.batch_norms = torch.nn.ModuleList() for layer in range(num_layers): self.convs.append(GINConv(emb_dim)) self.batch_norms.append(torch.nn.BatchNorm1d(emb_dim)) def forward(self, batched_data): x, edge_index, edge_attr = batched_data.x, batched_data.edge_index, batched_data.edge_attr # computing input node embedding h_list = [self.atom_encoder(x)] # 先将类别型原子属性转化为原子表征 for layer in range(self.num_layers): h = self.convs[layer](h_list[layer], edge_index, edge_attr) h = self.batch_norms[layer](h) if layer == self.num_layers - 1: # remove relu for the last layer h = F.dropout(h, self.drop_ratio, training=self.training) else: h = F.dropout(F.relu(h), self.drop_ratio, training=self.training) if self.residual: h += h_list[layer] h_list.append(h) # Different implementations of Jk-concat if self.JK == \"last\": node_representation = h_list[-1] elif self.JK == \"sum\": node_representation = 0 for layer in range(self.num_layers + 1): node_representation += h_list[layer] return node_representationclass GINGraphPooling(nn.Module): def __init__(self, num_tasks=1, num_layers=5, emb_dim=300, residual=False, drop_ratio=0, JK=\"last\", graph_pooling=\"sum\"): \"\"\"GIN Graph Pooling Module Args: num_tasks (int, optional): number of labels to be predicted. Defaults to 1 (控制了图表征的维度，dimension of graph representation). num_layers (int, optional): number of GINConv layers. Defaults to 5. emb_dim (int, optional): dimension of node embedding. Defaults to 300. residual (bool, optional): adding residual connection or not. Defaults to False. drop_ratio (float, optional): dropout rate. Defaults to 0. JK (str, optional): 可选的值为\"last\"和\"sum\"。选\"last\"，只取最后一层的结点的嵌入，选\"sum\"对各层的结点的嵌入求和。Defaults to \"last\". graph_pooling (str, optional): pooling method of node embedding. 可选的值为\"sum\"，\"mean\"，\"max\"，\"attention\"和\"set2set\"。 Defaults to \"sum\". Out: graph representation \"\"\" super(GINGraphPooling, self).__init__() self.num_layers = num_layers self.drop_ratio = drop_ratio self.JK = JK self.emb_dim = emb_dim self.num_tasks = num_tasks if self.num_layers &lt; 2: raise ValueError(\"Number of GNN layers must be greater than 1.\") self.gnn_node = GINNodeEmbedding(num_layers, emb_dim, JK=JK, drop_ratio=drop_ratio, residual=residual) # Pooling function to generate whole-graph embeddings if graph_pooling == \"sum\": self.pool = global_add_pool elif graph_pooling == \"mean\": self.pool = global_mean_pool elif graph_pooling == \"max\": self.pool = global_max_pool elif graph_pooling == \"attention\": self.pool = GlobalAttention(gate_nn=nn.Sequential( nn.Linear(emb_dim, emb_dim), nn.BatchNorm1d(emb_dim), nn.ReLU(), nn.Linear(emb_dim, 1))) elif graph_pooling == \"set2set\": self.pool = Set2Set(emb_dim, processing_steps=2) else: raise ValueError(\"Invalid graph pooling type.\") if graph_pooling == \"set2set\": self.graph_pred_linear = nn.Linear(2*self.emb_dim, self.num_tasks) else: self.graph_pred_linear = nn.Linear(self.emb_dim, self.num_tasks) def forward(self, batched_data): h_node = self.gnn_node(batched_data) h_graph = self.pool(h_node, batched_data.batch) output = self.graph_pred_linear(h_graph) if self.training: return output else: # At inference time, relu is applied to output to ensure positivity # 因为预测目标的取值范围就在 (0, 50] 内 return torch.clamp(output, min=0, max=50) 6. Practice with GIN Regression Task using PCQM4M Dataset123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143# %load_ext tensorboard# %tensorboard --logdir=runsimport osimport os.path as ospimport pandas as pdimport torchfrom ogb.utils.mol import smiles2graphfrom ogb.utils.torch_util import replace_numpy_with_torchtensorfrom ogb.lsc import PCQM4MEvaluatorfrom ogb.utils.url import download_url, extract_zipfrom rdkit import RDLoggerfrom torch_geometric.data import Data, Datasetimport shutilfrom tqdm import tqdmimport torchfrom torch import nnfrom torch.utils.tensorboard import SummaryWriterfrom torchvision import datasets, transformsRDLogger.DisableLog('rdApp.*')class MyPCQM4MDataset(Dataset): def __init__(self, root): self.url = 'https://dgl-data.s3-accelerate.amazonaws.com/dataset/OGB-LSC/pcqm4m_kddcup2021.zip' super(MyPCQM4MDataset, self).__init__(root) filepath = osp.join(root, 'raw/data.csv.gz') data_df = pd.read_csv(filepath) self.smiles_list = data_df['smiles'] self.homolumogap_list = data_df['homolumogap'] @property def raw_file_names(self): return 'data.csv.gz' def download(self): path = download_url(self.url, self.root) extract_zip(path, self.root) os.unlink(path) shutil.move(osp.join(self.root, 'pcqm4m_kddcup2021/raw/data.csv.gz'), osp.join(self.root, 'raw/data.csv.gz')) def len(self): return len(self.smiles_list) def get(self, idx): smiles, homolumogap = self.smiles_list[idx], self.homolumogap_list[idx] graph = smiles2graph(smiles) assert(len(graph['edge_feat']) == graph['edge_index'].shape[1]) assert(len(graph['node_feat']) == graph['num_nodes']) x = torch.from_numpy(graph['node_feat']).to(torch.int64) edge_index = torch.from_numpy(graph['edge_index']).to(torch.int64) edge_attr = torch.from_numpy(graph['edge_feat']).to(torch.int64) y = torch.Tensor([homolumogap]) num_nodes = int(graph['num_nodes']) data = Data(x, edge_index, edge_attr, y, num_nodes=num_nodes) return data # 获取数据集划分 def get_idx_split(self): split_dict = replace_numpy_with_torchtensor(torch.load(osp.join(self.root, 'pcqm4m_kddcup2021/split_dict.pt'))) return split_dict def train(model, optimizer, loss_f, loader, scheduler ,device ): model.train() total_loss = 0. total_nodes = 0. for batch in tqdm(loader): batch = batch.to(device) optimizer.zero_grad() out = model(batch).to(device) loss = loss_f(out, batch.y) loss.backward() optimizer.step() nodes = batch.num_nodes total_loss += loss.item() * nodes total_nodes += nodes scheduler.step() return total_loss/total_nodesdef evaluate(model, loader, device, evaluator): model.eval() y_true = [] y_pred = [] for batch in tqdm(loader): batch = batch.to(device) out = model(batch).view(-1, ).to(device) y_true.append(batch.y.view(out.shape).detach().cpu()) y_pred.append(out.detach().cpu()) #convert list to tensor y_true = torch.cat(y_true, dim=0) y_pred = torch.cat(y_pred, dim=0) dic ={'y_true': y_true, \"y_pred\":y_pred} return evaluator.eval(dic)['mae'] if __name__ == \"__main__\": dataset = MyPCQM4MDataset('dataset2') split_data = dataset.get_idx_split() output_file =open( \"./task-7-runs/logging.txt\" ,\"a\") from torch_geometric.data import DataLoader from tqdm import tqdm train_loader = DataLoader(dataset[split_data['train']], batch_size=256, shuffle=True, num_workers=4) val_loader = DataLoader(dataset[split_data['valid']], batch_size=256, shuffle=True, num_workers=4) test_loader = DataLoader(dataset[split_data['test']], batch_size=256, shuffle=True, num_workers=4) device = torch.device('cuda' if torch.cuda.is_available() else \"cpu\") print(\"Using device: \",device) # loss used to train model, MSE loss loss_f = torch.nn.MSELoss() # evaluator used to evaluate regression output and prediction, MAE (mean absolute error) evaluator = PCQM4MEvaluator() model = GINGraphPooling(num_tasks=1, num_layers=4, emb_dim=300, residual=False, drop_ratio=0.5, JK=\"last\", graph_pooling=\"sum\").to(device) optimizer = torch.optim.Adam(model.parameters(), lr =1e-3, weight_decay= 1e-3 ) scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.1) # tensorboard writer writer = SummaryWriter(log_dir =\"task-7-runs\" ) epochs = 20 for e in range(epochs): train_loss = train(model, optimizer, loss_f, train_loader, scheduler,device ) print(f\"Epoch: {e}, Train MAE: {train_loss} \",file=output_file, flush=True) writer.add_scalar(\"Loss/train\", train_loss, e) if e%2==0: val_mae = evaluate(model, val_loader, device, evaluator) writer.add_scalar(\"MAE/val\", val_mae, e) print(f\"Epoch: {e}, Valid MAE: {val_mae} \",file=output_file, flush=True) 0%| | 0/11896 [00:00&lt;?, ?it/s] Using device: cuda 100%|██████████| 11896/11896 [06:44&lt;00:00, 29.38it/s] 100%|██████████| 1487/1487 [00:51&lt;00:00, 28.91it/s] 100%|██████████| 11896/11896 [06:40&lt;00:00, 29.72it/s] 100%|██████████| 11896/11896 [06:43&lt;00:00, 29.49it/s] 100%|██████████| 1487/1487 [00:50&lt;00:00, 29.23it/s] 100%|██████████| 11896/11896 [06:43&lt;00:00, 29.51it/s] 100%|██████████| 11896/11896 [06:42&lt;00:00, 29.58it/s] 100%|██████████| 1487/1487 [00:51&lt;00:00, 28.71it/s] 100%|██████████| 11896/11896 [06:40&lt;00:00, 29.74it/s] 100%|██████████| 11896/11896 [06:39&lt;00:00, 29.81it/s] 100%|██████████| 1487/1487 [00:50&lt;00:00, 29.54it/s] 100%|██████████| 11896/11896 [06:40&lt;00:00, 29.68it/s] 100%|██████████| 11896/11896 [06:41&lt;00:00, 29.65it/s] 100%|██████████| 1487/1487 [00:50&lt;00:00, 29.25it/s] 100%|██████████| 11896/11896 [06:40&lt;00:00, 29.67it/s] 100%|██████████| 11896/11896 [06:46&lt;00:00, 29.24it/s] 100%|██████████| 1487/1487 [00:49&lt;00:00, 29.96it/s] 100%|██████████| 11896/11896 [06:38&lt;00:00, 29.83it/s] 100%|██████████| 11896/11896 [06:42&lt;00:00, 29.57it/s] 100%|██████████| 1487/1487 [00:49&lt;00:00, 29.92it/s] 100%|██████████| 11896/11896 [06:41&lt;00:00, 29.63it/s] 100%|██████████| 11896/11896 [06:47&lt;00:00, 29.22it/s] 100%|██████████| 1487/1487 [00:49&lt;00:00, 29.78it/s] 100%|██████████| 11896/11896 [06:39&lt;00:00, 29.76it/s] 100%|██████████| 11896/11896 [06:42&lt;00:00, 29.53it/s] 100%|██████████| 1487/1487 [00:49&lt;00:00, 30.23it/s] 100%|██████████| 11896/11896 [06:42&lt;00:00, 29.56it/s] 100%|██████████| 11896/11896 [06:42&lt;00:00, 29.55it/s] 100%|██████████| 1487/1487 [00:49&lt;00:00, 29.90it/s] 100%|██████████| 11896/11896 [06:35&lt;00:00, 30.08it/s] Result Training MSE Loss MAE (Mean Absolute Error) for evaluation 12 7. Reference[1] OGB document: https://ogb.stanford.edu/kddcup2021/pcqm4m/#evaluator [2] Dataset类官方文档： torch_geometric.data.Dataset [3] 将图样本封装成批（BATCHING）：ADVANCED MINI-BATCHING [4] 分子图的量子特性回归数据集：PCQM4M-LSC [5] Get Started | Open Graph Benchmark (stanford.edu) [6] Datawhale: https://github.com/datawhalechina/team-learning-nlp/blob/master/GNN/Markdown%E7%89%88%E6%9C%AC/9-1-%E6%8C%89%E9%9C%80%E8%8E%B7%E5%8F%96%E7%9A%84%E6%95%B0%E6%8D%AE%E9%9B%86%E7%B1%BB%E7%9A%84%E5%88%9B%E5%BB%BA.md https://github.com/datawhalechina/team-learning-nlp/blob/master/GNN/Markdown%E7%89%88%E6%9C%AC/9-2-%E5%9B%BE%E9%A2%84%E6%B5%8B%E4%BB%BB%E5%8A%A1%E5%AE%9E%E8%B7%B5.md","link":"/2021/07/09/GNN-7-Mini-Batching-Practice/"},{"title":"Java-1-Setup","text":"Introduction: JavaJava是一门面向对象编程语言（OOD, Object-Oriented Design），不仅吸收了C++语言的各种优点，还摒弃了C++里难以理解的多继承、指针等概念，因此Java语言具有功能强大和简单易用两个特征。Java语言作为静态面向对象编程语言的代表，极好地实现了面向对象理论，允许程序员以优雅的思维方式进行复杂的编程。 Java Properties从互联网到企业平台，Java是应用最广泛的编程语言，原因在于： Java是基于JVM虚拟机的跨平台语言，一次编写，到处运行； Java程序易于编写，而且有内置垃圾收集，不必考虑内存管理； Java虚拟机拥有工业级的稳定性和高度优化的性能，且经过了长时期的考验； Java拥有最广泛的开源社区支持，各种高质量组件随时可用。 从上面的特点，我们可以看到java一些特性: 简单性Java 相对于C, C++而言，除了保留 C/C++相似的语法外， Java也把一些复杂的特性去除掉，比如说C/C++里面的显性指针 和 操作符重载。 面向对象 (OOD)另外Java是面向对象的编程语言，它的程序都是用Class类进行封装的，Java实现的了OOD的特点和概念，比如抽象性(abstraction), 封装性(Encapsulation), 继承性(inheritance) 和多态性(polymorphism) 软件可移植性(Portability) 和平台的独立性Java 不像C/C++编译时那样要生成机械码， Java编译时把程序编译成byte code 字节码，而 Java字节码能够不依赖机器平台，Java程序可以在任何实现了Java解释程序和运行系统（run-time system）的系统上运行， 下图就是一个例子， window编写好的 java类可以在带有JRE环境的Linux, Mac 等不同环境运行，不用重新编写 (图来自 https://www.topperskills.com/tutorials/java/features-properties-java-language.html) 稳健性Robustness Java不支持指针，它消除重写存储和讹误数据的可能性。Java自动的“无用单元收集”预防存储漏泄和其它有关动态存储分配和解除分配的有害错误。Java解释程序也执行许多运行时的检查，诸如验证所有数组和串访问是否在界限之内。(JVM通过一定机制进行内存自动的回收，也避免像C/C++容易内存的问题) Java中的异常检测是使得程序更稳健的一个特征。异常是某种类似于错误的异常条件出现的信号。使用try/catch/finally语句，程序员可以找到出错的处理代码，这就简化了出错处理和恢复的任务。 Java是一个强类型语言，它允许扩展编译时检查潜在类型不匹配问题的功能。Java要求显式的方法声明，它不支持C风格的隐式声明。这些严格的要求保证编译程序能捕捉调用错误，这就导致更可靠的程序 High Performance:Java JVM 每次通过 Just In Time compiler (JIT) 来转换相似的byte code到依赖于机器的环境的机器码，从而减少程序的运行时间 多线程MultithreadJava 提供支持多线程的执行 也称为轻便过程，能处理不同任务，使具有线索的程序设计很容易。Java的lang包提供一个Thread类，它支持开始线索、运行线索、停止线索和检查线索状态的方法。(这个C++, Python 之类的语言也支持)。 Java的线索支持也包括一组同步原语。这些原语是基于监督程序和条件变量风范，由C.A.R.Haore开发的广泛使用的同步化方案。用关键词synchronized，程序员可以说明某些方法在一个类中不能并发地运行。这些方法在监督程序控制之下，确保变量维持在一个一致的状态。 安全性SecurityJava的安全性体现在 存储分配模型里面没有指针， 用户不能直接通过指针直接访问和修改内存 Java编译程序不处理存储安排决策，所以程序员不能通过查看声明去猜测类的实际存储安排。编译的Java代码中的存储引用在运行时由Java解释程序决定实际存储地址。 分布性 DistributedJava设计成支持在网络上应用，它是分布式语言。Java既支持各种层次的网络连接，又以Socket类支持可靠的流（stream）网络连接，所以用户可以产生分布式的客户机和服务器 可解释性Java 是高度的解释性语言意味着它不依赖操作系统的运行去把byte code翻译成机器码，也不像传统的编译过程把代码重新”编译-&gt;链接-&gt; 测试 “， 而是有它自己独立的编译器把新类装进环境，, 它是增量式的、轻量级的过程。 动态性 dynamicJava语言设计成适应于变化的环境，它是一个动态的语言。例如，Java中的类是根据需要载入的，甚至有些是通过网络获取的。 Installation Install JDKJava需要在JVM上面才能运行，所以我们首先要把Java的环境安装，所以我们要安装 JDK安装时根据电脑环境的不同，安装不同的版本，这里了我安装的是Window的版本 Set Environment Variable for java在安装JDK后， 我们要让电脑和terminal检测到java的环境就要通过设置环境变量来指向Java的安装位置Window的安装目录大概是这样子 1C:\\Program Files\\Java\\jdk-16 然后，把JAVA_HOME的bin目录附加到系统环境变量PATH上。在Windows下，它长这样： 1Path=%JAVA_HOME%\\bin;&lt;现有的其他路径&gt; 在Mac里面是在Mac或者linux下，它在/.bash_profile或/.zprofile里，它是： 1export JAVA_HOME=`/usr/libexec/java_home -v 16` Test在安装好Java后，可以在terminal里面简单看看java的版本 来看看jdk有没有安装完成 1java --version Tools java: 这个可执行程序其实就是JVM，运行Java程序，就是启动JVM，然后让JVM执行指定的编译后的代码 javac: 这是Java的编译器，它用于把Java源码文件（以.java后缀结尾）编译为Java字节码文件（以.class后缀结尾）； jar: 用于把一组.class文件打包成一个.jar文件，便于发布； javadoc: 用于从Java源码中自动提取注释并生成文档； jdb: Java调试器，用于开发阶段的运行调试。（类似C/C++里面的gdb） IDE 环境安装Java常用的IDE（Integrated development environment）集成环境有Eclipse, IntelliJ IDEA等 Simple Codes这里简单写个Hello World的demo测试一下IntelliJ IDEA的开发环境，项目步骤创建如下: 点进在IDE左上角 file -&gt; new -&gt; Project 在Project里面选择java 在创建后的项目目录里面的src（源码的文件夹）里面右键选择new-&gt; Package 在创建后的Package文件夹里面创建一个叫做demo的java文件 在新建的java文件里面添加一下代码, 这样可以把新建的java文件当作新的class,可以把这文件当成出现的入口 1234567package Practice.demo;public class demo { public static void main(String[] args){ System.out.println(\"Hello World\"); }} 按 Alt + Shift + F10 对main入口这个文件进行运行 Reference[1] https://www.topperskills.com/tutorials/java/features-properties-java-language.html [2] Datawhale: https://github.com/datawhalechina/team-learning-program/blob/master/Java/0.%20Java%E7%AE%80%E4%BB%8B%E4%B8%8E%E5%AE%89%E8%A3%85%E9%85%8D%E7%BD%AE.md [3] IntelliJ IDEA IDE使用: https://blog.csdn.net/oschina_41790905/article/details/79475187 [4] Java Tutorial: https://www.liaoxuefeng.com/wiki/1252599548343744/1280507291631649","link":"/2021/07/13/Java-1-Installation/"},{"title":"GNN-8-Conclusion","text":"GNN-Task-8-Conclusion1. Introduction这一节我们来总结一些之前学过的GNN的常见任务，GNN模型, 以及torch_geometric的常用类型，比如Dataset， ClusterData, Mini-batching等。另外也总结一下不同的GNN模型的常用场景和特点等。 2. GNN Tasks Definition of Graph 一个图被记为 $\\mathcal{G}={\\mathcal{V}, \\mathcal{E}}$，其中 $\\mathcal{V}={v_{1}, \\ldots, v_{N}}$ 是数量为 $N=|\\mathcal{V}|$ 的结点的集合， $\\mathcal{E}={e_{1}, \\ldots, e_{M}}$ 是数量为 $M$ 的边的集合。 图用节点表示实体（entities ），用边表示实体间的关系（relations）。 节点和边的信息可以是类别型的（categorical），类别型数据的取值只能是哪一类别。一般称类别型的信息为标签（label）。 节点和边的信息可以是数值型的（numeric），类别型数据的取值范围为实数。一般称类别型的信息为属性（attribute）。 大部分情况中，节点含有信息，边可能含有信息。 Types of Graph 同质图（Homogeneous Graph）：只有一种类型的节点和一种类型的边的图。 异质图（Heterogeneous Graph）：存在多种类型的节点和多种类型的边的图。 二部图（Bipartite Graphs）：节点分为两类，只有不同类的节点之间存在边。 Node Embedding在Graph里面, node representation和edge的表示是分别用两个矩阵表示，其中node representation/embedding 的shape = [Num_nodes, dimension of node embedding], 而 edge的matrix的shape = [2, number of edge], 其中 edge_index[0] = source nodes, edge_index[1]= target nodes Node Prediction(Classification , Regression)预测节点的类别或某类属性的取值, 例子：对是否是潜在客户分类(Node Classification) ,对游戏玩家的可以消费的金额做预测(Node Regression) Link Prediction预测两个节点间是否存在链接, 例子：Knowledge graph completion、好友推荐、商品推荐(有点类似Matrix Factorization里面的对item和user相似度关联度预测) Graph EmbeddingGraph embedding/representation 是可以通过node embedding通过pooling或者concatenate 拼接得到的 Graph Prediction对不同的图进行分类或预测图的属性, 例子：分子属性预测, 对分子结构组成(分子的组成结构就是一个图)进行分类 Other Tasks Graph Generation：例如药物发现 Graph Evolution：例如物理模拟 图演变 3. PyG ToolkitsDataset InMemoryDataset Link: GNN-Task-4-LinkPrediction torch_geometirc source code: https://pytorch-geometric.readthedocs.io/en/latest/_modules/torch_geometric/data/in_memory_dataset.html#InMemoryDataset InMemoryDataset是一个用于把raw数据全部转换成torch的 .pt 数据之后，把所有数据加载到内存里面的的数据集。由于它把数据集全部加载到内存里面，所有它的数据读取速度快，但是不能存放超大型的图数据。因此它一般用于中小型图数据 ClusterDataset Link: GNN-Task-5-ClusterData torch_geometirc source code: https://pytorch-geometric.readthedocs.io/en/latest/_modules/torch_geometric/data/cluster.html#ClusterData ClusterDataset是根据ClusterGCN的paper针对超大型图数据计算时neighborhood expansion problem 导致梯度计算复杂度指数增长的问题进行优化。它把整个图通过clustering partition的方法得到独立的subgraph然后再梯度更新。 Mini-Batching Dataset Link: GNN-Task-6-GIN-GraphEmbedding , GNN-Task-7-Mini-Batching torch_geometirc source code: https://pytorch-geometric.readthedocs.io/en/latest/_modules/torch_geometric/data/data.html#Data 根据官方文档, Mini-Batching 简单地通过把一个大型图的Adjacency matrix 按照对角线方向拆分成多个小batch，每个batch代表一个独立的subgraph. 然后它把这些subgraph作为一个batch去feed GNN。Note：虽然整个adjacency matrix被划分成多个batch，但是GNN的输入的shape (node的个数等)不用变，因为Adjacency Matrix里面每个subgraph相互独立没有相互连接，所以训练时不会涉及不同的graph的node的计算。(这样就有些想把整个图通过一个mask把一些node和edge隐藏起来进行计算) 和前面的ClusterDataset对比，Mini-Batching 有以下不同: Mini-Batching(像 PCQM4M分子数据集)它是基于InMemoryDataset,用起来和普通图数据一样，它可以通过 split_idx函数获取train, test, valid的mask对图进行拆分，并且可以像普通数据集一样直接用DataLoader 进行分batch训练 而它也不像ClusterDataset那样要想通过cluster进行subgraph clustering和采样，所以数据加载更快 Mini-Batching的dataset是储存在Sparse Matrix里面只对非零的数据进行储存，所以没有大量的存储开销 4. GNN Models4.1 MessagePassing在图神经网络里面，在对数据和样本之间的关系进行建模得到图的edge， node之后，我们需要在图里面把每个节点的信息根据它的neighbor的信息进行更新，从而达到node的信息更新和节点特征(Node Representation)的特征表达。而这个把node节点信息相互传递从而更新节点表征的方法也叫Message Passing。MessagePassing是一种聚合邻接节点信息来更新中心节点信息的范式(通用架构)**，它将卷积算子推广到了不规则数据领域，实现了图与神经网络的连接。遵循消息传递范式的图神经网络被称为消息传递图神经网络。**Message Passing GNN的通用公式可以描述为 $$ \\mathbf{x}_ i^{(k)} = \\gamma^{(k)} ( \\mathbf{x}_ i^{(k-1)}, \\square_{j \\in \\mathcal{N}(i)} , \\phi^{(k)}(\\mathbf{x}_ i^{(k-1)}, \\mathbf{x}_ j^{(k-1)},\\mathbf{e}_{j,i}) ), $$ 根据官方文档 以及CREATING MESSAGE PASSING NETWORKS, 我们定义 + $\\mathbf{x}^{(k-1)}_i\\in\\mathbb{R}^F$表示神经网络的$(k-1)$层中节点$i$的节点表征 + $\\mathbf{e}_{j,i} \\in \\mathbb{R}^D$ 表示从节点$j$到节点$i$的边的属性信息。 + $\\square$表示**可微分**的、具有排列不变性（**函数输出结果与输入参数的排列无关**）的函数, 比如aggregation 函数。比如sum， mean, min等函数和输入的参数顺序无关的函数。 + $\\gamma$ : **可微分可导**的update 函数，比如MLPs（多层感知器） + $\\phi$: **可微分可导**的message 函数，比如MLPs（多层感知器）和 linear Projection等 4.2 GCN GCN 原理 根据PyG的官方文档，**GCNConv** 的公式是 $$ \\mathbf{x}_ i^{(k)} = \\sum_{j \\in \\mathcal{N}(i) \\cup { i }} \\frac{1}{\\sqrt{\\deg(i)} \\cdot \\sqrt{\\deg(j)}} \\cdot ( \\mathbf{\\Theta} \\cdot \\mathbf{x}_ j^{(k-1)} ), $$ 矩阵的形式是 $$ \\mathbf{X}^{(k)} = \\mathbf{D}^{-0.5}\\mathbf{A}\\mathbf{D}^{-0.5}\\mathbf{X}^{(k-1)}\\mathbf{\\Theta} $$ 其中，$\\mathbf{x}_i$ 的节点的特征是由它的近邻的node的信息(包括node i自己)进行更新，所以计算时j是节点i的邻居(包括节点i本身)的子集里面的node。 邻接节点的表征$\\mathbf{x}_j^{(k-1)}$首先通过与权重矩阵$\\mathbf{\\Theta}$相乘进行变换，然后按端点的度$\\deg(i), \\deg(j)$进行归一化处理，最后进行求和。这个公式可以分为以下几个步骤： 向邻接矩阵添加自环边。 对节点表征做线性转换。 计算归一化系数。 归一化邻接节点的节点表征。 将相邻节点表征相加（”求和 “聚合）。 GCN存在的问题: Oversmoothing根据paper Tackling Over-Smoothing for General Graph Convolutional Networks, over-smoothing issue drives the output of GCN towards a space that contains limited distinguished information among nodes, leading to poor expressivity. 即随着GCN层数加深, GCN的node embedding的信息就会越来越相似缺少差异性(就相当于丢失信息了)，导致特征的表达能力不足。不过这个现象也是很正常的，回想一下CNN, 如果Convolution layer 过多，就会导致对输入特征作用的范围越来越大，把一大块的特征过滤成一小块输出，表达的信息就会丢失更多。 另外，也可以把Convolution看成是一个低通滤波器，如果这个filter作用范围越大，输出的信号越平滑，那么就会丢失很多信息了。 4.3 GAT paper link: https://arxiv.org/pdf/1710.10903.pdf Graph Attention Network 的attention公式如下: $$\\alpha_ {i,j} = \\frac{ \\exp(\\mathrm{LeakyReLU}(\\mathbf{a}^{\\top}[\\mathbf{W}\\mathbf{h}_ i , \\Vert , \\mathbf{W}\\mathbf{h}_ j]))}{\\sum_ {k \\in \\mathcal{N}(i) \\cup { i }}\\exp(\\mathrm{LeakyReLU}(\\mathbf{a}^{\\top}[\\mathbf{W} \\mathbf{h}_ i , \\Vert , \\mathbf{W}\\mathbf{h}_ k]))}.$$ 节点信息更新$$\\mathbf{h}_ i^{‘} = \\sigma(\\frac{1}{K} \\sum_ {k=1}^K\\sum_ {j \\in N(i)} a_{ij}^{k}\\mathbf{W}^k\\mathbf{h}_ {i})$$ 实际上GAT就是在每个节点把邻居的信息聚合时根据邻居节点的node representation和这个节点的node representation的相似度对聚合的信息有侧重地聚合其中每个参数的代表: $\\mathbf{h}_i$: 节点 i的node representation。这个node representation可以是GNN的某一层的输出 $\\mathbf{W}$: shared linear transformation. 用于每个节点的共享的线性投映矩阵，所有节点都用相同的W进行投映 $k \\in \\mathcal{N}(i) \\cup { i }$: 第i个节点的邻居节点(包括第i个节点本身)。注意因为这里涉及两个sum，两个loop所以计算有点慢 $\\Vert$: 把两个向量拼接 4.4 GraphSAGE (Graph Sample and Aggreate Graph Embedding) GraphSAGE是一种 inductive的representation learning的方法，就是归纳法。它是用于预测之前没有见过的node的embed的ing的特征。它的主要思想是通过学习多个aggregate函数(paper里面提出来mean, LSTM, pooling 三个)，然后这些aggregate函数用neighbor的信息来生成之前没有见过的node的embedding之后再做预测。下面是GraphSAGE的流程图： GraphSAGE 的node embedding的其中一个生成公式为(还有其他用于生成embedding的aggregate函数公式可以参考原文):$$\\mathbf{x}_ {i}^{‘} = \\mathbf{W}_ {1}x_{i} + \\textbf{mean}_ {j \\in N(i)}(\\mathbf{x}_{j})$$ GraphSAGE 的graph-based unsupervised loss function 定义为 $$\\mathbf{J}_ {G}(z_{u}) = -log(\\sigma(\\mathbf{z}_ {u}^{T}\\mathbf{z}_ {v})) - \\mathbf{Q} \\cdot \\mathbf{E}_ {v_ {n} \\in P_ {n}(v)}log(\\sigma(-\\mathbf{z}_ {u}^{T} \\mathbf{z}_ {v_{n}}))$$ 其中: $j \\in N(i)$ 为第i个节点的第j个neighbor节点 $v$ 是和 $u$ 在定长的random walk采样路径出现的节点 $Q$ 是负样本的个数， $P_{n}(v)$ 是负采样的分布 $z_{u}$是node representation特征 这里$\\sigma()$里面计算的是节点和random walk采样时同时出现的其他节点的相似度。相似度越大，loss越小 4.5 GINGIN 全称是Graph Isomorphsim Network, 同构图网络，是用于学习Graph Embedding的一种网络，它也可以用来测量两个图之间的相似度。 在GIN里面node representation的update公式是$$h_ {v}^{k} = \\text{MLP}^{k}((1+ \\epsilon^{k})h_ {v}^{(k-1)} + \\sum_ {u \\in \\mathbf{N}(v)} h_ {u}^{(k-1)})$$ 在生成节点的表征后仍需要执行图池化（或称为图读出）操作得到图表征，最简单的图读出操作是做求和。由于每一层的节点表征都可能是重要的，因此在图同构网络中，不同层的节点表征在求和后被拼接，其数学定义如下，$$h_ {G} = \\text{CONCAT}(\\text{READOUT}({h_{v}^{(k)}|v\\in G})|k=0,1,\\cdots, K)$$采用拼接而不是相加的原因在于不同层节点的表征属于不同的特征空间。未做严格的证明，这样得到的图的表示与WL Subtree Kernel得到的图的表征是等价的。 5. Reference[0] Datawhale: https://github.com/datawhalechina/team-learning-nlp/tree/master/GNN [1] Torch_Geometric: https://pytorch-geometric.readthedocs.io/en/latest/_modules/torch_geometric/data/in_memory_dataset.html#InMemoryDataset [2] Dataset Class: https://pytorch-geometric.readthedocs.io/en/latest/modules/data.html [3] Stanford OGB library (提供图数据) : https://github.com/snap-stanford/ogb [4] GNN model Torch Geometric: https://pytorch-geometric.readthedocs.io/en/latest/notes/cheatsheet.html [5] GCN: https://arxiv.org/pdf/2007.02133.pdf [6] GAT: https://arxiv.org/abs/1710.10903 [7] GraphSAGE: https://arxiv.org/pdf/1706.02216.pdf [8] GIN: https://arxiv.org/pdf/1810.00826.pdf [9] 书籍 Deep Learning on Graph: https://cse.msu.edu/~mayao4/dlg_book/chapters/","link":"/2021/07/10/GNN-8-Conclusion/"},{"title":"Java-3-Array","text":"Introduction这一章介绍Java 里面的Array数组的操作和原理。 这里会先简单介绍一下数组的概念和以及数组的基本操作(创建，遍历修改，排序，更加高维度的数组的操作等) 1. 数组基本概念 Concept1.1 什么是数组首先什么是数组? 数组是一种数据结构不管在哪种编程语言都会用到， 它就好比是我们的有很多抽屉的柜子，每个抽屉都有自己的ID(比如位置)来识别每个抽屉的位置，而根据ID我们打开相应的抽屉时就能够有对应的空间让我们放东西。和这个柜子的特点一样， 数组有以下特点: 固定长度(柜子的大小和抽屉的个数是固定的) 类型相同(这个柜子只能放一类的物品) 可以通ID寻址(比如按照柜子的抽屉的高度顺序来标号) 1.2 数组的底层内存构成 (Memory of Array)在Java里面假设我有一个array名字叫做a, 我们可以把它看成是一个柜子的名字a, 然后我这个柜子里面存放的是叫做 int整型的数据(物品)， 那么我这个这个柜子有5个抽屉，那么在创建数组时就是写成int a[] = new int[5]。 而这个时候这个array的名字地址a就放到stack栈里面，相对于把这个柜子的名字放到一个列表，方便知道柜子在什么地方。而柜子的内容放到一个叫heap堆的空间里面。当我们要用时就先找a的地址，然后再再找第i个抽屉，把数据拿出来。即 读取a[i]的数据。 上面提及的存放array的名字和数据的内存有以下: Stack 栈内存：栈内存存储的都是局部变量，变量一旦出了自己的作用域，那么就会马上从内存的消失，释放内存空间。 Heap 堆内存：堆内存存储的都是对象内存，对象一旦被使用完，并不会马上从内存中消失，而是等待垃圾回收器不定时的把垃圾对象回收，这时候该对象才会消失，释放内存。 凡是以new关键字创建的对象，JVM都会在堆内存中开辟一个新的空间，创建一个新的对象。 对象如果没有变量引用了，那么该对象就是一个垃圾对象了。 2. 数组基本操作Array Operations2.1 数组创建Creation 数组申明 declaration当我们想要告诉程序，我们要定义一个数组，但还没有初始化赋予空间时，我们可以简单通过下面语句declare。 第一种是Java的常见写法，第二种是C++的写法。但是这样做，数组还没有开储存空间，所以不能直接用，这样就涉及到数组的初始化。1234int [] arr;//orint arr[]; 数组初始化 Initialization在Java里面初始化数组一般要new关键词， 除非是定义数组同时直接赋值。在数组的初始化我们可以用下面方法定义数组并赋值： 先定义后赋值 12345int [] a;a = new int[5];a[0] = 1;a[1] = 2; ... 或者 12int [] a;a = new int[5]{1,2,3,4,5}; 定义的同时赋值 1int [] a = {1,2,3,4,5} 2.2 数组遍历Iteration 方法1: 用index遍历数组 在Java的数组里， 数组的index都是从0开始的，所以如果有n个元素在数组，那么就最大的index = n-1. 比如以下代码里面对a数组打印是从0打印到4， a数组的大小为5. 123456789public class Demo{ public static void main(){ int [] a = {1,2,3,4,5}; for(int i=0; i&lt; a.length; i++){ System.out.println(a[i]); } }} 方法2: 用for each 遍历数组 另外一种数组的遍历是用for each来遍历有点类似python里面的for x in ls:的用法，以及C++里面的for(auto x : ls)的用法。 for each的方法要比index遍历的方法简洁，但是不能拿到index，只能直接拿数据item。 12345678public class Main { public static void main(String[] args) { int[] ns = { 1, 4, 9, 16, 25 }; for (int n : ns) { System.out.println(n); } }} 2.3 数组的常用的API Arrays.sort() 排序是数据结构算法的很基础的一个操作，因此Array的类也有对应的API. 这里我们先调用这个API，再尝试自己写一个。 这里我们先通过java.util.Arrays调用Array 类然后再调用这个类里面的Array.sort()的方法。 Note: Array.sort()的方法的输入可以是不同的类，如int, float, double, String 等，如果是String会按照第一个字母的ASCII码进行排序 123456789import java.util.Arrays;public class Main { public static void main(String[] args) { int[] ns = { 12,0,-4,-9, 20, 30, -100}; Arrays.sort(ns); System.out.println(Arrays.toString(ns)); }} 这里实现一个insertion sort 12345678910111213141516171819202122232425262728293031// 降序排序import java.util.Arrays;public class Main { public static void main(String[] args) { int[] ns = { 28, 12, 89, 73, 65, 18, 96, 50, 8, 36 }; // 排序前: System.out.println(Arrays.toString(ns)); // TODO: insertion sort int tmp; for(int i=0; i&lt; ns.length-1;i++){ tmp = ns[i]; //find max for(int j= i+1; j&lt; ns.length;j++ ){ if(ns[j]&gt; tmp){ tmp = ns[j]; ns[j] = ns[i]; ns[i] = tmp; } } } // 排序后: System.out.println(Arrays.toString(ns)); if (Arrays.toString(ns).equals(\"[96, 89, 73, 65, 50, 36, 28, 18, 12, 8]\")) { System.out.println(\"测试成功\"); } else { System.out.println(\"测试失败\"); } }} Arrays.toString() Array的toString函数可以把一个array(包括中括号[]) 转成一个String， 比如把[96, 89, 73, 65, 50, 36, 28, 18, 12, 8] 转成 “[96, 89, 73, 65, 50, 36, 28, 18, 12, 8]”。 1234567891011121314public class Main { public static void main(String[] args) { int[] ns = { 28, 12, 89, 73, 65, 18, 96, 50, 8, 36 }; // 排序前: System.out.println(Arrays.toString(ns)); System.out.println(Arrays.toString(ns)); if (Arrays.toString(ns).equals(\"[28, 12, 89, 73, 65, 18, 96, 50, 8, 36]\")) { System.out.println(\"测试成功\"); } else { System.out.println(\"测试失败\"); } }} Arrays.fill()数组中的元素定义完成后，可通过Arrays类的静态方法fill()方法来对数组中的元素进行分配，起到填充和替换的效果。fill()方法可将指定的int值分配给int型数组的每个元素。用法∶Arrays.fill(int[] a ,int value) 2.4 更高维度的数组Higher dimensional array2.4.1 2D array2D array的创建和1D的差不多,一样用new关键词进行创建。不过2D array里面每一行的长度可以不一样。array[row][col] 2D array的第一个index代表row的index，第二个index是column的index. 以上面图为例， array是一个存放了每个row的起始的地址的列表，而 array[i] 地址指向了第i个row的数组。而2D array的每个element都是放到Heap里面的 1234int[][] a = {{1}, {1,2}, {1,2,3}};//orint[][] a= new int[][] {{1}, {1,2}, {1,2,3}}; 12345678910public class Main { public static void main(String[] args) { int[][] ns = { { 1, 2, 3, 4 }, { 5, 6, 7, 8 }, { 9, 10, 11, 12 } }; System.out.println(ns.length); // 3 }} 在上面这个数组里面，它的结构是 12345678910 ns[0][1].........ns[0][3] ┌───┬───┬───┬───┐ ┌─────┐ ┌──&gt;│ 1 │ 2 │ 3 │ 4 │ns ─────&gt;│ns[0]│──┘ └───┴───┴───┴───┘ ├─────┤ ┌───┬───┬───┬───┐ │ns[1]│─────&gt;│ 5 │ 6 │ 7 │ 8 │ ├─────┤ └───┴───┴───┴───┘ │ns[2]│──┐ ┌───┬───┬───┬───┐ └─────┘ └──&gt;│ 9 │10 │11 │12 │ └───┴───┴───┴───┘ 3. Coding Practice这里我自己写一个2D binary Search 的leetcode的题目来练习一下2D array 12345678910111213141516171819202122232425262728293031323334353637import java.util.Arrays;public class Task3{public static void main(String[] args){ //2D array int[][] arr = new int[][]{ {1,2,3,4,5}, {3,5,7,8,10}, {6, 9, 11, 16, 21}, {9, 11, 14, 18, 25}, {15, 16, 19, 40, 50} }; System.out.println(\"Array: \"); for(int i=0; i&lt; arr.length;i++){ System.out.println(Arrays.toString(arr[i])); } //binary search in 2D ascending array int r = 0, c = arr[0].length-1; int target = 18; System.out.println(\"Target: \"+ target); while(r &lt; arr.length &amp;&amp; c &gt;=0){ if(arr[r][c] &lt; target){ r += 1; } else if(arr[r][c]&gt;target){c -= 1;} else{ System.out.printf(\"Position: row: %d, col: %d\\n\", r,c); break; } } if(r&gt;= arr.length &amp;&amp; c&lt;0) {System.out.println(\"Target not found\");}}} Compile Commands: 12$ javac Task3.java$ java Task3 Output: 12345678Array:[1, 2, 3, 4, 5][3, 5, 7, 8, 10][6, 9, 11, 16, 21][9, 11, 14, 18, 25][15, 16, 19, 40, 50]Target: 18Position: row: 3, col: 3 4. Comparison between C++ Array and Java Array Properties C++ Array Java Array Type NOT Object/Class Java Array is a Class Declaration int arr[size]= {…} int[] arr = new int[size] //usually use “new” Initialization int arr[size] int[] arr check size of array sizeof() array.length 5. Reference[1] https://stackoverflow.com/questions/17185906/what-is-the-difference-between-int-arr1-null-and-int-arr1-int-5-re [2] Java Tutorial: https://www.liaoxuefeng.com/wiki/1252599548343744/1259544232593792[3] Datawhale: https://github.com/datawhalechina/team-learning-program/blob/master/Java/4.%E6%95%B0%E7%BB%84.md","link":"/2021/07/16/Java-3-Array/"},{"title":"Java-2-BasicGrammar","text":"1. Introduction这篇文章主要介绍Java的基本语法: 数据类型, 注释类型， 泛型(Generic Type, 等同于C++的Template)。之后再简单写个Java Demo去跑一遍程序 2. Data Type整型（byte、short、int、long） 整型数据有四种，它们的取值范围不同 byte 的取值范围：-128～127（-2的7次方到2的7次方-1） short 的取值范围：-32768～32767（-2的15次方到2的15次方-1） int 的取值范围：-2147483648～2147483647（-2的31次方到2的31次方-1） long 的取值范围：-9223372036854774808～9223372036854774807（-2的63次方到2的63次方-1） 2.1 浮点型浮点型包括float和double两种，区别在与精度，float是单精度、32位、符合IEEE 754标准的浮点数；double 数据类型是双精度、64 位、符合 IEEE 754 标准的浮点数。 float（单精度浮点型）取值范围：3.402823e+38～1.401298e-45 double（双精度浮点型）取值范围：1.797693e+308～4.9000000e-324 Note: java里面的浮点数，比如2.4， java默认会使用double。所以如果像 float a = 1.5 这样Java会自动把 a先设成double然后强制转换为float，这样编译时会报错。 2.2 字符类型char 类型是一个单一的 16 位 Unicode 字符； 最小值是 \\u0000（十进制等效值为 0）； 最大值是 \\uffff（即为 65535）； char 数据类型可以储存任何字符； 2.3 布尔类型布尔类型只有两个取值，分别是true 、false 。 java默认是false。 2.4 ArrayJava 的Array数组类型使用方法类似C/C++的array， 不过创建时要用new keyword 1234//Indicate an arrayint[] arr;// initializearr = new int[10]; Notes: array must be initialized by keyword new and size of array must be provided. Or we can directly initialize array by setting element values 1234567int[] arr = {1,2,3,4,5};//orint[] arr = new arr[5];int[] arr = {1,2,3,4,5};int[] arr;arr= {1,2,3,4,5}; //error 在初始化后， array的size不能更改 3. Variable 变量3.1 类 变量Java里面的变量分为: local variable局部变量:变量只能在作用域{..}或方法里面使用，在方法使用后，变量内存会被释放销毁，局部变量在使用时必须进行赋值操作或被初始化，否则出现编译错误。 例如 123456public class Cat{ public void print(){ String name = \"cat\"; // local variable, it can be used only inside method print() System.out.println(name); }} 类静态变量静态变量在类中以 static 关键字声明，但必须在方法之外。无论一个类创建了多少个对象，类只拥有类变量的一份拷贝。生命周期与类一致。类的静态变量用于描述类的信息而不是实例的信息。 instance variable实例变量实例变量也叫类成员变量，声明在一个类当中，方法、构造方法和语句块之外。实例变量的生命周期与对象一致。 constant常量初始化之后数据就不能变动的变量，因此叫做常量。一般用final修饰的variable都是常量123public class Cat{ final String name=\"Mike\";} 3.1 类型变换数据处理时常会要把一个数据类型转换成另外一种类型。而通常把占用字节空间大的类型变成占用字节少的类型时会导致数据精度的丢失，下面这幅图展示了类型变换时的精度丢失情况。 3. Annotation注释注解，一种元数据形式，提供有关程序的数据，该数据不属于程序本身。注释对其注释的代码的操作没有直接影响。注释的作用有: 编译前：为编译器提供编译检查的依据，辅助检查代码错误或抑制检查异常。 编译中或发布时：给编译器提供信息生成代码或给其他工具提供信息生成文档等。 运行时：在运行过程中提供信息给解释器，辅助程序执行。 注解经常和反射结合起来用，多用于框架中。 4. Generic Type泛型在介绍 Generic 泛型前先来考虑一下这个问题, 在Java的ArrayList Class里面 1234567public class ArrayList { private Object[] array; private int size; public void add(Object e) {...} public void remove(int index) {...} public Object get(int index) {...}} 如果我们用这个ArrayList 去存放 String类型数据， 就必须强制转换Object到String类型，比如 1234ArrayList list = new ArrayList();list.add(\"Hello\");// 获取到Object，必须强制转型为String:String first = (String) list.get(0); 这样的缺点有: 需要强制转换 不方便，容易类型转换不当而报错 为了解决这个问题， 其中一个解决方法就是把ArrayList里面的Object类型在每次使用前都变成我们所需的储存类型(如String, Integer,….). 而最好的方法就是ArrayList除了数据类型外，其他的所有variable， method都不变。这个时候就需要用到泛型Generic. 泛型的本质是为了参数化类型（在不创建新的类型的情况下，通过泛型指定的不同类型来控制形参具体限制的类型）。也就是说在泛型使用过程中，操作的数据类型被指定为一个参数，这种参数类型可以用在类、接口和方法中，分别被称为泛型类、泛型接口、泛型方法.例子: 1234// 创建ArrayList&lt;Integer&gt;类型：ArrayList&lt;String&gt; integerList = new ArrayList&lt;String&gt;();// 添加一个Integer：integerList.add(\"Hello World\"); 创建一个Generic Type Class： 123456789101112131415161718192021222324252627282930313233343536373839404142class House&lt;T&gt;{// note: in Python, when defining an array: array = [...]// in Java/C/C++, T[] array={....}, T[] indicates this is an array and use bracket {} to initializeprivate int size;private int pt=0;private T[] array;public House(int size){ this.size = size; this.array = (T[])new Object[size];}public int size(){ return this.size;}public T At(int i){return this.array[i];}public void add(T e){ this.array[this.pt] = e; if(this.pt&lt; this.size) { this.pt+=1; this.size += 1; }}public void remove(T e){ if(this.pt&gt;=0){ this.array[this.pt] = null; this.pt -= 1; this.size -= 1; }}} Note: 泛型Class里面的array数组不能直接初始化，因为还不知道T的类型，所以要先通过用Object创建数据类型，然后强制转换为 (T[])数组类型 5. Coding Practice1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162/* Java Review Task 2: DataType * Author: Wenkang Wei * This file is to test the * * */// Test enumerate classenum Color {Orange, Black, White, Brown};enum Gender {Male, Female};class Cat{/*Test different data types: * * */static String category = \"mammal\";int num_legs = 4;char id = 'a';boolean has_wing = false;byte body_temperature = 37;float tall = 20.5f; // note: if using float tall=20.5, 20.5 is default to be double, then it is converted to be floatdouble length = 40.1; String name = \"Mike\";long num_teeth = 32;Color skin_color = Color.Orange;final Gender gender = Gender.Male; //constant variable, it can not be changed anymorepublic float speed(){ float speed = 10; //m/s return speed;}public void printInfo(){System.out.println(\"Name: \"+this.name);System.out.println(\"Gender: \"+this.gender);System.out.println(\"#_legs: \"+this.num_legs);System.out.println(\"id: \"+this.id);System.out.println(\"Color: \"+this.skin_color);System.out.println(\"#Teeth: \"+this.num_teeth);System.out.println(\"Tall: \"+this.tall + \"Length:\"+ this.length+\"Body Temperature: \"+ this.body_temperature);}}public class DataType{// define a class called DataTypepublic static void main(String[] args){//main function entry. JVM will run this function firstSystem.out.println(\"Testing datatype...\");Cat c = new Cat();c.printInfo();}} 编译一下java文件 12javac DataType.java #use java compiler to compile java source code to byte codejava DataType # run program on JVM 输出结果: 12345678Testing datatype...Name: MikeGender: Male#_legs: 4id: aColor: Orange#Teeth: 32Tall: 20.5Length:40.1Body Temperature: 37 6. Conclusion总结一下Java相对于C/C++, Python要注意的地方: 凡是像 3.14，这样的带小数点数字，java默认为double类型，如果要把它变成float，就要float a= 3.14f 后面加个f. Java 创建数组时，要么像C/C++那样直接int array[]; 或者int[] array申明，要么直接int[] array ={1,2,3...}来申明并初始化。还有一种是int[] array = new int[100] 这样来初始化但不赋值。Java里面一般是用int[] array 把type和[]一起写的写法 C/C++ 初始化数组时是int arr[]= {1,2,3....} 或者 int arr[100] Python 初始化数组是直接arr=[], arr=[0]*n之类的 Generic Type泛型里面一般不直接用T 类型 来初始化数组因为T 类型是未定的，但可以用Object类型再强制转换或者其他方式初始化 Java里面的System.out.println()是把input当成字符串那样直接通过”+”加号把多个输入串起来， 而printf()是和C的printf一样要输入格式如%d之类的。 6. Reference[1] Datawhale: https://github.com/datawhalechina/team-learning-program/blob/master/Java/11.%E6%B3%9B%E5%9E%8B.md[2] Java Tutorial: https://www.liaoxuefeng.com/wiki/1252599548343744/1265102638843296","link":"/2021/07/15/Java-2-BasicGrammar/"},{"title":"Java-5-OOD","text":"","link":"/2021/07/28/Java-5-OOD/"},{"title":"Java-4-class","text":"Introduction在面向对象编程里面， 类是一个很重要的概念，它可以用来抽象化事物的特性并封装到一个类，就比如猫这个类可以有像是颜色，年龄等等特点以及像是跑，跳等行为动作。 在类里面我们可以把它的特点描述成class member参数而动作等可以描述成函数/方法。而面向对象的编程主要有3个特性: 封装性: 即把一个对象的所以特性都抽象化参数化并用一个类去描述 继承: 一个类可以从另外一个类里面继承得到一些特征和函数，像是猫这类可以继承哺乳动物这类的一些特征，比如体温恒温，有四肢等。 通俗来说就是子类继承父类的特征和行为，使得子类对象（实例）具有父类的实例域和方法，或子类从父类继承方法，使得子类具有父类相同的行为。 多态: 多态是指同一个行为具有多个不同表现形式或形态的能力，同一个接口，使用不同的实例而执行不同操作Java OODClass Member and Object Class MethodConstructor在类中除了成员方法之外，还存在一种特殊类型的方法，那就是构造方法。构造方法是一个与类同名的方法，对象的创建就是通过构造方法完成的。每当类实例化一个对象时，类都会自动调用构造方法。构造方法的特点如下∶ 构造方法没有返回类型，也不能定义为void。 构造方法的名称要与本类的名称相同。 构造方法的主要作用是完成对象的初始化工作，它能把定义对象的参数传给对象成员。 public 修饰时是可以让构造函数在对象创建时被调用 例子： 12345public class Bird{ public Bird(){ }} 另外也可以将构建函数设计成有输入参数，并对成员变量进行初始化。 12345class Book { public Bird(int args){ //有参构造方法 //对成员变量进行初始化 }} keywords在面向对象编程里面，有些关键词也是很重要的 this 在类对象里面， this 关键词代指目前这个实例对象本身，然后在method里面可以通过this关键词去调用这个对象的参数. 当使用 this.var = … 来给类参数赋值，就是对类的成员对象进行赋值。如果没有this，只是var=… 那么就只是给一个变量进行赋值而不是成员变量赋值 其实，this关键字除了可以调用成员变量或成员方法之外，还可以作为方法的返回值。例如，在项目中创建一个类文件，在该类中定义Book类型的方法，并通过过this关键字进行返回。 1234567891011121314public class BorrowABook2 { // 创建借书类 String name; // 属性：书名 public BorrowABook2(String name) { // 参数为name的构造方法 this.name = name; // 将参数name的值付给属性name } public void borrow() { // 借书方法 System.out.println(\"请前往借阅登记处领取\" + name + \"。\"); // 输出借出的书名 } public static void main(String[] args) { // 创建参数为“《战争与和平》”的借书类对象， BorrowABook2 book = new BorrowABook2(\"《战争与和平》\"); book.borrow(); // 调用借书方法 }} 1234567// 通过this返回book的引用//之后用返回的引用对book进行操作public class Book{ public getBook(){ return this; //返回Book类引用 }} static很多时候，不同的类之间需要对同一个变量进行操作，比如一个水池，同时打开入水口和出水口，进水和出水这两个动作会同时影响到水池中的水量，此时水池中的水量就可以认为是一个共享的变量。在Java程序中，如果把共享的变量用static修饰，那么该变量就是静态变量。 Static Method Static method如果想要使用类中的成员方法，需要先将这个类进行实例化，但有些时候不想或者无法创建类的对象时，还要调用类中的方法才能够完成业务逻辑，这种情况下就可以使用静态方法. 1234567891011121314151617181920212223public class Pool { // 创建水池类 public static int water = 0; // 初始化静态变量之水池中的水量为0 public static void outlet() { // 放水，一次放出2个单位 if (water &gt;= 2) { // 如果水池中的水量大于等于2个单位 water = water - 2; // 放出2个单位的水 } else { // 如果水池中的水量小于2个单位 water = 0; // 水池中的水量为0 } } public static void inlet() { // 注水，一次注入3个单位 water = water + 3; // 注入3个单位的水 } public static void main(String[] args) { System.out.println(\"水池的水量：\" + Pool.water); // 输出水池当前水量 System.out.println(\"水池注水两次。\"); Pool2.inlet(); // 调用静态的注水方法 Pool2.inlet(); // 调用静态的注水方法 System.out.println(\"水池的水量：\" + Pool.water); // 输出水池当前水量 System.out.println(\"水池放水一次。\"); Pool2.outlet(); // 调用静态的放水方法 System.out.println(\"水池的水量：\" + Pool.water); // 输出水池当前水量 }} Static block1234public class StaticTest { static {// 此处编辑执行语句 }} Main Method主方法是类的入口点，它指定了程序从何处开始，提供对程序流向的控制。Java编译器通过主方法来执行程序。 主方法的语法如下∶ 123public static void main(String[] args){ // 方法体} 在主方法的定义中可以看到主方法具有以下特性∶ 主方法是静态的，所以如要直接在主方法中调用其他方法，则该方法必须也是静态的。 主方法没有返回值。和C/C++一样。 主方法的形参为数组。其中 args[0]～args[n] 分别代表程序的第一个参数到第n+1个参数，可以使用 args.length 获取参数的个数。 Coding Practice 练习1：设计加油站类和汽车类，加油站提供一个给车加油的方法，参数为剩余汽油数量。每次执行加油方法，汽车的剩余汽油数量都会加 2。 123456789101112class Car{ public Car(){ this.remaining_gas =0 }}class GasStation{ public GasStation(){} public void charge(Car c){ c.remaining_gas += 2; }} 练习2：智能手机的默认语言为英文。，但制造手机时可以将默认语言设置为中文。编写手机类， 无，参构造方法使用默认语言设计，利用有参构造方法修改手机的默认语言。 123456class SmartPhone{ String language = \"Chinese\"; public SmartPhone(){ this.language = \"English\"; }} 练习3：张三去KFC买可乐，商家默认不加冰块。但是张三可以要求加 3 个冰块。请利用有参构造方法实现上述功能。 123456class Cola{ int ice_cube = 0; public Cola(int num){ this.ice_cube = num; }} 练习4：创建教师类，类中有姓名、性别和年龄三个属性，在构造方法中使用 this 关键字分别为这三个成员属性赋值。 123456789101112Enum Gender {Male, Female};class Teacher{ String name; int age; Gender gender; public Teacher(String name, int age, Gender g){ this.name = name; this.age = age; this.gender = g; }} 练习5：一只大熊猫，长 1.3 米，重 90千克。在自定义方法中使用this关键字调用类的成员变量并在控制台输出这只大熊猫的信息。 1234567class Panda{ double tall = 1.3; // meter double weight = 90; //kg void printInfo(){ System.out.println(\"Panda info: Tall: \"+ this.tall + \" weight: \"+this.weight); }} 练习6：创建信用卡类，有两个成员变量分别是卡号和密码，如果用户开户时没有设置初始密码，则使用”123456”作为默认密码。设计两个不同的构造方法，分别用于用户设置密码和用户未设置密码两种构造场景。 123456789class CreditCard{ int password; public CreditCard(){ this.password = \"123456\"; } public CreditCard(String pw){ this.password = pw; }} Reference[1] Datawhale: https://github.com/datawhalechina/team-learning-program/blob/master/Java/5.%E7%B1%BB%E4%B8%8E%E6%96%B9%E6%B3%95.md [2] Runoob教程: https://www.runoob.com/java/java-inheritance.html[3] Java tutorial: https://www.liaoxuefeng.com/wiki/1022910821149312/1023442583285984","link":"/2021/07/20/Java-4-class/"},{"title":"Quick tutorial JavaScript with D3.js","text":"IntroductionAbout this passageThis passage simply introduces how to combine HTML, CSS and Javascript together to design a web page. Then D3.js, a javascript package for data visualization is introduced and applied to visualize some simple data. About HTML, CSS, JavaScriptHTML: HTML stands for Hyper Text Markup Language. It is the standard markup language for Web pages. Elements in HTML are the building blocks of HTML pages, represented by &lt;&gt; tags CSS: it stands for Cascading Style Sheets. It describes how HTML elements are to be displayed. JavaScript: JavaScript is the Programming Language for the Web. It can update and change both HTML and CSS. It can calculate, manipulate and validate data Combining HTML, CSS, JavaScript for a webpageTo start with, Let’s create a folder that will contain all HTML, CSS, JavaScript files we will create. Write a HTML fileFirst, we write an empty HTML file with title “Demo” and empty content for passage &lt;p&gt; &lt;\\p&gt; and heading &lt;h1&gt;&lt;\\h1&gt; 12345678910&lt;html&gt; &lt;head&gt; &lt;meta charset=\"utf-8\"&gt; &lt;title&gt; Demo&lt;/title&gt; &lt;/head&gt; &lt;body&gt; &lt;h1&gt;&lt;/h1&gt; &lt;p&gt; &lt;/p&gt; &lt;/body&gt; &lt;/html&gt; Then we put Javascript into HTML directly 1234567891011121314151617181920212223&lt;html&gt; &lt;head&gt; &lt;meta charset=\"utf-8\"&gt; &lt;title&gt; Demo&lt;/title&gt; &lt;/head&gt; &lt;body&gt; &lt;h1&gt;&lt;/h1&gt; &lt;p&gt; &lt;/p&gt; &lt;script&gt; var headers = document.getElementsByTagName(\"h1\"); for (var i = 0; i &lt; headers.length; i++) { var header = headers.item(i); header.innerHTML = \"Hello I'm Header\"; } var paragraphs = document.getElementsByTagName(\"p\"); for (var i = 0; i &lt; paragraphs.length; i++) { var paragraph = paragraphs.item(i); paragraph.innerHTML = \"I'm Passage\"; } &lt;/script&gt; &lt;/body&gt; &lt;/html&gt; The tag &lt;\\script&gt;&lt;\\script&gt; tells HTML the code inside this block is Javascript. In this script, we select the element with tag name ‘h1’ and ‘p’ using document.getElementsByTagName() and then input text “Hello I’m Header” and “I’m Passage”. This will let HTML display such texts in corresponding elements. Using Javascript file instead of internal scriptTo use JavaScript file rather than script inside HTML, we first create a JavaScript file called “myjs.js” in the same folder. Then copy the JavaScript code from HTML to Javascirpt. 12345678910var headers = document.getElementsByTagName(\"h1\"); for (var i = 0; i &lt; headers.length; i++) { var header = headers.item(i); header.innerHTML = \"Hello I'm Header\"; }var paragraphs = document.getElementsByTagName(\"p\"); for (var i = 0; i &lt; paragraphs.length; i++) { var paragraph = paragraphs.item(i); paragraph.innerHTML = \"I'm Passage\"; } In HTML file, we replace the script with a sentence, shown as below. 1234567891011&lt;html&gt; &lt;head&gt; &lt;meta charset=\"utf-8\"&gt; &lt;title&gt;D3 Demo&lt;/title&gt; &lt;/head&gt; &lt;body&gt; &lt;p&gt;Hello World &lt;/p&gt; &lt;h1&gt;&lt;/h1&gt; &lt;script id=\"myjs\" src=\"myjs.js\" type=\"text/javascript\"&gt;&lt;/script&gt; &lt;/body&gt; &lt;/html&gt; “id”，”type” attributes tell HTML we are looking for a file with name “myjs” and file type of JavaScript. Define style with CSS file Internal CSS code Elements’ styles, like color, font size, etc can be defined using CSS code. To add CSS inside HTML, here is an example: 1234567891011121314151617181920212223242526272829303132333435363738394041424344 &lt;html&gt; &lt;head&gt; &lt;meta charset=\"utf-8\"&gt; &lt;title&gt;D3 Demo&lt;/title&gt; &lt;/head&gt; &lt;style&gt; h2 { color: blue; font-size: 12px; font-family: \"Open Sans\"; text-anchor: middle; } &lt;/style&gt; &lt;body&gt; &lt;p&gt;Hello World &lt;/p&gt; &lt;h1&gt;&lt;/h1&gt; &lt;script id=\"myjs\" src=\"myjs.js\" type=\"text/javascript\"&gt;&lt;/script&gt; &lt;/body&gt; &lt;/html&gt; ~~~ The tag \\&lt;style&gt; tells the HTML the style settings (blue color, font-size = 12 pixel, ...) for all h2 element. &lt;br&gt;- __External CSS code__Usually, using a separated css file, rather than internal css code inside HTML file, can help further improvement for our project. It is more flexible to use css for element style design. To use external CSS file, replace the style codes in HTML with \\&lt;link&gt; tag~~~HTML&lt;html&gt; &lt;head&gt; &lt;meta charset=\"utf-8\"&gt; &lt;title&gt;D3 Demo&lt;/title&gt; &lt;/head&gt; &lt;link id =\"mystyle\" href=\"mystyle.css\" rel=\"stylesheet\" type=\"text/css\" /&gt; &lt;body&gt; &lt;p&gt;Hello World &lt;/p&gt; &lt;h1&gt;&lt;/h1&gt; &lt;script id=\"myjs\" src=\"myjs.js\" type=\"text/javascript\"&gt;&lt;/script&gt; &lt;/body&gt; &lt;/html&gt; Then Create a CSS file, called “mystyle.css” and copy the style code to CSS file 123456h2 { color: blue; font-size: 12px; font-family: \"Open Sans\"; text-anchor: middle; } The CSS file is linked to the HTML file by &lt;link&gt; tags in HTML. “id” is the identity of element “link” and “href” is set to be “mystyle.css” file. Attribute “type” is the file type of “mycss.css” file, hence it is css or text type. What is D3.jsD3.js is a JavaScript library for manipulating documents based on data. D3 helps you bring data to life using HTML, SVG, and CSS. D3’s emphasis on web standards gives you the full capabilities of modern browsers without tying yourself to a proprietary framework, combining powerful visualization components and a data-driven approach to DOM manipulation Import D3.jsTo import D3.js library, we can either add this line to the HTML file, then the D3.js library from the website will be applied. 1&lt;script src=\"http://d3js.org/d3.v3.min.js\" charset=\"utf-8\"&gt;&lt;/script&gt; Or download D3.js library from https://d3js.org/ to the folder directly.Here is an example of importing D3.js library and my javascript (D3-demo.js), css files (mystyle.css) 1234567891011121314&lt;html&gt; &lt;link id=\"mystyle\" href= 'mystyle.css' rel=\"stylesheet\" type=\"text/css\" /&gt; &lt;script src=\"http://d3js.org/d3.v3.min.js\" charset=\"utf-8\"&gt;&lt;/script&gt; &lt;head&gt; &lt;meta charset=\"utf-8\"&gt; &lt;title&gt;D3 Demo&lt;/title&gt; &lt;/head&gt; &lt;body&gt; &lt;p&gt;Hello World &lt;/p&gt; &lt;h1&gt;&lt;/h1&gt; &lt;script id=\"myjs\" src='D3-demo.js' type=\"text/javascript\"&gt;&lt;/script&gt; &lt;div id=\"chart\"&gt;&lt;/div&gt; &lt;/body&gt; &lt;/html&gt; Selection, Insertion, Remove for elements Selection and Modification 123456789101112131415161718192021222324252627// Using JavaScript to change URL to stylesheet and JavaScript in HTML// #mystyle ： element with id \"mystyle\"// #myjs ： element with id \"myjs\"// Note: to Select element by ID, we need to add # ahead id namecss_src= \"mystyle.css\"js_src=\"D3-demo.js\"var mycss= d3.select(\"#mystyle\").attr(\"href\",css_src)var myjs = d3.select(\"#myjs\").attr (\"src\",js_src)// Change Passage textvar p = d3.select(\"body\") .selectAll(\"p\") .text(\"d3js.org\");// Change color of passage text//修改段落的颜色和字体大小p.style(\"color\",\"red\") .style(\"font-size\",\"72px\");//Change &lt;h1&gt; element's text and text style: color, font sizevar h = d3.select(\"body\") .selectAll(\"h1\") .text(\"Heading\"); h.style(\"color\",\"red\") .style(\"font-size\",\"72px\");// Append &lt;h2&gt; tag to &lt;p&gt; section, with text \"Here is h2\"var h2 = d3.select(\"body\").select(\"p\").append(\"h2\").text(\"Here is h2\"); Bar Plot 1234567891011121314151617181920212223242526// Setup Canvas element by appending tag &lt;svg&gt; into &lt;body&gt; section// with width=300 pixels and height= 300 pixelsvar width = 300; //画布的宽度var height = 300; //画布的高度var svg = d3.select(\"body\") //选择文档中的body元素 .append(\"svg\") //添加一个svg元素 .attr(\"width\", width) //设定宽度 .attr(\"height\", height); // Append Rectangle bars to Canvas// Set width of each bar = each dataset value var dataset = [ 250 , 210 , 170 , 130 , 90 ]; var rectHeight = 25; //每个矩形所占的像素高度(包括空白)svg.selectAll(\"rect\") .data(dataset) .enter() .append(\"rect\") .attr(\"x\",50) .attr(\"y\",function(d,i){ return i * rectHeight; }) .attr(\"width\",function(d){ return d; }) .attr(\"height\",rectHeight-2) .attr(\"fill\",\"steelblue\"); Copy all codes above and open our HTML file in browser, we can see something like this. Reference[1] https://wiki.jikexueyuan.com/project/d3wiki/selection.html[2] https://d3js.org/[3] https://www.w3schools.com/","link":"/2020/07/13/JS-tutorial-with-D3js/"},{"title":"Some Useful Linux-commands","text":"Here is some useful linux commands du -h file check the file size pwd show the current directory location locate file find the file name alias name=”…” alias some commands tar -xzvf file.tar.gz -x extract file, -v show extraction process, -f output to given file name, -z using gzip or unzip to extract gz format cat /proc/cpuinfo display information of cpus nvidia-smi -L list the gpus. need to install nvidia first diff a b show difference between a and b files chmod ugo file-nameugo is a oct number used to set the access mode of the file, indicating r (read), w (write), x (executive) mode of the file to different groups of users.For example if ugo = 666, then it means user, group and other groups can read, write the file, but can not execute the file. 12345678910111213permission to: user(u) group(g) other(o) /¯¯¯\\ /¯¯¯\\ /¯¯¯\\octal: 6 6 6binary: 1 1 0 1 1 0 1 1 0what to permit: r w x r w x r w xbinary - 1: enabled, 0: disabledwhat to permit - r: read, w: write, x: executepermission to - user: the owner that create the file/folder group: the users from group that owner is member other: all other users Palmetto Commands: Need to be in login node (name node), not computing node or shell in jupyter Hub checkquota: check my disk space quota module avail : list available packages qstat -xf jobid: check the status of job with id: jobid qstat -Qf queuename : check status of a queue References[1] https://www.runoob.com/linux/linux-comm-tar.html","link":"/2020/11/13/Linux-commands/"},{"title":"ML-K-mean-Cluster","text":"Introduction to K-mean ClusteringIn Chinese, we usually say “物以类聚”， which means somethings in the same class can be grouped together based on their similar attributes. For example, when we group different types of fruit, like apple, cherry, blackberry, together based on their color, then we can group apple and cherry into the same class and the blackberry into another class. In K-mean clustering, it uses the same idea, by assigning different data points to the same cluster center based on the distance between data point and different cluster centers. By grouping data together simply, we can explore the similarity among data points and potential data pattern, which can help us analyze the internal features between data. Note that K-mean clustering is an unsupervised, non-parameter learning method, since it doesn’t use labels created by human or any numerical parameters/ weights like linear, logistic regression to estimate the distribution of data. How K-mean Clustering works?The steps in K-Mean-Clustering are following: Pick the number of clusters K Randomly assign K cluster centers in feature space In the feature space, each data point is assigned to the cluster, whose center is closest to this point, based on distance measurement (Usually Euclidean Distance) In each cluster, re-compute and update the center/mean value of this cluster based on data points in the cluster. Repeat Step 3 to 4 until the cluster of each data point doesn’t change, or called Converged. Here is an image to demonstrate the process of K-mean clustering. Distributed Version of K-mean ClusteringAs K-mean clustering is a simple straight-forward unsupervised learning method, it can be extended to distributed version easily.In distributed version of K-mean Clustering, we have: Partition big data data points evenly into n processes. Mapper Function: Each process uses its local data points to do K-mean clustering. Each process has the same K value Shuffle / Sort K cluster centers from each process. Then we have K * n cluster centers Reducer Function: In the Kn cluster center points, we do K-mean clustering again to find K cluster centers of Kn center points Send the K cluster centers found in step 4 back to each process (then each process has the same K centers) and repeat step 2 to 5 until K centers don’t move (Converge) Properties of K-mean ClusteringAlthough K-Mean Clustering is very easy to compute, it has different advantages and disadvantages. Advantage Computing mean of data is easy and fast Can explore potential similarity on low-dimensional data points Disadvantage Need to specify the K value and we don’t know which K is the best. Cluster Pattern can be affected by scale of data, Since K-mean clustering is distance-based method. If different attributes are not in same scale, the cluster pattern will be distorted and some data points may be assigned to wrong clusterSo K-mean Cluster need normalization of data Cluster is Sensitive to outlier data point (data outside the normal range of cluster) and cluster will shift a lot when computing mean.For example:I have a dataset like [-100, -1, 0, 1, 5,6,7], where -100 is a outlier point as it is pretty far away from other data points. If I have two cluster centers 0, 8, then [-100, -1, 0, 1] will belong to cluster 0 and [5,6, 7] belongs to cluster 8.When I update the centers of two clusters, they become (-100-1+0+1) / 4 = -25, and (5+6+7)/3 = 6. We can see that -25 this center is actually very far away from data points (-1, 0, 1) due to outlier -100. In the next assignment of points, all (-1, 0, 1) points will be assigned to another cluster based on distance. Hence the clustering pattern will be distorted. K-mean Clustering needs to store all data points to compute. It can lead to large space complexity when handling large data. K-mean Clustering can detect Convex patterns only, such as circle, retangle, triangle. But for non-convex patterns, like U-shape, V-shape, patterns, it can not make cluster correctly. Not work directly in high dimensional data and unstructured data like image. When to Use When we want to simply visualize the data distribution, we can use PCA, t-SNE to reduce dimension of data and use K-mean clustering to visualize them When the data is in low dimension Some extension of clustering K-medoid clustering: using median rather than mean to update clustring Hierarchical Clustering: use distance matrix as clustering criteria. Without the need of value K but need terminal state BIRCH CF-Tree: tree-based hierarchical clustering Density clustering DBSCAN OPTICS … Reference[1] https://i.ytimg.com/vi/_aWzGGNrcic/hqdefault.jpg","link":"/2020/11/23/ML-K-mean-Cluster/"},{"title":"ML Model - Regression models","text":"IntroductionThis article is to first introduce types of machine learning models and summarize properties of seven regression models: Linear Regression and Logistic Regression, Lasso Regression, Ridge Regression, Elastic Net Regression and Stepwise regression. Type of machine learning modelParametric/ non-parametric modelsIn general, Machine Learning model can be classified into two types of models based on the critic that if model uses parameters to estimate model or not.In this case, machine learning model can be divided into two types: parametric model and non-parametric model.The following are some useful machine models: parametric: Linear regression, Logistic regression Neural Network non-parametric: K-Nearest neighbor (K-NN) K-Mean Clustering Decision Tree Random Forest Naive Bayesian etc…Note that the “parametric” here is the parameters the model use to estimate the function, distribution. The parameters that control the complexity, performance of model are excluded. For example, K-Mean clustering and K-NN requires us to choose a k value to do clustering/ classification tasks. This k value is not the parameter we consider here. linear/non-linear model Linear modelThe linear model is the model that can be expressed by the formula$y’ = w_0 + w_1x_1 +….+w_nx_n = w_0+ \\sum_{i}^nw_ix_i$where x is the data point from dataset, $w_i$ is the parameters used to adjust the model and $w_0$ is the bias term. $y’$ is the prediction from the estimator.This formula is linear since it just involves first order terms and linear combination.Note that the real label is$y = \\epsilon + w_0+ \\sum_{i}^nw_ix_i$where $\\epsilon$ is the irreducible error between prediction $y’$ and true label $y$. In the following discussion, I discuss $y’$ prediction from model rather than label $y$. Non-linear modelNon-linear model in other word is the model that can not be expressed by the linear formula above. If there are second order term like $x^2, x^3, x_1x_2$, it is non-linear model as wellHence we can easly know that models like Naive Bayesian, K-NN, K-mean clustering, logistic regression, etc are non-linear models Linear RegressionThe formula of Linear Regression is$$y’ = w_0 + w_1x_1 +….+w_nx_n $$where $w_0$ is a constant and $y’$ is the prediction from model. To simplify it, we have: $$y’ = \\sum_{i=1}^nw_ix_i + w_0$$Obviously, the linear regression model is a linear model. Optimize Model Maximum Likelihood Estimation (MLE) Assume the data distribution is normal distribution, then the likelihood function used to estimate the real data distribution is: $$P(y=y_i|x) = \\frac{1}{\\sigma\\sqrt{2\\pi}} e^{-\\frac{(y_i-f(x))^2}{2\\sigma}}$$ where f(x) is the linear regression estimator, estimating the mean of normal distribution $\\mu$. Then the weight solution of the regression model is $$W = argmax_w P(Y|X)$$ Loss Function to minimize The loss function used to update the parameter in Linear regression is mean square error (MSE) $$ MSE = \\frac{1}{N}\\sum_i^N{ (y_i - y’_i)^2 }$$ or sum square error (SSE) $$ SSE = \\sum_i^N{ (y_i - y’_i)^2 }$$ where $y_i$ is the $i^{th}$ label and $y’_i$ is the $i^{th}$ prediction from model Properties of MSE/SSE: SSE / MSE error actually assumes that the distribution of data is Normal distribution and the Linear regression model is actually estimating the mean of Normal distribution with a fixed variance $\\sigma$. The negative log likelihood of normal distribution is as follow: $$ -log(\\Pi_i^nP(y=y_i|x_i)) \\ \\propto -log(e^{\\sum_i^n\\frac{-(y_i-f(x_i)^2)}{2\\sigma}}) \\ \\propto \\sum_i^n(y_i-f(x_i))^2 $$ where $y_i$ is the $i^{th}$ label and $x_i$ is the $i^{th}$ feature vector. It is easy to see that maximize the likelihood function is equivalent to minimize the negative log likelihood function or the SSE /MSE. Hence to minimize the MSE error is actually estimating the normal distribution function $P(Y|X)$. Properties Linear regression is an unbias model, which is sensitive to outliers.For example, In the following linear regression There is an outlier at x= 50, y= 30.During update with gradient descent w = w - 2x(y - wx). The outlier value changes the weight a lot and shift the line away from the $y=4x$. This is because in the update of parameter $w$, each data point is given equal weight to change the parameter $w$.Hence linear regression is sensitive to outliers. The output/prediction from linear regression is to estimate the expected value/ mean of normal distribution Logistic RegressionLogistic regression is an non-linear model as it can not be expressed into the linear form.The formula of Logistic Regression model: $$ P(y=y_i|x) = p_i = \\frac{1}{1+e^{-wx}} $$ Optimize Model Maximum Likelihood Estimation (MLE): we want to maximize the log likelihood function (The distribution estimated by the model) to get close to the real distribution function. In binary classification problem, we assume the data distribution is Bernoulli distribution, $ P(y=y_i|x) = p^{y_i}(1-p)^{1-y_i}$， since this distribution considers the possibility of P(y=0|x) and P(y=1|x). Then the log likelihood function / log Bernoulli distribution is following: $$ logP(y_1,y_2,..y_n| x_1, x_2..x_n) =log(P(y_1|x_1)P(y_2|x_2)..P(y_n|x_n))$$ $$ =\\sum_i^n log(p_i^{y_i}(1-p_i)^{1-y_i})= \\sum_i^n[y_ilog(p_i)+(1-y_i)log(1-p_i)]$$ Since we only care the effect of weight vector and data X to the likelihood function, it can be re-written as $$ \\sum_i^n[y_ilog(h_w(x_i))+(1-y_i)log(1-h_w(x_i))]$$ where $x_i, y_i$ are feature vector and binary label, $h_w(x_i) = p_i$ is the logistic regression, estimating the possiibility that if $x_i$ belongs to class 0 or class 1. The log function here is used to simplify the likelihood function and convert the multiplication into summation. This gives us easier way to analyze it. Maximizing the log likelihood is equivalent to maximize the original likelihood function. Notice that when $p_i$ predicted by the model is as same as its label $y_i$, then $p_i^{y_i}(1-p_i)^{1-y_i}$=1, the likelihood function is maximized. The optimal solution of weight matrix in logistic regression is $$W = argmax_W (\\sum_i^n(y_ilog(h_w(x_i))+(1-y_i)log(1-h_w(x_i)))) $$ Binary Cross-Entropy When we add the negative sign to the log likelihood function, we can get $$ - \\sum_i^n log(p_i^{y_i}(1-p_i)^{1-y_i})= -\\sum_i^n(y_ilog(p_i)+(1-y_i)log(1-p_i)) $$ This form is also called binary cross-entropy. To maximize the likelihood function to real distribution of data, is actually equivalent to minimize the binary cross entropy. Cross entropy is to measure the uncertainty between $y_i$ and $p_i$, $1-y_i$ and $1-p_i$. Higher the entropy is, less similar they are. So binary cross entropy assume the data is in Bernoulli distribution The goal to train the logisitic regression using binary cross-entropy is to let $h_w(x_i)$ get as close to $y_i$ as possible. Properties Logistic function actually projects the range of linear regression into range of [0,1]. Y-axis in logistic regression= possibility that input x belongs to one class, while the linear regression output range is ($-\\infty, +\\infty$). Logisitic regression is an bias model, since it considers the feature values X close to 0 are different and hence the gradient close to x=0 is large. However, when x is far away from x=0, such as $+\\infty, -\\infty $, the gradient of logistic function is close to 0 and it “considers” there is no much difference between two features x1 and x2 when both x1 and x2 $\\to \\infty$. Logistic Regression has vanishing gradient problem. Since the gradient of logistic regression is : $$(\\frac{1}{1+e^{-wx}})’ =\\frac{ e^{-wx}}{(1+e^{-wx})^2} = (\\frac{1}{1+e^{-wx}})(1-\\frac{1}{1+e^{-wx}})$$ when it predicts some sample X as a value very close to 0 or 1, the gradient will be very close to 0 and the weight is hard to update any more. The gradient is actually vanishing in this case. When it think a sample X is class y, then it won’t be willing to change its mind. This is a reason why it is “biased model” as well Logistic Regression is widely used for binary classification problem, it can be regarded as a two-classes version of softmax function. Using Maximum Likelihood estimation requires large data sample size in order to estimate the real distribution. Assumption in Logistic Regression is that the logarithem value of the ratio of possibility of class =1 to possibility of class =0 is linear, which can be estimated by linear regression.Proof:$$ log(\\frac{P(Y=1|X)}{P(Y=0|X)}) = log(\\frac{P(Y=1|X)}{1- P(Y=1|X)})=wx \\$$ $$ (1- P(Y=1|X))*e^{wx} = P(Y=1|X)$$ $$ P(Y=1|X) = \\frac{e^{wx}}{1+e^{wx}} = \\frac{1}{1+e^{-wx}}$$ Logistic Regression is to find a linear separation boundary /a line that separate the data in sample space for classification problem. Since in logistic regression $\\frac{1}{1+e^{-wx}}$, when $-wx&gt;0$, $h_w(x)&gt;0.5$ and it predicts class 1, otherwise, class 0. The line $-wx=0$ in 2-D space shown in the left figure below is actually the decision boundary. The data x with $-wx&gt;0$ is above the line and is classified as class1. Polynomial Regression$$y’ = w_0 + w_1x_1 + w_2x_2^2 +w_3x_3^3+…w_nx_n^n$$where y’, $w_i$, $x_i$ are all scalar values in this case and y’ is the predicted value from model. Different from linear regression, polynomial regression involves higher order terms. Optimize ModelSum square error$$ SSE = \\sum_i^N{ (y_i - y’_i)^2}$$It is as same as the linear regression Properties It is very easy for Polynomial regression to over-fitting as the order increase. It is an biased model as well. It predicts the expected value of Y, which is as same as linear regression To implement polynomial regression, we can first construct polynomial features, like $x_1^2, x_1x_2, x_2^2$, etc. Then apply linear regression to polynomial features Lasso regressionThe prediction model of lasso regression is still linear regression:$$y’=\\sum_{i=1}^nw_ix_i +w_0$$ Optimize Model Loss function to minimize in Lasso regression: $$min \\sum_i^n(y_i - wx_i -w_0)^2 + \\lambda||w||_{1}$$ where $y_i$ is a scalar value, a label, $x_i$ is a feacture column vector, $w$ is the weight vector. $\\sum_i^n(y_i - wx_i)^2$ is the least square error (or called L2 norm distance between y and x). Note that mean square error is scaled version of sum square error. Minimize the sum square error is equivalent to minimize the mean square error, so this term is as same as loss function in Linear regression The second term in the loss function is the L1- regularization term used to reduce overfitting effect, in which $||w||_1 = \\sum_i^n|w_i|$ is the L1 norm format. Principle of L1-Regularization term $$min \\sum_i^n(y_i - wx_i -w_0)^2 + \\lambda||w||_{1} $$ is equivalent to optimize the problem: $$min \\sum_i^n(y_i - wx_i -w_0)^2 \\ \\text{ subject to } ||w||_1 \\leq C $$ where C is a constant. Assume there are two variables in feature vector $x_i$ and only two weight variables, then to visualize it in 2-D space, we have: The x, y axis represent the weight variables. The blue point is the optimal point of the minimum least square error term. The orange region describes the constraints $||w||_1 \\leq C$, hence the intercepts on x, y axis are equal to $C$. The main idea of L1 regularization term is that when the weight vector satisfies the constraint, then the weight vector must be inside the orange region. We need to find the point with the smallest distance to blue point inside this region. When C is small enough such that weight vector can not reach the optimal point, then the model can reduce over-fitting effect. By using lagrange multiplier, the constraint problem can be converted to: $$min \\sum_i^n(y_i - wx_i-w_0)^2 + \\lambda (||w||_{1}-C) $$ where $\\lambda$ is the lagrange multiplier coefficient used to weigh the regularization term and $\\lambda$ is adjusted by user manually. Larger $\\lambda$ is, more regularization term affect. We can see that during minimization process, $C$ and $\\lambda$ are actually constants, so we can ignore this term and get: $$min \\sum_i^n(y_i - wx_i)^2 + \\lambda||w||_{1} $$ Properties It involves L1 regularization term to limit the range of weights and reduce over-fitting effect Lasso regularization can be used to do feature selectionSince in the figure above, we can see when the optimal solution to this constraint problem is at one of the angles of the square, some weight parameters are driven to be 0 and this leads to a sparse model. That is, the model filters out some features in $x_i$. As $\\lambda$ increases, more weights are driven to 0 and more features are not selected. We can not decide which features should be excluded in Lasso regression, since we don’t know which parameters become 0. Feature selection with lasso regression is not stable. Since during training the model, some weights may be very close to 0, but not equal to 0. That is, the features that are expected to be filtered out are actually not filtered out and still affect the model performance.In addition, different initialization of weights may lead to different feature selection. Ridge RegressionThe prediction model of Ridge regression is linear regression as well: $$y = \\sum_{i=1}^nw_ix_i +w_0$$ But the loss function is different. Read the following. Optimize Model Loss function to minimize in Ridge regression: $$min \\sum_i^n(y_i - wx_i -w_0)^2+ \\lambda||w||_{2}^2$$ Ridge regression is similar to Lasso regression, except that the regularization term here use L2 norm distance. Then the constraint region in 2-D space becomes a circle, shown in below. Since the constraint region becomes a circle, it is almost impossible to drive some weights to be zeros unless the blue point (optimal point of least square error) is on the axis. Hence, Ridge regression can not be used for feature selection. Properties Ridge regression can not be used for feature selection Ridge regression can be used to reduce multi-collinearity effect of features and soothe the over-fitting effect.multi-collinearity means that some features have a linear correlated relationship. For example, one feature value increases, another feature value may increase or decrease linearly. If model focuses on the similar features, it may become over-fitting on such feature. The reason is that in the circle region, assume features $x_1$ and $x_2$ are collinear, if $w_1$ increases and the model weigh more on feature $x_1$, then $w_2$ will decrease and model weighs less on feature $x_2$. In this case, model could avoid focusing on learning similar features heavily and reduce the multi-collinearity effect.3. Reduce the variance in model error and reduce over-fitting effect Ridge regression is more stable than Lasso regression for avoiding over-fitting Elastic Net RegressionThe prediction model of Ridge regression is linear regression as well: $$y’ = \\sum_{i=1}^nw_ix_i +w_0$$ Optimize Model Loss function to minimize$$min \\sum_i^n(y_i - wx_i- w_0)^2 + \\lambda_1||w||_{1} + \\lambda_2 ||w||_{2}$$ where $y_i$ is the label, $w$ is the weight vector and $w_0$ is the scalar bias. $x_i$ is the feature vector.Elastic net regression combines L1, L2 regularization terms together. Hence it can be regarded as the combination of weighed Ridge regression and weighed Lasso regression. Properties Combine the advantages of Lasso regression and Ridge regression. Stepwise RegressionIn statistics, stepwise regression is a method of fitting regression models in which the choice of predictive variables is carried out by an automatic procedure.Since there are multiple variable / features in feature vectors, we want to select the most important features to construct our regression model. Different combinations of variables lead to different regression model. Stepwise regression is to select the different feature variables and fit different models and then get the best one with least Residual sum square error Stepwise regression approachesAssume there are p features in dataset to pick. There are several types of approaches in stepwise regression for variable selection. Forward Stepwise Regression /selection Begin with the null model { a model that contains an intercept but no predictors. Fit p simple linear regressions and add to the null model the predictor that results in the lowest RSS. Add to that model the predictor that results in the lowest RSS among all two-predictor models. Continue until some stopping rule is satisfied, for example when all remaining variables have a p-value above some threshold. P-value is to measure how significantly different two models are using statistic technique There are p(p+1)/2 models to evaluate Backward Stepwise Regression (Backward elimination) Start with all predictors in the model. Remove the predictor with the largest p-value { that is, the predictor that is the least statistically significant. The new (p -1 )-predictor model is fit, and the predictor with the largest p-value is removed. Continue until a stopping rule is reached. There are p(p+1)/2 models to evaluate Bidirectional Stepwise Regression combine two methods above to see which predictor should be included or excluded from model Subset SelectionSubset selection is another variable selection method, in addition to stepwise regression Assume there are p features in dataset to pick fit all models for all subset pick the best model with smallest residual sum square error There are $2^p$ models to evaluate SummaryThis article summarizes the seven regression methods and their properties.Key things to note: Linear regression is to estimate the mean of normal distribution MSE assumes data is in normal distribution Logistic regression is an biased model to estimate the possibility that input x belongs to a class. Logistic regression assume data is in Bernoulli distribution. Binary cross entropy assumes data distribution is Bernoulli distribution Lasso Regression (L1 regularization) is good to do feature selection, but could be unstable. It can also reduce the over-fitting effect Ridge Regression can not be used for feature selection, but is good to reduce multi-collinearity effect and avoid overfitting Elastic Net Regression combines Lasso and Ridge regression Stepwise Regression and Subset selection are used to combine and select feature variables to find the estimators with the least SSE error. Reference[1] Stepwise Regression [2] https://www.analyticssteps.com/blogs/7-types-regression-technique-you-should-know-machine-learning [3] https://www.listendata.com/2018/03/regression-analysis.html [4] BIshop-PatternRecognition-MachineLearning.pdf [5] https://strata.uga.edu/8370/rtips/images/outlier.png","link":"/2020/10/01/ML-Model-LR/"},{"title":"Model Evaluation and Selection","text":"IntroductionIn order to do something better in our study, our jobs or even our life, evaluation step is an indispensable part of improvement. Otherwise, How do we know our work is good or bad? In addition, “good” and “bad” are ambiguous terms if we don’t have any evaluation methods. Similarly, in machine learning, in order to training a “better” model, we need concrete evaluation metrics to tell us if our models perform well, so that we can choose the best one. The goal of this passage is to conclude the common useful ways to evaluate and improve our machine learning models. My Thoughts are also provided. Evaluation Metrics Confusion MatrixIn classification task, we have predictions from our model on the test set and ground truth labels/targets indicating the real class of each sample in dataset.Let consider we are predicting if data belongs to class “C” or not. Then Confusion matrix is Actual class\\prediction C not C sum C TP FN actual P= TP+FN Not C FP TN actual N = FP+TN sum predicted P’ = TP+FP predicted N’ = FN+TN All True positive (TP): the amount of samples that are predicted as class “C” and they actually belong to class “C”. False positive (FP): the amount of samples that are predicted as class “C” and they actually DON’T belong to class “C”. False Negative (FN): the amount of samples that are predicted as Not class “C” and they actually belong to class “C”. True Negative (TN): the amount of samples that are predicted as not class “C” and they actually don’t belong to class “C”. Accuracy:measure how accurate the predictions are $acc = \\frac{TP+TN}{TP+TN+FP+FN}$ Error Rate: 1-acc Sensitivity/Recall: (Precentage of correct positive prediction on Actual positive )Recognition rate on True positive:$sens = \\frac{TP}{predicted P’} = \\frac{TP}{TP+FN}$ Specificity:(Precentage of correct negative prediction on Actual negative )How many positive predictions in true data are recognized by model?Recognition rate on True Negative: $spec = \\frac{TN}{predicted N’} = \\frac{TN}{TN+FP}$ Precision: (Percentage of correct positive prediction on all positive prediction) Measure what % of tuples that the classifier labeled as positive are actually positive How many positive predictions of your model are correct? $precision = \\frac{TP}{predicted P’} = \\frac{TP}{TP+FP}$ __F-score: $F_\\beta$__: It weighs precision and recall $$ F_\\beta = \\frac{(1+\\beta^2) * precision*recall}{\\beta^2 * precision+recall}$$ $\\beta$ controls the weight of precision. Higher $\\beta$ weigh more to precision than recall and hence precision is given more attention. F-1 score ($\\beta =1$): $$ F_1 = \\frac{2 * precision * recall}{precision+recall}$$ Both precision and recall are given equal weights. When to use Recall, Sensitivity and F-1 to evaluate model? Sensitivity/Recall:When we want the model to be more sensitive to positive cases and would like to predict more False Positive than False Negative.Example:In disease detection, we would like to use recall more than precision, since we want to detect disease and cure earily, we only care if we get disease or not (Positive or not)Cybersecurity, fault detection. We care if fault occurs/ prediction is positive or not only. Even if more FP than FN are in prediction, it help us reduce the possibility of missing faults (FN) Precision:when we care real positive cases, or both FN and TP. That is, we don’t want the model to predict more FP than FN. False negative also matters.Example:Spam detection: when mailbox detects a spam, it will directly delete/remove that email. However, if that email is actually not a spam (False positive), but an important email, then deleting it leads to an unexpected result.In this case, we want to reduce the false positive cases to avoid deleting an important email. Hence, we don’t use FP, but use FN and precision instead. F-Score:when we care both precision and recall, but want to weigh them.This depends on the assumption in the real cases. If the real case assumes both precision and recall are important, then we use F-score. Score used for GAN:Inception Score, FID. For more details, Please read this article Estimation of Model Accuracy Holdout The purpose of holdout is to evaluate how well the model performs on the unseen dataset. It is used to evaluate the both the framework of model and the hyperparameters of model Process: Split dataset into two independent datasets randomly: training set (usually 80%) and test set (usually 20%) Train the initial model with training set and then test model with test set Repeat steps 1~2 k times and calculate average accuracy of the k accuracies obtained Note: The test sets in k iterations may be repeated Accuracy of model could be unstable due to the split method on dataset.For example, if after randomly spliting dataset into training set and test set, all samples belonging to class “C” are in test set, then model can not learn features from class “C” and hence perform worse. Otherwise, it could perform better if it learn features from class “C” from training set. Cross Validation The purpose of Cross Validation is to evaluate the framework of the model, rather than how good the hyper parameters of the model is. K-fold 1. Split the training set into k-subsets: {D1, D2,..Dk} with (almost) equal size. 2. Loop through k iterations. At $i^th$ iteration, select subset Di as validation set and the remaining k-1 subsets as training set. 3. Compute accuracy of each iteration 4. Compute the mean of accuracies obtained in k iterations. Leave One out It is a case of k-fold cross validation with setting K= N, the number of data point in training set. It hence takes N-1 iterations to evaluate model. Stratified Cross validation It is a case of K-fold cross validation with each setting: each set contains approximately the same percentage of samples of each target class as the complete set. __For example:__ In a dataset S= {A1,A2,A3,B1,B2,B3,C1,C2,C3}, which has classes A,B,C. Each class occupies 30% of total dataset.In Stratified 3-fold Cross-validation, S= {K1,K2,K3}. To make each class of data in each fold have the same percentage, we have: K1 ={A1,B1,C1}, K2 ={A2,B2,C2}, K3 ={A3,B3,C3} such that the percentage of each class of sample in each fold is still 30%. Note for Cross-Validation When K is small, the variance in performance of model could be large, since each fold contains more data and hence becomes noisy. When K is large, eg. in LeaveOneOut, K=N, variance of performance of model would be smaller. In each fold of training set, we need to start training the initial model again, rather than training the model from the last fold. Cross validation is to evaluate the performance of the framework of the model, rather than of the parameters of the model. Bootstrap (Sample-based) Samples the given training tuples uniformly with replacement d times as training set. The remaining data tuples form the test set (or validation set).That is,the tuples/samples that have been chosen can still be chosen with equal possibility Train model with training set Compute average accuracy of model on both training set and test set Repeat steps 1~3 k times, then compute average accuracy of all accuracy obtained in k iterations. Comparison among methods Above holdout Purpose Holdout is mainly used to evaluate how well the model performs on the unseen dataset. It evaluates both framework and hyperparameter of the model. Different from cross-validation, the holdout data can be any size (usually 20% of the training dataset), while cross-validation requires validation data has the same size as each fold. When the amount of fold is small (like 2) in cross-validation, it could waste the training set. Advantages Less expensive in computation, Easy to compute, compared with cross-validation, since it splits test set randomly Disadvantages Estimated accuracy may be unstable (Accuracy is easy to change), since holdout depends on the dataset split methods on training set and test set. When to Use When the dataset is very large and hard to compute multiple subset Cross-validation Advantages The estimated accuracy is much stable than holdout , since it trains model on multiple different train-test set splits. It guarantees to train model with all samples. Disadvantages The Cost for computation is expensive when dataset is very large. When value K is smaller, the variance in performance of model will be larger and model is easier to overfit (training with more data samples could be more noisy). When value K is smaller, it could waste the training set. For example, in 2-fold, we use only half of training set to train the model, which is a waste of training data. When to Use when dataset is not very large (10000 samples or even more) when you have powerful computational device. Bootstrap Advantages Performance of model doesn’t depend on the split method on dataset When dataset is small or insufficient, or imbalanced (some classes are more than other significantly), it may reduce overfitting effect by sampling with replacement Less expensive on computation compared with cross validation Disadvantages Need to determine what sampling method to use When to Use when dataset is small or insufficient (In this case, we may use Over-sampling to repeat some data) when dataset is imbalanced when dataset is pretty large (In this case, we may use down-sampling to select part of data for training) Comparison of performances among different modelHow do we know the performances between models are similar? t-Test / Student’s t test Read this paper Model Selection ROC curve: receiver operating charatistic curve (true positive rate -VS- False positive rate)we first let model output possibility of each class and then set the threshold to convert possibility to 0, 1 binary classification labels.The threshold is usually default as 0.5.Based on the binary classification, compute true positive rate TPR/recall (TP/(TP+FN)) and false positive rate/FPR (FP/(FP+TN) = FP/real negative)Since threshold is set to 0.5, during training of model, the performance of model will change and hence TPR and FPR will change.The changing TPR and FPR leads to the ROC curve AUC: area under curve ( ROC curve)Since TPR and FPR range from 0 to 1, hence AUC has range [0, 1]. AUC usually is inside [0.5, 1] because a good model usually can classify sample correctly with 0.5 possibility. Physical Meaning of AUC:AUC is the possibility that a random-chosen positive sample is ranked more highly than a randomly-chosen negative sample.In other words, AUC is the possibility that a model’s prediction possibility of a random positive sample is more higher than a random negative sample. Then when AUC of a model is larger, it is more likely for the model to predict positive sample as 1 , rather than negative sample as 1. Example: when threshold =0.5 Case 1 Sample A B C D Possibility 0.9 0.8 0.51 0.3 Prediction 1 1 1 0 Ground Truth 1 1 0 0 In case 1 we see that possibility that positive sample is ranked higher than negative sample, since all positive sample has possibility greater than negative sample and TPR = 2/2=1 as FPR = 1/2 =0.5 &lt; TPR. Hence in this case, it is a good model. Case 2 Sample A B C D Possibility 0.9 0.3 0.51 0.6 Prediction 1 0 1 1 Ground Truth 1 1 0 0 In case 2, we see that TPR = 1/2 &lt; FPR= 2/2=1 and AUC is small. The possibility that positive sample ranked higher than negative sample is small. The prediction possibilty of positive sample is also small than negative sample in sample B,D. Hence the model performance is not good enough. Improvement on Accuracy: Ensembling method Bagging (bootstrap aggregation) Main Idea:Its goal is to reduce variance by using multiple classifiers, like decision tree. It uses boostrap method to sample data and train multiple classifiers and then average the prediction over a collection of classifiers (for continuous value prediction, regression), or return the prediction with maximum votes (forclassification)Random forest is a bagging approach, which bags a set of decision trees together. Assumptions we have training set with size of D and a set of models with size of K Training: Similar to Bootstrap, at each iteration i, a training set Di of d tuples is sampled with replacement from training set. A classifier model Mi is learned for each training set Di Prediction: Each Classifier Mi returns prediction for input X. Discrete value output: The bagged classifier counts the votes and assigns the class with the most votes to X Continous value output:take the average value of each prediction for a given test sample. Advantages: Better than a single classifier from the classifier set. More robust in noisy data and hence smaller variance Disadvantages: 1.Its training depends on sampling techniques, which could affect the accuracy The prediction may be not precise, since it uses average value of classifiers’ predictions.For example, if valid prediction values are 1,2,or 3, then the average of predictions from different model could lead to a floating point number. Properties of Random Forest Comparable in accuracy to Adaboost, but more robust to errors and outliers. Insensitive to the number of attributes selected for consideration at each split, and faster than boosting Boosting Main Idea:Its goal is to improve accuracy, let models better fit training set. It uses weighted votes from a collection of classifiers Training: Each training sample/tuple is given a weight, eg $w_i$ for the $i^{th}$ tuple. Then we have training set {(X0,y0, w0), … ,(Xi,yi, wi)}where $X_i$ and $y_i$ are training sample and target We have k classifiers {M0, M1,…Mk}. Each classifier is learned from the whole training set iteratively. That is, if we have k classifiers, then we need to iterate the training set at least k times (at $i^{th}$ iteration, we train the $i^{th}$ classifier), so that each classifier can learn the training set. After classifier $M_i$ is learned on training set, classifier $M_{i+1}$ paies more attention to the training samples that are misclassified by $M_i$ Prediction: The final Model combines the votes of each individual classifier. Either find the prediction with largest sum of weights (Classification), or find the average of all prediction values (Regression) The weight of each classifier’s vote is a function of its accuracy Advantages Boosting can be extended to numeric prediction Better fit the training set since it adjusts the weights of training set and gives more attention to the misclassified sample. Disadvantages Easy to overfit. Need Additional techniques to avoid overfitting. (I will discuss the methods dealing with Overfitting ). Questions: How to pay more attention to misclassified samples? give Higher weights? But How to compute weights?Answer: This depends on the actual boosting algorithm, like GradientBoosting, AdaBoosting AdaBoosting Assumption:Assume we have training set with size of D and a set of classifier models with size of T __Error of model $M_i$__ Error($M_i$) = $\\sum_i^D (w_i \\times err(X_i))$ if using normalized weight (weight in range [0,1]), then Error($M_i$) = $\\frac{\\sum_i^D (w_i \\times err(X_i))}{(\\sum_j^D w_j)} $ Note: In classification, $err(X_i)= 1(C_i(X_i) !=Y_i)$, $C_i(X_I)$ means the prediction of model $M_i$ on sample $X_i$. If the prediction is correction $error(X_i) =0$, otherwise 1. Weight of model $M_i$’s voting: $\\alpha_i$ $\\alpha_i = log\\frac{1-error(M_i)}{error(M_i)} + log(K-1)$. Note: K = the number of classes in dataset. When K=1, log(K-1) term can be ignored Update of weight $w_i = w_i \\cdot exp(\\alpha_i \\cdot 1(M_j(X_i) != Y_i))$ The weight $w_i$ of the $i^{th}$ training tuple $X_i$ is updated by timing exponential value of weight of model only when this model $M_j$ misclassifies the $X_i$ ( That is $M_j(X_i) !=Y_i$ and hence $1(M_j(X_i) !=Y_i) =1 $). Prediction $C(X_i) = argmax_{k} \\sum_{j=1}^T \\alpha_{m}\\cdot 1(M_j(X_i)== Y_i)$ where $1(M_j(X_i)== Y_i)$ is equal to 1 if prediction is correct, otherwise, 0. The prediction of the whole model has the largest sum of weight of models, NOT the weight of training tuple! More detail for AdaBoosting, Read this paper Stacking Main Idea:It combines and trains a set of heterogeneous classifiers in parallel.It consists of 2-level models:level-0: base modelModels fit on the training data and whose predictions are compiled.level-1: Meta-ModelIt learns how to best combine the predictions of the base models. Training: split the training data into K-folds one of base models is fitted on the K-1 parts and predictions are made for Kth part. Repeat step 2 for each fold Fit the base model on the whole train data set to calculate its performance on the test set. repeat step 2~4 for each base model Predictions from the train set are used as features for the second level model. Classification: Second level model is used to make a prediction on the test set. Advantage: It harness the capabilities of a range of well-performing models on a classification or regression task and make predictions that have better performance than any single model in the ensemble. Disadvantage: It could be computational expensive since it uses k-fold method and use multiple level models. Comparison among Ensembling, boosting and bagging Goal of bagging is to reduce variance and noise while boosting is to improve accuracy using weighted models. Stacking is to improve accuracy of model using hetergenerous models. Adaboost let classifiers pay more attention to the misclassified samples, but if those misclassified samples are outlier or noisy data, it will affect a lot and lead to larger variance.However, bagging and ensemble uses averaging and voting methods and each classifier has equal weight, which is less sensitive to the noise data and outlier. Reference[1] https://blog.csdn.net/weixin_37352167/article/details/85028835[2] https://machinelearningmastery.com/stacking-ensemble-machine-learning-with-python/[3] https://en.wikipedia.org/wiki/Student%27s_t-test[4] https://media.springernature.com/lw685/springer-static/image/art%3A10.1038%2Fnmeth.3945/MediaObjects/41592_2016_Article_BFnmeth3945_Fig1_HTML.jpg","link":"/2020/08/14/Model-Eval-Selection/"},{"title":"Markdown Quick Tutorial","text":"Source File For this tutorial: Click Here Example 1: Markdown Code: 123456_Italic_ *Italic* __Bold__ **Bold**&lt;span style=\"color:blue\"&gt;Blue&lt;/span&gt;_&lt;span style=\"color:red\"&gt;Red and Italic&lt;/span&gt;____&lt;span style=\"color:red\"&gt;Red and Italic and Bold&lt;/span&gt;___**_&lt;span style=\"color:red\"&gt;Red and Italic and Bold&lt;/span&gt;_** Output: Italic ItalicBold BoldBlueRed and ItalicRed and Italic and BoldRed and Italic and Bold Example 2:Markdown Code: 123456789~~Strickout~~&lt;u&gt;Underline&lt;/u&gt;创建脚注格式类似这样 [^Footnote]。[^Footnote]: Here is footnoteoutput \"*\" \\* \\_ Output StrickoutUnderline 创建脚注格式类似这样 [^Footnote]。 [^Footnote]: Here is footnote output ““ \\ _ Example 3Markdown Code: 12345678910111213141516__Here is check list__- [ ] my- [ ] check- [x] list__Here is Emoji__:blush::smile::angry::cry::joy: &gt;Block&gt;1. Block 1&gt;2. Blokc 2 Output Here is check list my check list Here is Emoji:blush::smile::angry::cry::joy: Block Block 1 Blokc 2 Example 4Here is code function() 1Code Block 1int text= C Code Block Col1 Col2 Col3 a b c 左对齐(left) 居中(center) 右对齐(right) 默认左对齐(default) a b c d list 1 list 2 list 3 list 1 list 2 list 3 Example 5Markdown Code: 1234567891011121314151617181920&lt;image src=\"https://gst-online.com/wp-content/uploads/2018/07/16679084-abstract-word-cloud-for-representation-with-related-tags-and-terms.jpg\"&gt;[Here is a link](https://google.com/)[1]: http://static.runoob.com/images/runoob-logo.png&lt;h1&gt;Embeded HTML&lt;/h1&gt;&lt;h2&gt;Hearder&lt;/h2&gt;&lt;h3&gt;Hearder&lt;/h3&gt;使用 &lt;kbd&gt;Ctrl&lt;/kbd&gt;+&lt;kbd&gt;Alt&lt;/kbd&gt;+&lt;kbd&gt;Del&lt;/kbd&gt; 重启电脑## math formula： Need to be loaded on browser$$\\mathbf{V}_1 \\times \\mathbf{V}_2 = \\begin{vmatrix} \\mathbf{i} &amp; \\mathbf{j} &amp; \\mathbf{k} \\\\\\frac{\\partial X}{\\partial u} &amp; \\frac{\\partial Y}{\\partial u} &amp; 0 \\\\\\frac{\\partial X}{\\partial v} &amp; \\frac{\\partial Y}{\\partial v} &amp; 0 \\\\\\end{vmatrix}${$tep1}{\\style{visibility:hidden}{(x+1)(x+1)}}$$ Output Here is a link[1]: http://static.runoob.com/images/runoob-logo.png Embeded HTML Hearder Hearder 使用 Ctrl+Alt+Del 重启电脑 math formula： Need to be loaded on browser$$\\mathbf{V}_1 \\times \\mathbf{V}_2 = \\begin{vmatrix}\\mathbf{i} &amp; \\mathbf{j} &amp; \\mathbf{k} \\\\frac{\\partial X}{\\partial u} &amp; \\frac{\\partial Y}{\\partial u} &amp; 0 \\\\frac{\\partial X}{\\partial v} &amp; \\frac{\\partial Y}{\\partial v} &amp; 0 \\\\end{vmatrix}${$tep1}{\\style{visibility:hidden}{(x+1)(x+1)}}$$ Hello","link":"/2020/07/09/Markdown-Tutorial/"},{"title":"NLP Improvement on Word2Vector","text":"IntroductionAlthough word2Vec for word embedding has been widely used, it has some obvious shortages that affect computation of word vector. This passage is to identify those shortages and introduce a solution called Negative Sampling to solve the problems. Review to Word2VecWord2Vec is a framework to convert vocabulary in texts into dense numeric vector representation such that our machine learning models can realize the vocabulary and human language and learn something.The main idea of word2Vec (skip-gram) is following: Start with random word vectors in neural network Iterate each world in word corpus Predict the context words (surrounding words) of a center word by computing posterior distribution: $P(w_{t+1}|w_t)$, where $w_t$ is the center word at position t and $w_{t+1}$ is the surrounding word. $$P(o|c) = \\frac{exp(\\textbf{u}_o^T\\textbf{v}c )}{\\sum{w\\in V} exp(\\textbf{u}_w^T\\textbf{v}_c)}$$ where $\\textbf{v}_c$ is the word vector of center word and $\\textbf{u}_o$ is the word vector of surrounding words (or weights). Update word vector using Gradient Descent based on cost function Note: $\\theta$ in loss function here is the weight matrix used in softmax. The weight matrix in the network is $\\textbf{u}_w$. Disadvantages of Word2VectorWe can notice that bigger vocabulary it is, larger the word vector becomes. Usually, there are thousands of different words in text, using gradient descent to update the whole weight matrix leads to expensive computation cost and each update become super slow. We need to repeat updating each weight using the following equation and the time complexity increases linearly as the amount of words increases. ImprovementStochastic Gradient Descent with sampled windowAssume we are using a window centered at center word and hence it has size of 2m+1. The the update is Repeatedly sample windows and iterate each window, rather than iterate all windows. Compute the gradient of the words that actually appear. The graident of words in dictionary that don’t appear in text won’t be updated. Notes: the gradient of the words that don’t appear in text, but in vector is all 0. In this case the vector would be very sparse (many zeros in vectors). It is a waste of time to compute those 0 update. We need a sampling technique to sample windows and words for updating part of weights. This leads to our next section Negative Sampling Negtive SamplingIn word2vec (skip-gram), the input to the neural network is one-hot word vector of the center word. In training step, output of neural network is a vector of possibility that each word can appear given the center word.In prediction step, the output is converted from possibility vector to one-hot vectors of the predcited context words that are most likely to appear given the center word. For example, if we have possibility output [0.1, 0.1, 0.5, 0.3] and expect predictions of 2 context words. Then we output one-hot vectors of the words with possibility of 0.5 and 0.3.The targets corresponding to the given center word are one-hot vector of context words surrounding this center word in the window. Negative wordsIn one-hot vector output from neural network, we call the word with value equal to 1 as postive word and those words with values equal to 0 as negative word. For example. Assume we have a word vector with dimension of 100 (100 words in dictionary). Then in a window such as “I like my dog”, I like my dog if “like” is center word $w_t$, as the input to the neural network. Then “I” , “my”, “dog” are context words, or positive words that are expected to output “1” in the output one-hot vector of neural network(it is expected to output 3 one-hot vectors). Then other words that don’t appear in this window / context and are expected to be 0 in one-hot vector are negative words. Selection of negative wordsHowever, there would be a large amount of negative words that don’t appear in context. If we update weights of all negative words, the update could be very inefficient.In negative sampling, we sample negative words based on the possiblity that word may occur. The possibility is given by: $$P(w_i)= \\frac{f(w_i)}{\\sum_j^Nf(w_j)}$$ where $f(w_i)$ is the number of word $w_i$ appears in corpus and the denominator is the number of all words appear/ the amount of words in corpus. However, the author proposes this equation since it gives the best performance. $$P(w_i)= \\frac{f(w_i)^{3/4}}{\\sum_j^Nf(w_j)^{3/4}}$$ Based on the possibility of the occurence of words, we can sample a number of negative words that are most likely to appear and update the corresponding weights by backward update in neural network. The number of samples is set by user.For example, If we have a dictionary with size of 100 words (hence one-hot vector with size of 100) and each word has corresponding weights with size of 100. Then there are 100x100 weights to update when updating all weights in network. However, if we only sample 20 negative words to update, we need to update 20x100 weights only. This allows us to speed up training step. References[1] http://mccormickml.com/2017/01/11/word2vec-tutorial-part-2-negative-sampling/ [2] https://arxiv.org/pdf/1310.4546.pdf","link":"/2020/07/29/NLP-Word2Vec-Improvement/"},{"title":"NLP Word Representations","text":"1. Representation:1.1 WordNeta thesaurus containing lists of synonyms and hpernyms, using “is-a” to denote the relationship among words. Advantages: easy to understand the relationship. It is interpretable. Disadvantages: good as resource, but missing nuance in different context. Eg: “proficient” doesn’t always mean “good” Hard to update with new meanings of words, since the meanings of words may change along time subjective and bias Require Human to label Can’t compute accurate word similarity 1.2 One-hotRepresent word as discrete symbol, eg. 0 or 1Example: In sentence “I like dog”, the vector for “I” =[1,0,0], the vector for “like” =[0,1,0],the vector for “dog” =[0,0,1] word one-hot vector “I” 1 0 0 “like” 0 1 0 “Dog” 0 0 1 Note: Usually, the number of of vocabulary is equal to the number of entry in the one-hot vector. This enables the (Manhatton) distance between any two words/labels is same and the feature of each word is independent with each other.The feature of a word contains no information about another word and hence don’t affect other words. Moreover, this method also has physical meaning and easy to understand.For example:Assume in linear regression,we have $y =w_1x_1 + w_2x_2 + w_3x_3 $, in vector representation it is $Y = [w_1, w_2, w_3] \\cdot [x_1, x_2, x_3]^T = WX$Let vector X be a one-hot vector. That is, one of $x_1, x_2, x_3$ must be one. In this case, $WX$ becomes a lookup table to choose which weight $w_1, w_2, w_3$ should be learned. In this case, weight $w_1$ is affected by feature $x_1$ only, without being affected other features. This can lead to faster convergence of $w_1$ in learning . Advantage: able to represent words into number for computing Easy to understand, since it has physical meaning Features/ labels are independent from each other Disadvantage: Vector dimension equal to the number of words in vocabulary, it could be very big No natural notion of similarity, since word vectors are orthogonal. Hard to extend the representation of labels when more labels are added. Could be memory expensive when a large group of labels involved. Eg, represent vocabulary in a book as one-hot vector, the vectors could be very large and sparse. 1.3 Bag of Words (BOW)Similar to one-hot, it represents each word as orthogonal vector, but the number of vector is the frequency the word appears. In BOW, a text (such as a sentence or a document) is represented as the bag (multiset)/set of its words, disregarding grammar and even word order but keeping multiplicity. Advantage: Easy to realize and compute Easy to use for simple case (the order of words don’t matter a lot) Disadvantage: Without considering similarity among different words it has the same drawbacks as one-hot vector 1.4 n-gram modelN-gram model is an extension of bag of word model. While bag-of-word model considers each word only, N-gram model considers a tuple of n consecutive words as an element in the collection of words. Example: Consider a sentence: “a boy is playing grames”. When using 3-gram model, the collection of words becomes {(a boy is), (boy is playing),(is playing games)} Then if we have a 3-gramm vector for a document: [1, 0,0 ], it means the element (a boy is) appear once in the document. Advantage: More flexible and robust than bag-of-world model, since it could considers different structure of words. Some names like “deep learning”, “machine learning” with more than one words can be detected easier. Disadvantage: The collection could be large when using different grams. Usually, bigram and trigram are used. 1.5 Word Vectors/ Word Embeddings/Word distributed representationsWord Vectors represent words by context. Context of a word is a set of words that appear nearby, or in a fixed window A word meaning is given by the words that frequently appear close-by. Each entry in a word vector for a word is the similarity between this center word and the words in context. Eg: In “I like dog”, vector for “like” = [0.01,0.7 ,0.5], where 0.5 is the possibility “dog” will occur, given center word “like”. More general, given the center word $w_{t}$, the possibility of context word $w_{t+1}$ will occur is $P(w_{t+1}|w_{t} )$. Then in word vector, our goal is to find the best condition distribution to represent the vector. There are many frameworks to find word vector, such as word2Vec, Glove, Fasttext. Advantage: Able to compute similarity between words No need to label by human Disadvantage: Dimension of vector is equal to the number of words in vocabulary. It could be very large Need to learn the similarity by word2vector framework. Hard to compute similarity when there is a large corpus of words 2. Word2Vector: Skip-gramA framework to learn and find dense word vectors, rather than sparse orthogonal vector like bag-of-word model. Here is details about word2vector The dense word vectors measure similarity between two words. If two words are similar, then they have similar word vectors. Goal: to learn the word vector that measure the similarity between context and center word, Or possibility that the next word will occur, given the center word. 2.1 Summary of Idea in Word2Vector: Collect a large corpus of text Represent each word in text as a vector (count vector or one-hot model) Go through each position t in text, find center word c and outside words o (context words) Find similarity of the word vectors for c and o (c and o are actually from the output of the neural network), compute possibility of context o, given center word vector c Keep adjusting word vectors / train the network to maximize possibility/likelihood function and find the optimal word vectors that contains similarity between c and o. Note: input to network is one-hot vector , output to network is similarity/possibility vector. The output is the vector we want 2.2 One-hot Vector In Skip gram, it first find the set of vocabularies and then converts each word into its one-hot vector. In one-hot vector, 1 represents the word Example:consider a sentence “I like dog and you like cat”.Then there is a set of words [“I”,”you” ,”like””,”dog”,”cat”,”and”]. word one-hot vector “I” [1,0,0,0,0,0] “like” [0,1,0,0,0,0] “dog” [0,0,1,0,0,0] “and” [0,0,0,1,0,0] … … Then one-hot vector for “like” = [0,0,1,0,0,0], where 1 at the corresponding position represents “like” in the word set. Dimension of vector = |V| , where V is vocabulary set of the text. 2.3 Skip-gram neural network The neural network takes one-hot vector as input, with size |V|. Let denote one-hot vector as v for convenience There are only one hidden layer. There is no activation function in this layer and hence it is linear. The number of neurons N is defined by user. Softmax activation function for output with |V| neurons There is a input weight matrix W with dimension |V|-by-N between input layer and hidden layer (|V| rows and N columns) There is a input weight matrix W ‘ with dimension N-by-|V| between hidden layer and output layer Since input is 0,1 vector, when it times input matrix W, it actually selects a row of the matrix.Hence, we can consider W as a look-up table and output from hidden layer c = Wv is the real “word vector”. “Word vector” is acutually one-hot vector or bag-of-word vector times input weight matrix W The i^th entry in the output vector from softmax function is the possibility that if you pick up a word nearby the input word, that is the i^th word in the vocabulary. Note: when training neural network, its output is one-hot vector ( set the maximum softmax output value as 1, others as 0s), indicating the predicted nearby context words When evaluating network, using softmax output value, possibility as output, to compute the cost value __Since there are different actual context words $w_{t-1}, w_{t-2}, …$, corresponding to the single input vector $w_{t}$.__If we consider P($w_{t-1},w_{t-2},w_{t-3}|w_t$) with window with size of 4 containing central word $w_t$ and context words $w_{t-1},w_{t-2},w_{t-3}$,then the output look something like this: 2.4 Evaluation of Skip-gramSince the input to the network is a single word, it is also regarded as center word $w_t$, at the t position in text. The words nearby it are called context, or outside words. The context word at t+k position, is denoted as $w_{t+k}$. Hence our goal, or objective is to estimate the distribution $P(w_{t+k}| w_{t})$.The distribution has the meaning that given center word $w_t$, the possibility that context word $w_{t+k}$ will appears. In order to estimate the distribution, we need to maximize the likelihood function, or its log value, called loss, or cost function, shown as below. Advantages It is unsupervised learning hence can work on any raw text given. It requires less memory comparing with other words to vector representations. It requires two weight matrix of dimension [N, |v|] each instead of [|v|, |v|]. And usually, N is around 300 while |v| is in millions. So, we can see the advantage of using this algorithm. Disadvantages Finding the best value for N and the context position is difficult. Softmax function is computationally expensive. The time complexity for training is high Models for Word2vector Skip-gramPredict context (outside) words (surrounding the center word) given centerword Continuous Bag of words （CBOW）Predict center word from (bag of) context words. It is an inverse version of skip-gram. Reference:[1] http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/[2] http://web.stanford.edu/class/cs224n/slides/cs224n-2020-lecture02-wordvecs2.pdf[3] https://towardsdatascience.com/skip-gram-nlp-context-words-prediction-algorithm-5bbf34f84e0c[4] https://zhuanlan.zhihu.com/p/50243702[5] http://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf","link":"/2020/07/08/NLP-Word-Representation/"},{"title":"PySpark-Note-1","text":"IntroductionThis note is to introduce some useful functions in PySpark and also do some practice with them to analyze SF crime dataset. Then KMean Clustering model is applied to show how to use ML model in PySpark. List of Useful Functions Create Spark Session SparkSession Create spark session, the main entry to create DataFrame, register DataFrame as tables, execute SQL over tables, cache tables, and read parquet files. 123456from pyspark.sql import SparkSessionspark = SparkSession.builder \\.master(\"local\") \\.appName(\"Word Count\") \\.config(\"spark.some.config.option\", \"some-value\") \\.getOrCreate() Explanation: .master(‘local’): sets the Spark master URL to connect to. “local” mean run spark locally. Just like a local server .appName(“Word Count”) : application name .config(“spark.some.config.option”, “some-value”): configure some key-value pair in application .getOrCreate(): Gets an existing SparkSession or, if there is no existing one, creates a new one SQLContext As of Spark 2.0, this is replaced by SparkSession. However, this class is kept here for backward compatibility. A SQLContext can be used create DataFrame, register DataFrame as tables, execute SQL over tables, cache tables,and read parquet files. Load data After we create a SparkSession, we can use the following code to read data from CSV or JSON file. Returned Object: PySpark DataFrame object csv12345678# Set delimiter = \";\" between columns# Skip header of file if header=True. # Otherwise, load header as data recordspark.read.options(header= True,delimiter=\";\") \\.csv(\"path-to-dataset/dataset.csv\")# orspark.read.options(header= True,delimiter=\";\") \\.format('csv').load(\"path-to-dataset/dataset.csv\") json123spark.read.json(\"path-to-dataset/dataset.json\")#orspark.read.format('json').load(\"path-to-dataset/dataset.json\") Useful functions of pyspark.sql.DataFrame with SQL Create DataFrame and SQL table df.createDataFrame(data, schema=None): create PySpark dataframe data: a list of tuples,each tuple contains the data of a row schema: a list of column names of dataframe 12345spark = SparkSession.builder\\.master(\"local\").appName(\"Word Count\")\\.config(\"spark.some.config.option\", \"some-value\").getOrCreate()l1 = [('Alice', 1)]new_row1 = spark.createDataFrame(l1,[\"name\",\"age\"]) df.createGlobalTempView(view_name) Creates a global temporary view with this DataFrame. df.createOrReplaceTempView(view_name): Creates or replaces a local temporary view with this DataFrame. df.registerDataFrameAsTable(table_name): Registers the given DataFrame as a temporary table in the catalog. Show Columns, Rows, Statistic description df.columns: a list of column names df.summary(): Computes specified statistics for numeric and string columns, with count - mean - stddev - min - max. df.describe(): Computes basic statistics for numeric and string columns. Similar to summary() df.printSchema(): print Schema / decriptions of each column, like data type Query and Selection df.head(10): return the top 10 rows in Row type, not DataFrame df.tail(10):return the last 10 rows in Row type, not DataFrame df.where(condition) or df.filter(condition) : select the rows which satisfy the conditions. Return a DataFrame 12345678# Select ages that are &lt; 100 and City name = \"SF\"# Using SQL expression in conditionsdf.where(\"age &lt; 100 and city in ('SF') \")#Using Spark APIfrom pyspark.sql import Rowfrom pyspark.sql.functions import coldf.where(col(\"age\")&lt;100 &amp; col('city').isin([\"SF\"])) df.column.isin([‘a’,’b’])Check if the value in a column is in the list or not df.column.between(lower_bound, upper_bound)check if the value is within a range or not df.select([“column-name”]):select the columns based on a list of given column-names df.take(N):Return a list the top N rows in Row() format df.collect():convert pyspark dataframe to a list of row in Row() format Handle data df.withColumn(‘column_name’, col):append a column to dataframe with name “column_name” col: a Column expression for the new column. we can use UDF (user defined function) to it as well. 1df = df.withColumn(\"age\", df.age+2) df.withColumnRenamed(“old name”,”new name”):rename a column in the dataframe df.drop([“column-name”]), df.dropna(subset =[“column-name”])):drop columns and drop the rows with NaN in selected columns df.dropDuplicates([“column-name-to-remove-duplicated-value”]):drop duplicated rows in selected coumns df.fillna(), df.fill_duplicates(): fill na with given values df.spark.sql.Row(age=10, name=’A’):Return a row with elements: age=10, name=’A’ df.orderBy([“column-name”], ascending=False), df.sortBy():orderBy is an alias of sortBy, they sort the dataframe along given column names df.groupby().agg() / df.groupby().count()Apply aggregation function, such as count() to a group 123# First select group based on column A, B. Then count the amount of group \"A\"from pyspark.sql import functions as Fdf.groupby([\"A\"]).agg(F.count(df.A)) Append New Rows df.union():Return a new DataFrame containing union of rows in this and another DataFrame, based on position. df.unionByName([“colunm-name”])The difference between this function and union() is that this function resolves columns by name (not by position) 12new_row = spark.createDataFrame([(\"A\",1)],[\"name\",\"count\"])df = df.union() SQL df = SparkSession.sql(“select * from table”) Display Data df.show(n) Data Types Convertion with Pandas df.toPandas()convert dataframe to pandas dataframe df.toDF()convert a list of Rows to PySpark dataframe Convert Pandas DataFrame to PySpark DataFrame using Schema 12345from pyspark.sql.types import *mySchema = StructType([ StructField(\"col name1\", IntegerType(), True)\\ ,StructField(\"col name2\", StringType(), True)\\ ,StructField(\"col name3\", IntegerType(), True)])spark_df = spark.createDataFrame(df, mySchema) Using Resilient distributed Dataset (RDD) RDD represents an immutable, partitioned collection of elements that can be operated on in parallel. Please refer to the official website about RDD Practice with Example: SF Crime data Requirements: Find a platform for distributed computing, like databricks Install PySpark Download SF Crime Data for Demo 12345import requestsr = requests.get(\"https://data.sfgov.org/api/views/tmnf-yvry/rows.csv?accessType=DOWNLOAD\")with open(\"sf_crime.csv\",\"w\") as f f.write(r.content) f.close() Load Data with PySpark 12345678910111213from pyspark.sql import SparkSessionfrom pyspark.sql.functions import to_date, to_timestamp, hourfrom pyspark.sql.functions import year, month, dayofmonth, date_formatfrom pyspark.sql.functions import from_unixtime, unix_timestamp# create SparkSession entry to handle dataspark = SparkSession \\ .builder \\ .appName(\"crime analysis\") \\ .config(\"spark.some.config.option\", \"some-value\") \\ .getOrCreate()# load csv datadf_opt1 = spark.read.option(\"header\", \"true\").csv(data_path) Visualize Data 1234567891011# Create a view called \"sf_crime\" to the dataframe, so that we can use SQL# to query data laterdf_opt1.createOrReplaceTempView('sf_crime')# if using databricks, we can use display function to see the datadisplay(df_opt1 )# ordf_opt1.show()# or # show the schema of dataframedf_opt1.printSchema() Clean DataChange the string data type to date type, integer type and other suitable data type 12345678910# convert data type and replace datadf_opt1 = df_opt1.withColumn('Hour', hour(df_opt1['Time']))# convert string to date type using unix_timestampdf_opt1 = df_opt1.withColumn(\"Date\", to_date( from_unixtime(unix_timestamp(df_opt1['Date'], 'MM/dd/yyy'))))df_opt1 = df_opt1.withColumn(\"Year\", year(df_opt1.Date))df_opt1 = df_opt1.withColumn(\"Month\", month(df_opt1.Date))df_opt1 = df_opt1.withColumn(\"Day\", dayofmonth(df_opt1.Date))df_opt1 = df_opt1.withColumn('HasCriminal', (df_opt1[\"category\"]!=\"NON-CRIMINAL\"))df_opt1 = df_opt1.withColumn(\"X\", df_opt1[\"X\"].cast(\"double\"))df_opt1 = df_opt1.withColumn(\"Y\", df_opt1[\"Y\"].cast(\"double\")) Query and Select data I’m interested inI want to analyze the count of crime of each category here, so there are two ways to do this. 12345678910# Display Count of cime using SQLcrimeCategory = spark.sql(\"SELECT category, COUNT(*) AS crime_counts FROM sf_crime GROUP BY category ORDER BY crime_counts DESC\")crimes_pd_df = crimeCategory.toPandas()display(crimeCategory)# Display Count of crime of each category Using PySparkcrime_category_df = df_opt1.groupby('category').count().orderBy('count',ascending=False)crime_category_df = crime_category_df.withColumnRenamed('count', 'crime_counts')crime_category_df = crime_category_df.toPandas()display(crime_category_df) Result Advance Topic: Machine Learning ModelUsing KMean Clustering to find the 5 centers in which crimes occur frequently Select Position data X,Y and then Use Interquantile Range method to find outliers of positionSince KMean Clustering is sensitive to the outlier as it uses mean method to find the center of clusters, we need to remove outliers first. Quantile Based method to remove outlier: The outlier is defined as the data point that drop outside the range [Q1-1.5IQR , Q3+1.5IQR],where Q1 and Q3 are the first and third quantile of dataset and IQR = Q3-Q1 is the interquantile range. API in PySpark to find quantile: df.approxQuantile(col, probabilities, relativeError):col: column to find quantileprobabilities: a list of quantile probabilities we want to find. Here I want to find Q1 =0.25 and Q3=0.75return: the a list values that correspond to the quantile in probabilities list. 1234567891011121314151617# select the positions where crimes occurcrime_cluster_df = df_opt1.where(\"hasCriminal =true\").select([\"Hour\", \"PdDistrict\", \"X\",\"Y\", \"DayOfWeek\", \"category\",\"Resolution\",\"hasCriminal\"])# crime_cluster_df.show()#Find the Q1, Q 3 Quantilebounds = { c: dict( zip([\"q1\", \"q3\"], crime_cluster_df.approxQuantile(c, [0.25, 0.75], 0)) ) for c in crime_cluster_df.select([\"X\",\"Y\"]).columns}# compute lower bound and upper bound of normal datafor c in bounds: iqr = bounds[c]['q3'] - bounds[c]['q1'] bounds[c]['lower'] = bounds[c]['q1'] - (iqr * 1.5) bounds[c]['upper'] = bounds[c]['q3'] + (iqr * 1.5) Remove Outliers based on upper bound and lower bound of quantile 1234from pyspark.sql.functions import *crime_cluster_df = crime_cluster_df.select([\"X\",\"Y\"])\\.where(col(\"X\").between(bounds['X']['lower'], bounds['X']['upper'])) \\.where(col(\"Y\").between(bounds['Y']['lower'], bounds['Y']['upper'])) Assemble columns into one feature column before trainingIn PySpark, we need to put all feature columns into one single feature column before we train the model. VectorAssembler provides a way to assemble those features into one column. 1234# ensemble multiple columns into one single feature column for training KMean Clusteringfrom pyspark.ml.feature import VectorAssemblervecAssembler = VectorAssembler(inputCols=[\"X\", \"Y\"], outputCol=\"features\")crime_cluster_df = vecAssembler.transform(crime_cluster_df.select([\"X\",\"Y\"])) KMean Clustering to learn dataIn Machine Learning of PySpark, we need to set the name of feature column to “features”, otherwise, set featuresCol=&quot;column-name&quot; to select which feature column to learn in dataframe 123456789101112# Training KMean clusteringfrom pyspark.ml.clustering import BisectingKMeansK=5bkm = BisectingKMeans(k=K, minDivisibleClusterSize=1.0)model = bkm.fit(crime_cluster_df)# predict at one single point# print(model.predict(crime_cluster_df.head().features))# predict clusters# Output the prediction to the column called \"Prediction\"model.setPredictionCol(\"Prediction\")transformed = model.transform(crime_cluster_df).select(\"X\",\"Y\", \"Prediction\") Result SummaryThis tutorial introduce: List of useful PySpark functions and their basic usage Use SF Crime dataset as a demo to see how to use PySpark to manipulate data and visualize them How to use machine learning model: KMean Clustering in PySpark to learn data. Note: the APIs of ML in PySpark are different from sklearn. Need to Pay attention to the difference. Reference[1] SparkReader,Writer[2] PySpark,SQL_module[3] PySpark,ML_module[4] PySpark,outlier_detection[5] logo","link":"/2020/10/13/PySpark-Note-1/"},{"title":"Recommendation-System-2-WideDeep","text":"Background2016 年Google通过用wide&amp; Deep+ FTRL 优化器对数据稀疏性sparsity和特征组合学习的问进行研究原文： In the experiments, we used Follow- the-regularized-leader (FTRL) algorithm with L1 regularization as the optimizer for the wide part of the model, and AdaGrad for the deep part. 来自 https://zhuanlan.zhihu.com/p/142958834 Motivation目的 CTR预估任务里面需要精细的特征工程对特征进行组合使模型对出现频率高的特征进行学习。但问题是特征工程需要花费时间长而且麻烦 对于sparse的数据进行特征组合进行训练容易模型过拟合，泛化性差，因此需要一种generalization的方法对数据进行处理 Wide&amp;Deep模型就是围绕记忆性和泛化性进行讨论的，模型能够从历史数据中学习到高频共现的特征组合的能力，称为是模型的Memorization。能够利用特征之间的传递性去探索历史数据中从未出现过的特征组合，称为是模型的Generalization。Wide&amp;Deep兼顾Memorization与Generalization并在Google Play store的场景中成功落地。 特点 用了Embedding的deep model对学习general的feature为了解决sparse 数据的问题，Google把DNN，embedding结合用于处理sparse的数据。通过dense embedding的方法把sparse的高维数据进行降维的同时，也把0,1的sparse的数据变成连续的数据使得梯度下降学习的时候不会因为梯度消失的问题而变得模型难以训练。而这个模块也成为deep model 用了wide model对特征进行自动组合。为了解决特征组合的问题，google通过利用wide model方法直接把其他类别的数据进行线性投影从而达到特征选择的作用。因为线性投影的时候每个特征都有一个weight进行权衡。而模型学习的时候就是把这些weight进行调整来选择。这样可解释性也比较强 联合训练W&amp;D模型是将两部分输出的结果结合起来联合训练，将deep和wide部分的输出重新使用一个逻辑回归模型做最终的预测，输出概率值。联合训练的数学形式如下：需要注意的是，因为Wide侧的数据是高维稀疏的，所以作者使用了FTRL算法优化，而Deep侧使用的是 Adagrad 原理 我们可以看到 Wide&amp; Deep model基本就是wide model 和deep model的输出的相加，再通过logistics 或softmax(如果是多分类)，输出的结果是CTR (click or not click)的概率 Wide modelwide部分是一个广义的线性模型，输入的特征主要有两部分组成，一部分是原始的部分特征，另一部分是原始特征的交叉特征(cross-product transformation)，对于交互特征可以定义为 Deep modelDeep部分是一个DNN模型，输入的特征主要分为两大类，一类是数值特征(可直接输入DNN)，一类是类别特征(需要经过Embedding之后才能输入到DNN中)，DNN模型随着层数的增加，中间的特征就越抽象，也就提高了模型的泛化能力。对于Deep部分的DNN模型作者使用了深度学习常用的优化器AdaGrad，这也是为了使得模型可以得到更精确的解。 W&amp;D模型是将两部分输出的结果结合起来联合训练，将deep和wide部分的输出重新使用一个逻辑回归模型做最终的预测，输出概率值。联合训练的数学形式如下：需要注意的是，因为Wide侧的数据是高维稀疏的，所以作者使用了FTRL算法优化，而Deep侧使用的是 Adagrad 优缺点 优点 通过结合wide和deep model同时解决sparse data和memorization， generalization的问题 不用特意人工选择特征进行组合 数据特征可以通过embedding方式降维降低参数的数目 缺点 可能需要考虑把哪些特征放到deep model哪些放到wide model 需要通过联合训练方式对不同模型优化6. 思考 在你的应用场景中，哪些特征适合放在Wide侧，哪些特征适合放在Deep侧，为什么呢？个人认为categorical的数据如果distinct的value很多的feature应该用于deep model，比如说像电影类别id的特征，或者用户地址，如果细分的话会有上百个value，而每个value这样需要one-hot来表示的话就会变得很大这样需要deep model的embedding进行降维。 而如果value个数不多的categorical feature，比如性别，又或者连续数据，比如年龄，可以通过wide的 model进行处理。不过感觉也可以放到deep model里面 为什么Wide部分要用L1 FTRL训练？Wide model 用L1 - FTRL 的优化器主要原因是 L1-FTRL 在结合L1-FOBOS 和 L1-RDA的优化方法后再梯度下降有较高的精度同时，也能产生稀疏性进行特征的筛选，而这个在wide-model里有利于wide mode通过weight的稀疏性进行特征组合和选择。比如说通过L1-FTRL 我们可以把 y=w1x1 +w2x2 +w3x3 里面的w1， w2的weight降到接近0从而达到排除特征x1和x2 而选择x3的效果 为什么Deep部分不特别考虑稀疏性的问题？Deep model 之所以不用太考虑了稀疏性是因为embedding已经把稀疏的特征进行降维成dense的vector，而对于本来就不sparse的feature如年龄也就更加不用考虑稀疏性了 Future work 有没有可以不用专门分开两种训练方法，更加general的训练方法对不同的模型进行训练但不会影响效果？ 能不能不专门考虑什么feature要放到deep model什么feature放到wide model达到更好的自动化特征选择的效果？ Reference[1] https://github.com/datawhalechina/team-learning-rs/blob/master/DeepRecommendationModel/Wide%26Deep.md [2] https://zhuanlan.zhihu.com/p/142958834","link":"/2021/03/18/Recommendation-System-2-WideDeep/"},{"title":"Recommendation-System-1- Collaborative Filtering and Content-based Filtering","text":"Introduction to Recommendation systemIn recommendation system, we want to let it give recommendations of items to users based on users’ visit history or other data.if we represent the items and users in a table, it would be something like this: users\\items item 1 item 2 item 3 user 1 1 - - user 2 - 1 - user 3 - - 1 user 4 1 1 - In this table, each row represent a user and column represent an item. The cell with value 1 means that user buys the corresponding item. For example, the cell of user1 - item 1 =1 means user 1 buys item 1. Then recommendation system is to predict if the empty cells in the table will become 1 or not. Or predict if that user will buy the item or not. Building such recommendation system can help encourage users to consume items by recommending somethings attracting them and let companies make profits Collaborative Filtering is one type of common methods in constructing recommendation system. Its main idea is to find the similarity between users or between items to rank the items and then pick and recommend the top K items to users.In content-based filtering, it utilizes the content features of items and the contents users may like to compute similarity between item and user and make recommendation. There are three common ways to do recommendation user-based It first computes similarity between users, then pick top K items that the most similar user like to the current user Item-based It recommends new items to current user based on the similarity between new items and items that user has brought before content-based It computes the similarity between user and item based on the tags/topics the user like and the tags/topics that item contains. Then it recommends the most similar item to the user. Cold Start ProblemIn recommendation system, we also face a problem that when a new user or a new item comes to the platform, we don’t have enough information (like the preference of new users, tags/contents of new items) about them to make recommendation. Hence Cold start problem mainly can be divided into two types: Cold start of user: We don’t have enough information about new users and can not determine the preference of users to do recommendation Cold start of item: We don’t have enough information of new items and don’t know the tags and contents those items may have User-based filtering (similarity between users)The main idea in user-based filtering is that we first find the similarity between each two users based on the item vectors of the two users.Then pick the user who is most similar to the current user. In the item list of the selected user, we pick the K top frequent visited items to the current user. How it works AssumptionAssume we have a user-item table users\\items item 1 item 2 item 3 item 4 user 1 1 - 1 1 user 2 1 1 1 - user 3 - - 1 1 user 4 1 1 - 1 where 1 represent the ratign from the user to the item the user purchase, - mean the user doesn’t buy the item. Denote the $i^{th}$ user and the $j^{th}$ user as ui, uj, respectively and N(ui), N(uj) are the set of items the $i^{th}$ user purchased and the set of items $j^{th}$ user purchased respectively. |N(ui)| is the number of items the $i^{th}$ user purchased. Step 1: Compute Similarity between a pair of usersIn order to compute the similarity between a pair of users, we need a way to measure the distance between users.One common way is cosine distance, denoted as cos(ui, uj): $$cos(ui, uj) = \\frac{ |N(ui) \\cap N(uj) |}{\\sqrt{|N(ui)| \\times|N(uj)|} }$$ where $|N(ui) \\cap N(uj)|$ means the size of the intersection set between the set of items purchased by ui and the set of items purchased by uj. Example:In the table above, we want to compute cos(u1, u2), then we can see the intersection of the items from u1 and items from u2 is [item1, item3], so $|N(ui) \\cap N(uj)|=2$. Both u1 and u2 users purchase 3 items, so $|N(u1)|=|N(u2)|=3$ Then similarity between user 1 and user 2 is:$$cos(u1, u2) = \\frac{ 2}{\\sqrt{3 \\times 3} }$$ Based on this, we can compute similarity between every pair of users in table.There are other choices to compute similarity, such as Euclidean distance, dot product, Pearson’s Correlation. Step 2: Predict the interest of the $i^{th}$ user, ui, on the $x^{th}$ item, ix In this step, we first pick the top K users who are most similar to ui. Then We pick the users who rated the item ix from the K users.Finally we compute the estimated rating of ui on item ix using user similarity and ratings from those users. Estimated rating of ui on item ix is $$R(ui, ix) = \\sum_{v\\in M} { cos(ui, v) \\times R(v, ix)}$$ where M is the intersection between the set of the top K similar users and users who rated the item ix. The $R(v, ix)$ is the rating of user v on item ix. Note that we can also use K-Nearest Neighbor method, rather than cosine similarity to find the top K simiar users as well. In this case, we may use Euclidean distance or other distance to measure similarity. Properties Advantages Easy to implement It is good to explore group interests on items, since it measures similarity between users in a group Good to use this method when the update of incoming new items is faster than the update of incoming new users(Computing similarity between new user and other existing users is time expensive when there is millions of users. But updating similarity after adding new items is much easy, we just need to update N(ui)) Disadvantages Sparsity. The percentage of rating from users is low User-based collaborative filtering is also a memory-based mehtod, since it requires memory to store similarity between users. when update frequency of user is faster than the update frequency of items, it is time-expensive, especially there are millions of users Cold start problem. When there is no sufficient information about users or item, it is hard to estimate similarity efficiently update is slow when there are large amount of users or items When to use when we want to explore group interests or similarity between users in a group The number of users is smaller than the number of items. Or Incoming new users amount is smaller than incoming new items amount. Item-based filtering (similarity between items)The main idea in Item-based is to compute the similarity between new item and the items purchased by users to predict if users will buy the new item. How it worksUsing the user-item table above users\\items item 1 item 2 item 3 item 4 user 1 1 - 1 1 user 2 1 1 1 - user 3 - - 1 1 user 4 1 1 - 1 Denote the $i^{th}$ item as xi. M(xi) is the set of users who purchased item xi and |M(xi)| is the size of this set Step 1: Compute similarity between a pair of itemsThe cosine similarity of item xi and item xj is similarity to the similarity between users: $$cos(xi, xj) = \\frac{ |M(xi) \\cap M(xj) |}{\\sqrt{|M(xi)| \\times|M(xj)|} }$$ In this example, $cos(x1, x2) = \\frac{ |M(x1) \\cap M(x2) |}{\\sqrt{|M(x1)| \\times|M(x2)|} } = \\frac{2}{\\sqrt{3\\times 2} } $ Step 2: Predict Rating of user ui on item xiWe pick the K items which are most similar to item xi based on computed item similarity. Then find the intersection Q between these K items and the set of items user ui purchased.Then estimated rating of user ui on item xi is $$R(ui, xi) = \\sum_{y \\in Q} { cos(xi, y) \\times R(ui, y)}$$ where Q is the intersection set between K items and the set of items user ui purchased. y is item from Q. $R(ui, y)$ is rating of user ui on item y. Properties Advantages Item-based Collaborative filtering is good for personalized recommendation, since it is based on similarity of items user purchased, which shows user’s preference information. Good to use this method when there are a large amount of users. Easy to compute as well Good Explainability Disadvantages It is memory-based model as well. Hence it needs to store information of items, users. If there are millions of items, the computation is expensive Cold Start problem. For new item, it is hard for us to compute similarity between items since there is lack of users using the new item Sparsity. The number of users using new items could be small and it is hard to compute similarity Time complexity is high when there are millions of items and users When to use when we want to make personalized recommendations to users when the amount of items is not pretty large. Or the update frequency of items is smaller than update frequency of users’ information. When we have enough information about items and cold start effect is small. Content-based filtering (based on tags/content of items)Let’s consider that there are some tags/contents in items and users may prefer some tags/contents and hence like the items that contain such tags/contents. Based on this setting, we can construct an item vector $v_i$ and an user vector $u_i$. $v_i$ and $u_i$ have the same shape Each entry in the vector represents whether this item contains the corresponding tags/contents, or whether this user like or dislike the corresponding content / tag. The user vector is given by this formula:$$u_i = \\frac{1}{n}\\sum_i^nu_i - \\frac{1}{m}\\sum_j^mu_j$$ where $u_i$ is the item vector that user likes and $u_j$ is the item vector that user dislikes. Each entry in user vector is the difference between the level of prefering this contents and the level of disliking this contents. If entry is negative, then user may dislike that content. After computing the user vector, we can compute the similarity between user vector and item vectors to see which item is most likely to be purchased by user based on user’s interests. The similarity can be computed by dot product similarity: $$similarity(ui, vi) = &lt;ui, vi&gt;$$ where &lt;ui, vi&gt; is dot product of ui, vi. Note that for ui, vi, we don’t use normalization here, since when there are large amount of contents but item has only a few contents, the vector will be very sparse. Normalization in a sparse vector will make the non-zero values pretty small and even close to 0, which make computation difficult and may loss precision. Steps in content-based filtering are as follow: Compute item-content vectors and user vectors Compute similarity between user and items Rank items to recommend Examplein a item-content table: item tag1 tag2 tag3 v1 1 0 1 v2 1 0 0 v3 0 1 0 v4 0 1 1 in a user-item table: user item1 item2 item3 item4 u1 1 1 -1 -1 Step1: compute item vectors and user vectorin item-content table, 1 represents item vi contains this tag/content, otherwise, it doesnt. In user-item table, 1 indicates this user like this item and -1 means user dislike this item.Hence item vectors are v1 = [1, 0, 1], v2 = [1, 0, 0], v3= [0, 1, 0], v4=[0, 1, 1]Then user vector u1 = $\\frac{v1+v2}{2} - \\frac{v3+v4}{2}$ = [1, 0, 0.5] - [0, 1, 0.5] = [1, -1, 0]. In this case, we can see user1 like tag1 and dislike tag2 and be neutral about tag3. Step2: compute similarity between user and itemsSim(u1, v1) = &lt;u1, v1&gt; = 1 * 1 + 0 * (-1) + 1 * 0 = 1Sim(u1, v2) = &lt;u1, v2&gt; = 1 * 1 + 0 * (-1) + 0 * 0 = 1Sim(u1, v3) = &lt;u1, v3&gt; = 0 * 1 + 1 * (-1) + 0 * 0 = -1Sim(u1, v4) = &lt;u1, v4&gt; = 0 * 1 + 1 * (-1) + 1 * 0 = -1 Step3: ranking and recommenduser likes v1, v2 and dislikes v3, v4. Update of user vectorSince user vector is just the difference between the average of like item vectors and the average of dislike item vectors. When the $k^{th}$ item comes, we can update either the like vector or dislike vector to update the user vector. Let the average of K1 item vectors that user likes as $$u^+_{k-1}= \\frac{1}{k1-1}\\sum^{k1-1}_iv_i$$ the average of K2 item vectors that user dislikes as $$u^-_{k-1} = \\frac{1}{k2-1}\\sum^{k2-1}_jv_j$$ and K1 + K2 = k-1 user vector for k-1 items is $$u_{k-1} = u_{k-1}^+ -u_{k-1}^-$$ When the $K^{th}$ item comes, if user like this item, then we update like vector $$u_{k}^+ = \\frac{(k1-1)u_{k-1}^+ + v_k }{k1} , u_{k}^- = u_{k-1}^-$$ if user dislikes the item, just update dislike vector $$u_{k}^- = \\frac{(k2-1)u_{k-1}^- + v_k }{k2} , u_{k}^+ = u_{k-1}^+$$ This enable us to update the user vector on the fly. Properties Advantages Easy and very fast to compute similarity even when there are large amount of items or users. Utilize the content information to explore potential contents that user likes Compared with Item-based and user-based method, it is less sensitive to Cold Start problem, since contents of item can be defined easily. Disadvantages Sparsity of item vectors. Item vector could be sparse when there are a lot of contents and content follow long-tail distribution (Note long-tail distribution of feature/item can also lead to the sparsity) Very easy to converge to certain scope. Since content/tag could be long-tail distribution and most of items have similar content, then a small change in the item vector will lead to large change of similarity.For example, a user vector u= [100, 0, 1, -100], due to long-tail distribution of contents, the values between contents are quick different. For item vectors v1 = [1, 0, 0, 0 ] and v2= [0, 0, 1, 0], sim(u1,v1) and sim(u1,v2) would be very different. When to useWhen we have more information about items and want to use those contents in recommendation. Reference[1] https://medium.com/@cfpinela/recommender-systems-user-based-and-item-based-collaborative-filtering-5d5f375a127f [2] https://zh.wikipedia.org/wiki/%E5%8D%94%E5%90%8C%E9%81%8E%E6%BF%BE [3] https://ars.els-cdn.com/content/image/1-s2.0-S1110866515000341-gr3.jpg [4] https://towardsdatascience.com/introduction-to-recommender-systems-1-971bd274f421","link":"/2020/12/16/Recommendation-System-1/"},{"title":"Accuracy Improvement-Ensemble Learning","text":"IntroductionIn our life, we know that the powerful of a single person is much weaker than a group of people. One simple example is tug-of-war. A group of people have stronger power than a single person and much easier to win the competition. Similarly, when making a decision about if a person should be punished, the judgement from a group of jurors is less biased than that from a single juror, since they consider different aspects of the case.In machine learning, ensemble method applies such idea to combine decisions from different models and improve the accuracy. Three common ensemble methods are: bagging, boosting and stacking. Bagging (bootstrap aggregation) Main Idea: Its goal is to reduce variance by using multiple classifiers, like decision tree. It uses boostrap method to sample data and train multiple classifiers and then average the prediction over a collection of classifiers (for continuous value prediction, regression), or return the prediction with maximum votes (for classification) Random forest is a bagging approach, which bags a set of decision trees together. Assumptions we have training set with size of D and a set of models with size of K Training: Similar to Bootstrap, at each iteration i, a training set Di of d tuples is sampled with replacement from training set. A classifier model Mi is learned for each training set Di Prediction: Each Classifier Mi returns prediction for input X. Discrete value output: The bagged classifier counts the votes and assigns the class with the most votes to X Continous value output: take the average value of each prediction for a given test sample. Advantages: Better than a single classifier from the classifier set. More robust in noisy data and hence smaller variance Disadvantages: 1.Its training depends on sampling techniques, which could affect the accuracy The prediction may be not precise, since it uses average value of classifiers’ predictions. For example, if valid prediction values are 1,2,or 3, then the average of predictions from different model could lead to a floating point number. Features of Random Forest Comparable in accuracy to Adaboost, but more robust to errors and outliers. Insensitive to the number of attributes selected for consideration at each split, and faster than boosting Boosting Main Idea: Its goal is to improve accuracy, let models better fit training set. It uses weighted votes from a collection of classifiers Training: Each training sample/tuple is given a weight, eg $w_i$ for the $i^{th}$ tuple. Then we have training set {(X0,y0, w0), … ,(Xi,yi, wi)}where $X_i$ and $y_i$ are training sample and target We have k classifiers {M0, M1,…Mk}. Each classifier is learned from the whole training set iteratively. That is, if we have k classifiers, then we need to iterate the training set at least k times (at $i^{th}$ iteration, we train the $i^{th}$ classifier), so that each classifier can learn the training set. After classifier $M_i$ is learned on training set, classifier $M_{i+1}$ pays more attention to the training samples that are misclassified by $M_i$ Prediction: The final Model combines the votes of each individual classifier. Either find the prediction with largest sum of weights (Classification), or find the average of all prediction values (Regression) The weight of each classifier's vote is a function of its accuracy Advantages Boosting can be extended to numeric prediction Better fit the training set since it adjusts the weights of training set and gives more attention to the misclassified sample. Disadvantages Easy to overfit. Need Additional techniques to avoid overfitting. (I will discuss the methods dealing with Overfitting ). Questions: How to pay more attention to misclassified samples? give Higher weights? But How to compute weights?Answer: This depends on the actual boosting algorithm, like GradientBoosting, AdaBoosting AdaBoosting Assumption: Assume we have training set with size of D and a set of classifier models with size of T __Error of model $M_i$__ Error($M_i$) = $\\sum_i^D (w_i \\times err(X_i))$ if using normalized weight (weight in range [0,1]), then Error($M_i$) = $\\frac{\\sum_i^D (w_i \\times err(X_i))}{(\\sum_j^D w_j)} $ Note: In classification, $err(X_i)= 1(C_i(X_i) !=Y_i)$, $C_i(X_I)$ means the prediction of model $M_i$ on sample $X_i$. If the prediction is correction $error(X_i) =0$, otherwise 1. Weight of model $M_i$’s voting: $\\alpha_i$ $\\alpha_i = log\\frac{1-error(M_i)}{error(M_i)} + log(K-1)$. Note: K = the number of classes in dataset. When K=1, log(K-1) term can be ignored Update of weight $w_i = w_i \\cdot exp(\\alpha_i \\cdot 1(M_j(X_i) != Y_i))$ The weight $w_i$ of the $i^{th}$ training tuple $X_i$ is updated by timing exponential value of weight of model only when this model $M_j$ misclassifies the $X_i$ ( That is $M_j(X_i) !=Y_i$ and hence $1(M_j(X_i) !=Y_i) =1 $). Prediction $C(X_i) = argmax_{k} \\sum_{j=1}^T \\alpha_{m}\\cdot 1(M_j(X_i)== Y_i)$ where $1(M_j(X_i)== Y_i)$ is equal to 1 if prediction is correct, otherwise, 0. The prediction of the whole model has the largest sum of weight of models, NOT the weight of training tuple! More detail for AdaBoosting, Read this paper Stacking Main Idea: It combines and trains a set of heterogeneous classifiers in parallel. It consists of 2-level models: level-0: base model Models fit on the training data and whose predictions are compiled. level-1: Meta-Model It learns how to best combine the predictions of the base models. Training: split the training data into K-folds one of base models is fitted on the K-1 parts and predictions are made for Kth part. Repeat step 2 for each fold Fit the base model on the whole train data set to calculate its performance on the test set. repeat step 2~4 for each base model Predictions from the train set are used as features for the second level model. Classification: Second level model is used to make a prediction on the test set. Advantage: It harness the capabilities of a range of well-performing models on a classification or regression task and make predictions that have better performance than any single model in the ensemble. Disadvantage: It could be computational expensive since it uses k-fold method and use multiple level models. Comparison among Ensembling, boosting and bagging Goal of bagging is to reduce variance and noise while boosting is to improve accuracy using weighted models. Stacking is to improve accuracy of model using hetergenerous models. Adaboost let classifiers pay more attention to the misclassified samples, but if those misclassified samples are outlier or noisy data, it will affect a lot and lead to larger variance. However, bagging and ensemble uses averaging and voting methods and each classifier has equal weight, which is less sensitive to the noise data and outlier. Reference[1] https://blog.csdn.net/weixin_37352167/article/details/85028835[2] https://machinelearningmastery.com/stacking-ensemble-machine-learning-with-python/[3] https://miro.medium.com/max/497/0*Bbf8eeslDtVKog7U.png","link":"/2020/07/24/Model-Acc-Improvement/"},{"title":"Recommendation-System-3-DeepFM","text":"Background在推荐系统里面学习低阶和高阶的feature，将feature 进行交叉组合挖掘数据的信息一直需要花费大量的时间精力进行feature engineering。组合特征大牛们研究过组合二阶特征，三阶甚至更高阶，但是面临一个问题就是随着阶数的提升，复杂度就成几何倍的升高。这样即使模型的表现更好了，但是推荐系统在实时性的要求也不能满足了。所以很多模型的出现都是为了解决另外一个更加深入的问题：如何更高效的学习特征组合？ 为了进行通过特征交叉更加高效学习高阶和低阶的特征，大牛们曾经对研究过FM(factorization machine), FFM (field-aware factorization machine) 对特征进行交叉和研究更加高阶的特征(注意FM的特征交叉相对于直接在logistics regression对特征进行相乘交叉而已，通过用embedding的思想先把sparse feature $x_j$ 进行embedding转换成dense的vector 然后再对不同的特征交叉， 这样的好处在避免了$x_ix_j$特征交叉时只要有一个特征是0就会变成0这样更加容易稀疏的缺陷).然而FM， FFM的方法把特征进行交叉的问题在于随着交叉的order越高，计算复杂度越大。 另外一种方法是利用DNN对更加高阶的非线性特征进行学习。但是这种方法会导致model的参数因为sparse的feature维度太高而导致参数过多的难以训练的问题。 为了解决这几个问题，研究人员曾经把FM， DNN和embedding进行结合。先通过embedding把sparse feature进行降维学习到dense的vector同时也泛化了feature，之后再通过类似FM的feature intersection 特征交叉的形式(比如inner product和outer product)将feature 进行组合并得到高阶的feature，最后再用DNN进行特征学习，而这也引向了PNN的思想。下图为PNN的结构。 不过PNN的问题在于它通过串行的形式把feature变成高阶feature之后，低阶的feature不能很好表达(因为所有输入的feature都被投影转换后丢失了原来低阶特征的信息)。 为了解决这个问题，后来研究人员把Google的wide&amp;deep的model的并行结构(wide model+ deep model)和 类似FM的feature crossing的方式进行结合，把高阶特征和低阶特征同时并行分开学习而这个也就是DeepFM的思想 Motivation在DeepFM (deep factorization machine) 里面，它通过把Factorization Machine， Embedding，Wide&amp;Deep model 的思想进行结合从而达到以下的效果： No Feature Engineering和wide&amp;deep 相比， DeepFM 不用做feature engineering对feature进行组合，DeepFM可以直接通过FM方式把数据特征自动组合 Learn low-order Feature Intersection and high-order Feature Intersection相对于PNN， DeepFM通过利用并行的方式对高阶和低阶特征进行组合，而不会影响低阶特征的表达和学习 Share the same input and embedding vector对于不同的用户的数据输入，DeepFM用相同的embedding vectors进行转换，用户的sparse的数据相对于是对embedding的vector进行选择，这样可以有效降低模型的空间复杂度 How DeepFM work模型的结构与原理 前面的Field和Embedding处理是和前面的方法是相同的，如上图中的绿色部分；DeepFM将Wide部分替换为了FM layer如上图中的蓝色部分 这幅图其实有很多的点需要注意，很多人都一眼略过了，这里我个人认为在DeepFM模型中有三点需要注意： Deep模型部分 FM模型部分 Sparse Feature中黄色和灰色节点代表什么意思 FM model详细内容参考FM模型部分的内容，下图是FM的一个结构图，从图中大致可以看出FM Layer是由一阶特征和二阶特征Concatenate到一起在经过一个Sigmoid得到logits（结合FM的公式一起看），所以在实现的时候需要单独考虑linear部分和FM交叉特征部分。 $$y_{FM}(x) = w_0+\\sum_{i=1}^N w_ix_i + \\sum_{i=1}^N \\sum_{j=i+1}^N v_i^T v_j x_ix_j$$ Deep modelDeep架构图 Deep Module是为了学习高阶的特征组合，在上图中使用用全连接的方式将Dense Embedding输入到Hidden Layer，这里面Dense Embeddings就是为了解决DNN中的参数爆炸问题，这也是推荐模型中常用的处理方法。 Embedding层的输出是将所有id类特征对应的embedding向量concat到到一起输入到DNN中。其中$v_i$表示第i个field的embedding，m是field的数量。$$z_1=[v_1, v_2, …, v_m]$$上一层的输出作为下一层的输入，我们得到：$$z_L=\\sigma(W_{L-1} z_{L-1}+b_{L-1})$$其中$\\sigma$表示激活函数，$z, W, b $分别表示该层的输入、权重和偏置。 最后进入DNN部分输出使用sigmod激活函数进行激活：$$y_{DNN}=\\sigma(W^{L}a^L+b^L)$$ Properties优点 结合了Wide&amp;Deep, FM的特点，在通过特征进行交叉(feature intersection)来得到更高阶的特征并同时学习高阶特征和低阶特征 不用进行特别精细的feature engineering （wide&amp;Deep 在wide的模型里面还是需要人工特征组合，比较wide model里面feature intersection不够） 通过embedding和field input的思想将sparse的特征进行降维同时可以share相同的embedding vector，降低模型的复杂度 Comparison of deep models for CTR prediction model No Feature Pre-training High-order Low-order No Features Engineering FNN × √ × √ PNN √ √ × √ Wide &amp; Deep √ √ √ × DeepFM √ √ √ √ 缺点 对于 FM部分的训练会相对较慢 FM的公式是一个通用的拟合方程，可以采用不同的损失函数用于解决regression、classification等问题，比如可以采用MSE（Mean Square Error）loss function来求解回归问题，也可以采用Hinge/Cross-Entropy loss来求解分类问题。当然，在进行二元分类时，FM的输出需要使用sigmoid函数进行变换，该原理与LR是一样的。直观上看，FM的复杂度是 $O(kn^2)$ 。但是FM的二次项可以化简，其复杂度可以优化到 $O(kn)$ 。由此可见，FM可以在线性时间对新样本作出预测。 证明推理见 link： $\\begin{array}{cc}\\sum_{i=1}^{n-1}{\\sum_{j=i+1}^{n}{&lt;v_i,v_j&gt;x_ix_j}}\\\\= \\frac{1}{2}\\sum_{i=1}^n\\sum_{j=1}^n &lt;v_i,v_j&gt;x_ix_j -\\frac{1}{2}\\sum_{i=1}^n &lt;v_i,v_i&gt;x_ix_i\\\\= \\frac{1}{2}\\sum_{i=1}^n\\sum_{j=1}^n\\sum_{f=1}^k v_{if}v_{jf}x_ix_j - \\frac{1}{2}\\sum_{i=1}^n\\sum_{f=1}^k v_{if}v_{if}x_ix_i\\\\= \\frac{1}{2}\\sum_{f=1}^k [(\\sum_{i=1}^nv_{if}x_i)(\\sum_{j=1}^nv_{jf}x_j) - \\sum_{i=1}^nv_{if}^2x_{i}^2]\\\\\\end{array}$由于第一个sum的loop时间是O(k), 而计算$(\\sum_{j=1}^nv_{jf}x_j)$只需要1次就可以得到中括号里面的term所以是O(n)最后相乘起来时间复杂度变成O(kn) 其中 没有考虑到用户的兴趣和过去浏览的历史的问题，只是单纯在对feature进行交叉，没有考虑用户在时间上的行为变化，也不能进行个性化推荐 FM的部分可能需要FTRL优化器进行更新 QuestionsCode12345678910111213141516171819202122232425262728293031323334def DeepFM(linear_feature_columns, dnn_feature_columns): # 构建输入层，即所有特征对应的Input()层，这里使用字典的形式返回，方便后续构建模型 dense_input_dict, sparse_input_dict = build_input_layers(linear_feature_columns + dnn_feature_columns) # 将linear部分的特征中sparse特征筛选出来，后面用来做1维的embedding linear_sparse_feature_columns = list(filter(lambda x: isinstance(x, SparseFeat), linear_feature_columns)) # 构建模型的输入层，模型的输入层不能是字典的形式，应该将字典的形式转换成列表的形式 # 注意：这里实际的输入与Input()层的对应，是通过模型输入时候的字典数据的key与对应name的Input层 input_layers = list(dense_input_dict.values()) + list(sparse_input_dict.values()) # linear_logits由两部分组成，分别是dense特征的logits和sparse特征的logits linear_logits = get_linear_logits(dense_input_dict, sparse_input_dict, linear_sparse_feature_columns) # 构建维度为k的embedding层，这里使用字典的形式返回，方便后面搭建模型 # embedding层用户构建FM交叉部分和DNN的输入部分 embedding_layers = build_embedding_layers(dnn_feature_columns, sparse_input_dict, is_linear=False) # 将输入到dnn中的所有sparse特征筛选出来 dnn_sparse_feature_columns = list(filter(lambda x: isinstance(x, SparseFeat), dnn_feature_columns)) fm_logits = get_fm_logits(sparse_input_dict, dnn_sparse_feature_columns, embedding_layers) # 只考虑二阶项 # 将所有的Embedding都拼起来，一起输入到dnn中 dnn_logits = get_dnn_logits(sparse_input_dict, dnn_sparse_feature_columns, embedding_layers) # 将linear,FM,dnn的logits相加作为最终的logits output_logits = Add()([linear_logits, fm_logits, dnn_logits]) # 这里的激活函数使用sigmoid output_layers = Activation(\"sigmoid\")(output_logits) model = Model(input_layers, output_layers) return model Reference[1] Datawhale: https://github.com/datawhalechina/team-learning-rs/blob/master/DeepRecommendationModel/DeepFM.md[2] zhihu: https://zhuanlan.zhihu.com/p/50692817[3] CSDN: https://blog.csdn.net/ISMedal/article/details/100578354[4] https://blog.csdn.net/a819825294/article/details/51218296[5] DeepFM Paper: https://arxiv.org/pdf/1703.04247.pdf","link":"/2021/03/21/Recommendation-System-3-DeepFM/"},{"title":"Recommendation-System-4-NFM","text":"BackgroundNeuralFM (Neural Factorization Machines)是2017年由新加坡国立大学的何向南教授等人在SIGIR会议上提出的一个模型，这个模型可以看成是直接把FM，Neural network 和embedding的简单粗暴的结合 （原来论文这么好发的吗？） Motivation这个模型考虑的问题是 FM 模型只考虑了一阶和二阶的特征，然而对更加高阶的特征没有学习到，这样无法对生活中更加复杂和有规律的数据进行挖掘和学习。FM的公式如下， 它只考虑到一阶和二阶的特征组合。第二个term是inner product。$$y_{N F M}(x)=w_{0}+\\sum_{i=1}^{n} w_{i} x_{i}+ \\sum_{i=1}^{n}\\sum_{j=i+1}^{n} &lt;v_iv_j , x_{i}x_{j}&gt;$$ 为了解决这个局限性问题，何向南教授通过简单粗暴的方法直接把FM的二阶特征的组合的部分换成神经网络，并将sparse的feature在网络中先进行embedding降维再交叉结合从而得到学习更加高阶非线性特征的效果， 更新后的公式如下： $$y_{N F M}(x)=w_{0}+\\sum_{i=1}^{n} w_{i} x_{i}+f(x)$$ 其中f(x) 是DNN的部分. NeuralFM 原理Neural FM 的模型结构如下：橙色框框代表FM里面linear feature 的一阶特征组合，而对于sparse的特征先通过embedding生成dense vector然后把dense vector用于一阶的特征组合(橙色框框)以及bi-interactoin pooling layer 的高阶特征交叉进行高阶特征的学习（绿色框框） NeuralFM 的$f(x)$ 部分的结构如下：NeuralFM的DNN部分由 Sparse input, Embedding layer, B-intersection Layer, Hidden layers (Deep model), prediction output组成 Input Layer输入层的特征里面 每一个cell相当于一个 sparse feature， 每个feature一般是先one-hot, 然后会通过embedding，生成纬度低的dense vector，假设$v_i \\in R^{k}$为第$i$个特征的embedding向量， 那么$V_{x}={x_{1} v_{1}, \\ldots, x_{n} v_{n}}$表示的下一层的输入特征。这里带上了$x_i$是因为很多$x_i$转成了One-hot之后，出现很多为0的， 这里的$x_iv_i$ 是一个embedding的vector 而 $x_i$不等于0的那些特征向量，相当于$x_i$ 通过lookup table 方式选择 embedding vector $v_i$. Bi-Interaction Pooling layer在Embedding层和神经网络之间加入了特征交叉池化层是本网络的核心创新了，正是因为这个结构，实现了FM与DNN的无缝连接， 组成了一个大的网络，且能够正常的反向传播。假设$V_{x}$是所有特征embedding的集合， 那么在特征交叉池化层的操作： $$f_{B I}(V_{x})=\\sum_{i=1}^{n} \\sum_{j=i+1}^{n} x_{i} v_{i} \\odot x_{j} v_{j}$$ $\\odot$表示两个向量的元素积操作(element-wise multiplication, 这个不清楚可以google一下)，即两个向量对应维度相乘得到的元素积向量（可不是点乘) 个人认为在 $x_{i} v_{i} \\odot x_{j} v_{j}$ 里面它的结构是和$v_i,v_j$相同大小的特征dense vector。$f_{B I}(V_{x})$是多个dense vector的简单直接交叉element-wise相乘后相加的结果。这个方法其实也是挺直接Bi-Interaction层不需要额外的模型学习参数，更重要的是它在一个线性的时间内完成计算，和FM一致的，即时间复杂度为$O(k N_{x})$，$N_x$为embedding向量的数量。参考FM，可以将上式转化为证明推理见 link： $$f_{B I}(V_{x})=\\frac{1}{2}[(\\sum_{i=1}^{n} x_{i} v_{i})^{2}-\\sum_{i=1}^{n}(x_{i} v_{i})^{2}]$$ Hidden Layer这一层就是全连接的神经网络， DNN在进行特征的高层非线性交互上有着天然的学习优势，公式如下： $$\\begin{array}{cc}a_1 = \\sigma(W_1f_{BI}(V_x) +b_1) \\\\a_2 = \\sigma(W_2a_1 +b_2) \\\\… \\\\a_{i+1} = \\sigma(W_ia_i +b_i) \\\\\\end{array}$$ 这里的$\\sigma_i$是第$i$层的激活函数，hidden layer里面一般是ReLu函数而不是logistics 来防止梯度消失问题， $f_{BI}(V_x)$是交叉后的embedding的dense vector。 Prediction Layer这个就是最后一层的结果直接过一个隐藏层，但如果这里是回归问题，没有加sigmoid激活，如果是分类问题，需要加上logistic 或softmax的 activation function： $$f({x})={h}^{T} {z}_{L}$$ 在NeuralFM的DNN这一部分，为了减少DNN的很多负担，一般只需要很少的隐藏层就可以学习到高阶特征信息。当然可以像其他深度模型一样通过添加Dropout, Batch normalization, ResBlock 等方法进行进行更加深度的feature的学习已经抑制过拟合，梯度消失等问题。 Properties优点 NFM 通过简单粗暴直接的方式把FM的二阶特征组合部分换成DNN的方式学习到更加高阶非线性的特征，很容易理解 计算简单直接linear logit + embedding 的DNN的部分就完事了，而且由于DNN一般来说比较浅就能学习高阶的特征，训练也不难 能够学习低阶特征和高阶特征 能够通过embedding有效解决sparse feature带来的训练看你的问题，不用像wide&amp;deep那样需要额外的FTRL的optimization的方法进行优化 缺点 和DeepFM相比，NeuralFM 有点像把DeepFM里面的FMmodel和Deep model 串起来的感觉。相对于DeepFM, NFM没有把低阶特征直接交叉，只是要么把低阶特征直接线性相加，要么把他们通过embedding进行投影到高阶general的特征再直接相加，所以感觉对低阶特征交叉组合方面不太好。 Code123456789101112131415161718192021222324252627282930313233343536def NFM(linear_feature_columns, dnn_feature_columns): \"\"\" 搭建NFM模型，上面已经把所有组块都写好了，这里拼起来就好 :param linear_feature_columns: A list. 里面的每个元素是namedtuple(元组的一种扩展类型，同时支持序号和属性名访问组件)类型，表示的是linear数据的特征封装版 :param dnn_feature_columns: A list. 里面的每个元素是namedtuple(元组的一种扩展类型，同时支持序号和属性名访问组件)类型，表示的是DNN数据的特征封装版 \"\"\" # 构建输入层，即所有特征对应的Input()层， 这里使用字典的形式返回， 方便后续构建模型 # 构建模型的输入层，模型的输入层不能是字典的形式，应该将字典的形式转换成列表的形式 # 注意：这里实际的输入与Input()层的对应，是通过模型输入时候的字典数据的key与对应name的Input层 dense_input_dict, sparse_input_dict = build_input_layers(linear_feature_columns+dnn_feature_columns) input_layers = list(dense_input_dict.values()) + list(sparse_input_dict.values()) # 线性部分的计算 w1x1 + w2x2 + ..wnxn + b部分，dense特征和sparse两部分的计算结果组成，具体看上面细节 linear_logits = get_linear_logits(dense_input_dict, sparse_input_dict, linear_feature_columns) # DNN部分的计算 # 首先，在这里构建DNN部分的embedding层，之所以写在这里，是为了灵活的迁移到其他网络上，这里用字典的形式返回 # embedding层用于构建FM交叉部分以及DNN的输入部分 embedding_layers = build_embedding_layers(dnn_feature_columns, sparse_input_dict, is_linear=False) # 过特征交叉池化层 pooling_output = get_bi_interaction_pooling_output(sparse_input_dict, dnn_feature_columns, embedding_layers) # 加个BatchNormalization pooling_output = BatchNormalization()(pooling_output) # dnn部分的计算 dnn_logits = get_dnn_logits(pooling_output) # 线性部分和dnn部分的结果相加，最后再过个sigmoid output_logits = Add()([linear_logits, dnn_logits]) output_layers = Activation(\"sigmoid\")(output_logits) model = Model(inputs=input_layers, outputs=output_layers) return model Reference[1] datawhale: https://github.com/datawhalechina/team-learning-rs/blob/master/DeepRecommendationModel/NFM.md [2] FM https://www.jianshu.com/p/152ae633fb00 [3] 原paper https://arxiv.org/pdf/1708.05027.pdf","link":"/2021/03/25/Recommendation-System-4-NFM/"},{"title":"Recommendation-System-5-DIN","text":"Introduction在过去的DeepCrossing, Wide&amp;Deep, DeepFM， NFM等模型里面，这些模型都是通过把sparse feature变成embedding的vector，然后把dense的vectors进行特征交叉(feature intersection) 从而学习低阶和更加高阶的特征。Embeding&amp;MLP模型对于这种推荐任务一般有着差不多的固定处理套路，就是大量稀疏特征先经过embedding层， 转成低维稠密的，然后进行拼接，最后喂入到多层神经网络中去。 然而，这些模型都是只考虑现有的特征进行交叉学习，而没有考虑用户的历史信息和过去的行为信息，没有从时间的角度考虑用户的行为变化。而对于历史行为信息挖掘的问题，又有很多不同的模型，比如用于推荐系统的GNN图神经网络， DIN （deep interest network）等。 而这次要介绍的是Deep Interest Network(DIIN)模型，它是2018年阿里巴巴提出来的模型， 该模型基于业务的观察，从实际应用的角度进行改进，相比于之前很多“学术风”的深度模型， 该模型更加具有业务气息。 MotivationDIN 模型的应用场景是阿里巴巴的电商广告推荐业务， 这样的场景下一般会有大量的用户历史行为信息， 这个其实是很关键的。因而DIN为了解决这个业务场景有一些特点： Assumption:DIN 假设了应用场景里面有大量的用户的历史信息，而这些信息能够提供用户的兴趣和爱好以及购买/浏览等行为的变化。而DIN主要是关注和利用这些历史信息进行信息的挖掘 Novelty:基于DIN的assumption下，DIN模型的创新点或者解决的问题就是使用了注意力机制来对用户的兴趣动态模拟， 这样就能猜测出用户的大致兴趣来，这样我们的推荐才能做的更加到位，所以这个模型的使用场景是非常注重用户的历史行为特征（历史购买过的商品或者类别信息）。而和之前学习的DeepFM, wide&amp;deep等模型相比，之前的模型都没有考虑历史信息的问题。 Deep Interest Network (DIN)由于DIN考虑了用户的历史浏览行为特征，在输入的特征表达里面和过去的模型有出入，所以这个部分先考虑输入特征的表达之后考虑DIN的网络架构 DIN 的特征表达 （feature representation）在过去的DeepFM和NeuralFM里面它们的特征交叉的方式都以下的形式，要么直接通过embedding投映然后将特征进行直接交叉，要么用神经网络把特征通过DNN的方式进行高阶特征学习 而这些输入的$x_i$的特征是sparse的one-hot vector里面要么是1 表明某个特征有出现，要么0 表示没出现。 Multi-one-hot但是在考虑到用户历史行为里面，用一个feature来代表用户的历史浏览的item时，它的可以千变万化并且也有很多数量的长度不确定。举个例子：输入的特征是 [age =18, gender = Female, product_cat = book, visited_cat_ids = {book, bag, computer, paper}，visited_shop_id={TV,movie} ], 那么这里的 visited_cat_ids 这个list就是过去看过的item的history。而这个list根据用户不同的浏览历史，它的长度和item的值也是不一样的。如果直接把这个feature变成one-hot是难以表达的因为one-hot里面只能有一个1代表有浏览某一个item，但是不能代表多个。 面对这个问题DIN的处理方法是将数据集里面的所有的visit过的items进行合并成一个集合，然后对这个集合变成一个multi-onehot的vector用来表示用户历史浏览过的多个item。 而每个浏览过的item可以通过embedding方法变成对应的dense vector，这样就能得到多个embedding vector。而embedding vector的个数也会因为每个用户浏览的历史的item个数不同而不同。 对multi-onehot 举个例子就像之前的NFM的结构一样，输入的sparse vector里面有多个1，而每个1代表浏览过的item，比如一个multi-onhot的vector里面有10个item因此vector的长度 是10，而 vector每个scalar value对应一个item，如果这个value=1那么这个item就是浏览过的。比如 visited_cat_ids = {book, bag, computer, paper}那么对应book, bag, computer, paper的4个item对应位置的scalar value就是1，其他位置的value就是0。 Pooling for embedding vectors那么问题又来了，每个用户的embedding的vector的个数不一样，但DNN的输入的shape是固定的，怎么才能把他们这些不同数目的embedding的vectors变成相同大小的特征? 方法就是pooling，把多出来的vectors通过pooling的方法进行concate 拼接的方法，形成一个固定大小的DNN的输入表达形式，如果有连续的feature value也把它的embedding拼接上去，它的公式是$$ e_i=pooling(e_{i1}, e_{i2}, …e_{ik})$$ 这里的$i$表示第$i$个历史特征组(是历史行为，比如历史的商品id，历史的商品类别id等， 这些特征组都是通过multi-onehot 来表示)， 这里的$k$表示对应历史特种组里面用户购买过的商品数量，也就是历史embedding的数量。 特征表达的小总结 Dense型特征: 由于是数值型了，这里为每个这样的特征建立Input层接收这种输入， 然后拼接起来先放着，等离散的那边处理好之后，和离散的拼接起来进DNN Sparse型特征: 为离散型特征建立Input层接收输入，然后需要先通过embedding层转成低维稠密向量，然后拼接起来放着，等变长离散那边处理好之后， 一块拼起来进DNN， 但是这里面要注意有个特征的embedding向量还得拿出来用（广告商品的embedding），就是候选商品的embedding向量，这个还得和后面的计算相关性，对历史行为序列加权。 VarlenSparse型特征: 这个一般指的用户的历史行为特征，变长数据， 首先会进行padding操作成等长， 然后建立Input层接收输入，然后通过embedding层得到各自历史行为的embedding向量， 拿着这些向量与上面的候选商品embedding向量进入+ AttentionPoolingLayer去对这些历史行为特征加权合并，最后得到输出。 DIN 网络结构DIN Base model: DIN 的改进结构如下： Embedding layer：这个层的作用是把高维稀疏的输入转成低维稠密向量， 每个离散特征下面都会对应着一个embedding词典， 维度是$D\\times K$， 这里的$D$表示的是隐向量的维度， 而$K$表示的是当前离散特征的唯一取值个数。 其他离散特征也是同理，只不过上面那个multi-hot编码的那个，会得到一个embedding向量的列表，因为他开始的那个multi-hot向量不止有一个是1，这样乘以embedding矩阵，就会得到一个列表了。通过这个层，上面的输入特征都可以拿到相应的稠密embedding向量了。 pooling layer and Concat layer： pooling层的作用是将用户的历史行为embedding这个最终变成一个定长的向量，因为每个用户历史购买的商品数是不一样的， 也就是每个用户multi-hot中1的个数不一致，这样经过embedding层，得到的用户历史行为embedding的个数不一样多，也就是上面的embedding列表不一样长， 那么这样的话，每个用户的历史行为特征拼起来就不一样长了。 而后面如果加全连接网络的话，我们知道，他需要定长的特征输入。 所以往往用一个pooling layer先把用户历史行为embedding变成固定长度(统一长度)，就像之前所说的pooling的公式一样。我们可以考的上面的图片里面Goods1的各个纵向的embedding vector在通过Concat之后变成单个横向的vector，而横向的vector里面的颜色对应之前的embedding的vector，但是长度变小了，这个是因为用了pooling的原因。Concat layer层的作用就是拼接了，就是把这所有的特征embedding向量，如果再有连续特征的话也算上，从特征维度拼接整合，作为MLP的输入。 MLP：这个就是普通的全连接，用了学习特征之间的各种交互。 Loss: 由于这里是点击率预测任务， 二分类的问题，所以这里的损失函数用的负的log对数似然：$$L=-\\frac{1}{N} \\sum_{(\\boldsymbol{x}, y) \\in S}(y \\log p(\\boldsymbol{x})+(1-y) \\log (1-p(\\boldsymbol{x})))$$ 这里改进的地方已经框出来了，这里会发现相比于base model， 这里加了一个local activation unit， 这里面是一个前馈神经网络，输入是用户历史行为商品和当前的候选商品， 输出是它俩之间的相关性， 这个相关性相当于每个历史商品的权重，把这个权重与原来的历史行为embedding相乘求和就得到了用户的兴趣表示$v_{U}(A)$, 这个东西的计算公式如下： $$\\begin{array}{cc}v_U(A)=f(v_{A}, e_{1}, e_{2}, \\ldots, e_{H})\\\\=\\sum_{j=1}^{H} a(e_{j}, v_{A}) e_{j} \\\\=\\sum_{j=1}^{H} w_{j} e_{j}\\\\\\end{array}$$ 这里的${v_{A}, e_{1},e_{2}, \\ldots, e_{H}}$是用户$U$的历史行为特征embedding， $v_{A}$表示的是候选广告$A$的embedding向量， $a(e_j, v_A)=w_j$表示的权重或者历史行为商品与当前广告$A$的相关性程度。$a(\\cdot)$表示的上面那个前馈神经网络，也就是那个所谓的注意力机制， 当然，看图里的话，输入除了历史行为向量和候选广告向量外，还加了一个它俩的外积操作，作者说这里是有利于模型相关性建模的显性知识。 这里有一点需要特别注意，就是这里的权重加和不是1， 准确的说这里不是权重， 而是直接算的相关性的那种分数作为了权重，也就是平时的那种scores(softmax之前的那个值)，这个是为了保留用户的兴趣强度。 Properties优点 考虑了用户的历史浏览信息，更加贴近真实的业务场景 也用embedding 和特征交叉的方式进行特征的学习和挖掘以及降维缺点 考虑用户浏览信息时没有考虑历史的先后顺序，只是把历史的信息和广告的信息特征交叉，没有考虑历史浏览的item的先后关联，比如一个物品被浏览后用户容易浏览下一个相关的物品 用户最新浏览的商品很有可能不在用户过去的浏览的历史里面，这样的话multi-onehot vector的长度就会变化， embedding也要重新训练，并且随着浏览的商品增多，onehot的长度越大，embedding训练也会越困难，在随着时间变化而特征也会变化这也是个问题。所以embedding要定期更新。 一般来说，用户浏览的历史有很多，那样embedding的个数就需要很多。但是对于一些浏览的历史里面，很有可能会存在大量的用户浏览了但是不感兴趣的内容，是否能够把这些内容进行过滤(比如更加浏览的时间来判断是否应该把item加入历史的list里面)从而降低onehot 的大小从而来加速模型的训练？ Code1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556# DIN网络搭建def DIN(feature_columns, behavior_feature_list, behavior_seq_feature_list): \"\"\" 这里搭建DIN网络，有了上面的各个模块，这里直接拼起来 :param feature_columns: A list. 里面的每个元素是namedtuple(元组的一种扩展类型，同时支持序号和属性名访问组件)类型，表示的是数据的特征封装版 :param behavior_feature_list: A list. 用户的候选行为列表 :param behavior_seq_feature_list: A list. 用户的历史行为列表 \"\"\" # 构建Input层并将Input层转成列表作为模型的输入 input_layer_dict = build_input_layers(feature_columns) input_layers = list(input_layer_dict.values()) # 筛选出特征中的sparse和Dense特征， 后面要单独处理 sparse_feature_columns = list(filter(lambda x: isinstance(x, SparseFeat), feature_columns)) dense_feature_columns = list(filter(lambda x: isinstance(x, DenseFeat), feature_columns)) # 获取Dense Input dnn_dense_input = [] for fc in dense_feature_columns: dnn_dense_input.append(input_layer_dict[fc.name]) # 将所有的dense特征拼接 dnn_dense_input = concat_input_list(dnn_dense_input) # (None, dense_fea_nums) # 构建embedding字典 embedding_layer_dict = build_embedding_layers(feature_columns, input_layer_dict) # 离散的这些特特征embedding之后，然后拼接，然后直接作为全连接层Dense的输入，所以需要进行Flatten dnn_sparse_embed_input = concat_embedding_list(sparse_feature_columns, input_layer_dict, embedding_layer_dict, flatten=True) # 将所有的sparse特征embedding特征拼接 dnn_sparse_input = concat_input_list(dnn_sparse_embed_input) # (None, sparse_fea_nums*embed_dim) # 获取当前行为特征的embedding， 这里有可能有多个行为产生了行为列表，所以需要列表将其放在一起 query_embed_list = embedding_lookup(behavior_feature_list, input_layer_dict, embedding_layer_dict) # 获取历史行为的embedding， 这里有可能有多个行为产生了行为列表，所以需要列表将其放在一起 keys_embed_list = embedding_lookup(behavior_seq_feature_list, input_layer_dict, embedding_layer_dict) # 使用注意力机制将历史行为的序列池化，得到用户的兴趣 dnn_seq_input_list = [] for i in range(len(keys_embed_list)): seq_embed = AttentionPoolingLayer()([query_embed_list[i], keys_embed_list[i]]) # (None, embed_dim) dnn_seq_input_list.append(seq_embed) # 将多个行为序列的embedding进行拼接 dnn_seq_input = concat_input_list(dnn_seq_input_list) # (None, hist_len*embed_dim) # 将dense特征，sparse特征， 即通过注意力机制加权的序列特征拼接起来 dnn_input = Concatenate(axis=1)([dnn_dense_input, dnn_sparse_input, dnn_seq_input]) # (None, dense_fea_num+sparse_fea_nums*embed_dim+hist_len*embed_dim) # 获取最终的DNN的预测值 dnn_logits = get_dnn_logits(dnn_input, activation='prelu') model = Model(inputs=input_layers, outputs=dnn_logits) return model Reference[1] paper: https://arxiv.org/pdf/1706.06978.pdf[2] datawhale: https://github.com/wenkangwei/team-learning-rs/blob/master/DeepRecommendationModel/DIN.md[3] https://cloud.tencent.com/developer/article/1164761[4] DeepCTR github https://github.com/wenkangwei/DeepCTR","link":"/2021/03/28/Recommendation-System-5-DIN/"},{"title":"Recommendation-System-9-Swing","text":"Motivation在推荐系统里面处理上亿级别的用户是一个challenging的问题，特别是在捕捉用户的实时行为，比如click， add bag等行为。另外，过去的协调滤波CF的方法里面有以下问题： Accuracy： 只考虑到user-item之间的关系，而忽略了用户内在的行为之间的关系，从而导致accuracy受限制 Sparity：用户购买行为的数据是非常稀疏，在大量的用户和商品里面， 可能被购买的商品就只有那么几个，从而导致对用户的购买行为捕捉困难度增加 Direction: 在Co-purchase联合购买行为里面，是存在方向性的，有可能购买A商品会也购买B商品，但是反过来的行为不一定频繁 Scalability: 在上亿级别用户里面用户数和商品数是在不断增长的，模型的可延展性scalability也是一个重要的问题 而Swing是 阿里淘宝提出的一个计算item之间相似度的模型，它主要把用户，商品之间的二分图bi-graph进行搭建，对user-item-user的关联行为考虑进来从而进一步利用用户的内在行为之间的关联。 How does Swing work Assumption在用户的点击行为里面是经常充满噪声的，比如用户误点或者随机点击的行为，从而导致这些行为可行度不大。因此， Swing这里提出了一个assumption： 如果一个用户同时点击两个item i和j那么这个行为有可能是随机的。 但是如果同时有两个user都同时点击item i和j，那么item i和j的关系就更加可行那么 user和item之间的关系图就变成下面的bi-graph图片 FormulaSwing的计算公式如下： 它计算item i和j之间的相似度时，先通过找出购买过item i 的user集合Ui 以及购买过item j 的user 集合Uj， 之后找到Ui， Uj之间的user交集。 在都购买过i，j商品的user里面再看看这些user之间的购买的商品的交集数的倒数。 如果这些用户是因为经常购买商品从而同时购买item i和就的话，导致交集很大，那么相似度就会下降，也就是这些用户的行为不可信。而alpha是一个smoothing factor。 另外因为active user购买的商品数目一般很大，容易导致Iu 和Iv的交集很大，使score偏小 存在bias，这里引入了一个penalty的weight对score进行调整 PropertiesAdvantages 计算简单而且能够并行计算，适用于召回层 无序考虑购买item的顺序 结合图结构，item之间的关联可靠性强Disadvantages 计算慢，Time Complexity O(T x N^2 x M), T: # of item, N: average degree of item, M: the average node degree of a user When to Use 推荐召回层里面可以使用，无需使用太多特征，只需user， item购买列表Reference[1] https://arxiv.org/pdf/2010.05525.pdf","link":"/2021/09/13/Recommendation-System-9-Swing/"},{"title":"Recommendation-System-EGES","text":"Introduction这篇文章主要回顾和介绍2018年Alibaba在KDD里面提出的的EGES(Enhanced Graph Embedding with Side information) 模型。 这个模型主要是在DeepWalk 学习到embedding的基础上增添了其他item的补充信息(side inforamtion). 在介绍EGES之前，先介绍一下过去Item2Vec以及DeepWalk 学习item的embedding的特点以及一些不足。 之后再介绍EGES的思路和训练方法。 Item2VecItem2Vec可以看成是Word2Vec 在推荐系统里面的推广。 Word2Vec 假设的是 在一个长为T的窗口/句子里面有{w1, w2,… wT}个单词，然后计算第i个单词(central word)和第j个单词（context word）的同时出现的隔离. 而ItemVec则是不考虑窗口的概念，而是考虑在用户的一个浏览的历史里面浏览过的商品{w1, w2,… wT}， 把单词替换成item。它的objective function是 $$\\mathbf{L} = \\frac{1}{K} \\sum_ {i=1}^k\\sum_ {j \\neq i}^k log(w _i | w _j)$$ Note: 在item2Vec的objective function里面它是计算每对商品之间的log的概率， 而word2Vec里面是计算给定的central word和其他context word之间的概率。这样看起来有点像是FM里面的特征两两交叉的操作 Item2Vec 的一些特点 广义上来讲， 只要能通过item的特征生成特征向量的方法都是item2Vec. Advantages: Item2Vec 是一个很广泛的概念，只要是序列数据都能够用来做embedding的学习，对item的embedding vector进行学习 Disadvantages: 和Word2Vec 一样， Item2Vec只用了序列的数据，比如item的购买序列，但是却没有用到其他的特征信息，比如item和item, user和user, user和item之间的关联信息(Relationship data), item的tag是补充信息。 为了解决Item2Vec只能用序列信息的问题， GraphEmbedding提供了很好的解决方案。Graph Embedding里面充分利用了item的补充信息外还能学习item，user之间的关联信息。而DeepWalk 就是其中一种GraphEmbedding的方法 EGES正如上面说的Item2Vec 只能用sequential data进行item的embedding vector学习，但是不能用到其他信息，比如item-item直接的关联，item的补充信息(categorical or numerical tags)。 而Graph Embedding的出现能够有效解决这个问题，之后也出现大量的graph embedding的学习方法。而 EGES (Enhanced Graph Embedding with Side Information) 就是其中之一。 MotivationEGES (Enhanced Graph Embedding with Side Information) 是有Alibaba在2018年KDD里面提出来的基于DeepWalk的GraphEmbedding 方法结合了item补充信息而得到的模型。它先基于DeepWalk得到基本的GraphEmbedding 模型，然后逐步把Side Information添加到模型里面进行优化. 所以下面先解释DeepWalk, 之后再解释EGES模型 DeepWalk / Basic Graph Embedding (BGE)DeepWalk 是在2014年KDD里面有Byran等人提出来的模型. Link：https://arxiv.org/pdf/1403.6652.pdf 在应用到Alibaba的推荐系统里面后它的基本流程如下: 收集用户的历史浏览数据 根据浏览数据搭建item之间的有向概率图。图里面每个节点代表一个item，而item之间的边是有向边，每条边都有对应的跳转概率。概率的计算公式如下， $P(v _j | v _i)$ 是从节点vi跳到vj 的概率，$M _{ij}$ 是从节点vi跳到vj的边的权重， 而 $N _{+} (v _i)$ 是节点vi的out-degree(出边)的节点的集合。 $\\epsilon$是图中所有边的集合 在概率图中用RandomWalk 随机游走采样多个物品序列 用这些序列放到Word2Vec里面训练得到Embedding，而其中的Objective/loss function 和Word2Vec的一样（如下形式）, 都是minimize在center node已知的情况下context node的出现概率的negative log的值。 Graph Embedding with Side information (BES)虽然DeepWalk学习到的embedding能够学习到Collabortive Filtering学不到的行为序列的关系，但是它依然有以下问题: 不能解决冷启动问题，面对新进来的item, item的graph是没有新进来的item信息 item的其他side information比如 标签信息等并没有充分利用 所以这里做的一个改进就是把side information也做embedding。得到item embedding以及side information的embedding。 其中$W _v^0$ 是原来学习到的item v 的embedding， $W _v^s$是第s个的side information的embedding。如果side information的embedding有n个，那么这个item v的embedding个数总共有n+1个，之后item v的所有embedding通过average pooling的方法进行信息融合得到对应item的embedding, 即$H _v$ Enhanced Graph Embedding with Side information (EBES)那么问题又来了， 虽然GES能够通过side information解决冷启动的问题，但是每个用户对一个item的喜欢的信息是有倾向性的，比如一个用户喜欢一台手机是因为它是IPhone, 另一个用户喜欢一台手机是因为它是续航能力强,那么用户对这些特征的倾向性也应该反映到对embedding的倾向性上面，如对”Iphone” 或”续航能力强”这个tag的embedding的权重应该提高。 因此它对每个embedding应该加上一个权重。于是， item的embedding的形式被调整成下面形式 其中： $a _v^j$ 是第v个item第j个embedding的权重值，由于要保证这个weight是大于0，于是用来exponent 作为底数 $H _v$是对第v个item所有embedding的weighted sum 在更新了Embedding的公式之后，EGES的embedding再乘上一个output matrix然后做softmax进行概率的预测，这一点和Word2Vec 一样。 loss用的是binary cross entropy. $Z _u$ 是context node u的embedding的信息，$H _v$是center node的embedding信息。这里的context node在原文中是通过negative sampling方法来采样负样本的节点，和Word2Vec一样。 Experiment原文里面用的评估方式也是传统的线下AUC和线上A/B CTR testing的评估。这个就不多说了 Opinions这里对上面一些算法的思考: 在DeepWalk 生成概率图来重新采样时，这个跳转的概率应该是要逼近实际的数据中两个item之间的有向关系，这种情况下，一般是要数据越大越好，才能逼近真实的关系 Deepwalk 里面的图只能计算已经知道的item之间的关系，对应新进来的item是始终有冷启动的问题。 而面对这个问题EGES是通过引入side information的embedding进行新的item信息填充来解决一开始新item没有embedding的情况 个人认为EGES创新性一般，主要还是在原因的deepwalk模型里面引入其他信息的embedding，而其他步骤基本上和Word2Vec一样。 不过这个其实能够解决冷启动问题而且也一定程度上提高了CTR的表现，算是比较实用的模型 Source Code 这里基于SparrowRecSys 开源推荐系统的github代码自己重新写了一遍 DeepWalk的graph embedding的计算. 我自己在弄的推荐系统的github：https://github.com/wenkangwei/RecSys 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278%%writefile embedding.pydef getItemSeqs(spark, samplesRating): \"\"\" extract item sequences for each user from dataframe 1. for each user, collect the corresponding visited movies and timestamp into a list 2. use UDF to process movie list and timestamp list to sort the movie sequence for each user 3. join the movie list to get a string for each user \"\"\" def sortF(movie_list, timestamp_list): \"\"\" sort by time and return the corresponding movie sequence eg: input: movie_list:[1,2,3] timestamp_list:[1112486027,1212546032,1012486033] return [3,1,2] \"\"\" pairs = [] # concat timestamp with movie id for m, t in zip(movie_list, timestamp_list): pairs.append((m, t)) # sort by time pairs = sorted(pairs, key=lambda x: x[1]) return [x[0] for x in pairs] sortUDF = udf(sortF, ArrayType(StringType())) # rating data #ratingSamples.show(5) # ratingSamples.printSchema() userSequence = samplesRating.where(F.col(\"rating\") &gt; 3) \\ .groupBy(\"userId\")\\ .agg(sortUDF(F.collect_list(\"movieId\"), F.collect_list(\"timestamp\")).alias(\"movieIds\"))\\ .withColumn(\"movieIdStr\", F.array_join(F.col(\"movieIds\"), \" \")) seq = userSequence.select(\"movieIdStr\").rdd.map(lambda x : x[0].split(\" \")) #print(seq.collect()[:5]) return seqdef embeddingLSH(spark, movieEmbMap): \"\"\" Local sensitive hashing using bucketedRandomProjection \"\"\" movieEmbSeq = [] for key, embedding_list in movieEmbMap.items(): embedding_list = [np.float64(embedding) for embedding in embedding_list] movieEmbSeq.append((key, Vectors.dense(embedding_list))) movieEmbDF = spark.createDataFrame(movieEmbSeq).toDF(\"movieId\", \"emb\") bucketProjectionLSH = BucketedRandomProjectionLSH(inputCol=\"emb\", outputCol=\"bucketId\", bucketLength=0.1, numHashTables=3) bucketModel = bucketProjectionLSH.fit(movieEmbDF) embBucketResult = bucketModel.transform(movieEmbDF) print(\"movieId, emb, bucketId schema:\") embBucketResult.printSchema() print(\"movieId, emb, bucketId data result:\") embBucketResult.show(10, truncate=False) print(\"Approximately searching for 5 nearest neighbors of the sample embedding:\") sampleEmb = Vectors.dense(0.795, 0.583, 1.120, 0.850, 0.174, -0.839, -0.0633, 0.249, 0.673, -0.237) bucketModel.approxNearestNeighbors(movieEmbDF, sampleEmb, 5).show(truncate=False)def getTransitionMatrix(item_seq): \"\"\" build graph and transition matrix based on input item sequences input: list of item sequence in RDD format output: transition matrix and item distribution in dictionary format \"\"\" def generate_pair(x): \"\"\" use a sliding window with size of 2 to generate item pairs input: ls = list of items output: list of pairs example: input: [86, 90, 11, 100,] output: [[86,90], [90, 11], [11,100]] \"\"\" res = [] prev = None print(x) for i in range(len(x)): if i &gt;0: res.append((x[i-1],x[i])) return res # convert item sequences to pair list pair_seq = item_seq.flatMap(lambda x: generate_pair(x)) # convert pair list to dictionary, key = pair, value = count pair_count_dict = pair_seq.countByValue() tot_count = pair_seq.count() trans_matrix = defaultdict(dict) item_count = defaultdict(int) item_dist = defaultdict(float) # consider out-degree only for item, cnt in pair_count_dict.items(): item1, item2 = item[0], item[1] item_count[item1] += cnt trans_matrix[item1][item2] = cnt for item, cnt in pair_count_dict.items(): item1, item2 = item[0], item[1] # possibility of transition trans_matrix[item1][item2] /= item_count[item1] # distribution of each source node (item) item_dist[item1] = item_count[item1]/tot_count return trans_matrix, item_distdef oneRandomWalk(trans_mat, item_dist, sample_length): \"\"\" generate one random walk sequence based on transition matrix input: - trans_mat: transition matrix - item_dist: distribution of item - sample length: number of node in a path = length of a walk -1 = length of edges - 1 \"\"\" rand_val = random.random() # randomly pick item based on CDF , cumulative density function, obtained from the item distribution # we can also randomly pick a item based on the distribution using choice () function from numpy as well cdf_prob =0 first_item = '' for item, prob in item_dist.items(): cdf_prob += prob if cdf_prob &gt;= rand_val: first_item = item break item_list = [first_item] cur_item = first_item while len(item_list) &lt; sample_length: if (cur_item not in item_dist) or (cur_item not in trans_mat): break cdf_prob = 0 rand_val = random.random() dist = trans_mat[cur_item] for item, prob in dist.items(): cdf_prob += prob if cdf_prob &gt;= rand_val: cur_item = item break item_list.append(cur_item) return item_listdef generateItemSeqs(trans_mat, item_dist, num_seq=20000, sample_length = 10 ): \"\"\" use random walk to generate multiple item sequences \"\"\" samples = [] for i in range(num_seq): samples.append(oneRandomWalk(trans_mat, item_dist, sample_length)) return samplesdef trainItem2Vec(spark, item_seqs, emb_length, output_path, save_to_redis=False, redis_keyprefix=None): \"\"\" use Word2Vec to train item embedding input: - item_seqs: RDD pipeline instance, rather than dataframe Note: - Word2Vec from mllib is a function that take RDD pipeline as input. - Word2Vec from ml is a function that take Dataframe as input \"\"\" # train word2Vec# w2v = Word2Vec(vectorSize=emb_length, windowSize = 5, maxIter = 10, seed=42) w2v = Word2Vec().setVectorSize(embLength).setWindowSize(5).setNumIterations(10) model = w2v.fit(item_seqs) # test word2vec synonyms = model.findSynonyms(\"157\", 20) for synonym, cos_similarity in synonyms: print(synonym, cos_similarity) # save word2Vec to input path if not os.path.exists(output_path): os.makedirs(output_path) with open(output_path, \"w\") as fp: for movie_id in model.getVectors(): # convert vector to string type and store it vector = \" \".join([str(emb) for emb in model.getVectors()[movie_id]]) pair = movie_id + \":\" + vector + \"\\n\" fp.write(pair) return modeldef getDeepWalk(spark, item_seq, sample_length=10, num_walk=20000, output_file='../../data/modeldata/embedding.csv', save_to_redis=False, redis_key_prefix=None): \"\"\" use DeepWalk to generate graph embeddings input: - item_seq: RDD based sequence of item visited by a user \"\"\" # construct probability graph trans_mat, item_dist = getTransitionMatrix(item_seq) # generate sequence samples randomly samples = generateItemSeqs(trans_mat, item_dist,num_seq=num_walk, sample_length = sample_length ) # convert list of samples to spark rdd samples_rdd = spark.sparkContext.parallelize(samples) # train item2Vec graphEmbModel = trainItem2Vec(spark, samples_rdd, emb_length=10, output_path=output_file , save_to_redis=False, redis_keyprefix=None) return graphEmbModeldef getUserEmb( spark ,samples_rating, item_emb_model, output_file): \"\"\" generate user embedding based on item embedding use map reduce to sum up embeddings of items purchased by user to generate user embedding input: - spark: spark session - samples_rating: dataframe with rating, movieId, userId data - item_emb_model: word2Vec/Item2Vec model trained by deep walk. - output_file: file name of user embedding \"\"\" # assert not item_emb or not item_emb_path, \"Must input either item embedding vectors or path\"# if item_emb_path != None:# item_emb = spark.read.csv(item_emb_path, header=True) emb_dict = item_emb_model.getVectors() item_emb_ls=[] for item, emb in emb_dict.items(): #print((item, emb)) item_emb_ls.append((item, list(emb))) fields = [StructField('movieId', StringType(),False), StructField('emb', ArrayType(FloatType()),False),] item_emb_schema = StructType(fields) item_emb_df = spark.createDataFrame(item_emb_ls, item_emb_schema) # apply mapreduce to sum up item embeddings for each user to obtain user embedding # Note: we need inner join here to avoid empty item embedding during mapreduce calculation user_emb = samples_rating.join(item_emb_df, on=\"movieId\", how=\"inner\") print() print(\"User Embdding\") user_emb.show(5) user_emb.printSchema() user_emb = user_emb.select(\"userId\",\"emb\").rdd.map(lambda row: (row[0], row[1]) ).reduceByKey(lambda emb1, emb2: [ float(emb1[i]) + float(emb2[i]) for i in range(len(emb1))] ).collect() print(user_emb[:5]) #save user embedding with open(output_file,\"w\") as fp: for userId, emb in user_emb: row = \" \".join([str(e) for e in emb]) row = str(userId)+ \":\"+ row + \"\\n\" fp.write(row) print(\"User Embedding Saved!\") returnif __name__ == '__main__': conf = SparkConf().setAppName('ctrModel').setMaster('local') spark = SparkSession.builder.config(conf=conf).getOrCreate() # Change to your own filepath file_path = '../../data/' rawSampleDataPath = file_path + \"ratings.csv\" embLength = 10 print(\"Process ItemSquence...\") samplesRating = spark.read.csv(rawSampleDataPath, header = True) item_seqs = getItemSeqs(spark, samplesRating) #print(samples) #trainItem2Vec(item_seqs, emb_length=10, output_path=file_path+\"modeldata/itemGraphEmb.csv\", save_to_redis=False, redis_keyprefix=None) graphEmb = getDeepWalk(spark, item_seqs, sample_length=10, num_walk=20000, output_file=file_path+\"modeldata/itemGraphEmb.csv\", save_to_redis=False, redis_key_prefix=None) getUserEmb( spark ,samples_rating= samplesRating, item_emb_model= graphEmb, output_file= file_path+\"modeldata/userEmb.csv\") print(\"Done!\") Reference[1] Alibaba-EGES: https://arxiv.org/pdf/1803.02349v2.pdf [2] DeepWalk： https://arxiv.org/pdf/1403.6652.pdf [3] SparrowRecSys: https://time.geekbang.org/column/article/296932 [4] 推荐算法大佬的github： https://github.com/shenweichen/GraphEmbedding [5] DeepWalk的原文: http://www.perozzi.net/publications/14_kdd_deepwalk.pdf","link":"/2021/08/03/Recommendation-System-GraphEmbedding/"},{"title":"Recommendation-System-7-FFM","text":"IntroductionField-aware Factorization Machine (FFM)是由Criteo Research 的YuChin Juan 在2016年发表到ACM的预测CTR的推荐算法，它这里引入了Field场的概念到M里面 Motivation在Field-aware FM 里面， 它的目的是通过引入field场的概念来将之前的 PITF（pairwise intereaction tensor factorization）和FM的特征交叉的方式更加generalize。 和PITF相比，FFM使得特征交叉是不限于User, Tag, Item这三种，而是更多不同的fields的embedding之间的交叉。 Field-Factorization Machine (FFM)Field在说明FFM的原理之前，先举个例子说明一下field场的概念。一般来说， 推荐系统里面的数据会有大量的稀疏特征，而在处理的时候会用OneHot encoding把类别特征变成0, 1 向量，encoding之后每个binary的值xi就相对于一个特征值。 而一个field场就是把同一类的特征值进行拼在一起。 这里举个例子。如果原来的特征有Publisher, Advertiser, Gender 三类别特征个，而其中Publisher有{ESPN, Vogue, NBC}三个特征值， Advertiser有{Nike, Gucci, Addidas} 三个特征值， Gender 有{Male, Female}两个特征值. 那么我们把这些类别特征OneHot Encoding 变成0,1 特征之后，{ESPN, Vogue, NBC} 3个binary 特征属于Publisher一类， {Nike, Gucci, Addidas}3个binary特征属于Advertiser一类，而{Male, Female} 2个binary特征属于Gender类，那么我们可以把Publisher, Advertiser, Gender 3个类看成是3 个fields. Field 1: Publisher Field 2: Advertiser Field 3: Gender ESPN Vogue NBC Nike Gucci Addidas Male Female Field-aware Factorization Machine在FM那样每个稀疏特征只对应一个embedding，比如在Advertiser里面Nike对应一个embedding，Addidas对应一个embedding。那么如果输入特征是Publisher = ESPN, Advertiser=Nike, Gender=Male, 那么FM的特征交叉 $\\Phi _{FM} = w _{ESPN} \\cdot w _{Nike} + w _{ESPN} \\cdot w _{Male} + w _{Male} \\cdot w _{Nike}$。 是每个embedding 向量 w 两两相互点乘相加。 在FM 里面，由于每个特征只有一个embedding，使得一个embedding会用于计算不同的特征之间的潜在影响关系。 比如 $w _{ESPN}$ 用于计算$w _{ESPN} \\cdot w _{Nike}, w _{ESPN} \\cdot w _{Male}$ 会同时影响到 (ESPN, Nike)和 (ESPN, Male) 两组交叉特征的隐性信息。而不同于FM的是， FFM在引入field的概念后假设Nike, Male这两个特征是属于不同的field的，每两组特征交叉的隐性向量应该相互独立，即像(ESPN, Nike)交叉向量和 (ESPN, Male) 交叉向量要用不同的$w _{ESPN}$进行计算， 这样才能使交叉后的隐性特征之间是相互独立的 因此，在FFM里面，每个特征都有多个embedding， 每个embedding对应一个field。 比如我现在有3个fields: Publisher, Advertiser, Gender, 那么我Nike这个特征就会有3个embedding vectors，分别是$w _{Nike, Publisher}, w _{Nike, Advertiser}, w _{Nike, Gender}$而FFM的二阶特征交叉的公式变成: 而它要优化的目标函数是 Comparison with FM, DeepFM 在FM和DeepFm里面，每个feature都只有一个embedding，而FFM里面每个feature的Embedding的个数等于field的个数 Space Complexity里面FM是 nk, 而FFM是nfk，乘上了field的个数。而Computation Complexity里面， FM是O(nk)而FFM是O(n^2k) Source Code这里参考DeepCTR里面的FFM源码 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273import tensorflow as tffrom tensorflow.keras.layers import Layerfrom tensorflow.keras.regularizers import l2class FFM_Layer(Layer): def __init__(self, sparse_feature_columns, k, w_reg=1e-6, v_reg=1e-6): \"\"\" :param dense_feature_columns: A list. sparse column feature information. :param k: A scalar. The latent vector :param w_reg: A scalar. The regularization coefficient of parameter w :param v_reg: A scalar. The regularization coefficient of parameter v \"\"\" super(FFM_Layer, self).__init__() self.sparse_feature_columns = sparse_feature_columns self.k = k self.w_reg = w_reg self.v_reg = v_reg self.index_mapping = [] self.feature_length = 0 for feat in self.sparse_feature_columns: self.index_mapping.append(self.feature_length) self.feature_length += feat['feat_num'] self.field_num = len(self.sparse_feature_columns) def build(self, input_shape): self.w0 = self.add_weight(name='w0', shape=(1,), initializer=tf.zeros_initializer(), trainable=True) self.w = self.add_weight(name='w', shape=(self.feature_length, 1), initializer='random_normal', regularizer=l2(self.w_reg), trainable=True) self.v = self.add_weight(name='v', shape=(self.feature_length, self.field_num, self.k), initializer='random_normal', regularizer=l2(self.v_reg), trainable=True) def call(self, inputs, **kwargs): inputs = inputs + tf.convert_to_tensor(self.index_mapping) # first order first_order = self.w0 + tf.reduce_sum(tf.nn.embedding_lookup(self.w, inputs), axis=1) # (batch_size, 1) # field second order second_order = 0 latent_vector = tf.reduce_sum(tf.nn.embedding_lookup(self.v, inputs), axis=1) # (batch_size, field_num, k) for i in range(self.field_num): for j in range(i+1, self.field_num): second_order += tf.reduce_sum(latent_vector[:, i] * latent_vector[:, j], axis=1, keepdims=True) return first_order + second_orderclass FFM(Model): def __init__(self, feature_columns, k, w_reg=1e-6, v_reg=1e-6): \"\"\" FFM architecture :param feature_columns: A list. sparse column feature information. :param k: the latent vector :param w_reg: the regularization coefficient of parameter w :param field_reg_reg: the regularization coefficient of parameter v \"\"\" super(FFM, self).__init__() self.sparse_feature_columns = feature_columns self.ffm = FFM_Layer(self.sparse_feature_columns, k, w_reg, v_reg) def call(self, inputs, **kwargs): ffm_out = self.ffm(inputs) outputs = tf.nn.sigmoid(ffm_out) return outputs def summary(self, **kwargs): sparse_inputs = Input(shape=(len(self.sparse_feature_columns),), dtype=tf.int32) tf.keras.Model(inputs=sparse_inputs, outputs=self.call(sparse_inputs)).summary() Reference[1] Paper: https://www.csie.ntu.edu.tw/~cjlin/papers/ffm.pdf[2] Github: https://github.com/shenweichen/DeepCTR/tree/9f155590cc44c14821dcb691811656eb2ef2f49b","link":"/2021/07/31/Recommendation-System-7-FFM/"},{"title":"PySpark-0-Installation","text":"IntroductionThis article is to introduce how to install PySpark on Linux using different methods (pip and install manually). Then I will talk about some issues I met during installation. Note: before installing PySpark, we need to install Java and set the JAVA_HOME environment variable first Install PySpark via pipInstall PySpark with pipThe simplest way to install PySpark in linux is to use pip tool to install pyspark. The commands are as follow: 1pip install pyspark To specify the extra dependencies for extra components, we can add [component_name, ...]. For example, if we want to install components for SQL, then 1pip install pyspark[sql] In the default distribution, PySpark uses Hadoop3.2 and Hive2.3 as default. If we want to specify the version of Haddop / Hive version or distribution URL, we can do the following (by setting PYSPARK_HADOOP_VERSION=… before pip): 1PYSPARK_RELEASE_MIRROR=http://mirror.apache-kr.org PYSPARK_HADOOP_VERSION=2.7 pip install Setting Environment VariablesAfter installing pyspark using pip, we need to put the following environment variables to ~/.bashrc file to tell pyspark about the settings we want. 12export PYSPARK_PYTHON=/usr/bin/python3export PYSPARK_DRIVER_PYTHON=/usr/bin/python3 Note: if we have multiple python versions, like python2 and python3, we need to set these two environment variables to tell PySpark which python we use. Install PySpark ManuallyInstall PySpark with source code First, we need to download a suitable version of PySpark from the official website: https://www.apache.org/dyn/closer.lua/spark/spark-3.1.2/spark-3.1.2-bin-hadoop3.2.tgz and then extract pyspark by this command (here I choose pyspark 3.1.2 with hadoop3.2). 1tar xf spark-3.1.2-bin-hadoop3.2.tgz Install the following dependencies using pip install pandas, numpy, pyarrow, py4j. We should also check the minimum versions of those packages: http://spark.apache.org/docs/latest/api/python/getting_started/install.html pandas: optional for SQL Numpy: required pyarrow: optional for SQL Py4J: it interprets python to JVM code findspark: it tells the python program about where to find pyspark Note that when using pip to install pyspark, it automatically installs those dependencies and tell the system Setting Environment VariablesWe also need to tell the system about where to find pyspark and some settings about pyspark, so we need to do the following: 123456cd spark-3.0.0-bin-hadoop2.7 #the path to the root directory of pysparkexport SPARK_HOME=`pwd`export PYTHONPATH=$(ZIPS=(\"$SPARK_HOME\"/python/lib/*.zip); IFS=:; echo \"${ZIPS[*]}\"):$PYTHONPATHexport PYSPARK_PYTHON=/usr/bin/python3export PYSPARK_DRIVER_PYTHON=/usr/bin/python3export JAVA_HOME= directory_to_your_jdk Alternatively, we can add the following codes to ~/.bashrc file so that we can avoid repeatting using these codes before running the program in the future. 123456# in ~/.bashrcexport SPARK_HOME=spark-3.0.0-bin-hadoop2.7 #the path to the root directory of pysparkexport PYTHONPATH=$(ZIPS=(\"$SPARK_HOME\"/python/lib/*.zip); IFS=:; echo \"${ZIPS[*]}\"):$PYTHONPATHexport PYSPARK_PYTHON=/usr/bin/python3export PYSPARK_DRIVER_PYTHON=/usr/bin/python3export JAVA_HOME= directory_to_your_jdk Then 1source ~/.bashrc Setting Environment Variables during RuntimeWe can also use os.environ[..] = &quot;..&quot;method to setup the environment variables as well 12345678910import osos.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\" # path to java jdkos.environ[\"SPARK_HOME\"] = \"/content/spark-3.1.1-bin-hadoop2.7\" # path to spark homeos.environ[\"PYSPARK_PYTHON\"] = \"/usr/bin/python3\"os.environ[\"PYSPARK_DRIVER_PYTHON\"] = \"/usr/bin/python3\"import findsparkfindspark.init(\"spark-3.1.1-bin-hadoop2.7\")# use findspark to find the root of pyspark, SPARK_HOMEfrom pyspark.sql import SparkSessionspark = SparkSession.builder.master(\"local[*]\").getOrCreate() Template when using PySpark with Google ColabHere is a template of using pyspark in runtime in Google colab.Please also update the link of pyspark with the url of the latest version of PySpark. We can check it here： https://www.apache.org/dyn/closer.lua/spark/spark-3.1.2/spark-3.1.2-bin-hadoop3.2.tgz 12345678910!apt-get update!apt-get install openjdk-8-jdk-headless -qq &gt; /dev/null!wget -q http://apache.forsale.plus/spark/spark-3.1.1/spark-3.1.1-bin-hadoop2.7.tgz !tar xf spark-3.1.1-bin-hadoop2.7.tgz!pip install -q findspark!pip install py4j!export JAVA_HOME=$(/usr/lib/jvm/java-8-openjdk-amd64 -v 1.8)! echo $JAVA_HOME 1234567891011import osos.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"os.environ[\"SPARK_HOME\"] = \"/content/spark-3.1.1-bin-hadoop2.7\"os.environ[\"PYSPARK_PYTHON\"] = \"/usr/bin/python3\"os.environ[\"PYSPARK_DRIVER_PYTHON\"] = \"/usr/bin/python3\"import findsparkfindspark.init(\"spark-3.1.1-bin-hadoop2.7\")# path to the home of pyspark: SPARK_HOMEfrom pyspark.sql import SparkSessionspark = SparkSession.builder.master(\"local[*]\").getOrCreate() Install Spark in Distributed ClusterIn industry, we usually need to install spark in distributed cluster rather than local machine, so that we can leverage the power of distributed computing. In order to install Spark in distributed cluster, please check this link: https://www.hadoopdoc.com/spark/spark-install-distribution Problems during Installation Without specifying path to python for pyspark If we don’t add the following env variables to specify path of python when we have different versions of python, like python2 and python3, it will pop up exception when we use show() method to display dataframe. 12export PYSPARK_PYTHON=/usr/bin/python3export PYSPARK_DRIVER_PYTHON=/usr/bin/python3 Error when using df.show() to display spark dataframe 123Exception: Python in worker has different version 2.7 than that in driver 3.5, PySpark cannot run with different minor versions. Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.... Set the environment variables for Pyspark when we use pip method to install pyspark, there is no need to specificy SPARK_HOME or PYTHONPATH. However, when we install it manually, we do need to specify them and also use findspark.init(…) to search the home directory of pyspark. For other common issues, check this: https://towardsdatascience.com/pyspark-debugging-6-common-issues-8ab6e7b1bde8 Reference[1] PySpark Org: http://spark.apache.org/docs/latest/api/python/getting_started/install.html [2] StackOverflow: https://stackoverflow.com/questions/48260412/environment-variables-pyspark-python-and-pyspark-driver-python [3] https://towardsdatascience.com/how-to-use-pyspark-on-your-computer-9c7180075617[4] issues in pyspark: https://towardsdatascience.com/pyspark-debugging-6-common-issues-8ab6e7b1bde8 [5] Tutorial of Spark( using scala): https://www.hadoopdoc.com/spark/spark-sparkcontext","link":"/2020/10/14/PySpark-0-Installation/"},{"title":"SQL-Installation","text":"IntroductionThis article is to introduce how to install PostgreSQL server database and client software used to connect this server. We will introduce the following contents: Install PostgreSQL server on Window and Linux Install Client software (I choose VSCode here) Connect to PostgreSQL to create database Some useful commands to manage database PostgreSQL Installation为了安装 PostgreSQL， 我们首先要安装一个SQL的数据库服务器用于存放和管理数据库里面的数据。另外， 我们要访问这个PostgreSQL的数据库我们就要有一个client客户端对这个服务器进行远程连接访问，并通过在Client端编写SQL代码对数据进行增删查改等操作。所以这里我们除了安装PostgreSQL的 server外，我们需要选择client客户端安装。 其中常见的Client可以是VSCode的PostgreSQL插件， SQL WorkBench PostgreQL Server InstallationWindow这里介绍PostgresSQL在Window里面的安装。 下载地址: https://www.enterprisedb.com/downloads/postgres-postgresql-downloads 安装的时候根据系统的版本进行下载安装： 在安装时候会它问你要不要安装一些元件，这里我们可以默认安装所有元件 之后我们可以选择database的存放位置，一般都是默认在C盘里面。 在选择存放位置后， 我们需要设置这个PostgresSQL的登录的password，而它的默认的user name是 postgres 以及访问的端口port(默认是5432). 这一步很重要,之后在Client里面连接PostgreSQL server时需要用到这个 username 和password以及port 在所有设定设置好后它会显示所有的summary，之后只管点next完成安装就可以了。 Ubuntu在Ubuntu的系统里面，我们可以很简单通过command line的方式直接安装PostgreSQL: 12sudo apt-get updatesudo apt-get install postgresql postgresql-client 安装后，PostgreSQL会默认创建名为postgres的用户，密码为空 123456# sudo -i -u postgres# use PostgresSQL$ psql# Command to quit SQL\\q 启动和停止PostgreSQL: 123sudo /etc/init.d/postgresql start # 开启sudo /etc/init.d/postgresql stop # 关闭sudo /etc/init.d/postgresql restart # 重启 Client Installation在安装了PostgreSQL server后，一般会有SQL Shell窗口程序进入server，并直接用SQL对server进行处理。 但是如果我们想远程操作服务器就需要用到Client。这里我们介绍如何安装VSCode的PostgreSQL插件对database进行访问。 在VSCode的左侧一栏里面的查找按键里面查找到以下的PostgreSQL插件，并安装。 在安装之后我们可以看到左侧栏下面有个大象的icon，那么就是安装成功了 打开这个插件，并在上面找的一个”+”加号的icon的add connection的按键，点击去就把下面一些信息填写进去就可以在VSCode里面添加登录到PostgreSQL server的信息: The host of the database: 如果是本地连接就填localhost或者127.0.0.1, 否则要填写server的IP地址 The PostgreSQL user to authenticate as：这里填写之前默认的username: postgres The password of the PostgreSQL user: 这里填写之前安装时自己设定的密码 The port number to connect to: 这里默认5432 Use an ssl connection?： 这里选择Standard Connection Show All Databases: 这里选择PostgreSQL server里面已经有的database。如果之前没有重建的话，一般会有一个database叫做postgres, 直接默认选择这个就好了 The display name of the database connection: 这个是我们重建了这个connection后，在VSCode左侧栏里面显示的SQL connection的名字。 除了VSCode外， 我们可以通过安装 SQL Workbench对PostgreSQL server进行连接，link: https://www.sql-workbench.eu/MainWindow_png.html Useful Commands in PostgreSQLSQL Shell在Client和 Server的shell里面，我们都可以直接通过PostgreSQL的语句对databases, table进行增删查改。 不过在Server的command line的SQL shell里面，我们可以直接通过一些命令进行查询. 在进入SQL shell后它会出现postgres=#输入行， 这个是指当前的database的名字 \\q: 退出SQL \\l: 显示所有database名字 \\c database_name: 切换到指定的database \\d: 显示当前database里面所有的table名字 \\help: 显示help信息(SQL的语法信息) \\help SQL_keyword: 显示SQL关键词的用法 登录进去psql的语句：psql -h host_name -p port_num -U user_name database_name Some useful SQL commands Create and drop database CREATE DATABASE dbname; DROP DATABASE IF EXISTS database_name;: 这里的IF EXISTS 可以加也可不加，加了之后如果database是不存在的话不会报错 Create and drop tables在SQL里面创建table时我们要先创建一个空的table定义每一列的datatype，之后才能够把数据一行行insert导入进去。 create empty table 12345678CREATE TABLE table_name(column1 datatype,column2 datatype,column3 datatype,.....columnN datatype,PRIMARY KEY( 一个或多个列 )); example: 1234567CREATE TABLE COMPANY(ID INT PRIMARY KEY NOT NULL,NAME TEXT NOT NULL,AGE INT NOT NULL,ADDRESS CHAR(50),SALARY REAL); Copy data from CSV file using COPY 123# table in data.csvid_count,tenure_count10000,11 123456789DROP TABLE IF EXISTS RESULT3;CREATE TABLE IF NOT EXISTS RESULT3( id_count varchar(100) NOT NULL, tenure_count varchar(100) NOT NULL);COPY RESULT3 FROM 'E:\\data.csv' WITH (FORMAT CSV, HEADER TRUE);SELECT * FROM RESULT3; Reference[1] Runoob: https://www.runoob.com/postgresql/windows-install-postgresql.html [2] SQL Workbench: https://www.sql-workbench.eu/MainWindow_png.html [3] Microsot VSCode: https://marketplace.visualstudio.com/items?itemName=ms-ossdata.vscode-postgresql","link":"/2021/07/28/SQL-Installation/"},{"title":"Statistic- 1 Hypothesis-Testing","text":"Introduction: Hypothesis testingThis article will introduce what is hypothesis testing and some teminologies, like p-value, significant level, confidence level, etc.The outline of this article is as follow: what is hypothesis testing What components are in Hypothesis testing and How Hypothesis testing Distribution of the null hypothesis (Center limit theorem) Significant level and confidence level P-value Overall Steps in Hypothesis testing When to use hypothesis testing What is Hypothesis testingHypothesis testing is a procedure to determine if a hypothesis about an estimated difference is statistically meaningful and should be rejected or not. If the hypothesis is rejected, then we will choose alternative competing hypothesis For example, when we are designing an APP and have two different styles of the interface: Red and Green. Then we make an hypothesis that Red color style can attract more users than the Green style. The alternative competing hypothesis is that Red color style can not attract more users than Green style. Then in hypothesis testing is to determine if we should reject the hypothesis that red style can attract more users than green and choose another competing hypothesis that there is no difference between two styles. Main idea behind Hypothesis testingComponents in Hypothesis testingThe components in Hypothesis testing are as follow: Null Hypothesis H0: the original hypothesis assuming that the estimated difference is slight enough and can be regarded as no difference. Alternative Hypothesis H1: the alternative competing hypothesis that we use to challenge the null hypothesis. Usually H1 assumes that observed difference between groups should be considered.Note that alternative hypothesis is mutually exclusive from Null hypothesis. Evidence/ Observation that is used to challenge the Null hypothesis. Note that if Null hypothesis is statistically meaningful and held, then the possibility of the occurence of this evidence will be small. Idea behind Hypothesis testingThe idea behind hypothesis testing is to use proof of contradiction to reject the null hypothesis. It is to use the evidence/observation, which seems like an extreme case under hypothesis H0 and may be able to challenge the null hypothesis, to work as a contradiction to reject H0. However, whether the evidence/ observation is strong enough to work as an contradiction to reject H0 should be determined. Example 1:H0: All bird can fly. H1: Not all bird can fly. Evidence: penguin can not fly.We can see that in this case the observation that penguin can not fly is a strong enough to be a contradiction to H0, since all penguins can not fly. Example2:H0: bird eats food. H1: bird doesn’t eat foodEvidence: one bird in the zoo didn’t eat my food when I try to feed it.Here we can see the evidence is just a specific case while that all birds eat food is a fact. Hence the evidence is not strong enough to reject H0. In order to test whether an evidence is strong enough to reject H0, P-value, significant level, confidence level, power will be introduced later. One important note is that in hypothesis testing, we can only determine if we should reject the hypothesis or not. We CAN NOT prove that the hypothesis we specify is correct or the hypothesis is accepted.If the hypothesis is not rejected, it just means that the possibility that the hypothesis goes wrong is statistically small enough so that we can not observe the wrong cases using current observation method. It doesn’t mean the hypothesis is actually correct or should be accepted. Center limit theoremThe central limit theorem states that if you have a population with mean μ and standard deviation σ and take sufficiently large random samples from the population with replacementtext annotation indicator, then the distribution of the sample means will be approximately normally distributed， even the real distribution of samples is not Normal distribution. Due to Center Limit theorem, when we samples large amount data for H0 and H1, the distribution of H0 and H1 are normal distribution, shown as below When we plot the distribution of H0 and H1 in the same space, it would look like this (assume the left one is H0, and the right one is H1) where the x axis represent feature X and y-axis represent the possibility density function value.We can see that two distributions have the overlapping region. If we want to determine when to reject H0, we need a threshold of feature x (a vertical line) to separate H0 and H1.Then to find such threshold, it involves the concept of significant level and p-value P-valueWhat is p-valueP-value is the probability under the assumption of no effect or no difference (null hypothesis H0), of obtaining a result equal to or more extreme than what was actually observed. (在H0=true的假设前提下/H0=True的distribution下,取得比目前的evidence/observation 同等极端或更加极端更加极端的事件的概率). The P stands for probability and measures how likely it is that any observed difference between groups is due to chance. In other words, P-value actually measure how strong the evidence is to reject H0. If P-value is larger, then the strenght of evidence to reject H0 is weaker. Otherwise, the strenght is stronger. Example: Assume I am sampling a random number from a range [-1000, 1000] and assumes that the distributions of number in this range is normal distribution with mean =0. An observation is a sample with value =900. Then the P-value of this observation is the possibility of obtaining sample &gt;=900. In the plot of possbility density function, P-value represents the area under curve in X&gt;=900. Significant level and confidence levelIn order to determine whether reject H0 or not, we need to define a threshold for P-value and this leads to the concepts of significant level and confidence level. Significant level“Significant” means how significant the difference between groups is so that we should consider such difference is meaningful. Significant level is usually notated as $\\alpha$ and $\\alpha$=P(reject H0 | H0 =true). significant level is the possibility of rejecting H0 when H0 is true. Moreover, the error that rejecting H0 when H0 is true is called Type I error. Hence Significant level is also the possibility of the occurence of Type I error / Type I error rate (Significant error = Type I error) confidence levelConfidence level is the possibility of retaining H0 when H0 is true. Hence we know that confidence level = 1- significant level = $1-\\alpha$Confidence interval is the region outside the shadow region of $\\alpha$ in the distribution of H0=true. In the possibility density function, We can see that the shadow region in the figure represent a single side significant level (as it is possibility, in density function, it represents an area). Then the area of shadow region = $\\alpha$ When to reject H0if P-value &lt; $\\alpha$, we reject H0 and choose H1. if P-value &gt;=$\\alpha$, retain H0 if an observation X on the x-axis is outside of shadow region, that is, P-value, P(x&gt;=X) is greater than or equal to significant level, then we think observation X is common under null hypothesis H0 and we can not use X to reject H0. if P-value is smaller than significant level / observation X fall inside shadow region, then the difference between groups will be considered and we reject H0 and choose H1. In other words, the observation X is too rare so that we don’t consider it satisfies null hypothesis and hence it becomes a contradiction to reject H0. Note: significant level and confidence level is set by user and the value of significant level is usually 0.05 or 5%. This also means that users assume the possibility of rejecting a true hypothesis H0 due to some extreme cases under current observation method should be smaller than significant level $\\alpha$. Retain region and powerNow, we are considering the alternative hypothesis H1 rather than H0. Assume the distribution from H1 is normal distribution as well. Beta/retain regionThe yellow region of distribution of H1=True is the retain region for H0 and the area of the yellow region is $\\beta$, which represents the possibility of retaining H0 when H1=True. If the observation value X on x-axis falls into the yellow region, then we retain Hypothesis H0.$\\beta$ = P(retain H0 | H0 = False) = Type II error rate ( possibility to retain H0 when H0 is false under assumption H1=True) Powerpower is possibility of rejecting null hypothesis H0 and power = 1- $\\beta$.if power is set to be larger, then testing could be easier to detect and reject false Null hypothesis. But if power is too large, it would reject a good hypothesis as well, despite the null hypothesis is true. Note that there is a tradeoff between $\\alpha$ and $\\beta$. When we set smaller $\\alpha$ value to decrease the Type I error rate (reject H0 when H0 is True), then $\\beta$ / Type II Error rate (retain H0 when H0 is False)will increase . Usually $\\beta$ is not set by user, but it can be calculated using p-value and $\\alpha$. Usually $\\beta$ is around 20% Type I and Type II ErrorNow Let’s consider the distributions of hypothesis that H0=True (left) and the hypothesis that H1 = True (right) Together. Both distributions are estimated by sampling. Since the yellow region $\\beta$ in distribution of H1= True and confidence region of distribution of H0 = True represent the regions to retain H0, we can put two distribution together. Then the vertical line separates the region of retaining H0 (left) and rejecting H0 (right). Remember that $\\alpha$ = Type I error rate = P(reject H0 | H0 = True) and $\\beta$ = Type II error rate = P(retain H0 | H0 = False) and there is trade-off between Type I error rate and Type II error rate. When we lower significant level $\\alpha$, $\\beta$ will increase. In other words, lowering threshold to reject H0 can increase possibility of retaining H0, but also null hypothesis H0 may not be true, in this case, rate of accepting False H0 will increase. Factors affecting Power and $\\beta$ Size of the effect / effect size The size of the effect is the distance between the mean of null hypothesis distributions and the mean of true distribution.If $\\alpha$ is fixed, then Greater the size of effect is, smaller $\\beta$ / Type II error rate is and more different two distributions are. We can see that the first figure has the effect size smaller than the effect size in second figure, as the $\\beta$ in the top figure is larger than the bottom one. Assume distribution of H0=True is estimated from samples and the real distribution of population is distribution of H1=True. Then Equation to compute size of effect is: effect size = $\\frac{\\mu_{H0} -\\mu_{H1} }{\\sigma_{H1}}$where $\\mu_{H0}$, $\\mu_{H1}$ are mean of H0, H1 $\\sigma_{H1}$ is standard variance of distribution of H1=True. Note that effect size can be computed, but can not be controlled by users directly. It depends on the samples and population. Size of samplesThe sample size controls the variance of a distribution (width of distribution) and hence can affect $\\beta$ / Type II error rate. In the following figure, when $\\alpha$ and means of two distributions H0=True, H1 = True are fixed, large sample size is, smaller variance is and smaller $beta$ / Type II error is Control Type I error $\\alpha$ To control Type I error rate/ $\\alpha$, we can change $\\alpha$ directly. Decrease $\\alpha$ is to decrease Type I error rate (rejecting H0 when H0 =True) Usually $\\alpha$ is set to be 0.05 or 0.01 Control Type II error $\\beta$ or (1- power) Since there is trade-off between Type I error rate $\\alpha$ and Type II error rate $\\beta$, we can increase $\\alpha$ to decrease $\\beta$ and increase power Increase sample size to decrease variance of distribution to decrease $\\beta$, the rate of accepting H0 when H0=False. Two-tailed Hypthesis TestingThe hypothesis testing above is called single-tailed hypothesis testing, since we only consider the significant level $\\alpha$ in on one side only. In the figure below $H_a: \\mu &lt;\\mu_0 $ means the sample mean $\\mu$ is smaller than population mean $\\mu_0$. In single-tailed test, we only consider one-side of relationship and disregard another side Now we consider the Two-tailed Hypothesis testing with the sum of area of left tail and right tail in distribution of H0 equal to $\\alpha$. The figure is shown below. In two-tailed test, we regard the extreme cases on both sides. *We reject H0 when 2P-value&lt; $\\alpha$ or P-value &lt; $\\frac{\\alpha}{2}$ in Two-tailed Hypothesis Testing** For example, we want to analysis how tall the people are in a region. Then Example 1:H0: the mean tall of population = 1.75 mH1: the mean tall of population is not equal to 1.75 m.Observation: the mean of a sample group of people = 1.9m.In this case, if we care whether tall mean of people is &gt;1.75m or &lt;=1.75m, then we use two-tailed test and compare P-value with $\\frac{\\alpha}{2}$ Example 2:H0: the mean tall of population = 1.75 mH1: the mean tall of population is greater than 1.75 m.Observation: the mean of a sample group of people = 1.9m.If we care if tall mean of people &gt;1.75m or not only, then we use one-tailed test and compare P-value with $\\alpha$. Example 3:H0: the mean tall of population = 1.75 mH1: the mean tall of population is less than or equal to 1.75 m.Observation: the mean of a sample group of people = 1.5m.In this case, we care if mean &lt;=1.75m or not only, then we use one-tailed test again. Choosing which tail in one-tailed test depends on the observation and your alternative hypothesis H1. When to use single-tailed test or two-tailed test Choosing two-tailed test when: when you care extreme cases on both sides of distribution without direction when effects exist on both sides based on your observations. Note that if effect exists on two sides, but we choose single-tailed test, then the expected significant level =$\\alpha$ but we choose $\\frac{\\alpha}{2}$, this may increase Type II error rate and decrease power as we decrease $\\alpha$ Choosing one-tailed test when: when you care one side of effect / extreme cases on one side only when effects exist on only one side based on your observations Note that if effect exists on two tails and we choose one-tailed test, then expected significant level =$\\frac{\\alpha}{2}$, but we use $\\alpha$ instead, this may increase Type I error rate. Methods to compute p-valueThe common methods to compute P-value as follow, I will introduce them in the next article z-test Z-distribution/normal distribution Assume data follow normal distribution usually used with continuous variables t-test Use t-distribution usually used with continuous variables Pearson’s Chi-Square score Use Chi sqare distribution (Square Sum of multiple normal distribution) Used to measure independence between two variables Can be used for continuous variables or discrete variables Factors affecting Hypothesis testing Sample size As mentioned before, sample size can affect $\\beta$, type II error rate (retain H0/ can not reject H0 when H0 is false). Randomized Experiment In order to obtain representative samples from population in hypothesis testing to compute the p-value, we need to avoid sampling in bias. This leads to randomized experiment. There are some important terms in randomized experiment: Random sampling Random sampling is to obtain get some representative samples from the population (full traffic) randomly. If the samples are not obtained randomly, it could be biased which can affect the distribution of sample data even sample size is large Randomization / Random assignment Randomization is how you allocate samples to groups in experiment. It alleviates the bias that might cause a systematic difference between groups unrelated to the treatment itself. For example, If I want to test if the new UI design of a software can get higher ClickThrough rate (CRT) from users. Then I compare a group of users who use the new UI, called treatment group with another group of users who use original UI, called controlled group. Then After I randomly sample the users from different region, the way that I randomly assign those users to treatment group or controlled group is randomization/ random assignment. Confounding factor /variable Confounding factor is the factor that correlates with the independent and dependent variable confuse the effects and impact the results of experiments. Randomization of experiments is the key to controlling for confounding variables in machine learning experiments. Summary to use Hypothesis testing Specify a Null Hypothesis H0 and alternative Hypothesis H1 based on some observations (H0 and H1 should be mutually exclusive). Choose one-tailed test or two-tailed test based on H0 and H1 Specify significant level $\\alpha$ Compute P-value based on dataset (t-test/t-statistic, z-test, F-statistic,Chi-square test) Compare P-value with significant level $\\alpha$. If P-value &gt;= $\\alpha$, retain H0 (NOT accept !). If P-value &lt; $\\alpha$, reject H0 and retain H1 Reference[1] https://towardsdatascience.com/a-complete-guide-to-hypothesis-testing-2e0279fa9149 [2] https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4111019/ [3] http://web.mnstate.edu/malonech/Psy633/Notes/hyp.%20testing,%20effect%20size,%20power%20GW8.htm [4] https://statisticsbyjim.com/hypothesis-testing/one-tailed-two-tailed-hypothesis-tests/ [5] http://us.sagepub.com/sites/default/files/upm-binaries/83321_Chapter_8_Hypothesis_Testing_Significance,_Effect_Size,_and_Power.pdf [6] https://zhuanlan.zhihu.com/p/86178674 [7] https://machinelearningmastery.com/confounding-variables-in-machine-learning/ [8] https://online.stat.psu.edu/stat100/lesson/10","link":"/2021/02/19/Statistic-Hypothesis-Testing/"},{"title":"Web Scrapping and Regular Expression - 1","text":"IntroductionThis Quick tutorial is to use an web scrapping example to introduce how to use BeautifulSoup and Regular expression to mine the web data quickly and easily. For details, Please refer to the BeautifulSoup document. BeautifulSoupAPI Requests Import requests package 1import requests requests.get():using get(url) function from requests package, it allows us to send a request to the url webstite.It returns the response from that website, but this response is not HTML file, but a response packet from server response.content:By calling content after we get the response packet from get(), we can extract the HTML page from the packet and analyze it. BeautifulSoup soup = BeautifulSoup(html …) 12from bs4 import BeautifulSoupsoup = BeautifulSoup(html, 'html.parser') This creates beautifulsoup object and parse the HTML text with a html parser to store html content to the beautifulsoup object. .tag_nameAfter obtaining a beautiful soup object of a html file, we can use soup.tag_name to get name of current tag.Example: 12345678910111213 body = soup.b print(body.name) ~~~ This extracts the tag *\\&lt;b\\&gt;* &lt;br&gt;+ *t.get_text() / t.text* After extracting a tag, we can use .text or .get_text() function to extract all texts value under current tag ~~~Python soup = BeautifulSoup(\"&lt;html&gt;&lt;h1&gt;Head 1&lt;/h1&gt; &lt;h2&gt;Head 2&lt;/h2&gt;&lt;html&gt;\") soup.get_text() #or soup.text It returns “Head 1 Head 2” directly t.attrs[“href”] or t[“href”]:In HTML, every tag could have its attributes inside the tag. We can simply use tag.[“attribute-name”] or tag.attr[“attribute-name”] to extract the attributesThis example extracts the href link from tag &lt;a href=…&gt;&lt;/a&gt;Example: 1234567html = \"https://www.baidu.com\"soup = BeautifulSoup(html, \"html.parser\")# find a tag call \"a\", &lt;a href= ....&gt;tag= soup.find(\"a\")print(tag[\"href\"])#orprint(tag.attrs[\"href\"]) t.contents and t.children:A tag’s children are available in a list called .contents .contents:it stores all children into a list .children:it is a list_generator type object, we can not get child directly. We should use iteration method to get child from childrens123456789head_tag# &lt;head&gt;&lt;title&gt;The Dormouse's story&lt;/title&gt;&lt;/head&gt;head_tag.contents#[&lt;title&gt;The Dormouse's story&lt;/title&gt;]title_tag = head_tag.contents[0]title_tag# &lt;title&gt;The Dormouse's story&lt;/title&gt;title_tag.contents# [u'The Dormouse's story'] .string:If current tag doesn’t have children tags, but just have a string, then we can call .stringIf current tag doesn’t have text, but have children tags, we can not call .string (it returns nothing)Example: 123456head_tag# &lt;head&gt;&lt;title&gt;The Dormouse's story&lt;/title&gt;&lt;/head&gt;head_tag.string# Nothinghead_tag.title.string#The Dormouse's story find(…):It returns the first tag or string that satisfies the requirements in inputExample: 12tag = BeautifulSoup(\"&lt;html&gt; &lt;a&gt;text&lt;/a&gt; &lt;a&gt;text2&lt;/a&gt;&lt;html&gt;\")tag.find('a', string=\"text\") It returns the first tag with type of &lt;a&gt; which contains string “text”.soup.find() find_all()Similar to find(), but return a list of all tags that satisfy requirements find().find_next()Find the first tag and then find the next tag that satisfies the requirements inside current tag 12soup = BeautifulSoup(\"&lt;html&gt;&lt;h1&gt;Head 1 &lt;a href=www.baidu.com&gt;&lt;/h1&gt; &lt;h2&gt;Head 2&lt;/h2&gt;&lt;html&gt;\")soup.find('h1').find_next(attrs={'href':\"www.baidu.com\"}) it returns: &lt;a href=”www.baidu.com&quot;\\&gt;\\&lt;/a&gt; Example: Capture roster of football team in ESPN website 1234567import requestsfrom bs4 import BeautifulSoupteam_url = 'https://www.espn.com/college-football/team/roster/_/id/228/clemson-tigers'response = requests.get(team_url)soup = BeautifulSoup(response.content, \"html.parser\")tags= soup.find_all('h1') It finds all h1 tags in the HTML page and return them in a list Regular Expressionregular expression is a way to find any string pattern that match the expression we design. It helps us find the string pattern easier. API Import Regular expression package 1import re *Causes the resulting RE to match 0 or more repetitions.Example: ab*: match when 0 or more b follows a +Causes the resulting RE to match 1 or more repetitions.Example: ab+: match when 1 or more b follows a ?Causes the resulting RE to match 0 or 1 repetitions (…)Matches whatever regular expression is inside the parentheses, and indicates the start and end of a group […]Used to indicate a set of characters[a-z]: characters from a to z[a-zA-Z]: characters from a to z and from A to Z[a-zA-Z0-9]: characters from a to z and from A to Z and from 0 to 9 A|BMatch either A and B pattern (?=…):Matches if … matches next, but doesn’t consume any of the string (?!…):Matches if … doesn’t match next. (?&lt;=…), (?&gt;…)Matches if the current position in the string is preceded (the first one) or after (second one) by a match for … that ends at the current position re.search(“(text)”, input):Search a “text” pattern from input. The first input is regular expressionIt return a re object *re.compile(“([a-z]text)”):compile the regular expression object. This regular expression object can use match(), search() without inputing regular expression parameters re.group(0):It extracts matched string from re object and return the first pattern. Example: 123456import res = \"Hello World. Hello Everyone\"# match a set of character a-z or A-Z that repeat 0 or more before pattern \"one\".obj = re.search(\"([a-zA-Z]*one)\",s)if obj: print(obj.group(0))This example return pattern “Everyone” Example to Find Specific pattern from HTML fileSearch for all strings that contain pattern “SC” and end with “SC” 123456789import requestsfrom bs4 import BeautifulSoupimport reteam_url = 'https://www.espn.com/college-football/team/roster/_/id/228/clemson-tigers'response = requests.get(team_url)soup = BeautifulSoup(response.content, \"html.parser\")tags= soup.find_all(string= re.compile(\"[a-zA-Z ]*SC\"))tags Further workWe can mine more information from any websites. However, we also need to know the architecture and tag names or even some functions in webpage in the website we want to mine. To get to know how to know what functions or tags we have in the BeautifulSoup object, we can use a package called inspect in python to explore the structure of beautiful soup object Reference[1] BeautifulSoup[2] Regular Expression[3] Inspect package in Python","link":"/2020/09/21/Web-Scrapping/"},{"title":"Statistic-P-value","text":"IntroductionThis article introduces how to compute the P-value using different distributions / statistic methods, including Z-distribution, t-distribution, F-test ,Chi-Square $X^2$ Distribution.In all distributions, we first determine the random variable value in possibility density function (CDF) and then use the corresponding distribution look-up table to check the P-value (Possibility) of that distribution. Finally compare P-value with the significant level, $\\alpha$ to check should we reject Null hypothesis H0 or not. NotationsBefore talking about different testing for P-value, let’s denote some notations: x: sampled data , a small group of observations P: population size. Population is the total set of observations that can be made n: sample size $\\mu_0$: population mean $\\sigma_0$: population variance $\\mu’$: sample mean $\\sigma’$: sample variance Z-test / Z-statistic (standard normal distribution) AssumptionZ-test assumes distributions of both population and sample are Normal distributionNull Hypothesis in z-test is that two groups have no difference Computation of Z-Score of continuous random variable $$ z-score = \\frac{\\mu’ - \\mu_0 }{\\sigma_0/ \\sqrt{n}}$$ if we don’t know population standard variance, but know population mean only, we can use sample variance deviation to estimate the population variance deviation. Then it becomes $$ z-score = \\frac{\\mu’ - \\mu_0 }{\\sigma’/ \\sqrt{n}}$$ if compare two groups: $$ z-score = \\frac{\\mu_1 - \\mu_2 }{\\sqrt{(\\sigma_1^2/n_1)+(\\sigma_2^2/n_2)}}$$ Computation of Z-Score of binary random variable Denote $p_0$ as the Population possibility of obtaining current case X, $p’$ as the Sample possibility of current case X $$ z score = \\frac{p’ - p_0 }{ (p_0(1-p_0))/ \\sqrt{n}}$$ When to use when we know the population is normal distribution when the sample size is small, usually smaller than 30. When we know some distribution settings, like $\\mu_0$ = 0, $\\sigma_0$=1 in standard normal distribution ExampleTo test if two groups have no difference, we can let sample mean, variance of group A as $\\mu_A$ $\\sigma_A$, sample mean of group B as $\\mu_B$, $\\sigma_B$. Then let $\\mu’ = \\mu_A - \\mu_B$ if we assume population mean of group difference is $\\mu_0$ = 0, then we can model the group difference like this $$z-score = \\frac{(\\mu_A - \\mu_B) - 0 }{\\sigma_0/ \\sqrt{n}}$$ Or $$z-score = \\frac{(\\mu_A - \\mu_B) - 0 }{\\sqrt{(\\sigma_1^2/n_1)}}$$ t-test / t-statistic (similar to normal distribution) Assumptiont-test also assumes Normal distribution as the population distribution and sample distribution. Different from z-test, t-test doesn’t assume we know the parameters (mean, variance) of normal distribution.. Null Hypothesis in t-test is that two groups have no difference Note that as degree of freedom in t-distribution increases, it is more similar to normal distribution. Computation of t-Score for continuous random variable $$ t-score = \\frac{\\mu_1 - \\mu_2}{\\sqrt{(\\sigma_1^2/n_1)+(\\sigma_2^2/n_2)}}$$ since both t-test and z-test assume normal distribution, they have the same formula Find P-value of t-score After we determine the t-score in CDF, we can find the corresponding P-value in t-distribution using this table When to use t-test is used when population parameters (population mean and population variance) are not known when sample size is very large, usually &gt;30. (So usually in model evaluation, we use t-test with large dataset) when we assume data is normal distribution Note that since in t-test we don’t know the population distribution parameters, so the variances in t-test are sample variance. But in Z-test, we know the population distribution parameters, we usually use population variance and population mean instead In my word, z-test assumes we know normal distribution parameters, so we can use those parameters for testing and use less samples. But t-test doesn’t assume we know anything about distribution, so it uses samples parameters (sample mean and variance) to estimate the normal distribution for testing. Chi-Square-test / $X^2$-statistic AssumptionIn Chi-Square test, it is to test the independence/correlation between categorical variablesNote that Chi-Square test is a one(right)-tail test and the we can not use it for two-tail test. It meausres the correlation of two categorical variable. Null Hypothesis in Chi-Square test is that two groups are independent from each other Computation of Pearson’s Chi-Square valueConsider we have two binary categorical variable with total sample size of n. The table is shown as follow, we want to test their correlation. Then the Pearson’s Chi-Square Correlation is computed by where i, j mean the $i^{th}$ row and $j^{th}$ column. n is the total sample amount$$E_{i,j} = \\frac{X_iX_j}{n}$$ Find P-value from Chi-Square CDF function based on the Chi Square-value we find We can find the Chi-square distribution in this linkand the distribution table from here When to use when we are testing categorical variable and we need one-tail test only when we are testing independence between two variables Example: Measuring the independence between gender and salary, independence between smoking and cancer. F-test / F-statistic AssumptionF-score is to measure the ratio between explained variance over the unexplained variance Computation of F-Score for P-ValueTo understand the F-score, let consider a real linear regression model: $$ y = \\beta_0 + \\beta_1X_1 + \\beta_2X_2 + \\epsilon$$ and an estimated linear regression$$ y’ = \\beta_0 + \\beta_1X_1 + \\beta_2X_2$$where $\\epsilon$ is the unexplainable, irreducible error Then the Residual Sum Square Error (RSS) between two linear models among n samples is $$ RSS = \\sum_i^n(y_i - y’_i)^2$$ RSS Error is to measure the variability between two model. In F-statistic, we want to check if there is relationship between feature $X_i$ and target. If there is no relationship, then weight $\\beta_i$ = 0.(i&gt;0). Then Null hypothesis in F-statistic is that $\\beta_1 = \\beta_2=..=\\beta_p=0$ consider there is no relationship between features and targetThen alternative hypothesis is that $\\beta_i \\neq 0$ for some i&gt;0. In this case, for null hypothesis, we have a Restricted Linear Model$$ \\bar{y} = \\beta_0$$ and a Full Linear model $$ y_i’ = \\beta_0 + \\beta_1X_{1i}$$ where $X_{1i}$, $\\bar{y_i}$ means the the $1^{st}$ feature in the $i^{th}$ sample and the prediction of the $i^{th}$ sample. Denote the degree of freedom of restrict model as $df_r$, and the degree of freedom of Full model as $df_F$. There are n sample data points and p+1 weight cofficients $\\beta_i$ ($\\beta_0$ to $\\beta_p$ )to pick. Then degree of freedom = n -p.Hence in this case, we care about choosing $\\beta_1$ or not. We have $df_r$ = n-1 and $df_F$ = n-2 Total Sum Square Error (TSS) of Restrict Linear model:$$TSS = \\sum_i^n(y_i - \\bar{y})^2$$ Residual Sum Square Error (RSS) of Full Linear model:$$RSS = \\sum_i^n(y_i - y’_i)^2$$ The formula to compute F-score is $$F-score = \\frac{ (TSS-RSS)/(df_r - df_F)}{RSS/df_F} = \\frac{ (TSS-RSS)/1}{RSS/(n-2)}$$ More general, If there are p+1 weight coefficient in Full model, then we have$$F-score = \\frac{ (TSS-RSS)/p}{RSS/(n-p-1)}$$ where $TSS- RSS$ is the variability explained by model and $RSS$ measure the variability left unexplained after the regression (which is also the irreducible error mentioned before) If F-value &gt; 0 and &lt;= 1, we don’t reject H0. If F-value &gt;1, then we reject H0 and choose H1.If F-value &lt;0, then F-test fails, it doesn’t mean anything.Note that F-test is also a One-tail hypothesis test. When to use when we are measuring if some features matters in prediction / there is correlation between features and target F-test can be adjusted by the number of predictors, but individual t-test doesn’t Note that when the number of predictors p &gt; the number of samples n, F-test will fail since value $RSS/(n-p-1)$ becomes negative and this F-value is out of the range of random variable value in F-distribution ExampleThere are some examples from this link: https://online.stat.psu.edu/stat501/lesson/6/6.2 $R^2$ test Assumption$R^2$ measures the proportion of variability of prediction y that can be explained by feature X Computation of $R^2$-Score for P-Value$$ R^2 - value = \\frac{TSS- RSS}{RSS}$$ where RSS and TSS are computed as same as those in F-test $$TSS = \\sum_i^n(y_i - \\bar{y})^2$$ where $\\bar{y} =\\beta_0$ = mean of $y_i$since when we estimate $\\bar{y} =\\beta_0$ using samples to minimize the sum square loss, let gradient of $\\sum_i^n(y_i-\\bar{y})^2$ = 0, we have $-2\\sum_i^n(y_i-\\bar{y}) =0$ and $\\bar{y} = (\\sum_i^ny_i)/n$ Residual Sum Square Error (RSS) of Full Linear model:$$RSS = \\sum_i^n(y_i - y’_i)^2$$ Random Variable $R^2$ has range [0,1]. If $R^2$ fall outside this range, then the test fails. If $R^2$ value is close to 1, then reject H0 and consider H1 and there is relationship between features and y. if $R^2$ value is close to 0, then model is wrong and don’t fit well, then can not reject H0. When to use when we want to measures the proportion of variability of prediction y that can be explained by feature X Reference[1] https://towardsdatascience.com/statistical-tests-when-to-use-which-704557554740 [2] https://www.jmp.com/en_us/statistics-knowledge-portal/t-test/t-distribution.html [3] https://www.jmp.com/en_us/statistics-knowledge-portal/t-test/t-distribution.html [4] https://www2.palomar.edu/users/rmorrissette/Lectures/Stats/ttests/ttests.htm [5] https://online.stat.psu.edu/stat501/lesson/6/6.2 [6] https://stats.stackexchange.com/questions/130069/what-is-the-distribution-of-r2-in-linear-regression-under-the-null-hypothesis [7] https://www.statisticshowto.com/wp-content/uploads/2014/01/p-value1.jpg","link":"/2020/11/23/Statistic-P-value/"},{"title":"Recommendation System-6-DCN","text":"DCN: Deep&amp;Cross Network1. 动机Wide&amp;Deep模型的提出不仅综合了“记忆能力”和“泛化能力”， 而且开启了不同网络结构融合的新思路。 所以后面就有各式各样的模型改进Wide部分或者Deep部分， 而Deep&amp;Cross模型(DCN)就是其中比较典型的一个，这是2017年斯坦福大学和谷歌的研究人员在ADKDD会议上提出的， 该模型针对W&amp;D的wide部分进行了改进， 因为Wide部分有一个不足就是需要人工进行特征的组合筛选， 过程繁琐且需要经验， 而2阶的FM模型在线性的时间复杂度中自动进行特征交互，但是这些特征交互的表现能力并不够，并且随着阶数的上升，模型复杂度会大幅度提高。于是乎，作者用一个Cross Network替换掉了Wide部分，来自动进行特征之间的交叉，并且网络的时间和空间复杂度都是线性的。 通过与Deep部分相结合，构成了深度交叉网络（Deep &amp; Cross Network），简称DCN。 2. 模型结构及原理这个模型的结构是这个样子的： 这个模型的结构也是比较简洁的， 从上到下依次为：Embedding和Stacking层， Cross网络层与Deep网络层并列， 以及最后的输出层。下面也是一一为大家剖析。 2.1 Embedding和Stacking 层Embedding层我们已经非常的熟悉了吧， 这里的作用依然是把稀疏离散的类别型特征变成低维密集型。 $$x_{embed, i} = W_{embed, i} x_{i}$$ 其中对于某一类稀疏分类特征（如id），$X_{embed, i}$是第个$i$分类值（id序号）的embedding向量。$W_{embed,i}$是embedding矩阵， $n_e\\times n_v$维度， $n_e$是embedding维度， $n_v$是该类特征的唯一取值个数。$x_i$属于该特征的二元稀疏向量(one-hot)编码的。 【实质上就是在训练得到的Embedding参数矩阵中找到属于当前样本对应的Embedding向量】。其实绝大多数基于深度学习的推荐模型都需要Embedding操作，参数学习是通过神经网络进行训练。 最后，该层需要将所有的密集型特征与通过embedding转换后的特征进行联合（Stacking）：$$x_0= [x_{embed, 1}^{T}, \\ldots, x_{embed, k}^{T}, x_{dense }^{T}]$$ 一共$k$个类别特征， dense是数值型特征， 两者在特征维度拼在一块。 上面的这两个操作如果是看了前面的模型的话，应该非常容易理解了。 2.2 Cross Network这个就是本模型最大的亮点了—Cross网络， 这个思路感觉非常Nice。设计该网络的目的是增加特征之间的交互力度。交叉网络由多个交叉层组成， 假设$l$层的输出向量$x_l$， 那么对于第$l+1$层的输出向量$x_{l+1}$表示为： $$x_{l+1}=x_{0} x_{l}^{T} w_{l}+b_{l}+x_{l}=f(x_{l}, w_{l}, b_{l})+x_{l}$$ 可以看到， 交叉层的操作的二阶部分非常类似PNN提到的外积操作， 在此基础上增加了外积操作的权重向量$w_l$， 以及原输入向量$x_l$和偏置向量$b_l$。 交叉层的可视化如下： 可以看到， 每一层增加了一个$n$维的权重向量$w_l$（n表示输入向量维度）， 并且在每一层均保留了输入向量， 因此输入和输出之间的变化不会特别明显。关于这一层， 原论文里面有个具体的证明推导Cross Network为啥有效， 不过比较复杂，这里我拿一个式子简单的解释下上面这个公式的伟大之处： 我们根据上面这个公式， 尝试的写前面几层看看: $l$ =0: $x_{1} =x_{0} x_{0}^{T} w_{0}+ b_{0}+ x_{0}$ $l=1: x_{2} =x_{0} x_{1}^{T} w_{1}+ b_{1}+x_{1}=x_{0} [x_{0} x_{0}^{T} w_{0}+ b_{0}+x_{0}]^{T} w_{1}+ b_{1}+x_{1}$ $l=2: x_{3} =x_{0} x_{2}^{T} w_{2}+ b_{2}+ x_{2}=x_{0} [x_{0} [x_{0} x_{0}^{T} w_{0}+ b_{0}+ x_{0}]^{T} w_{1}+ b_{1}+ x_{1}]^{T} w_{2}+ b_{2}+x_{2}$ 我们暂且写到第3层的计算， 我们会发现什么结论呢？ 给大家总结一下： $x_1$中包含了所有的$x_0$的1,2阶特征的交互， $x_2$包含了所有的$x_1, x_0$的1， 2， 3阶特征的交互，$x_3$中包含了所有的$x_2$, $x_1$与$x_0$的交互，$\\mathrm{x}_0$的1,2,3,4阶特征交互。 因此， 交叉网络层的叉乘阶数是有限的。 第$l$层特征对应的最高的叉乘阶数$l+1$ Cross网络的参数是共享的， 每一层的这个权重特征之间共享， 这个可以使得模型泛化到看不见的特征交互作用， 并且对噪声更具有鲁棒性。 例如两个稀疏的特征$x_i,x_j$， 它们在数据中几乎不发生交互， 那么学习$x_i,x_j$的权重对于预测没有任何的意义。 计算交叉网络的参数数量。 假设交叉层的数量是$L_c$， 特征$x$的维度是$n$， 那么总共的参数是： $$n\\times L_c \\times 2$$这个就是每一层会有$w$和$b$。且$w$维度和$x$的维度是一致的。 交叉网络的时间和空间复杂度是线性的。这是因为， 每一层都只有$w$和$b$， 没有激活函数的存在，相对于深度学习网络， 交叉网络的复杂性可以忽略不计。 Cross网络是FM的泛化形式， 在FM模型中， 特征$x_i$的权重$v_i$， 那么交叉项$x_i,x_j$的权重为$&lt;x_i,x_j&gt;$。在DCN中， $x_i$的权重为${W_K^{(i)}}_{k=1}^l$, 交叉项$x_i,x_j$的权重是参数${W_K^{(i)}}_{k=1}^l$和${W_K^{(j)}}_{k=1}^l$的乘积，这个看上面那个例子展开感受下。因此两个模型都各自学习了独立于其他特征的一些参数，并且交叉项的权重是相应参数的某种组合。FM只局限于2阶的特征交叉(一般)，而DCN可以构建更高阶的特征交互， 阶数由网络深度决定，并且交叉网络的参数只依据输入的维度线性增长。 还有一点我们也要了解，对于每一层的计算中， 都会跟着$\\mathrm{x}_0$, 这个是咱们的原始输入， 之所以会乘以一个这个，是为了保证后面不管怎么交叉，都不能偏离我们的原始输入太远，别最后交叉交叉都跑偏了。 $x_{l+1}=f(x_{l}, w_{l}, b_{l})+x_{l}$, 这个东西其实有点跳远连接的意思，也就是和ResNet也有点相似，无形之中还能有效的缓解梯度消失现象。 好了， 关于本模型的交叉网络的细节就介绍到这里了。这应该也是本模型的精华之处了，后面就简单了。 2.3 Deep Network这个就和上面的D&amp;W的全连接层原理一样。这里不再过多的赘述。 $$h_{l+1}=f(W_{l} h_{l}+ b_{l})$$ 具体的可以参考W&amp;D模型。 2.4组合输出层这个层负责将两个网络的输出进行拼接， 并且通过简单的Logistics回归完成最后的预测：$$p=\\sigma ([x_{L_{1}}^{T}, h_{L_{2}}^{T}] w_{logits })$$其中$x_{L_{1}}^{T}$$h_{L_{2}}^{T}$表示交叉网络和深度网络的输出。最后二分类的损失函数依然是交叉熵损失： $$loss =-\\frac{1}{N} \\sum_{i=1}^{N} y_{i} \\log (p_{i})+(1-y_{i}) \\log (1-p_{i})+\\lambda \\sum_{l} |w_{i} |^{2}$$ Cross&amp;Deep模型的原理就是这些了，其核心部分就是Cross Network， 这个可以进行特征的自动交叉， 避免了更多基于业务理解的人工特征组合。 该模型相比于W&amp;D，Cross部分表达能力更强， 使得模型具备了更强的非线性学习能力。 3. 代码实现 (参考别人的代码)下面我们看下DCN的代码复现，这里主要是给大家说一下这个模型的设计逻辑，参考了deepctr的函数API的编程风格， 具体的代码以及示例大家可以去参考后面的GitHub，里面已经给出了详细的注释， 这里主要分析模型的逻辑这块。关于函数API的编程式风格，我们还给出了一份文档， 大家可以先看这个，再看后面的代码部分，会更加舒服些。下面开始： 从上面的结构图我们也可以看出， DCN的模型搭建，其实主要分为几大模块， 首先就是建立输入层，用到的函数式build_input_layers，有了输入层之后， 我们接下来是embedding层的搭建，用到的函数是build_embedding_layers， 这个层的作用是接收离散特征，变成低维稠密。 接下来就是把连续特征和embedding之后的离散特征进行拼接，分别进入wide端和deep端。 wide端就是交叉网络，而deep端是DNN网络， 这里分别是CrossNet()和get_dnn_output(), 接下来就是把这两块的输出拼接得到最后的输出了。所以整体代码如下： 12345678910111213141516171819202122232425262728293031323334def DCN(linear_feature_columns, dnn_feature_columns): # 构建输入层，即所有特征对应的Input()层，这里使用字典的形式返回，方便后续构建模型 dense_input_dict, sparse_input_dict = build_input_layers(linear_feature_columns + dnn_feature_columns) # 构建模型的输入层，模型的输入层不能是字典的形式，应该将字典的形式转换成列表的形式 # 注意：这里实际的输入与Input()层的对应，是通过模型输入时候的字典数据的key与对应name的Input层 input_layers = list(dense_input_dict.values()) + list(sparse_input_dict.values()) # 构建维度为k的embedding层，这里使用字典的形式返回，方便后面搭建模型 embedding_layer_dict = build_embedding_layers(dnn_feature_columns, sparse_input_dict, is_linear=False) concat_dense_inputs = Concatenate(axis=1)(list(dense_input_dict.values())) # 将特征中的sparse特征筛选出来 sparse_feature_columns = list(filter(lambda x: isinstance(x, SparseFeat), linear_feature_columns)) if linear_feature_columns else [] sparse_kd_embed = concat_embedding_list(sparse_feature_columns, sparse_input_dict, embedding_layer_dict, flatten=True) concat_sparse_kd_embed = Concatenate(axis=1)(sparse_kd_embed) dnn_input = Concatenate(axis=1)([concat_dense_inputs, concat_sparse_kd_embed]) dnn_output = get_dnn_output(dnn_input) cross_output = CrossNet()(dnn_input) # stack layer stack_output = Concatenate(axis=1)([dnn_output, cross_output]) # 这里的激活函数使用sigmoid output_layer = Dense(1, activation='sigmoid')(stack_output) model = Model(input_layers, output_layer) return model 这个模型的实现过程和DeepFM比较类似，这里不画草图了，如果想看的可以去参考DeepFM草图及代码之间的对应关系。 下面是一个通过keras画的模型结构图，为了更好的显示，类别特征都只是选择了一小部分，画图的代码也在github中。 4. DCN特点 计算Cross Network的复杂度： 时间复杂度: O(n*K), n = Cross network 里面x的维度， K= Cross Network的层数 空间复杂度: O(n* k *2), n = Cross network 里面x的维度， K= Cross Network的层数, 乘2是因为每层都有1个weight vector和1个bias vector 在实现矩阵计算$x_0*x_l^Tw$的过程中，有人说要先算前两个，有人说要先算后两个，请问那种方式更好？为什么？ 先计算后面xl和w的再计算前面x0会更好。 因为如果计算前面x0和xl的outer product会生成一个nxn的矩阵，不仅需要更多内存而且和后面的weight vector相乘时会变成 需要O(n^2)的 时间。 而计算后面w和xl相乘时会得到一个scalar value之后计算方便 5. Coding 实现（PyTorch）看这里： https://wenkangwei.gitbook.io/leetcode-notes/deep-learning/deep-and-cross-network-dcn 6. Reference 《深度学习推荐系统》 — 王喆 Deep&amp;Cross模型原论文 AI上推荐 之 Wide&amp;Deep与Deep&amp;Cross模型(记忆与泛化并存的华丽转身） Wide&amp;Deep模型的进阶—Cross&amp;Deep模型","link":"/2021/05/21/Recommendation-System-6-DCN/"},{"title":"XGBoost + LGBM","text":"XGBoost + LGBMGDBT介绍GBDT——Gradient Boosting Decision Tree，又称Multiple Additive Regression Tree，后面这种叫法不常看到，但Regression Tree却揭示了GBDT的本质——GBDT中的树都是回归树。GBDT的主要是思想是每次把上一棵树的输出和label的残差作为下一棵树的label进行拟合，前面的树做不好的地方后面的补上，不断把拟合后的预测的残差值叠加上去，进行梯度的提升。具体步骤如下 其中负梯度相当于残差，用做target给下个tree进行拟合 GBDT的正则化防止过拟合的方法，主要有以下措施： 1，Early Stopping，本质是在某项指标达标后就停止训练(比如树的高度之类的)，也就是设定了训练的轮数； 2，Shrinkage，其实就是学习率，具体做法是将每课树的效果进行缩减，比如说将每棵树的结果乘以0.1，也就是降低单棵树的决策权重，相信集体决策； 3，Subsampling，无放回抽样，具体含义是每轮训练随机使用部分训练样本，其实这里是借鉴了随机森林的思想； 4，Dropout，这个方法是论文里的方法，没有在SKLearn中看到实现，做法是每棵树拟合的不是之前全部树ensemble后的残差，而是随机挑选一些树的残差 XGBoost 介绍XGBoost是陈天奇等人开发的一个开源机器学习项目，高效地实现了GBDT算 法并进行了算法和工程上的许多改进，被广泛应用在Kaggle竞赛及其他许多机器 学习竞赛中并取得了不错的成绩。 XGBoost 原理XGBoost的建树的步骤分为以下几步： 定义目标函数Objective function 通过泰勒展开式对Objective function进行简化 对简化后的Objective function进行求解得到树的每个leaf node输出的值的计算公式，以及对树结构的评估函数 根据所得到的评估函数用Exact Greedy algorithm 或者 近邻算法估计得到每个feature的分裂点以及对应的gain，并按照分裂点和gain对feature进行选取和分裂建树 XGBoost 推导1. Objective function 优化目标函数： 在XGBoost的目标函数里面，它是基于GBDT的目标函数再加上一个 $\\sum_i^t\\Omega(f_i)$ 的对t棵树的输出regularization term正则化来抑制过拟合的问题. 而前面的loss 函数还是和之前一样 $y_i$ 是target $\\bar{y_i}^{(t)}$ 是对前面t棵树的输出的线性相加的输出值。 在对第t棵树的输出进行计算时， 我们可以把第t棵树的regularization term和 预测值拆开来得到以上表达式。其中 而regularization term $\\Omega(f_t) = \\gamma T + \\frac{1}{2}||w||^2$ 这里的gamm是一个penalty的系数而T是第t棵树 $f_t$ 的节点个数，w是这颗树的每个leaf node的输出预测。 2. 通过泰勒展示简化 Objective function 优化目标函数： 在XGBoost 里面，为了加速算法的计算，这里用了2阶的泰勒展开式对Objective function进行估计，泰勒展开式公式如下 之后我们可以把原来的目标函数简化成下面形式： 由于计算第t棵树的时候，前面t-1棵树的loss还有regularization的项是常数，我们可以不考虑。 把 $f_t(x_i)$ 树的输出替换成w的形式，我们得到关于w的一元二次函数 3. 通过对关于w的一元二次方程求解可得 由于我们的目标是要找出第t棵树使得我们当前的目标函数最小化，所以我们可以把上面的一元二次方程函数通过找极值的方法找到w的解以及对应的目标函数值，即 $x = - \\frac{b}{2a}, y = \\frac{4ac-b^2}{4a}$ 因此我们可以得到以下的解 $$w_ j^* = - \\frac{\\sum_ {i\\in I_ j} g_ i}{\\sum_ {i\\in I_ j} h_ i + \\lambda}$$ $$OBj^{t}（q(x)） = -\\frac{1}{2} \\sum_ {j=1}^T\\frac{(\\sum_ {i\\in I_ j} g_ i )^2}{\\sum_ {i\\in I_ j} h_ i + \\lambda} + \\gamma T$$ 这里的 $I_ j$ 是指属于第j个叶节点的样本的集合，wj 是第j个节点的输出值，而 $g_ i , h_ i$ 分别是关于之前t-1棵树的模型对第i个样本的输出的Loss的1阶和2阶的导数。 这里的目标函数 $OBj^t(q(x))$ 的值可以看成是对当前的第t棵树的loss，而q(x)代表了第t棵树的结构。 4. 根据前面的OBj score 的函数 对当前节点下的所有feature查找最大的gain，并根据概念找到每个feature的split 分裂点来建树 虽然计算第t棵树的每个leaf node的输出值的公式是有了， 但是我们其实还没有确定这棵树的结构。为了找到树的结构，我们需要给每个特征找到分裂点，以及在每一层对特征选取。而这里就需要用到第3步找到的leaf node 输出的值和目标函数值函数了。 这里举个例子， 在下图，假设我们先只考虑黄色看看的树。如果我们一开始还没建树，需要先选取一个feature (1个column)用来作为一个root node。那么为了使目标函数最小化我们最直接的方法是直接这个feature里面的出现的数值排序，然后把每两个值之间的间隙当成分裂点，然后看哪个分离点对应的 $OBj(q(x))$ 目标函数是最小就选那个作为当前的分裂点。所以这样在这个例子我们的分裂点是 x&lt;=6 , x&gt;6。这时我们的loss就是 $L^{old} = OBj(q(x))$ of old tree structure 由于我们想要通过添加剩下的feature作为tree的分支使得tree的loss 越小越好，那么对于old的框框下增加branch之后的tree的结构所对应的loss，标记为 $L^{new} = = OBj(q(x))$ of new tree structure。 我们就想要最大化 $$max(L^{old} - L^{new}) = OBj^{old} - OBj^{new}$$ 而我们可以标记 $$L^{split} = max(L^{old} - L^{new})$$ 作为分裂增益， split gain。split gain公式可以变成 之后我们可以找到当前可以选的feature所对应的最大split gain以及对应的分裂点，并选取split gain最大的那个feature作为当前的tree node分支。 不断重复这几步就可以把树建起来了。 而这个每次都要线性扫描分裂点的建树的方法也叫做 Exact Greedy Algorithm。在当前的节点里面，把归到这个节点里面的样本做以下操作： 由于这个方法每次都要把feature的数值排序并线性扫描，在计算连续特征的时候还要先排序并把每两个值的中点作为分裂点候选点来离散化，这样计算会很慢并且会在大量数据时内存容易不足。在XGBoost里面用了一种 近似算法(Approximate algorithm)按照预定的百分位对数值进行分桶得到候选的分裂点集合，然后再用Exact Greedy algorithm 进行分裂点选取。以下是原paper的描述： 例子： 5. 对缺失值或稀疏值处理 在XGBoost 里面，它能够自动处理缺失值和稀疏值时是把它们放到稀疏矩阵里面处理 对于训练时遇到的缺失值默认把它们试着分别放到右子树，左子树然后找到最大的gain的分裂点进行分裂。并且计算时间只考虑非缺失值 对于预测时遇到的缺失值，默认被分到右子树。 XGBoost把稀疏数据和缺失数据用相同方式处理。不过对类别特征还是要encoding成稀疏值后才能处理。 XGBoost 特点GBDT： GBDT 它的非线性变换比较多，表达能力强，而且不需要做复杂的特征工程和特征变换。 GBDT 的缺点也很明显，Boosting 是一个串行过程，不好并行化，而且计算复杂度高，同时不太适合高维稀疏特征； 传统 GBDT 在优化时只用到一阶导数信息。 Xgboost 它有以下几个优良的特性： 显示的把树模型复杂度作为正则项加到优化目标中。 公式推导中用到了二阶导数，用了二阶泰勒展开。 实现了分裂点寻找近似算法。 利用了特征的稀疏性, 对计算加速，计算的时间只和非缺失值的个数线性相关 xgboost在训练之前，预先对数据通过并行计算进行了排序，然后保存为block结构，后面的迭代中重复地使用这个结构，大大减小计算量。 传统GBDT用了所有数据，但XGBoost像RandomForest一样用了Column Subsampling 来抑制overfitting并且加速了计算。 传统的GBDT没有设计对缺失值进行处理需要人工对缺失值处理，XGBoost能够自动学习出缺 失值的处理策略。 树模型对outlier不敏感，不像Parameterized model,如LR之类的 XGBoost 相对于传统的GBDT可以处理缺失值，并且处理稀疏数据时比GBDT要好 思路总结： 加了Regularization term -&gt; 用二阶泰勒展开式对树模型求解-&gt;得到每个叶子的输出计算公式以及对树结构的gain的计算方式-&gt;通过gain的计算方式+ 贪心算法找到每个feature的分裂点 -&gt;按照分裂点的gain提出对稀疏值和缺失值的处理方法 -&gt; 其他特点： GBDT 用的CART tree分类时用的是Gini gain 而回归时用方差 variance，而XGBoost里面的启发函数用的是一个基于前面树的梯度的一阶二阶导数的Loss LGBMLGBM 相对于是对XGBoost的工程上的优化 LGBM 通过利用直方图分桶和采样的方式对XGBoost进行加速，计算速度远快于XGboost （用GOSS对样本采样， 用EFB对稀疏特征捆绑成新特征减少特征量） LGBM相对于XGboost需要更少的内存 LGBM和XGBoost一样能够处理缺失值 LGBM是不需要对类别特征进行onehot 编码处理，而XGboost需要onehot编码类别数据形成稀疏值后才能进行计算. 对于缺失值处理， XGBoost是把它当成稀疏值进行处理并且排序时把稀疏值放到左边或右边并排序来找最优分裂点，而LGBM是不需要输入数据编码，自动把特征编码并且通过EFB算法把多个稀疏值合并成更少特征。 LGBM对XGBoost有3个改进之处 在找分裂点时,LGBM用直方图方法进行采样 LGBM可以通过预先把特征值进行排序，找到分位数，并按照分位数把特征离散化得到多个bins，而不像XGBoost把每个分裂点都遍历一遍，从而提高计算速度 在样本采样中，用GOSS根据梯度大小对样本采样进行下一个模型更新Gradient-based One side sampling 单边采样 (GOSS) 算法的创新之处在于它只对梯度绝对值较小的样本按照一定比例进行随机采样，而保留了梯度绝对值较大的样本。这个梯度的大小可以由阀值进行决定 在特征采样中,LGBM用EFB互斥特征绑定方法进行特征采样EFB算法全称是Exclusive Feature Bundling。 EFB算法可以有效减少用于构建直方图的特征数量，从而降低计算复杂度，尤其是特征中包含大量稀疏特征的时候。在大量稀疏数据时相似ID数据，尽管用了之前的GOSS样本采样以及直方图找分裂点，但是太多稀疏特征依然会降低计算速度。 LGBM考虑多个稀疏的特征在很多情况下都是为0的，很少时候同时不为0，所以它假设这些特征是互斥的，因此可以把这些稀疏特征捆绑在一起作为一个新的特征。 对于指定为类别特征的特征，LightGBM可以直接将每个类别取值和一个bin关联，从而自动地处理它们，而无需预处理成onehot编码多此一举。 时间复杂度XGBoost时间复杂度 对每个连续特征预排序O(nlogn), n = sample 个数 建树时 O(nxm), n= 样本的数目， m=特征的数目，因为要对每个特征的排序后的样本进行扫描找分裂点。 LGBM时间复杂度: 在分桶的时候，LGBM里面用了直方图统计的方法，没有用到预排序，不用考虑预排序时间复杂度 在GOSS单边采样的时候，它先把样本根据梯度大小排序，然后按照梯度来选取样本。（注意LGBM里面的排序是为了采样，XGBoost里面的排序是为了找分裂点） 在搭建直方图时, 由于要对每个feature进行分桶，并且把样本分到每个桶里，所以O(number of data x number of feature)。并且由于子节点的直方图的和是等于父节点的直方图，所以可以做差加速对直方图做快速计算，不用重新统计 找直方图分裂点时， O(number of bins x number of feature),因为要遍历每个桶进行最大增益分裂点的查找。 Reference[1] 知乎 https://zhuanlan.zhihu.com/p/143009353 [2] 知乎 https://zhuanlan.zhihu.com/p/58269560 [3] 知乎 https://zhuanlan.zhihu.com/p/53980138 [4] XGBoost Paper: https://www.kdd.org/kdd2016/papers/files/rfp0697-chenAemb.pdf [5] LGBM Paper: https://papers.nips.cc/paper/2017/file/6449f44a102fde848669bdd9eb6b76fa-Paper.pdf [6] CSDN: https://blog.csdn.net/pearl8899/article/details/106159264 [7] Image: https://rohitgr7.github.io/content/images/2019/03/Screenshot-from-2019-03-27-23-09-47-1.png","link":"/2021/08/12/ensemble-learning-XGBoost/"},{"title":"ensemble-learning-project-HappinessPrediction","text":"背景介绍: 幸福感预测幸福感是一个古老而深刻的话题，是人类世代追求的方向。与幸福感相关的因素成千上万、因人而异，大如国计民生，小如路边烤红薯，都会对幸福感产生影响。这些错综复杂的因素中，我们能找到其中的共性，一窥幸福感的要义吗？ 另外，在社会科学领域，幸福感的研究占有重要的位置。这个涉及了哲学、心理学、社会学、经济学等多方学科的话题复杂而有趣；同时与大家生活息息相关，每个人对幸福感都有自己的衡量标准。如果能发现影响幸福感的共性，生活中是不是将多一些乐趣；如果能找到影响幸福感的政策因素，便能优化资源配置来提升国民的幸福感。目前社会科学研究注重变量的可解释性和未来政策的落地，主要采用了线性回归和逻辑回归的方法，在收入、健康、职业、社交关系、休闲方式等经济人口因素；以及政府公共服务、宏观经济环境、税负等宏观因素上有了一系列的推测和发现。 该案例为幸福感预测这一经典课题，希望在现有社会科学研究外有其他维度的算法尝试，结合多学科各自优势，挖掘潜在的影响因素，发现更多可解释、可理解的相关关系。具体来说，该案例就是一个数据挖掘类型的比赛——幸福感预测的baseline。具体来说，我们需要使用包括个体变量（性别、年龄、地域、职业、健康、婚姻与政治面貌等等）、家庭变量（父母、配偶、子女、家庭资本等等）、社会态度（公平、信用、公共服务等等）等139维度的信息来预测其对幸福感的影响。我们的数据来源于国家官方的《中国综合社会调查（CGSS）》文件中的调查结果中的数据，数据来源可靠可依赖:) 数据信息赛题要求使用以上 139 维的特征，使用 8000 余组数据进行对于个人幸福感的预测（预测值为1，2，3，4，5，其中1代表幸福感最低，5代表幸福感最高）。因为考虑到变量个数较多，部分变量间关系复杂，数据分为完整版和精简版两类。可从精简版入手熟悉赛题后，使用完整版挖掘更多信息。在这里我直接使用了完整版的数据。赛题也给出了index文件中包含每个变量对应的问卷题目，以及变量取值的含义；survey文件中为原版问卷，作为补充以方便理解问题背景。 评价指标MSE Loss: $$Score = \\sum_i^n (y_i - y_i^*)^2$$ $y_i$ 是target, $y_i^*$是prediction CodingGithub 源码以及运行结果： https://github.com/wenkangwei/Datawhale-Team-Learning/blob/main/Ensemble-Learning/Project_1_%E5%B9%B8%E7%A6%8F%E6%84%9F%E9%A2%84%E6%B5%8B.ipynb 1. 数据加载和分析123456789101112131415161718192021222324252627282930313233343536373839404142434445# utilitiesimport osimport timeimport pandas as pdimport numpy as npimport seaborn as sns# modelsfrom sklearn.linear_model import LogisticRegressionfrom sklearn.svm import SVC, LinearSVCfrom sklearn.ensemble import RandomForestClassifierfrom sklearn.neighbors import KNeighborsClassifierfrom sklearn.naive_bayes import GaussianNBfrom sklearn.linear_model import Perceptronfrom sklearn.linear_model import SGDClassifierfrom sklearn.tree import DecisionTreeClassifierfrom sklearn.ensemble import RandomForestRegressor as rfrfrom sklearn.ensemble import ExtraTreesRegressor as etrfrom sklearn.linear_model import BayesianRidge as brfrom sklearn.ensemble import GradientBoostingRegressor as gbrfrom sklearn.linear_model import Ridgefrom sklearn.linear_model import Lassofrom sklearn.linear_model import LinearRegression as lrfrom sklearn.linear_model import ElasticNet as enfrom sklearn.kernel_ridge import KernelRidge as krimport lightgbm as lgbimport xgboost as xgb# metricsfrom sklearn import metricsfrom datetime import datetimeimport matplotlib.pyplot as pltfrom sklearn.metrics import roc_auc_score, roc_curve, mean_squared_error,mean_absolute_error, f1_scorefrom sklearn.model_selection import KFold, StratifiedKFold,GroupKFold, RepeatedKFoldfrom sklearn.model_selection import train_test_splitfrom sklearn.model_selection import GridSearchCVfrom sklearn import preprocessingimport loggingimport warningswarnings.filterwarnings('ignore') #消除warning 12! git clone https://github.com/datawhalechina/team-learning-data-mining.git! cp /content/team-learning-data-mining/EnsembleLearning/CH6-集成学习之案例分享/集成学习案例分析1/* . 12345678910train = pd.read_csv(\"train.csv\", parse_dates=['survey_time'],encoding='latin-1')test = pd.read_csv(\"test.csv\", parse_dates=['survey_time'],encoding='latin-1') #latin-1向下兼容ASCIItrain = train[train[\"happiness\"]!=-8].reset_index(drop=True)train_data_copy = train.copy() #删去\"happiness\" 为-8的行target_col = \"happiness\" #目标列target = train_data_copy[target_col]del train_data_copy[target_col] #去除目标列data = pd.concat([train_data_copy,test],axis=0,ignore_index=True)train.happiness.describe() #数据的基本信息 1data.head() 1data.count() 1data.count() 看一下每个省份的幸福指数 12# 看一下每个省份的幸福指数train.happiness.groupby(by=train.province).mean().plot.bar() 看一下收入与幸福感的关系 12ax = train.income.groupby(by=train.happiness).mean().plot.bar(x='income',y='happiness')ax.set_ylabel('mean income') 看一下抑郁程度与幸福感的关系 12ax2 = train.happiness.groupby(by=train.depression).median() .plot.bar()ax2.set_ylabel('mean happiness') 看一下政治与幸福感的关系 1ax1 = sns.countplot(x= train.political,hue=train.happiness) 看一下放松与幸福感的关系 123ax1 = sns.countplot(x= train.relax,hue=train.happiness)# ax1 = train.happiness.groupby(by=train.relax).mean().plot.bar()ax1.set_ylabel('happiness count') 看一下教育程度与幸福感的关系 123# ax1 = sns.countplot(x= train.relax,hue=train.happiness)ax1 = sns.countplot(x= train.edu_status,hue=train.happiness)# train.happiness.groupby(by=train.edu_status).sum().plot.bar() 根据上面的可视化结果(详情和图请看我的github的源码结果)，简单分析了几个个feature，一下几点 发现收入越高越开心 越开心时婚姻关系好像有点不好，反倒大多数人越开心压力越大，有点意思，可能是因为带来开心的来源往往会伴随更大的压力。 越是和政治相关越是不开心，出来混不容易啊 适度放松会适度开心，过度放松(5)会不太开心，可能过度放纵没意思了吧 大多数调查的人的教育程度也是挺高的而且还挺开心。但是教育程度很低就不太开心了 2. 数据清洗和扩增12345678910111213141516171819#make feature +5#csv中有复数值：-1、-2、-3、-8，将他们视为有问题的特征，但是不删去def getres1(row): return len([x for x in row.values if type(x)==int and x&lt;0])def getres2(row): return len([x for x in row.values if type(x)==int and x==-8])def getres3(row): return len([x for x in row.values if type(x)==int and x==-1])def getres4(row): return len([x for x in row.values if type(x)==int and x==-2])def getres5(row): return len([x for x in row.values if type(x)==int and x==-3])#检查数据data['neg1'] = data[data.columns].apply(lambda row:getres1(row),axis=1)data.loc[data['neg1']&gt;20,'neg1'] = 20 #平滑处理,最多出现20次data['neg2'] = data[data.columns].apply(lambda row:getres2(row),axis=1)data['neg3'] = data[data.columns].apply(lambda row:getres3(row),axis=1)data['neg4'] = data[data.columns].apply(lambda row:getres4(row),axis=1)data['neg5'] = data[data.columns].apply(lambda row:getres5(row),axis=1) 1234567891011121314work_cols = [i for i in data.columns if 'work' in i.split('_')[0]] # columns start with work_s_cols = [i for i in data.columns if 's' == i.split('_')[0]] # columns start with s_edu_cols = [i for i in data.columns if 'edu' == i.split('_')[0]] # columns start with s_for col in work_cols: data[col]= data[col].fillna(0)for col in s_cols: data[col]= data[col].fillna(0)for col in ['edu_yr', 'edu_status','minor_child','marital_now','marital_1st','social_neighbor','social_friend']: data[col]= data[col].fillna(0)data['hukou_loc']=data['hukou_loc'].fillna(1) #最少为1，表示户口data['family_income']=data['family_income'].fillna(66365) #删除问题值后的平均值 对特殊数据进行特殊处理通过生日和调查时间计算年龄把年龄分区 12345678910#144+1 =145#继续进行特殊的列进行数据处理#读happiness_index.xlsxdata['survey_time'] = pd.to_datetime(data['survey_time'], format='%Y-%m-%d',errors='coerce')#防止时间格式不同的报错errors='coerce‘data['survey_time'] = data['survey_time'].dt.year #仅仅是year，方便计算年龄data['age'] = data['survey_time']-data['birth']# print(data['age'],data['survey_time'],data['birth'])#年龄分层 145+1=146bins = [0,17,26,34,50,63,100]data['age_bin'] = pd.cut(data['age'], bins, labels=[0,1,2,3,4,5]) 1234567891011121314151617181920212223242526272829303132#对‘宗教’处理data.loc[data['religion']&lt;0,'religion'] = 1 #1为不信仰宗教data.loc[data['religion_freq']&lt;0,'religion_freq'] = 1 #1为从来没有参加过#对‘教育程度’处理data.loc[data['edu']&lt;0,'edu'] = 4 #初中data.loc[data['edu_status']&lt;0,'edu_status'] = 0data.loc[data['edu_yr']&lt;0,'edu_yr'] = 0#对‘个人收入’处理data.loc[data['income']&lt;0,'income'] = 0 #认为无收入#对‘政治面貌’处理data.loc[data['political']&lt;0,'political'] = 1 #认为是群众#对体重处理data.loc[(data['weight_jin']&lt;=80)&amp;(data['height_cm']&gt;=160),'weight_jin']= data['weight_jin']*2data.loc[data['weight_jin']&lt;=60,'weight_jin']= data['weight_jin']*2 #个人的想法，哈哈哈，没有60斤的成年人吧#对身高处理data.loc[data['height_cm']&lt;150,'height_cm'] = 150 #成年人的实际情况#对‘健康’处理data.loc[data['health']&lt;0,'health'] = 4 #认为是比较健康data.loc[data['health_problem']&lt;0,'health_problem'] = 4#对‘沮丧’处理data.loc[data['depression']&lt;0,'depression'] = 4 #一般人都是很少吧#对‘媒体’处理data.loc[data['media_1']&lt;0,'media_1'] = 1 #都是从不data.loc[data['media_2']&lt;0,'media_2'] = 1data.loc[data['media_3']&lt;0,'media_3'] = 1data.loc[data['media_4']&lt;0,'media_4'] = 1data.loc[data['media_5']&lt;0,'media_5'] = 1data.loc[data['media_6']&lt;0,'media_6'] = 1#对‘空闲活动’处理data.loc[data['leisure_1']&lt;0,'leisure_1'] = 1 #都是根据自己的想法data.loc[data['leisure_2']&lt;0,'leisure_2'] = 5data.loc[data['leisure_3']&lt;0,'leisure_3'] = 3 还有一些数据处理和特征组合的部分这里不一一显示，详情看我的github 源码 3. 模型训练和预测LightBGM1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950#lightGBM决策树lgb_263_param = {'num_leaves': 7,'min_data_in_leaf': 20, #叶子可能具有的最小记录数'objective':'regression','max_depth': -1,'learning_rate': 0.003,\"boosting\": \"gbdt\", #用gbdt算法\"feature_fraction\": 0.18, #例如 0.18时，意味着在每次迭代中随机选择18％的参数来建树\"bagging_freq\": 1,\"bagging_fraction\": 0.55, #每次迭代时用的数据比例\"bagging_seed\": 14,\"metric\": 'mse',\"lambda_l1\": 0.1005,\"lambda_l2\": 0.1996,\"verbosity\": -1}RANDOM_STATE = 4folds = StratifiedKFold(n_splits=5, shuffle=True, random_state=RANDOM_STATE)oof_lgb_263 = np.zeros(len(X_train_263)) # 储存 LGBM的CV预测输出predictions_lgb_263 = np.zeros(len(X_test_263)) # 储存LGB的test set的输出用于后面的 Ensemble learning Stacking 的环节for i, (trn_idx, val_idx) in enumerate(folds.split(X_train_263,y_train)): trn_data = lgb.Dataset(X_train_263[trn_idx],y_train[trn_idx]) val_data = lgb.Dataset(X_train_263[val_idx], y_train[val_idx])#train:val=4:1 num_round = 10000 lgb_263 = lgb.train(lgb_263_param, trn_data, num_round, valid_sets = [trn_data, val_data], verbose_eval=500, early_stopping_rounds = 800) oof_lgb_263[val_idx] = lgb_263.predict(X_train_263[val_idx], num_iteration=lgb_263.best_iteration) # make prediction on test set with mean values predictions_lgb_263 += lgb_263.predict(X_test_263, num_iteration=lgb_263.best_iteration) / folds.n_splitsprint(\"CV score: {:&lt;8.8f}\".format(mean_squared_error(oof_lgb_263, target)))#---------------特征重要性pd.set_option('display.max_columns', None)#显示所有行pd.set_option('display.max_rows', None)#设置value的显示长度为100，默认为50pd.set_option('max_colwidth',100)df = pd.DataFrame(data[use_feature].columns.tolist(), columns=['feature'])# 利用LGBM的tree的feature importance 对特征进行分析df['importance']=list(lgb_263.feature_importance())df = df.sort_values(by='importance',ascending=False)plt.figure(figsize=(14,28))sns.barplot(x=\"importance\", y=\"feature\", data=df.head(50))plt.title('Features importance (averaged/folds)')plt.tight_layout() XGBoosting123456789101112131415161718192021222324252627282930313233##### xgb_263#xgboostxgb_263_params = {'eta': 0.02, #lr 'max_depth': 6, 'min_child_weight':3,#最小叶子节点样本权重和 'gamma':0, #指定节点分裂所需的最小损失函数下降值。 'subsample': 0.7, #控制对于每棵树，随机采样的比例 'colsample_bytree': 0.3, #用来控制每棵随机采样的列数的占比 (每一列是一个特征)。 'lambda':2, 'objective': 'reg:linear', 'eval_metric': 'rmse', 'silent': True, 'nthread': -1}folds = KFold(n_splits=5, shuffle=True, random_state=2019)# 存放 XGBoost 的预测输出结果oof_xgb_263 = np.zeros(len(X_train_263))predictions_xgb_263 = np.zeros(len(X_test_263))for fold_, (trn_idx, val_idx) in enumerate(folds.split(X_train_263, y_train)): print(\"fold n°{}\".format(fold_+1)) trn_data = xgb.DMatrix(X_train_263[trn_idx], y_train[trn_idx]) val_data = xgb.DMatrix(X_train_263[val_idx], y_train[val_idx]) watchlist = [(trn_data, 'train'), (val_data, 'valid_data')] xgb_263 = xgb.train(dtrain=trn_data, num_boost_round=3000, evals=watchlist, early_stopping_rounds=600, verbose_eval=500, params=xgb_263_params) oof_xgb_263[val_idx] = xgb_263.predict(xgb.DMatrix(X_train_263[val_idx]), ntree_limit=xgb_263.best_ntree_limit) predictions_xgb_263 += xgb_263.predict(xgb.DMatrix(X_test_263), ntree_limit=xgb_263.best_ntree_limit) / folds.n_splitsprint(\"CV score: {:&lt;8.8f}\".format(mean_squared_error(oof_xgb_263, target))) RandomForest12345678910111213141516171819#RandomForestRegressor随机森林folds = KFold(n_splits=5, shuffle=True, random_state=2019)oof_rfr_263 = np.zeros(len(X_train_263))predictions_rfr_263 = np.zeros(len(X_test_263))for fold_, (trn_idx, val_idx) in enumerate(folds.split(X_train_263, y_train)): print(\"fold n°{}\".format(fold_+1)) tr_x = X_train_263[trn_idx] tr_y = y_train[trn_idx] # using parallel job for training rfr_263 = rfr(n_estimators=1600,max_depth=9, min_samples_leaf=9, min_weight_fraction_leaf=0.0, max_features=0.25,verbose=1,n_jobs=-1) #verbose = 0 为不在标准输出流输出日志信息 #verbose = 1 为输出进度条记录 #verbose = 2 为每个epoch输出一行记录 rfr_263.fit(tr_x,tr_y) oof_rfr_263[val_idx] = rfr_263.predict(X_train_263[val_idx]) predictions_rfr_263 += rfr_263.predict(X_test_263) / folds.n_splitsprint(\"CV score: {:&lt;8.8f}\".format(mean_squared_error(oof_rfr_263, target))) Stacking把上面的模型的输出通过stacking的方式对stage2 模型进行训练 1234567891011121314151617181920# stacking predictions from LGBM, XGBoost and RandomForest# We need to transpose the results so that each row of matrix = predictions from different models# on the same sampletrain_stack2 = np.vstack([oof_lgb_263,oof_xgb_263,oof_rfr_263]).transpose()# transpose()函数的作用就是调换x,y,z的位置,也就是数组的索引值test_stack2 = np.vstack([predictions_lgb_263, predictions_xgb_263,predictions_rfr_263,]).transpose()#交叉验证:5折，重复2次folds_stack = RepeatedKFold(n_splits=5, n_repeats=2, random_state=7)oof_stack2 = np.zeros(train_stack2.shape[0])predictions_lr2 = np.zeros(test_stack2.shape[0])for fold_, (trn_idx, val_idx) in enumerate(folds_stack.split(train_stack2,target)): print(\"fold {}\".format(fold_)) trn_data, trn_y = train_stack2[trn_idx], target.iloc[trn_idx].values val_data, val_y = train_stack2[val_idx], target.iloc[val_idx].values #using ElasticNet as stage 2 model for prediction lr2 = en() lr2.fit(trn_data, trn_y) oof_stack2[val_idx] = lr2.predict(val_data) predictions_lr2 += lr2.predict(test_stack2) / 10mean_squared_error(target.values, oof_stack2) Reference[1] Datawhale https://github.com/datawhalechina/team-learning-data-mining/tree/master/EnsembleLearning [2] https://datawhale.feishu.cn/docs/doccnHM5ebRKr7u6mrhwcn3l5hc#","link":"/2021/05/18/ensemble-learning-project-HappinessPrediction/"},{"title":"ensemble-learning-Stacking","text":"IntroductionStacking集成算法可以理解为一个两层的集成，第一层含有多个基础分类器，把预测的结果(元特征)提供给第二层， 而第二层的分类 器通常是逻辑回归，他把一层分类器的结果当做特征做拟合输出预测结果。 Stacking而在Stacking里面， 在第一层的模型是通过K-fold Cross validation 形式进行训练和预测。 而Blending的思路相对于把K-fold Cross validation换成Hold-Out。以下是Stacking 步骤: (图片来源: https://zhuanlan.zhihu.com/p/91659366) Stacking 的步骤是： 先把training set 的数据进行K-fold cross validation (sklearn 里面default 2fold) 每个模型都有K个fold的prediction。之后如果有多个不同的模型，就有多个K-fold的prediction。比如说如果我先把训练集分成 5fold,那么就是[X1, X2 … X5]. 如果我有两个不同的模型, 比如KNN和RandomForest那么对应的prediction分别是[A1, A2…A5], [B1, B2, … B5]。之后我们可以把对应序号的prediction进行average计算。 比如 [(A1+B1)/2, (A2+B2)/2 … (A5+B5)/2 ]。 之后把这些模型的输出作为第二层模型的输入。第二层模型一般是logistics regression，这个取决于具体任务 在test set里面， 用第一层的模型对test set的输出进行平均值计算得到第二层模型的测试集的输入，之后第二层模型预测输出。 Source CodeGithub源码:https://github.com/wenkangwei/Datawhale-Team-Learning/blob/main/Ensemble-Learning/Stacking.ipynb PropertiesBlending与Stacking对比， Blending的优点在于: 使用Cross validation 进行数据的预测作为第二层输入，能充分利用数据 相对于Blending，因为用了Cross validation不容易过拟合 缺点在于： 用 Cross validation 之后计算慢 Reference[1] https://zhuanlan.zhihu.com/p/91659366[2] https://github.com/datawhalechina/team-learning-data-mining/tree/master/EnsembleLearning","link":"/2021/05/13/ensemble-learning-Stacking/"},{"title":"Ensemble Learning - Blending","text":"IntroductionBlending是基于Stacking的集成学习。 在Stacking集成学习的方法里面， 它分为两层，第一层里面多个模型(可以相同架构也可以不一样架构)直接从原来的训练集进行学习并预测输出。之后把第一层多个模型的输出作为第二层模型的输入对第二层模型进行训练。 Stacking的思路就好比上课时我上课迟到，我需要从其他同学的笔记里面进行学习归纳。这样子其他同学就相当于第一层的模型，直接从老师(数据)学习。而我就相当于第二层模型，从第一层模型的输出进行学习。 而在Stacking里面， 在第一层的模型是通过K-fold Cross validation 形式进行训练和预测。 而Blending的思路相对于把K-fold Cross validation换成Hold-Out。以下是Stacking 步骤: (图片来源: https://zhuanlan.zhihu.com/p/91659366) 在Stacking里面， 它先对training data进行分割成K份然后用K-fold Cross Validation 进行模型的训练和对validation set进行预测。 第一层模型对alidation set的预测用作第二层模型的训练集。而在测试集里面，第一层所有的模型对test set的预测可以通过求均值的方式得到第二层模型的test set。 第二层模型的输出就是整个stacking 模型的输出。Stacking 由于用了 K-Fold CV 进行第一层模型训练，第一层模型输出的特征更加robust对数据的利用率也比较充分，但是就是训练慢。 Blending 步骤 将数据划分为训练集和测试集(test_set)，其中训练集需要再次划分为训练集(train_set)和验证集(val_set)； 创建第一层的多个模型，这些模型可以使同质的也可以是异质的； 使用train_set训练步骤2中的多个模型，然后用训练好的模型预测val_set和test_set得到val_predict,test_predict1； 创建第二层的模型,使用val_predict作为训练集训练第二层的模型； 使用第二层训练好的模型对第二层测试集test_predict1进行预测，该结果为整个测试集的结果。 Blending图解 Properties优点 相对于Stacking, blending 用holdout更加简单 不需要太多理论分析缺点 只用了holdout的数据集预测作为第二层的训练集，浪费数据 因为holdout数据用的少，输入第二层的数据很有可能导致模型overfitting 相对于Stacking, 因为用了Holdout而不是K-fold CV,不够robust Source CodeGithub 链接：https://github.com/wenkangwei/Datawhale-Team-Learning/blob/main/Ensemble-Learning/Blending.ipynb Reference[1] https://zhuanlan.zhihu.com/p/91659366[2] https://github.com/datawhalechina/team-learning-data-mining/tree/master/EnsembleLearning","link":"/2021/05/12/ensemble-learning-blending/"},{"title":"Hello World","text":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new \"My New Post\" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment","link":"/2020/06/30/hello-world/"},{"title":"Use Hexo + ICarus to build a website","text":"IntroductionRecently, I realize that there are many benefits to write technical blog on a personal websites. It allows us to summarize the techniques we’ve learned, helping review what we did before, and to learn with each other.Hence, this blog is to show how to build a personal site using static website generator, such as Hugo, Hexo and Jekyll. Static site generate actually generates the general html, css and javascript,etc files we need in the framework of website. Comparison among Hugo, Hexo, JekyllHugo: Language GoLang Advantage The fastest framework for constructing website Good flexibility and no need to install many packages since Using Go language many built-in functions/plugins Templates for GoLang International Use Support Markdown Disadvantage Need to be familiar with GoLang Without default themes for website Lack of extensive plugins Hexo: Language Node.js Advantage Fast generating speed Support markdown Easy to setup Github page Good Chinese support (Most of users are Chinese) Many different themes for website Stable Disadvantage May be lack of supports for other languages, since most of users are Chinese Need to install some packages for plugins Jekyll: Language Ruby Advantage Simple to use Many free themes and plugins Easy to publish to Github Disadvantage Not international (may be blocked in some countries) Some plugins are not supported by Github Page Speed of constructing website could be slow as more documents/passages are included in website. Setup HexoI choose Hexo as the first generator for my website since it provides many different themes and plugins. To setup Hexo and publish your website, we first need to setup our work environment (Node.js) for Hexo and associate our website to our server (Github page). Setup Node.js and Git environment Install Node.js . Here is Chinese Reference Install Git to manage your repository of your website and open Git bash terminal. Open the git bash terminal and Install Hexo using command: 1$ npm install hexo-cli -g Initialize the folder that stores your website after install hexo: 1$ hexo init new-folder-name Then hexo will create the folder with the name “new-folder-name” and setup the environment. Inside the folder 1234567├── _config.yml├── package.json├── scaffolds├── source| ├── _drafts| └── _posts└── themes _config.yml contains the common configurations for the website. source contains the passages you are writing. themes is the folder containing all different themes for your website. Associate our website to server (I choose Gitpage here) Select your server There are many different servers for your website. To read more about choosing server for your websites, click here Since GithubPage provides a free way to publish our website and Github is a very powerful open source repository website. I use GithubPage for me website in this passage. create Github account create a new repository with name: &lt;Your github name&gt;. github.io Install git plugin for Hexo 1$ npm install hexo-deployer-git --save create SSH key 1$ ssh-keygen -t rsa -C \"your email address\" Copy your generated SSH key in the _id_rsa.pub file_ in the path you set like (C:\\Users\\Administrator.ssh\\id_rsa.pub) to your New ssh key Modify the deployment setting in _config.yml file in the root directory of your website. 123456# Deployment## Docs: https://hexo.io/docs/deployment.html deploy: type: git repo: git@github.com:&lt;Github账号名称&gt;/&lt;Github账号名称&gt;.github.io.git branch: master Upload your website to github Note: the default theme of your website in Hexo is landscape. so you don’t need to add any file to your website during testing. To learn more about how to write and post your passage, click here Choosing theme for your blogIf you dislike the default theme (landscape) in hexo, you can install other themes and change the _config.yml file setting to update your website.For example, if you want to install icarus theme, Go back to the root of your website, install icarus them using this command 1$ git clone https://github.com/ppoffice/hexo-theme-icarus.git themes/icarus Modify your config.yml file 12345# Extensions## Plugins: https://hexo.io/plugins/## Themes: https://hexo.io/themes/#theme: landscapetheme: icarus I annotate the theme of landscape and choose icarus here. Note: do not add any space before the keyword “theme”, otherwise, the theme will not update Generate static site files and update them to github generate static site files 1$ hexo g upload the files to github directory &lt;your github name&gt;.github.io 1$ hexo d Then you can type &lt;your github name&gt;.github.io in your browser to visit your website. Setup Comment pluginIn hexo, there are many different plugins for comment function, like valine, gitalk, gitment. Take valine as example here. Create an account on Leancloud Create a new App, then click the app you created -&gt; Settings-&gt;App keys Copy AppID and AppKey Modify the comment setting in _&lt;your hexo’s root&gt;/theme/&lt;directory name of icarus&gt;/config.yml 12345678910# Comment plugin configurations# https://ppoffice.github.io/hexo-theme-icarus/categories/Plugins/Comment/comment: # type: disqus # # Disqus shortname # shortname: 'Disqus' # Name of the comment plugin type: valine app_id: 'AppID' app_key: 'AppKey' Generate static site files and deploy them again 1$ hexo g -d Additional Notes Useful commands in Hexo 123456789101112131415161718192021222324#in the root directory of hexo$ hexo new page &lt;page-name&gt; #default generate a directory in source/$ hexo new &lt;file-name without postfix \".md\"&gt; #generate &lt;file-name&gt;.md file in /source/__post$ hexo clean #clean all static site files$ hexo g #generate static site files# or $ hexo generate$ hexo d #deploy website, push it to the server#or$ hexo deploy$ hexo g -d # generate and deploy website#move file from __drafts to __posts and publish the passage to website$ hexo publish draft &lt;filename without postfix \".md\"&gt; # Start local server and display your websiteat http://localhost:4000$ hexo s # or $ hexo server To enable catalogue in icarus to enable the passage to show up in catergory, tags, at the top of markdown file add -–type: [“category”, “tags”]-– To let catalogue show up in the page, add toc: true to the top of the file: -–toc: truetype: [“category”, “tags”]-– To change th background of your blog go to themes\\next\\source\\images folder to put your background image Then go to the file themes\\next\\source\\css_custom\\custom.styl to addthe following codes, where bg.jpg is your background image. 1234567body{ background:url(/images/bg.jpg); background-size:cover; background-repeat:no-repeat; background-attachment:fixed; background-position:center;} Then rebuild your HTML blog by commands 1hexo clean &amp;&amp; hexo g -d To change the text of button “next” and “previous” of page to be “next &gt;” and “&lt; prev”:In file: themes\\next\\layout_partials\\pagination.swig , Change 123456789&lt;nav class=&quot;pagination&quot;&gt; {{ paginator({ prev_text: '&lt;i class=&quot;fa fa-angle-left&quot;&gt;&lt;/i&gt;', next_text: '&lt;i class=&quot;fa fa-angle-right&quot;&gt;&lt;/i&gt;', mid_size: 1 }) }}&lt;/nav&gt; to 123456789&lt;nav class=&quot;pagination&quot;&gt; {{ paginator({ prev_text: '&lt; prev', next_text: 'next &gt;', mid_size: 1 }) }}&lt;/nav&gt; This enable you to change the text of next, prev button References https://blog.zhangruipeng.me/hexo-theme-icarus/Widgets/icarus%E7%94%A8%E6%88%B7%E6%8C%87%E5%8D%97-%E6%8C%82%E4%BB%B6/ https://lexcao.github.io/zh/posts/jekyll-hugo-hexo https://juejin.im/post/5bebfe51e51d45332a456de0#heading-9 https://www.vincentqin.tech/posts/build-a-website-using-hexo/ https://zsh2401.top/post/configure-theme-icarus/","link":"/2020/07/01/hexo-gitpage-website/"},{"title":"CPSC 6300 Final Report","text":"TITLE: KKBox Music Recommendation SystemTeam Members: Wenkang Wei, ShuLe ZhuDate: 12/5/2020Problem Statement and MotivationProblem statementAs the public is listening to all kinds of music and there is a diversity of the favors of different individuals in listening music, music users may choose different music platforms based on which platform can provide the kinds of music closest to their tastes. In order to help music platforms figure out how to recommend suitable kinds of music and songs to individuals or groups so that they can provide better services and retain users, it is necessary to analyze the tastes of individuals and the public in listening to music and explore the chances that users may repeat listening some songs based on data analysis. In addition, users, who listen to a large number of songs and add many songs to their favorite albums, usually forget what songs they have collected and which songs they like when trying to re-play those songs. In this case, it is necessary to build a recommendation system to help us predict the songs users may repeat listening to after the user’s a very first observable listening event. Motivation and GoalWe are to construct a recommendation system to predict the chances of a user listening to a song repetitively after the first observable listening event within a time window was triggered. If there are recurring listening event(s) triggered within a month after the user’s very first observable listening event, its target is marked 1, and 0 otherwise in the training set. By constructing such a recommendation system, we are able to remind music users of their favorite songs and increase the rate of the audience as well as the benefit to the music platform. Introduction and Description of DataMusic is important in our daily life and the role of music is determined by people and the environment. Music has a lot of help, for example, it can cultivate sentiment and reduce stress. However, some people don’t need music in their lives or live in music. People who never listen to music or harmony feel that music has a low sense of existence. But in fact, in their everyday lives, they actually have songs, ringing alarms, wind, and rain. So, the role of music performance is one of the most important companions in life. In order to recommend suitable music to users in music platforms efficiently, a recommendation system is an essential part of music platforms. A recommendation system is a subclass of an information filtering system that seeks to predict the “rating” or “preference” a user would give to an item. Due to the fact that there are millions of kinds of music, composers, artists, and other information from users, it is hard for users to pick the kinds of music they like from millions of songs and remember which songs they like and want to re-play. Usually, users forget the songs they have added to their favorite song album due to the large number of songs they have a play. Hence it is necessary to build a music recommendation system to recommend kinds of music, which users prefer and are likely to repeat listening to. The dataset we collected is KKBox Music 1.7GB dataset. Our dataset is from Kaggle KKBox-Music-Recommendation-Challenge. You can also get the dataset from our GoogleDrive . The description of the dataset is as follow: Descriptions of dataset: train.csv msno: user id song_id: song id source_system_tab: the name of the tab where the event was triggered. System tabs are used to categorize KKBOX mobile apps functions. For example, tab my library contains functions to manipulate the local storage, and tab search contains functions relating to search. source_screen_name: name of the layout a user sees. source_type: an entry point a user first plays music on mobile apps. An entry point could be album, online-playlist, song .. etc. target: this is the target variable. target=1 means there are recurring listening event(s) triggered within a month after the user’s very first observable listening event, target=0 otherwise . test.csv id: row id (will be used for submission) msno: user id song_id: song id source_system_tab: the name of the tab where the event was triggered. System tabs are used to categorize KKBOX mobile apps functions. For example, tab my library contains functions to manipulate the local storage, and tab search contains functions relating to search. source_screen_name: name of the layout a user sees. source_type: an entry point a user first plays music on mobile apps. An entry point could be album, online-playlist, song .. etc. sample_submission.csv sample submission file in the format that we expect you to submit id: same as id in test.csv target: this is the target variable. target=1 means there are recurring listening event(s) triggered within a month after the user’s very first observable listening event, target=0 otherwise . songs.csv The songs. Note that data is in unicode. song_id song_length: in ms genre_ids: genre category. Some songs have multiple genres and they are separated by | artist_name composer lyricist language members.csv user information. msno city bd: age. Note: this column has outlier values, please use your judgement. gender registered_via: registration method registration_init_time: format %Y%m%d expiration_date: format %Y%m%d song_extra_info.csv song_id song name - the name of the song. isrc - International Standard Recording Code, theoretically can be used as an identity of a song. However, what worth to note is, ISRCs generated from providers have not been officially verified; therefore the information in ISRC, such as country code and reference year, can be misleading/incorrect. Multiple songs could share one ISRC since a single recording could be re-published several times. In preliminary EDA, we take a look to the features in different data files. 1train_df.info() &lt;class 'pandas.core.frame.DataFrame'&gt; RangeIndex: 7377418 entries, 0 to 7377417 Data columns (total 6 columns): # Column Dtype --- ------ ----- 0 msno object 1 song_id object 2 source_system_tab object 3 source_screen_name object 4 source_type object 5 target int64 dtypes: int64(1), object(5) memory usage: 337.7+ MB 1test_df.info() &lt;class 'pandas.core.frame.DataFrame'&gt; RangeIndex: 2556790 entries, 0 to 2556789 Data columns (total 6 columns): # Column Dtype --- ------ ----- 0 id int64 1 msno object 2 song_id object 3 source_system_tab object 4 source_screen_name object 5 source_type object dtypes: int64(1), object(5) memory usage: 117.0+ MB 1song_df.info() &lt;class 'pandas.core.frame.DataFrame'&gt; RangeIndex: 2296320 entries, 0 to 2296319 Data columns (total 7 columns): # Column Dtype --- ------ ----- 0 song_id object 1 song_length int64 2 genre_ids object 3 artist_name object 4 composer object 5 lyricist object 6 language float64 dtypes: float64(1), int64(1), object(5) memory usage: 122.6+ MB 1members_df.info() &lt;class 'pandas.core.frame.DataFrame'&gt; RangeIndex: 34403 entries, 0 to 34402 Data columns (total 7 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 msno 34403 non-null object 1 city 34403 non-null int64 2 bd 34403 non-null int64 3 gender 14501 non-null object 4 registered_via 34403 non-null int64 5 registration_init_time 34403 non-null int64 6 expiration_date 34403 non-null int64 dtypes: int64(5), object(2) memory usage: 1.8+ MB In the dataset description above, we can obtain the information from both sources of songs, songs, and members. By using the information of sources of songs, we can know where the songs re-played by users are from. The information about songs and members can provide us with details about what kinds of songs are often re-played by users, or what types of members are more likely to repeat playing songs. Such kinds of data can give us insight into users’ behaviors of repeating listening to music. Based on the information from members, songs and sources of songs, we can use them as the input features to machine learning model. The target is the chance of a user listening to a song repetitively after the first observable listening event within a time window was triggered. If there are recurring listening event(s) triggered within a month after the user’s very first observable listening event, its target is marked 1, and 0 otherwise in the training set. Hence, the question can be defined as a binary classification problem and the prediction from machine learning model should be either possibility of the chance, or 0 , 1 binary value indicating if there is a chance. Literature Review/Related WorkAs for the KKBox music recommendation system project, many Kaggle users choose to use a light gradient boosting machine (LGBM) due to its simplicity in preprocessing a large amount of data, and its fast computation speed. Based on the blog [5], the LGBM model is a light gradient boosting machine model, which is based on multiple decision tree models and use gradient boosting ensemble learning method to improve the training accuracy. One work from Kaggle we refer to [3] is to use the LGBM boosting machine as a baseline to fit the dataset and train the model using binary cross-entropy loss since the prediction task in this project is a binary classification task. In addition to the LGBM model, there are some deep learning models used for the recommendation system. One of the neural network models is called the wide and deep model [1] . In this model, first uses an embedding network to transform the sparse categorical data into dense numerical data in one branch. Then it merges the branch of categorical data and the branch of numerical features together to feed the traditional network in the main branch. The main branch of the neural network works as a classifier while the embedding neural network works as a transformation network to preprocess the data. Since in this Kaggle project, no one tries such a neural network model for this task, we are interested in trying this model and see how it works in this problem. Then we compare it with the LGBM models. Modeling ApproachTask - Since this project is to predict the chances of a user listening to a song repetitively after the first observable listening event within a time window was triggered, the output from the machine learning model is the possibility that a user will repeat listening to a song after the first listening event. Hence, this problem can be modeled as a binary classification problem Loss Function - Since this task is a binary classification problem, we simply select binary cross-entropy loss as the loss function used to train parametric machine learning models, like deep learning models, logistic regression, etc. Evaluation Metric - The evaluation metric to measure the performance of the machine learning model is selected to be accuracy and AUC (area under the curve) since we care about how accurate the models could be. LGBM Modeling - Light Gradient Boosting Machine (LGBM) model is a tree-based model，this model merge and train multiple decision tree models to do classification or regression tasks, using the Boosting ensemble learning process. It will automatically encode categorical data into vectors and train models for labels or one-hot encoding. Since it provides a fast way to transform categorical data and train models with good accuracy performance based on the results from leaderboard in Kaggle, we try it here and tune its parameters to fit the data.Here is the reference to use LGBM: https://lightgbm.readthedocs.io/en/latest/Quick-Start.html Wide and Deep Neural Network model - This model using the embedding network in the neural network, this model converts categorical attributes into dense vectors, allowing one to reduce the dimension of categorical data and remove key features such as PCA. Then for feature selection and classification in the main branch, it blends dense embedded vectors with numerical data. The main branch is a typical neural network that acts as a classification function using linear layers, activation functions (relu, softmax, sigmoid). The output is the possibility that the user may repeat listening to the music.The architecture of Wide and Deep Model is referred to https://github.com/zenwan/Wide-and-Deep-PyTorch Baseline model - We choose our baseline model as LGBM boosting machine model with following setings: ‘objective’: ‘binary’, learning_rate = 0.3 , num_leaves = 110, bagging_fraction = 0.95, bagging_freq = 1, bagging_seed = 1, feature_fraction =0.9, feature_fraction_seed =1, max_bin = 256, max_depth = 10, num_rounds = 200.After we choose the baseline model, we tune the LGBM model by changing the max_depth parameter to be [10 , 15, 20 , 25, 30] to see how the depth affects our LGBM performance. After that, we also implement the Wide and Deep Model with learning rate = 0.001, binary cross-entropy loss, and 2 epochs. The reason why we use different settings on the Wide and Deep model is that the training process of the neural network is very slow and the convergence speed of the model is slower than the LGBM model. The training loss of the neural network is easy to increase after decreasing for some iterations. So we need to tune some parameters to train the neural network model. Project Trajectory, Results, and InterpretationOur goal in this project doesn’t change. It is to construct a recommendation system to predict the chances of a user listening to a song repetitively after the first observable listening event within a time window was triggered. If there are recurring listening event(s) triggered within a month after the user’s very first observable listening event, its target is marked 1, and 0 otherwise in the training set. Only one thing we change is that we choose to use neural network model and light gradient boosting machine (LGBM) model for this project, rather than using traditional models like KNN, logistic regression, Here is the trajectory of our project Timeline Date Assignment Wenkang Shule Oct 22 Proposal ✓ ✓ week 1 Set up environment and Exploratory Data Analysis (EDA) ✓ ✓ week 2 Data preprocessing ✓ week 3 Modeling ✓ Nov 12 Interim Report ✓ ✓ week 4 Training model and Evaluation ✓ week 5 Training model and Evaluation ✓ week 6 Wrap up and preparing website presentation ✓ ✓ week 7 Writting Final Report ✓ ✓ Dec 10 Website and Final Report ✓ ✓ Dec 13 Notebooks and other supporting materials ✓ ✓ In this project, we follow these steps to build our data pipeline and recommendation system model:First, in the exploratory data analysis step, we use visualization techniques to visualize and plot the features from the dataset to explore the correlation across features and find which features are most related to the target. Here are partial results from EDA section Genre Counts Artist Count Count of system tabs, screen names, source types In the Exploratory Data Analysis (EDA) part, we first analyze Song information by plotting and visualizing the counts of song genres, composers, artists, and the counts of source types, source screen names, system tabs. In this part, we find that the distributions of the counts of song genres, composers, artists are long-tailed distribution. That is, most users prefer listening to specific genres of music or songs created by specific artists, composers. In this case, we can know that those specific genres of music or songs from specific composers, artists are more likely to be re-played by users. In addition, when visualizing the counts of users vs source screen names, system tabs, and source types, we find that most users are more likely to repeat listening to music from their local sources, local library, rather than from online sources. That is, the features like source types, screen names, and system tabs provide important information for our recommendation system. Hence we choose to keep such kinds of features for modeling later. Later, we analyze Member information, like visualizing the count of bd/age attribute and analyzing the correlation between different attributes using the heatmap plot. In this part, we find that there are many outliers in bd/age features with values outside the range of [0, 100]. After we remove such kind of outliers, we find that most users have ages between 20 to 40. After this, we try to plot bivariate plots to visualize and analyze the relationship between attributes, like city and age/bd, expiration date, and target. what we find is that registration_init_year is negatively correlative to bd/age, city, registered_via. Song_length is also negatively correlative to language. In order to find which features are most related to targets, we plot the correlation matrix using a heatmap to visualize the correlation across features. In this part, since we find almost all other features have similar correlation values to the target, we choose to keep those features for modeling. Then in data preprocessing, we clean and transform the features to a suitable format, like converting String DateTime data to DateTime format and separate year, month, day as new features, removing the outliers in bd/age features, filling missing values, creating new features like count of composers, artist, genres and converting object data type to categorical data type before training model Later, we also construct a data pipeline to extract, transform, load data set by integrating the operations in the data preprocessing step into one single transformation function, which enable us to easily clean and transform dataset directly. After the data preprocessing and transformation step, we split the dataset into a training set (80% of the dataset) and a validation set (20% of the dataset) for training and validating our models. We also determine the Loss function (binary cross-entropy), evaluation metric (accuracy and AUC) to train our Light Gradient Boosting machines (LGBM) models and Wide and Deep Neural network model. During training our LGBM models, we are interested in how the max_depth affect the model performance, so we also try different max_depth parameters to tune our LGBM models. In the Model evaluation step, we simply use the validation dataset to validate the final trained models and then let models make predictions on the test set from Kaggle and submit predictions to Kaggle to see the final evaluation scores. Accuracy Results on our validation on LGBM model to see effect of max_depth on accuracy Index Lgbm with max_depth Validation Accuracy 0 10 0.709764 1 15 0.719106 2 20 0.723689 3 25 0.725822 4 30 0.728842 We can observe that as the value of max_depth of the decision tree in the Boosting machine increases, both validation accuracy, and test accuracy increase gradually. It implies that the performance of our LGBM models may be improved by increasing the max_depth. As increasing max_depth can improve the learning/fitting ability of the LGBM model, it is possible that tuning other parameters like the number of leaves, the number of training epochs may also help improve the accuracy and let models better fit the dataset. Accuracy Results on Kaggle Testset Model name private score public score LGBM Boosting Machine Model 4 0.67423 0.67256 LGBM Boosting Machine Model 3 0.67435 0.67241 LGBM Boosting Machine Model 2 0.67416 0.67208 LGBM Boosting Machine Model 1 0.67416 0.67188 LGBM Boosting Machine Model 0 0.67206 0.66940 Wide and Deep model 0.61628 0.61117 Best Accuracy Results on Kaggle Testset from Kaggle Leaderboard Ranking name Score Entries 1 Bing Bai 0.74787 263 In the final results on test data from Kaggle, we can see that light gradient boosting machines have the accuracy performance better than the Wide and Deep Neural Network model. The best score in LGBM models is 67.256% while the Wide and Deep model has an accuracy of 61.11%. Although the Wide and Deep model performance is not so well, we may improve its performance by tunning the parameters like dropout rate in the neural network, learning rate, training epochs in the future. In addition, in our experiments we try two epochs only, this is because we run the program in Google Colab and also try it in the Kaggle platform, but the hardware is not powerful enough to train the model quickly and there is a time limit in using GPU. Therefore, we can try better hardware to boost the training process in the future. What’s more, the best testing score from the Kaggle leaderboard is only 74.7887%, which means that this project is still challenging. It could be due to the difficulty in parsing and transforming the dataset to extract more meaningful patterns. The limitations of designing and training good models are also some factors since we don’t have enough computation power to train a large model, like Google, OpenAI. Overall, our works explore the effect of max_depth on LGBM models’ performance and also compare the performance of the Wide and Deep model with the performance of the LGBM model. Conclusions and Future WorkIn this project, there are still a few things that perform not very well. One obvious thing is the performance of the Wide and Deep model. We can easily see that the performance from the neural network model is low. This could be due to the small training epochs we use., as we use only 2 epochs to train the model using limited computation resources (Google colab). What’s more, we can also try different neural network architecture to better fit the dataset, or tune the parameters like learning rate, weight decay in the network to increase the learning ability of our network. As for the LGBM model, it seems like the LGBM model can better fit the dataset when using more training epochs and a larger max_depth value. We can also try to tune other parameters like the number of leaves, etc. Overall, in the future, we may do the followings to improve our project:In this project, there are several things we can improve in the future: We can use a better hardware platform for training models, rather than using Google Colab or Kaggle platform, so that we can better train the deep learning model. Tune the LGBM models using a grid search and choose larger max_depth values or tune other parameters Try to create more new features from text attributes like composer, lyricist, artist and use feature importance methods to pick features that most contribute to the prediction In conclusion, we collect a 1.7GB KKBOX music dataset from Kaggle and do exploratory data analysis (EDA) on the data by visualizing the attributes and compute the correlations among features. Then we clean the dataset by removing outliers from age/bd attributes, filling missing categorical data with new labels, and missing numerical data with median value. We also transform the text data and create new features. After that, we use 80% dataset as training set and 20% dataset as a validation set.In Modeling and Evaluation, we use LGBM models and Wide &amp; Deep Neural network models to fit the dataset and also tune the max_depth parameter in LGBM to do binary classification tasks. The best accuracy performance of our models is 67.25% while the best accuracy from the Kaggle leaderboard is about 74%. In the end, we also summarize the future works to improve the project, like using better hardware resources, tunning other parameters of models, and explore more useful features for training. References[1] https://github.com/zenwan/Wide-and-Deep-PyTorch/blob/master/wide_deep/torch_model.py [2] https://www.kaggle.com/c/kkbox-music-recommendation-challenge/submit [3] https://www.kaggle.com/asmitavikas/feature-engineered-0-68310 [4] https://blog.statsbot.co/recommendation-system-algorithms-ba67f39ac9a3 [5] https://medium.com/@pushkarmandot/https-medium-com-pushkarmandot-what-is-lightgbm-how-to-implement-it-how-to-fine-tune-the-parameters-60347819b7fc [6] https://towardsdatascience.com/how-to-build-a-wide-and-deep-model-using-keras-in-tensorflow-2-0-2f7a236b5a4b [7] https://lightgbm.readthedocs.io/en/latest/pythonapi/lightgbm.Dataset.html#lightgbm.Dataset Support Materials Dataset:You can find the dataset from Kaggle: https://www.kaggle.com/c/kkbox-music-recommendation-challenge/data Or you can get the dataset from our Google Drive: https://drive.google.com/file/d/1-WJHZUWFtz9ksfvFoX-dc-ZKjZ6fTk0D/view?usp=sharing Link to Our website/ notebook in databrick platform:https://databricks-prod-cloudfront.cloud.databricks.com/public/4027ec902e239c93eaaa8714f173bcfc/3546981394788271/3905787926422076/7844345949955417/latest.html Link to Our notebook in Google Colab (This notebook is as same as notebook in databrick, but in Colab, you can directly run it after copying it to your drive):https://colab.research.google.com/drive/1dssuTVKvDXM0zULihRt4tJUoOUoCxaFj?usp=sharing Github repository of this project:https://github.com/wenkangwei/cpsc6300-final-project Declaration of academic integrity and responsibilityIn your report, you should include a declaration of academic integrity as follows: 12345With my signature, I certify on my honor that:The submitted work is my and my teammates' original work and not copied from the work of someone else.Each use of existing work of others in the submitted is cited with proper reference.Signature: ___Wenkang Wei_________ Date: ______12/4/2020________ 12345With my signature, I certify on my honor that:The submitted work is my and my teammates' original work and not copied from the work of someone else.Each use of existing work of others in the submitted is cited with proper reference.Signature: ___ShuLe Zhu_________ Date: ______12/4/2020________ CreditThe above project template is based on a template developed by Harvard IACS CS109 staff (see https://github.com/Harvard-IACS/2019-CS109A/tree/master/content/projects).","link":"/2020/12/08/report/"},{"title":"tensorflow-practice","text":"Common packages for data12345import numpy as npimport pandas as pdimport matplotlib.pyplot as plt%matplotlib inlineimport seaborn as sns Tensorflow Practice Data set loading and visualization Data Preprocessing/transforming Modeling using tf.nn (more basic) / tf.keras (advance and convenient API for modeling) Performance Visualization Model saving and reuse, reload 123456import tensorflow as tfimport tensorflow.keras as keras# This one-hot convert words to index number rather than binary formfrom tensorflow.keras.preprocessing.text import one_hot#tf.one_hot import text/ labels into binary 0,1 form without compressionfrom numba import cuda 12# Check if GPU is enabledprint(cuda.gpus) &lt;Managed Device 0&gt;, &lt;Managed Device 1&gt; Basic Practise tf.constant contain types of data. Operation like +, -, *, / , should be applied to data with same types only 12345678910ls = np.random.randint(0,100,50)ls= tf.constant(ls)x1= tf.constant(1)x2= tf.constant(2)x4= tf.constant(5)x3 = x1+x2*x4tf.print(x3,[x1,x2])tf.print(tf.argsort(ls))c = tf.argsort(ls) 11 [1, 2] [32 24 27 ... 20 40 49] Practice with Text+Numeric type data: titanic data12345678TRAIN_DATA_URL = \"https://storage.googleapis.com/tf-datasets/titanic/train.csv\"TEST_DATA_URL = \"https://storage.googleapis.com/tf-datasets/titanic/eval.csv\"# The first param in get_file: the name of dataset to save# The second param: url to the dataset trainset_path = keras.utils.get_file('titanic_train.csv', TRAIN_DATA_URL)testset_path = keras.utils.get_file('titanic_test.csv', TEST_DATA_URL)trainset_path, testset_path Downloading data from https://storage.googleapis.com/tf-datasets/titanic/train.csv 32768/30874 [===============================] - 0s 0us/step Downloading data from https://storage.googleapis.com/tf-datasets/titanic/eval.csv 16384/13049 [=====================================] - 0s 0us/step ('/home/wenkanw/.keras/datasets/titanic_train.csv', '/home/wenkanw/.keras/datasets/titanic_test.csv') 123# Load CSV file with pandastrainset = pd.read_csv(trainset_path)testset = pd.read_csv(testset_path) 1trainset.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } survived sex age n_siblings_spouses parch fare class deck embark_town alone 0 0 male 22.0 1 0 7.2500 Third unknown Southampton n 1 1 female 38.0 1 0 71.2833 First C Cherbourg n 2 1 female 26.0 0 0 7.9250 Third unknown Southampton y 3 1 female 35.0 1 0 53.1000 First C Southampton n 4 0 male 28.0 0 0 8.4583 Third unknown Queenstown y 1trainset.groupby('sex').head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } survived sex age n_siblings_spouses parch fare class deck embark_town alone 0 0 male 22.0 1 0 7.2500 Third unknown Southampton n 1 1 female 38.0 1 0 71.2833 First C Cherbourg n 2 1 female 26.0 0 0 7.9250 Third unknown Southampton y 3 1 female 35.0 1 0 53.1000 First C Southampton n 4 0 male 28.0 0 0 8.4583 Third unknown Queenstown y 5 0 male 2.0 3 1 21.0750 Third unknown Southampton n 6 1 female 27.0 0 2 11.1333 Third unknown Southampton n 7 1 female 14.0 1 0 30.0708 Second unknown Cherbourg n 9 0 male 20.0 0 0 8.0500 Third unknown Southampton y 10 0 male 39.0 1 5 31.2750 Third unknown Southampton n","link":"/2020/07/15/tensorflow-practice/"},{"title":"Data Structure 2 -sorting","text":"Bubble sort (冒泡排序法)Main ideaBubble sort compares the array elements $(n-1) + (n-2)+…+1 = \\frac{n(n-1)}{2}$ times. At $k^{th}$ iteration, iterate (n-k) elements and swap two elements if previous one is larger than the next one in ascending sorting.It guarantees that in each iteration, there must be at least one element sorted into the correct positionsExample:in an array [5 1 4 2 8] with length = 5Begin:$1^{st}$ iteration: iteration starts from arr[0] to arr[4] and swap two element if arr[i] &gt; arr[i+1].[5 1 4 2 8] $\\to$ [1 5 4 2 8] $\\to$ [1 4 5 2 8] $\\to$ [1 4 2 5 8] $\\to$ [1 4 2 5 8] $2^{nd}$ iteration: iteration starts from arr[0] to arr[3] and swap two element if arr[i] &gt; arr[i+1]. Process is similar to the first iteration.The result is [1 4 2 5 8] $\\to$ [1 2 4 5 8] $3^{rd}$ iteration: iteration starts from arr[0] to arr[2].$4^{rd}$ iteration: iteration starts from arr[0] to arr[1].End Process Pesudo Code 1234Loop through n-1 iteration: at the k^th iteration, loop through arr[0] to arr[n-1 - k] element: if arr[i]&gt; arr[i+1] swap arr[i], arr[i+1] Python Code 1234567def bubble_sort(array): n = len(array) for i in range(n-1): for j in range(n-1-i): if array[j] &gt; array[j+1]: array[j], array[j+1] = array[j+1], array[j] return array Complexity Memory Complexity: O(1), since there is no memory used in algorithm Time Complexity: O(n^2), since there are two inner loops in function and need $\\frac{n(n-1)}{2}$ comparison. Selection Sort (选择排序法)Main IdeaSelection sort is to find the maximum value of current array and put it to the end of array and then exclude this sorted element to get the sub-array and repeat these two steps to sort the array. If find minimum value, need to put it to the beginning. Process Pesudo Code 1234Loop through n-1 iterations at k^th iteration find max of array[0:n-1-k] put max into array[n-1-k] Python Code 123456789def selection_sort(array): n = len(array) for i in range(n): max_v = 0 for j in range(n-i): if array[max_v] &lt; array[j]: max_v = j array[n-i-1] , array[max_v] =array[max_v], array[n-i-1] return array Complexity Memory Complexity: O(1) since in this code, we don’t use additional memory Time Complexity:: O(n^2), since selection sort use two loops to sort. One inner loop is used to find max value and the other loop is to sort the max values. Insertion sort (插入排序法)Main IdeaMain idea of Insertion Sort is to compare the current element with elements before this element and then insert this element to the correct position by moving some elements backward. Start from arr[1], i=1. Iterate elements arr[i] with index i from 1 to N-1 For each element arr[i], compare arr[i] with the element arr[k] right before arr[i]. If we found element arr[k] &gt; arr[i], swap arr[i] and arr[k]. Repeat Step 2 until the element arr[k] before arr[i] is less than or equal to arr[i]Example:Given an array arr = [4,3,2,10,12,1,5,6] with array length N= 8, initial index =0, we want to sort it in ascending order.iteration 1: [4,3,2,10,12,1,5,6] $\\to$ [3,4,2,10,12,1,5,6]iteration 2: [3,4,2,10,12,1,5,6] $\\to$ [3,2,4,10,12,1,5,6] $\\to$ [2,3,4,10,12,1,5,6]… Process Pseudo code12345678Let i = 1Loop until i ==N-1 j= i-1 // swap arr[i] and arr[j] if arr[j]&gt;arr[i] while arr[i]&lt; arr[j] and j&gt;0: swap(arr[i], arr[j]) j-- i++ Python Code1234567def insertion_sort(array): for i in range(1,len(array)): j = i-1 while j&gt;=0 and array[j] &gt;array[j+1]: array[j],array[j+1] = array[j+1], array[j] j -= 1 return array Complexity Memory Complexity: O(1) Speed Complexity: worst case: O(n^2) Extension Binary Search + Insertion sort. Reference Quick sort (快速排序法)Main IdeaIn quick sort, the main idea is that we first choose a pivot/key element used for comparison. Usually we choose the last element in the partition/array as the pivot (we can also choose middle element or the first element as well) Move the elements smaller than pivot to the left to form a subarray/partition P1 containing all elements smaller than pivot. Move the elements larger than pivot to the right to form a subarraysubarray/partition P2 containing all elements larger than pivot Move pivot to the position between P1 and P2 such that elements ahead pivot are smaller than pivot, elements after pivot are after than pivotNote: This structure is actually as same as binary search tree, in which elements on the left side of the parent node is smaller than parent as elements on the right side of the parent node is greater than parent Repeat steps 1~4 for each partition recursively until subarray can not be partitioned (only one element) Process In Partition step set the last element arr[len(arr)-1] to be pivot set pointer pointing to values smaller than pivot: left_pt =0 and pointer pointing to values greater than pivot: right_pt = len(arr)-2 Loop until reaching terminal state: left_pt&gt; right_pt left_pt++ until it finds arr[left_pt]&gt;pivot right_pt– until it finds arr[right_pt]&lt; pivot swap arr[left_pt] and arr[right_pt] after reaching terminal, swap pivot and arr[left_pt] such that pivot is at the correct position. Pseudo Code 12345678910111213141516171819202122partition(array, low, high): // check if input range is valid if begin-end &lt; 1: return end pivot = array[high] left_pt = low right_pt = high-1 //move left_pt to right, right_pt to left until left_pt&gt;right_pt //when left_pt&gt;right_pt, we know array[left_pt] &gt;= array[right_pt] //then we need to swap pivot with either array[left_pt] or array[right_pt] to set the boundary Loop if left_pt&lt;= right_pt: Loop if left_pt&lt; len(array) and array[left_pt] &lt;= pivot: left_pt++ Loop if right_pt&gt; 0 and array[right_pt]&gt;= pivot: right_pt-- if left_pt&lt; right_pt: swap array[left_pt] and array[right_pt] if array[left_pt] &gt; pivot swap array[left_pt], pivot return left_pt Note: array may have multiple values as same as pivot. In order to iterate each element in position 0~n-1, need to use &lt;= pivot, and &gt;= pivot to avoid trapping at some positions that doesn’t satisfy terminal conditioneg. arr[left_pt] a[1] arr[right_pt] arr[pivot] 2 1 2 2 In this case, if we use “&lt; pivot” and “&gt;pivot” rather than “&gt;=”,”&lt;=”, then left_pt and right_pt won’t check “1” in the array. __Need to compare array[left_pt] and pivot__, since when the input array has only 2 element, then left_pt = right_pt, it directly skips the loop and swap data without comparing the values, this could be wrong. Pseudo Code 12QuickSort( array): return quicksort(array, 0, len(array)-1) 1234567891011quicksort(array, begin,end) //Check if array is empty or length &gt;1 if end-begin&lt;1: return array // find the correct position of pivot mid = Partition(array, 0, len(array)-1) // sort the subarray with elements smaller than pivot array = quicksort(array, 0, mid-1) // sort the subarray with elements greater than pivot array = quicksort(array, mid+1,len(array)-1) return array Python Code 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748class Solution(object): def quickSort(self, array): \"\"\" input: int[] array return: int[] \"\"\" # write your solution here if array == None or len(array)==0: return array begin = 0 end = len(array)-1 return self.quick_sort(array, begin,end) def quick_sort(self, array, begin, end): # Check base case if begin &gt;=end: return array #pre-order operation # partition l_begin, l_end, r_begin, r_end = self.partition(array, begin, end) # recursively go to left and right sub-array array = self.quick_sort(array, l_begin, l_end) array = self.quick_sort(array, r_begin, r_end) return array def partition(self, array, begin, end): # check base case if begin &gt;= end: return begin, begin, begin,begin # choose pivot import random # randomly choose a pivot in array # then move the pivot to the end of the array, pivot=end pivot = random.randint(begin, end) array[pivot], array[end] = array[end], array[pivot] pivot = end stored_index = begin for i in range(begin, end+1): if array[i] &lt; array[pivot]: array[i], array[stored_index] = array[stored_index], array[i] stored_index += 1 array[pivot] , array[stored_index] = array[stored_index], array[pivot] l_begin = begin l_end = stored_index-1 r_begin = stored_index +1 r_end = end return l_begin, l_end, r_begin, r_end Complexity Memory Complexity: O(1) if we don’t consider the memory in stack. Otherwise, O(logn) Time Complexity: best case/average case；O(nlogn) worst case: O(n^2), since it is possible that every time we select the pivot of sub-array is the largest element of this sub-array, which lead to the case that the recursion tree becomes linked list.When the Recursion tree becomes linked list, time complexity becomes O(nh) = O(nn) rather than O(nh) = O(nlogn), where h is the height of tree. To solve this problem, we can use random selection method to select the pivot, rather than always pick the last element as pivot Note when it always picks the greatest/ smallest value as pivot or the array is already sorted, it comes to the worst case O(n^2) since the recursion tree becomes a list-like data structure with depth of recursion tree = O(n) and take O(n) to iterate every node and O(n) to sort at each node. Then it comes to O(n*n) time complexity Quick Sort use Pre-Order, Top-down, in-place operation to first partition array and then do recursion Merge Sort use Post-Order, Bottom-up, not in-place method to return array and then merge returned array Merge sort （归并排序）Main Idea：Merge sort is one of the divide and conquer algorithm like quicksort. Its main ideas is that Find the middle of the array Divide the array into two sub-array based on middle Repeat step 1~2 recursively until only 1 element in the array Merge left array and right array and return merged array recusively Example: In the array below, we recurively divide the array into left sub-array 2 and right sub-array 12, then sort the left array first by dividing it until step 4,5. We then merge two elements in step 6. After that, we go back to step 2 to sort the right sub-array. We do the same thing for the whole array. Process Pesudo Code 123456789101112131415161718192021222324252627282930MergeSort( array) check if array is valid array, if not ,return compute left , right boundary merge_subarray(array, left, right)merge_subarray(array, lef, right): check if left &lt; right. If not, return array[left] compute middle = (left+right)//2 // sort left array left_array = merge_subarray(array, left, middle) // sort right array right_array = merge_subarray(array, middle+1, right) // merge and sort left and right arrays sorted_array = merge(left_array, right_array) return sorted_arraymerge(left_array, right_array): use linear method to merge the left array and right array. // Note that since left_array and right array have //been sorted, merging two sorted array actually //lead to linear time complexity O(n) only create new array A left_index = 0 right_index = 0 while left_index &gt;0 and right_index&gt;0: add the min(left_array[left_index], right_array[right_index]) index += 1 Add the remaining elements in array to A return A Python code 12345678910111213141516171819202122232425262728293031323334353637383940414243class Solution(object): def merge_sort(self, array, left, right): if len(array)&lt;2 or left&gt;=right: return [array[left]] middle = left+ (right-left)//2 left_array = self.merge_sort(array, left, middle) right_array = self.merge_sort(array, middle+1, right) sorted_array = self.merge(left_array, right_array) return sorted_array def merge(self, left_array, right_array): if left_array == None or len(left_array) &lt;1: return right_array elif right_array == None or len(right_array) &lt;1: return left_array l_index = 0 r_index = 0 merged_array = [] while l_index &lt; len(left_array) and r_index&lt;len(right_array): if left_array[l_index] &lt; right_array[r_index]: merged_array.append(left_array[l_index]) l_index += 1 else: merged_array.append(right_array[r_index]) r_index += 1 if l_index &lt; len(left_array): merged_array.extend(left_array[l_index:]) elif r_index &lt; len(right_array): merged_array.extend(right_array[r_index:]) return merged_array def mergeSort(self, array): \"\"\" input: int[] array return: int[] \"\"\" # write your solution here if array == None or len(array) &lt;2: return array return self.merge_sort(array, 0, len(array)-1) Complexity Memory Complexity: if consider the memory used in stack during recursion, it is O(logn) Time Complexity: O(nlogn). In each layer of recursion tree, it takes O(n) to merge and sort two sub-arrays at a level of the tree. Since merge sort uses division method, that is, Assume that are n elements to sort and take k steps (depth of the recursion tree) to sort. There are $2^0+2^1+…+2^k = n$ elements. When only consider the largest term, we have $2^k = n$, and $k = O(log_2n) or O(logn)$. Since each level of recursion tree take O(n) to sort array, then have $O(nlogn)$ Compared with quick sort Both merge sort and quick sort use divide and conquer method (分治算法) to divide array and then solve each subset recursively. Hence both of them involve logn time complexity As for memory complexity, merge sort has O(nlogn) while quicksort has memory complexity O(logn). Quicksort is better than mergesort As for time complexity, merge sort has average/worst case time complexity: O(nlogn) while quick sort has O(nlogn) in average case and O(n^2) in worst case. Quick sort is less stable than merge sort. Quicksort and merge sort are much more useful than linear sorting(insertion, bubble, selection) due to its O(nlogn) time complexity without increasing memory complexity Notes: Usually, Recursion method is less optimal than iteration method (using while loop), since recursion method requires stack memory to store and return solved subset back to last sub-problem. Hence memory complexity of recursion is usually O(n) unless there are no memory used in recursion function and reduce n to 1 Recursion method is easy to implement, but usually costs more memory. Selection sort can be viewed as a version of bubble sort with more explicit physic meaning, since it explicitly finds the min/max value and put them to the beginning/end of subarray. Insertion sort compares current element with previous elements while bubble/selection sort compare current element with the next elements In-place algorithm: an algorithm which transforms input using no auxiliary data structure. However a small amount of extra storage space is allowed for auxiliary variables. The input is usually overwritten by the output as the algorithm executes. Source codeFor C++ source code, Please read my github repository here Reference[1] https://www.geeksforgeeks.org/bubble-sort/[2] https://www.geeksforgeeks.org/insertion-sort/?ref=lbp[3] https://www.geeksforgeeks.org/quick-sort/?ref=lbp","link":"/2020/08/25/data-structure-sorting/"},{"title":"KKBox Music Recommendation System","text":"12from google.colab import drivedrive.mount('/content/drive') Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(&quot;/content/drive&quot;, force_remount=True). 1!pip install kaggle Requirement already satisfied: kaggle in /usr/local/lib/python3.6/dist-packages (1.5.9) Requirement already satisfied: python-slugify in /usr/local/lib/python3.6/dist-packages (from kaggle) (4.0.1) Requirement already satisfied: python-dateutil in /usr/local/lib/python3.6/dist-packages (from kaggle) (2.8.1) Requirement already satisfied: six&gt;=1.10 in /usr/local/lib/python3.6/dist-packages (from kaggle) (1.15.0) Requirement already satisfied: certifi in /usr/local/lib/python3.6/dist-packages (from kaggle) (2020.11.8) Requirement already satisfied: urllib3 in /usr/local/lib/python3.6/dist-packages (from kaggle) (1.24.3) Requirement already satisfied: slugify in /usr/local/lib/python3.6/dist-packages (from kaggle) (0.0.1) Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from kaggle) (4.41.1) Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from kaggle) (2.23.0) Requirement already satisfied: text-unidecode&gt;=1.3 in /usr/local/lib/python3.6/dist-packages (from python-slugify-&gt;kaggle) (1.3) Requirement already satisfied: idna&lt;3,&gt;=2.5 in /usr/local/lib/python3.6/dist-packages (from requests-&gt;kaggle) (2.10) Requirement already satisfied: chardet&lt;4,&gt;=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests-&gt;kaggle) (3.0.4) 1!cp /content/drive/My\\ Drive/Colab\\ Notebooks/KKBox-MusicRecommendationSystem/kaggle.json . 1234567# !!chmod 600 kaggle.json!mkdir -p ~/.kaggle!cp kaggle.json ~/.kaggle/!ls ~/.kaggle!mkdir -p ./kaggle/!chmod 600 /root/.kaggle/kaggle.json!kaggle competitions download kkbox-music-recommendation-challenge kaggle.json Warning: Looks like you're using an outdated API Version, please consider updating (server 1.5.9 / client 1.5.4) song_extra_info.csv.7z: Skipping, found more recently modified local copy (use --force to force download) train.csv.7z: Skipping, found more recently modified local copy (use --force to force download) test.csv.7z: Skipping, found more recently modified local copy (use --force to force download) songs.csv.7z: Skipping, found more recently modified local copy (use --force to force download) members.csv.7z: Skipping, found more recently modified local copy (use --force to force download) sample_submission.csv.7z: Skipping, found more recently modified local copy (use --force to force download) 123456789101112!mkdir kaggle/working!mkdir kaggle/working/train!mkdir kaggle/working/train/data!apt-get install p7zip!apt-get install p7zip-full !7za e members.csv.7z !7za e songs.csv.7z !7za e song_extra_info.csv.7z !7za e train.csv.7z !7za e sample_submission.csv.7z !7za e test.csv.7z !mv *.csv kaggle/working/train/data mkdir: cannot create directory ‘kaggle/working’: File exists mkdir: cannot create directory ‘kaggle/working/train’: File exists mkdir: cannot create directory ‘kaggle/working/train/data’: File exists Reading package lists... Done Building dependency tree Reading state information... Done p7zip is already the newest version (16.02+dfsg-6). 0 upgraded, 0 newly installed, 0 to remove and 14 not upgraded. Reading package lists... Done Building dependency tree Reading state information... Done p7zip-full is already the newest version (16.02+dfsg-6). 0 upgraded, 0 newly installed, 0 to remove and 14 not upgraded. 7-Zip (a) [64] 16.02 : Copyright (c) 1999-2016 Igor Pavlov : 2016-05-21 p7zip Version 16.02 (locale=en_US.UTF-8,Utf16=on,HugeFiles=on,64 bits,2 CPUs AMD EPYC 7B12 (830F10),ASM,AES-NI) Scanning the drive for archives: 0M Scan\b\b\b\b\b\b\b\b\b \b\b\b\b\b\b\b\b\b1 file, 1349856 bytes (1319 KiB) Extracting archive: members.csv.7z -- Path = members.csv.7z Type = 7z Physical Size = 1349856 Headers Size = 130 Method = LZMA2:3m Solid = - Blocks = 1 0%\b\b\b\b \b\b\b\bEverything is Ok Size: 2503827 Compressed: 1349856 7-Zip (a) [64] 16.02 : Copyright (c) 1999-2016 Igor Pavlov : 2016-05-21 p7zip Version 16.02 (locale=en_US.UTF-8,Utf16=on,HugeFiles=on,64 bits,2 CPUs AMD EPYC 7B12 (830F10),ASM,AES-NI) Scanning the drive for archives: 0M Scan\b\b\b\b\b\b\b\b\b \b\b\b\b\b\b\b\b\b1 file, 105809525 bytes (101 MiB) Extracting archive: songs.csv.7z -- Path = songs.csv.7z Type = 7z Physical Size = 105809525 Headers Size = 122 Method = LZMA2:24 Solid = - Blocks = 1 0%\b\b\b\b \b\b\b\b 2% - songs.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 5% - songs.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 8% - songs.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 11% - songs.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 14% - songs.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 17% - songs.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 20% - songs.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 24% - songs.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 27% - songs.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 30% - songs.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 33% - songs.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 35% - songs.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 39% - songs.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 41% - songs.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 45% - songs.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 49% - songs.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 51% - songs.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 54% - songs.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 57% - songs.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 60% - songs.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 62% - songs.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 65% - songs.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 68% - songs.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 70% - songs.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 73% - songs.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 76% - songs.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 79% - songs.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 82% - songs.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 85% - songs.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 87% - songs.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 90% - songs.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 92% - songs.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 95% - songs.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 98% - songs.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bEverything is Ok Size: 221828666 Compressed: 105809525 7-Zip (a) [64] 16.02 : Copyright (c) 1999-2016 Igor Pavlov : 2016-05-21 p7zip Version 16.02 (locale=en_US.UTF-8,Utf16=on,HugeFiles=on,64 bits,2 CPUs AMD EPYC 7B12 (830F10),ASM,AES-NI) Scanning the drive for archives: 0M Scan\b\b\b\b\b\b\b\b\b \b\b\b\b\b\b\b\b\b1 file, 103608205 bytes (99 MiB) Extracting archive: song_extra_info.csv.7z -- Path = song_extra_info.csv.7z Type = 7z Physical Size = 103608205 Headers Size = 140 Method = LZMA:25 Solid = - Blocks = 1 0%\b\b\b\b \b\b\b\b 1% - song_extra_info.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 4% - song_extra_info.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 6% - song_extra_info.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 9% - song_extra_info.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 11% - song_extra_info.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 13% - song_extra_info.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 16% - song_extra_info.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 18% - song_extra_info.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 20% - song_extra_info.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 23% - song_extra_info.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 25% - song_extra_info.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 27% - song_extra_info.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 30% - song_extra_info.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 32% - song_extra_info.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 34% - song_extra_info.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 37% - song_extra_info.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 39% - song_extra_info.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 41% - song_extra_info.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 44% - song_extra_info.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 46% - song_extra_info.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 48% - song_extra_info.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 50% - song_extra_info.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 53% - song_extra_info.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 55% - song_extra_info.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 57% - song_extra_info.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 60% - song_extra_info.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 62% - song_extra_info.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 65% - song_extra_info.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 68% - song_extra_info.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 71% - song_extra_info.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 74% - song_extra_info.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 76% - song_extra_info.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 78% - song_extra_info.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 81% - song_extra_info.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 83% - song_extra_info.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 85% - song_extra_info.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 88% - song_extra_info.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 92% - song_extra_info.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 95% - song_extra_info.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 99% - song_extra_info.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bEverything is Ok Size: 181010294 Compressed: 103608205 7-Zip (a) [64] 16.02 : Copyright (c) 1999-2016 Igor Pavlov : 2016-05-21 p7zip Version 16.02 (locale=en_US.UTF-8,Utf16=on,HugeFiles=on,64 bits,2 CPUs AMD EPYC 7B12 (830F10),ASM,AES-NI) Scanning the drive for archives: 0M Scan\b\b\b\b\b\b\b\b\b \b\b\b\b\b\b\b\b\b1 file, 106420688 bytes (102 MiB) Extracting archive: train.csv.7z -- Path = train.csv.7z Type = 7z Physical Size = 106420688 Headers Size = 122 Method = LZMA2:24 Solid = - Blocks = 1 0%\b\b\b\b \b\b\b\b 2% - train.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 4% - train.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 6% - train.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 9% - train.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 10% - train.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 13% - train.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 14% - train.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 17% - train.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 19% - train.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 21% - train.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 23% - train.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 25% - train.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 27% - train.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 29% - train.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 31% - train.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 33% - train.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 34% - train.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 36% - train.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 38% - train.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 41% - train.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 43% - train.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 46% - train.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 48% - train.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 50% - train.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 52% - train.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 54% - train.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 56% - train.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 58% - train.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 59% - train.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 62% - train.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 65% - train.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 68% - train.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 70% - train.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 73% - train.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 75% - train.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 76% - train.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 78% - train.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 80% - train.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 82% - train.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 84% - train.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 86% - train.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 88% - train.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 90% - train.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 91% - train.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 93% - train.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 95% - train.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 97% - train.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 99% - train.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bEverything is Ok Size: 971675848 Compressed: 106420688 7-Zip (a) [64] 16.02 : Copyright (c) 1999-2016 Igor Pavlov : 2016-05-21 p7zip Version 16.02 (locale=en_US.UTF-8,Utf16=on,HugeFiles=on,64 bits,2 CPUs AMD EPYC 7B12 (830F10),ASM,AES-NI) Scanning the drive for archives: 0M Scan\b\b\b\b\b\b\b\b\b \b\b\b\b\b\b\b\b\b1 file, 463688 bytes (453 KiB) Extracting archive: sample_submission.csv.7z -- Path = sample_submission.csv.7z Type = 7z Physical Size = 463688 Headers Size = 146 Method = LZMA2:24 Solid = - Blocks = 1 0%\b\b\b\b \b\b\b\bEverything is Ok Size: 29570380 Compressed: 463688 7-Zip (a) [64] 16.02 : Copyright (c) 1999-2016 Igor Pavlov : 2016-05-21 p7zip Version 16.02 (locale=en_US.UTF-8,Utf16=on,HugeFiles=on,64 bits,2 CPUs AMD EPYC 7B12 (830F10),ASM,AES-NI) Scanning the drive for archives: 0M Scan\b\b\b\b\b\b\b\b\b \b\b\b\b\b\b\b\b\b1 file, 43925208 bytes (42 MiB) Extracting archive: test.csv.7z -- Path = test.csv.7z Type = 7z Physical Size = 43925208 Headers Size = 122 Method = LZMA2:24 Solid = - Blocks = 1 0%\b\b\b\b \b\b\b\b 4% - test.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 8% - test.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 13% - test.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 18% - test.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 22% - test.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 28% - test.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 34% - test.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 42% - test.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 49% - test.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 55% - test.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 61% - test.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 67% - test.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 74% - test.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 80% - test.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 86% - test.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 90% - test.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 96% - test.csv\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b \b\b\b\b\b\b\b\b\b\b\b\b\b\b\bEverything is Ok Size: 347789925 Compressed: 43925208 1234567891011121314151617# This Python 3 environment comes with many helpful analytics libraries installed# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python# For example, here's several helpful packages to loadimport numpy as np # linear algebraimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)# Input data files are available in the read-only \"../input/\" directory# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directoryimport osfor dirname, _, filenames in os.walk('./kaggle/input'): for filename in filenames: print(os.path.join(dirname, filename))# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save &amp; Run All\" # You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session Use 7z to uncompress the csv files1!ls ./kaggle/working/train/data/ members.csv song_extra_info.csv test.csv sample_submission.csv songs.csv train.csv 1!du -h ./kaggle/working/train/data/ 1.7G ./kaggle/working/train/data/ 1!cat /proc/cpuinfo processor : 0 vendor_id : AuthenticAMD cpu family : 23 model : 49 model name : AMD EPYC 7B12 stepping : 0 microcode : 0x1000065 cpu MHz : 2250.000 cache size : 512 KB physical id : 0 siblings : 2 core id : 0 cpu cores : 1 apicid : 0 initial apicid : 0 fpu : yes fpu_exception : yes cpuid level : 13 wp : yes flags : fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid extd_apicid tsc_known_freq pni pclmulqdq ssse3 fma cx16 sse4_1 sse4_2 movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm cmp_legacy cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw topoext ssbd ibrs ibpb stibp vmmcall fsgsbase tsc_adjust bmi1 avx2 smep bmi2 rdseed adx smap clflushopt clwb sha_ni xsaveopt xsavec xgetbv1 clzero xsaveerptr arat npt nrip_save umip rdpid bugs : sysret_ss_attrs spectre_v1 spectre_v2 spec_store_bypass bogomips : 4500.00 TLB size : 3072 4K pages clflush size : 64 cache_alignment : 64 address sizes : 48 bits physical, 48 bits virtual power management: processor : 1 vendor_id : AuthenticAMD cpu family : 23 model : 49 model name : AMD EPYC 7B12 stepping : 0 microcode : 0x1000065 cpu MHz : 2250.000 cache size : 512 KB physical id : 0 siblings : 2 core id : 0 cpu cores : 1 apicid : 1 initial apicid : 1 fpu : yes fpu_exception : yes cpuid level : 13 wp : yes flags : fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid extd_apicid tsc_known_freq pni pclmulqdq ssse3 fma cx16 sse4_1 sse4_2 movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm cmp_legacy cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw topoext ssbd ibrs ibpb stibp vmmcall fsgsbase tsc_adjust bmi1 avx2 smep bmi2 rdseed adx smap clflushopt clwb sha_ni xsaveopt xsavec xgetbv1 clzero xsaveerptr arat npt nrip_save umip rdpid bugs : sysret_ss_attrs spectre_v1 spectre_v2 spec_store_bypass bogomips : 4500.00 TLB size : 3072 4K pages clflush size : 64 cache_alignment : 64 address sizes : 48 bits physical, 48 bits virtual power management: KKBox-Music Recommendation SystemGoal of this projectIn this project, we are going to build a recommendation system to predict the chances of a user listening to a song repetitively after the first observable listening event within a time window was triggered. If there are recurring listening event(s) triggered within a month after the user’s very first observable listening event, its target is marked 1, and 0 otherwise in the training set. The same rule applies to the testing set. 0. Data Collection and DescriptionThe KKBox dataset is composed of following files: train.csv msno: user id song_id: song id source_system_tab: the name of the tab where the event was triggered. System tabs are used to categorize KKBOX mobile apps functions. For example, tab my library contains functions to manipulate the local storage, and tab search contains functions relating to search. source_screen_name: name of the layout a user sees. source_type: an entry point a user first plays music on mobile apps. An entry point could be album, online-playlist, song .. etc. target: this is the target variable. target=1 means there are recurring listening event(s) triggered within a month after the user’s very first observable listening event, target=0 otherwise . test.csv id: row id (will be used for submission) msno: user id song_id: song id source_system_tab: the name of the tab where the event was triggered. System tabs are used to categorize KKBOX mobile apps functions. For example, tab my library contains functions to manipulate the local storage, and tab search contains functions relating to search. source_screen_name: name of the layout a user sees. source_type: an entry point a user first plays music on mobile apps. An entry point could be album, online-playlist, song .. etc. sample_submission.csv sample submission file in the format that we expect you to submit id: same as id in test.csv target: this is the target variable. target=1 means there are recurring listening event(s) triggered within a month after the user’s very first observable listening event, target=0 otherwise . songs.csv The songs. Note that data is in unicode. song_id song_length: in ms genre_ids: genre category. Some songs have multiple genres and they are separated by | artist_name composer lyricist language members.csv user information. msno city bd: age. Note: this column has outlier values, please use your judgement. gender registered_via: registration method registration_init_time: format %Y%m%d expiration_date: format %Y%m%d song_extra_info.csv song_id song name - the name of the song. isrc - International Standard Recording Code, theoretically can be used as an identity of a song. However, what worth to note is, ISRCs generated from providers have not been officially verified; therefore the information in ISRC, such as country code and reference year, can be misleading/incorrect. Multiple songs could share one ISRC since a single recording could be re-published several times. 1. Data Cleaning and Exploratory Data Analysis (EDA) Find the Description and summary of each CSV file and Determine Null object, categorical attributes, numerical attributes Convert some attribute types to correct data type, like convert string to float, if necessary Handle Missing values Plot univariate, bivariate plots to visualize and analyze relationship between attributes and target Analysis Summary in this section 2. Data PreprocessingNote that This section is to give some examples to preprocess data like filling missing values and removing outliers. In order to train models, you should start from step 3 ETL to extract and transform data directly using integrated functions 3. Data Pipeline: Extract, Transformation, Load (ETL)4. Machine Learning Modeling LGBM Boosting machineIn the modeling part, we use LGBM model first, which is a light gradient boosting machine model using tree-based basic models for boosting. Since the dataset is large, 1.9 GB and the number of attributes can increase during transformation, LGBM provides a very fast way to train a machine model, so we try it here. In this part, we try LGBM model with different max_depth of tree: [10, 15,20, 25, 30] and see how max_depth affects the accuracy on prediction Wide and Deep Neural network modelIn addition to LGBM model, we are also interested in trying the Wide and Deep Neural network model since it is one of the popular neural network model in recommendation system and we want to see if this can help us improve the accuracy.In wide and deep model, It first uses a technique called embedding, which projects the sparse categorical features into dense features vectors with smaller dimension and extract the main features. Then it concatenates the embedded vectors with the numerical features together to train a traditional neural network classifier. 5. Model Training and validation In model training and validation step, we split the data set into training set(80% of dataset) and validation set (20% of dataset) and then use them to train and keep track of the performance of models. 6. Model EvaluationIn Model evaluation step, we simply use the validation set to validate the final trained models and then let models make predictions on testset from kaggle and submit predictions to kaggle to see the final evaluation scores. 7. Summary123456789101112#import necessary packages hereimport warningswarnings.filterwarnings('ignore')import numpy as np import pandas as pdimport matplotlib.pyplot as pltimport seaborn as snsimport lightgbm as lgbfrom subprocess import check_output# print(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))np.random.seed(2020) 1. Exploratory Data Analysis12345678root = './kaggle/working/train/data/'# !ls ../input/kkbox-music-recommendation-challengetrain_df = pd.read_csv(root+ \"train.csv\")test_df = pd.read_csv(root+ \"test.csv\")song_df = pd.read_csv(root+ \"songs.csv\")song_extra_df = pd.read_csv(root+ \"song_extra_info.csv\")members_df = pd.read_csv(root+ \"members.csv\")# sample_df = pd.read_csv(root+ \"sample_submission.csv\") 1.1 Take a look to the info and format of data1train_df.info() &lt;class 'pandas.core.frame.DataFrame'&gt; RangeIndex: 7377418 entries, 0 to 7377417 Data columns (total 6 columns): # Column Dtype --- ------ ----- 0 msno object 1 song_id object 2 source_system_tab object 3 source_screen_name object 4 source_type object 5 target int64 dtypes: int64(1), object(5) memory usage: 337.7+ MB 1train_df.count() msno 7377418 song_id 7377418 source_system_tab 7352569 source_screen_name 6962614 source_type 7355879 target 7377418 dtype: int64 We can see that attributes: source_system_tab, source_screen_name, source_type contain missing values1train_df.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } msno song_id source_system_tab source_screen_name source_type target 0 FGtllVqz18RPiwJj/edr2gV78zirAiY/9SmYvia+kCg= BBzumQNXUHKdEBOB7mAJuzok+IJA1c2Ryg/yzTF6tik= explore Explore online-playlist 1 1 Xumu+NIjS6QYVxDS4/t3SawvJ7viT9hPKXmf0RtLNx8= bhp/MpSNoqoxOIB+/l8WPqu6jldth4DIpCm3ayXnJqM= my library Local playlist more local-playlist 1 2 Xumu+NIjS6QYVxDS4/t3SawvJ7viT9hPKXmf0RtLNx8= JNWfrrC7zNN7BdMpsISKa4Mw+xVJYNnxXh3/Epw7QgY= my library Local playlist more local-playlist 1 3 Xumu+NIjS6QYVxDS4/t3SawvJ7viT9hPKXmf0RtLNx8= 2A87tzfnJTSWqD7gIZHisolhe4DMdzkbd6LzO1KHjNs= my library Local playlist more local-playlist 1 4 FGtllVqz18RPiwJj/edr2gV78zirAiY/9SmYvia+kCg= 3qm6XTZ6MOCU11x8FIVbAGH5l5uMkT3/ZalWG1oo2Gc= explore Explore online-playlist 1 1test_df.info() &lt;class 'pandas.core.frame.DataFrame'&gt; RangeIndex: 2556790 entries, 0 to 2556789 Data columns (total 6 columns): # Column Dtype --- ------ ----- 0 id int64 1 msno object 2 song_id object 3 source_system_tab object 4 source_screen_name object 5 source_type object dtypes: int64(1), object(5) memory usage: 117.0+ MB 1test_df.count() id 2556790 msno 2556790 song_id 2556790 source_system_tab 2548348 source_screen_name 2393907 source_type 2549493 dtype: int64 In test dataset, We can see that attributes that contain missing values: source_system_tab source_screen_name source_type 1test_df.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } id msno song_id source_system_tab source_screen_name source_type 0 0 V8ruy7SGk7tDm3zA51DPpn6qutt+vmKMBKa21dp54uM= WmHKgKMlp1lQMecNdNvDMkvIycZYHnFwDT72I5sIssc= my library Local playlist more local-library 1 1 V8ruy7SGk7tDm3zA51DPpn6qutt+vmKMBKa21dp54uM= y/rsZ9DC7FwK5F2PK2D5mj+aOBUJAjuu3dZ14NgE0vM= my library Local playlist more local-library 2 2 /uQAlrAkaczV+nWCd2sPF2ekvXPRipV7q0l+gbLuxjw= 8eZLFOdGVdXBSqoAv5nsLigeH2BvKXzTQYtUM53I0k4= discover NaN song-based-playlist 3 3 1a6oo/iXKatxQx4eS9zTVD+KlSVaAFbTIqVvwLC1Y0k= ztCf8thYsS4YN3GcIL/bvoxLm/T5mYBVKOO4C9NiVfQ= radio Radio radio 4 4 1a6oo/iXKatxQx4eS9zTVD+KlSVaAFbTIqVvwLC1Y0k= MKVMpslKcQhMaFEgcEQhEfi5+RZhMYlU3eRDpySrH8Y= radio Radio radio 1song_df.info() &lt;class 'pandas.core.frame.DataFrame'&gt; RangeIndex: 2296320 entries, 0 to 2296319 Data columns (total 7 columns): # Column Dtype --- ------ ----- 0 song_id object 1 song_length int64 2 genre_ids object 3 artist_name object 4 composer object 5 lyricist object 6 language float64 dtypes: float64(1), int64(1), object(5) memory usage: 122.6+ MB 1song_df.count() song_id 2296320 song_length 2296320 genre_ids 2202204 artist_name 2296320 composer 1224966 lyricist 351052 language 2296319 dtype: int64 Attributes that contain missing values are composer lyricist genre_ids language 1song_df.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } song_id song_length genre_ids artist_name composer lyricist language 0 CXoTN1eb7AI+DntdU1vbcwGRV4SCIDxZu+YD8JP8r4E= 247640 465 張信哲 (Jeff Chang) 董貞 何啟弘 3.0 1 o0kFgae9QtnYgRkVPqLJwa05zIhRlUjfF7O1tDw0ZDU= 197328 444 BLACKPINK TEDDY| FUTURE BOUNCE| Bekuh BOOM TEDDY 31.0 2 DwVvVurfpuz+XPuFvucclVQEyPqcpUkHR0ne1RQzPs0= 231781 465 SUPER JUNIOR NaN NaN 31.0 3 dKMBWoZyScdxSkihKG+Vf47nc18N9q4m58+b4e7dSSE= 273554 465 S.H.E 湯小康 徐世珍 3.0 4 W3bqWd3T+VeHFzHAUfARgW9AvVRaF4N5Yzm4Mr6Eo/o= 140329 726 貴族精選 Traditional Traditional 52.0 1song_extra_df.info() &lt;class 'pandas.core.frame.DataFrame'&gt; RangeIndex: 2295971 entries, 0 to 2295970 Data columns (total 3 columns): # Column Dtype --- ------ ----- 0 song_id object 1 name object 2 isrc object dtypes: object(3) memory usage: 52.6+ MB 1song_extra_df.count() song_id 2295971 name 2295969 isrc 2159423 dtype: int64 1song_extra_df.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } song_id name isrc 0 LP7pLJoJFBvyuUwvu+oLzjT+bI+UeBPURCecJsX1jjs= 我們 TWUM71200043 1 ClazTFnk6r0Bnuie44bocdNMM3rdlrq0bCGAsGUWcHE= Let Me Love You QMZSY1600015 2 u2ja/bZE3zhCGxvbbOB3zOoUjx27u40cf5g09UXMoKQ= 原諒我 TWA530887303 3 92Fqsy0+p6+RHe2EoLKjHahORHR1Kq1TBJoClW9v+Ts= Classic USSM11301446 4 0QFmz/+rJy1Q56C1DuYqT9hKKqi5TUqx0sN0IwvoHrw= 愛投羅網 TWA471306001 1members_df.info() &lt;class 'pandas.core.frame.DataFrame'&gt; RangeIndex: 34403 entries, 0 to 34402 Data columns (total 7 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 msno 34403 non-null object 1 city 34403 non-null int64 2 bd 34403 non-null int64 3 gender 14501 non-null object 4 registered_via 34403 non-null int64 5 registration_init_time 34403 non-null int64 6 expiration_date 34403 non-null int64 dtypes: int64(5), object(2) memory usage: 1.8+ MB 1train_df['song_id'].head() 0 BBzumQNXUHKdEBOB7mAJuzok+IJA1c2Ryg/yzTF6tik= 1 bhp/MpSNoqoxOIB+/l8WPqu6jldth4DIpCm3ayXnJqM= 2 JNWfrrC7zNN7BdMpsISKa4Mw+xVJYNnxXh3/Epw7QgY= 3 2A87tzfnJTSWqD7gIZHisolhe4DMdzkbd6LzO1KHjNs= 4 3qm6XTZ6MOCU11x8FIVbAGH5l5uMkT3/ZalWG1oo2Gc= Name: song_id, dtype: object 1song_df['song_id'].head() 0 CXoTN1eb7AI+DntdU1vbcwGRV4SCIDxZu+YD8JP8r4E= 1 o0kFgae9QtnYgRkVPqLJwa05zIhRlUjfF7O1tDw0ZDU= 2 DwVvVurfpuz+XPuFvucclVQEyPqcpUkHR0ne1RQzPs0= 3 dKMBWoZyScdxSkihKG+Vf47nc18N9q4m58+b4e7dSSE= 4 W3bqWd3T+VeHFzHAUfARgW9AvVRaF4N5Yzm4Mr6Eo/o= Name: song_id, dtype: object 123print(\"Unique Song amount in trainset:\",train_df['song_id'].nunique())print(\"Unique Song amount in testset:\", test_df['song_id'].nunique())print(\"Unique Song amount in song list:\",song_df['song_id'].nunique()) Unique Song amount in trainset: 359966 Unique Song amount in testset: 224753 Unique Song amount in song list: 2296320 1.2 Explore Song information12345# Merge two dataframe based on song_id so that we can analyze the song information together with training datauser_music_df = train_df.merge(song_df,on='song_id',how=\"left\", copy =False)user_music_df[\"song_id\"] = user_music_df[\"song_id\"].astype(\"category\")user_music_df.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } msno song_id source_system_tab source_screen_name source_type target song_length genre_ids artist_name composer lyricist language 0 FGtllVqz18RPiwJj/edr2gV78zirAiY/9SmYvia+kCg= BBzumQNXUHKdEBOB7mAJuzok+IJA1c2Ryg/yzTF6tik= explore Explore online-playlist 1 206471.0 359 Bastille Dan Smith| Mark Crew NaN 52.0 1 Xumu+NIjS6QYVxDS4/t3SawvJ7viT9hPKXmf0RtLNx8= bhp/MpSNoqoxOIB+/l8WPqu6jldth4DIpCm3ayXnJqM= my library Local playlist more local-playlist 1 284584.0 1259 Various Artists NaN NaN 52.0 2 Xumu+NIjS6QYVxDS4/t3SawvJ7viT9hPKXmf0RtLNx8= JNWfrrC7zNN7BdMpsISKa4Mw+xVJYNnxXh3/Epw7QgY= my library Local playlist more local-playlist 1 225396.0 1259 Nas N. Jones、W. Adams、J. Lordan、D. Ingle NaN 52.0 3 Xumu+NIjS6QYVxDS4/t3SawvJ7viT9hPKXmf0RtLNx8= 2A87tzfnJTSWqD7gIZHisolhe4DMdzkbd6LzO1KHjNs= my library Local playlist more local-playlist 1 255512.0 1019 Soundway Kwadwo Donkoh NaN -1.0 4 FGtllVqz18RPiwJj/edr2gV78zirAiY/9SmYvia+kCg= 3qm6XTZ6MOCU11x8FIVbAGH5l5uMkT3/ZalWG1oo2Gc= explore Explore online-playlist 1 187802.0 1011 Brett Young Brett Young| Kelly Archer| Justin Ebach NaN 52.0 1user_music_df['song_id'].nunique(), user_music_df['genre_ids'].nunique() (359966, 572) 1user_music_df.count() msno 7377418 song_id 7377418 source_system_tab 7352569 source_screen_name 6962614 source_type 7355879 target 7377418 song_length 7377304 genre_ids 7258963 artist_name 7377304 composer 5701712 lyricist 4198620 language 7377268 dtype: int64 1234567# plot the top-20 frequent genre_ids df_genre = user_music_df.sample(n=5000)df_genre = df_genre[\"genre_ids\"].value_counts().sort_values(ascending=False)[:20]df_genre = df_genre.sort_values(ascending=True)ax = df_genre.plot.barh(figsize=(15,8))ax.set_ylabel(\"song genre ids\")ax.set_xlabel(\"Count\") Text(0.5, 0, 'Count') 12345678910111213#selec the top-20 frequent artist_namedf_artist = user_music_df[\"artist_name\"].value_counts().sort_values(ascending=False)[:20]#plot in descending order in horizonal directiondf_artist = df_artist.sort_values(ascending=True)ax = df_artist.plot.barh(figsize=(15,10))ax.set_ylabel(\"song artist_name\")ax.set_xlabel(\"Count\")# artist_name # composer # lyricist Text(0.5, 0, 'Count') 1df_artist.head(10) The Chainsmokers 44215 梁靜茹 (Fish Leong) 44290 丁噹 (Della) 45762 楊丞琳 (Rainie Yang) 46006 蘇打綠 (Sodagreen) 47177 蔡依林 (Jolin Tsai) 49055 Eric 周興哲 49426 A-Lin 52913 Maroon 5 55151 謝和弦 (R-chord) 57040 Name: artist_name, dtype: int64 12345678fig, ax = plt.subplots(1, figsize=(15,8))df_composer = user_music_df[\"composer\"].value_counts().sort_values(ascending=False)[:20]ax = sns.barplot([i for i in df_composer.index],df_composer,ax= ax)ax.set_xlabel(\"song composer\")ax.set_ylabel(\"Count\") Text(0, 0.5, 'Count') 1df_composer.head(20).index Index(['周杰倫', '阿信', '林俊傑', '陳皓宇', 'JJ Lin', '張簡君偉', 'Eric Chou', '韋禮安', '八三夭 阿璞', 'R-chord', '怪獸', '吳青峰', '周湯豪', 'G.E.M. 鄧紫棋', '陳小霞', 'JerryC', '吳克群', '薛之謙', 'Rocoberry', '李榮浩'], dtype='object') Analyse the relationship between target and song 123456789101112131415161718192021222324252627fig, ax = plt.subplots(3,1,figsize=(15,18))# df = user_music_df[['source_system_tab','source_screen_name','source_type']]# df['source_system_tab'].value_counts().plot.bar(rot=20,ax=ax[0])# ax[0].set_xlabel(\"source_system_tab\")# ax[0].set_ylabel(\"count\")# df['source_screen_name'].value_counts().plot.bar(rot=30,ax=ax[1])# ax[1].set_xlabel(\"source_screen_name\")# ax[1].set_ylabel(\"count\")# df['source_type'].value_counts().plot.bar(rot=20,ax=ax[2])# ax[2].set_xlabel(\"source_type\")# ax[2].set_ylabel(\"count\")sns.countplot(y= 'source_system_tab',hue='target', order = user_music_df['source_system_tab'].value_counts().index, data=user_music_df,dodge=True, ax= ax[0])sns.countplot(y= 'source_screen_name',hue='target', order = user_music_df['source_screen_name'].value_counts().index, data=user_music_df,dodge=True, ax= ax[1])sns.countplot(y= 'source_type',hue='target', order = user_music_df['source_type'].value_counts().index, data=user_music_df,dodge=True, ax= ax[2]) &lt;matplotlib.axes._subplots.AxesSubplot at 0x7f269a90e128&gt; We can see that local library and local playlist are the main sources that users repeat playing music and Most of users more prefer to play music from local library than to play music onlineAnalyze Relationship between Target and members info1members_df.info() &lt;class 'pandas.core.frame.DataFrame'&gt; RangeIndex: 34403 entries, 0 to 34402 Data columns (total 7 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 msno 34403 non-null object 1 city 34403 non-null int64 2 bd 34403 non-null int64 3 gender 14501 non-null object 4 registered_via 34403 non-null int64 5 registration_init_time 34403 non-null int64 6 expiration_date 34403 non-null int64 dtypes: int64(5), object(2) memory usage: 1.8+ MB 12members_df[\"registration_init_time\"] = pd.to_datetime(members_df[\"registration_init_time\"], format=\"%Y%m%d\")members_df[\"expiration_date\"] = pd.to_datetime(members_df[\"expiration_date\"], format=\"%Y%m%d\") Parse the datetime data12345678members_df[\"registration_init_day\"] = members_df[\"registration_init_time\"].dt.daymembers_df[\"registration_init_month\"] = members_df[\"registration_init_time\"].dt.monthmembers_df[\"registration_init_year\"] = members_df[\"registration_init_time\"].dt.yearmembers_df[\"expiration_day\"] = members_df[\"expiration_date\"].dt.daymembers_df[\"expiration_month\"] = members_df[\"expiration_date\"].dt.monthmembers_df[\"expiration_year\"] = members_df[\"expiration_date\"].dt.yearmembers_df = members_df.drop(columns = [\"registration_init_time\", \"expiration_date\"],axis=1)members_df.info() &lt;class 'pandas.core.frame.DataFrame'&gt; RangeIndex: 34403 entries, 0 to 34402 Data columns (total 11 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 msno 34403 non-null object 1 city 34403 non-null int64 2 bd 34403 non-null int64 3 gender 14501 non-null object 4 registered_via 34403 non-null int64 5 registration_init_day 34403 non-null int64 6 registration_init_month 34403 non-null int64 7 registration_init_year 34403 non-null int64 8 expiration_day 34403 non-null int64 9 expiration_month 34403 non-null int64 10 expiration_year 34403 non-null int64 dtypes: int64(9), object(2) memory usage: 2.9+ MB 123456member_music_df = user_music_df.merge(members_df,on='msno',how=\"left\", copy=False)#after merging, the axis used to merge becomes object type,so need to convert it back to category typemember_music_df[\"msno\"] = member_music_df[\"msno\"].astype(\"category\")member_music_df[\"song_id\"] = member_music_df[\"song_id\"].astype(\"category\")member_music_df.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } msno song_id source_system_tab source_screen_name source_type target song_length genre_ids artist_name composer lyricist language city bd gender registered_via registration_init_day registration_init_month registration_init_year expiration_day expiration_month expiration_year 0 FGtllVqz18RPiwJj/edr2gV78zirAiY/9SmYvia+kCg= BBzumQNXUHKdEBOB7mAJuzok+IJA1c2Ryg/yzTF6tik= explore Explore online-playlist 1 206471.0 359 Bastille Dan Smith| Mark Crew NaN 52.0 1 0 NaN 7 2 1 2012 5 10 2017 1 Xumu+NIjS6QYVxDS4/t3SawvJ7viT9hPKXmf0RtLNx8= bhp/MpSNoqoxOIB+/l8WPqu6jldth4DIpCm3ayXnJqM= my library Local playlist more local-playlist 1 284584.0 1259 Various Artists NaN NaN 52.0 13 24 female 9 25 5 2011 11 9 2017 2 Xumu+NIjS6QYVxDS4/t3SawvJ7viT9hPKXmf0RtLNx8= JNWfrrC7zNN7BdMpsISKa4Mw+xVJYNnxXh3/Epw7QgY= my library Local playlist more local-playlist 1 225396.0 1259 Nas N. Jones、W. Adams、J. Lordan、D. Ingle NaN 52.0 13 24 female 9 25 5 2011 11 9 2017 3 Xumu+NIjS6QYVxDS4/t3SawvJ7viT9hPKXmf0RtLNx8= 2A87tzfnJTSWqD7gIZHisolhe4DMdzkbd6LzO1KHjNs= my library Local playlist more local-playlist 1 255512.0 1019 Soundway Kwadwo Donkoh NaN -1.0 13 24 female 9 25 5 2011 11 9 2017 4 FGtllVqz18RPiwJj/edr2gV78zirAiY/9SmYvia+kCg= 3qm6XTZ6MOCU11x8FIVbAGH5l5uMkT3/ZalWG1oo2Gc= explore Explore online-playlist 1 187802.0 1011 Brett Young Brett Young| Kelly Archer| Justin Ebach NaN 52.0 1 0 NaN 7 2 1 2012 5 10 2017 1member_music_df.info() &lt;class 'pandas.core.frame.DataFrame'&gt; Int64Index: 7377418 entries, 0 to 7377417 Data columns (total 22 columns): # Column Dtype --- ------ ----- 0 msno category 1 song_id category 2 source_system_tab object 3 source_screen_name object 4 source_type object 5 target int64 6 song_length float64 7 genre_ids object 8 artist_name object 9 composer object 10 lyricist object 11 language float64 12 city int64 13 bd int64 14 gender object 15 registered_via int64 16 registration_init_day int64 17 registration_init_month int64 18 registration_init_year int64 19 expiration_day int64 20 expiration_month int64 21 expiration_year int64 dtypes: category(2), float64(2), int64(10), object(8) memory usage: 1.2+ GB 1member_music_df.count() msno 7377418 song_id 7377418 source_system_tab 7352569 source_screen_name 6962614 source_type 7355879 target 7377418 song_length 7377304 genre_ids 7258963 artist_name 7377304 composer 5701712 lyricist 4198620 language 7377268 city 7377418 bd 7377418 gender 4415939 registered_via 7377418 registration_init_day 7377418 registration_init_month 7377418 registration_init_year 7377418 expiration_day 7377418 expiration_month 7377418 expiration_year 7377418 dtype: int64 1member_music_df['bd'].describe() count 7.377418e+06 mean 1.753927e+01 std 2.155447e+01 min -4.300000e+01 25% 0.000000e+00 50% 2.100000e+01 75% 2.900000e+01 max 1.051000e+03 Name: bd, dtype: float64 Visualize distribution of age: bd attributionNote: Since this attribute has outliers, I use remove the data that lies outside range [0,100] 123456789fig, ax = plt.subplots(2, figsize= (15,8))age_df = member_music_df['bd'].loc[(member_music_df['bd']&gt;0) &amp; (member_music_df['bd']&lt;100)]age_df.hist(ax = ax[0])ax[0].set_ylabel(\"count\")member_music_df['bd'].loc[(member_music_df['bd']&lt;0) | (member_music_df['bd']&gt;100)].hist(ax = ax[1])ax[1].set_xlabel(\"age\")ax[1].set_ylabel(\"count\") Text(0, 0.5, 'count') We can see that bd/age has outliers outside range [0,100], so we want to replace the incorrect bd with NaN1member_music_df['bd'].loc[(member_music_df['bd']&lt;=0) | (member_music_df['bd']&gt;=100)]= np.nan 12# member_music_df.describe() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } target song_length language city bd registered_via registration_init_day registration_init_month registration_init_year expiration_day expiration_month expiration_year count 7.377418e+06 7.377304e+06 7.377268e+06 7.377418e+06 4.430216e+06 7.377418e+06 7.377418e+06 7.377418e+06 7.377418e+06 7.377418e+06 7.377418e+06 7.377418e+06 mean 5.035171e-01 2.451210e+05 1.860933e+01 7.511399e+00 2.872200e+01 6.794068e+00 1.581532e+01 6.832306e+00 2.012741e+03 1.562338e+01 8.341742e+00 2.017072e+03 std 4.999877e-01 6.734471e+04 2.117681e+01 6.641625e+00 8.634326e+00 2.275774e+00 8.768549e+00 3.700723e+00 3.018861e+00 9.107235e+00 2.511360e+00 3.982536e-01 min 0.000000e+00 1.393000e+03 -1.000000e+00 1.000000e+00 2.000000e+00 3.000000e+00 1.000000e+00 1.000000e+00 2.004000e+03 1.000000e+00 1.000000e+00 1.970000e+03 25% 0.000000e+00 2.147260e+05 3.000000e+00 1.000000e+00 2.300000e+01 4.000000e+00 8.000000e+00 3.000000e+00 2.011000e+03 8.000000e+00 9.000000e+00 2.017000e+03 50% 1.000000e+00 2.418120e+05 3.000000e+00 5.000000e+00 2.700000e+01 7.000000e+00 1.600000e+01 7.000000e+00 2.013000e+03 1.500000e+01 9.000000e+00 2.017000e+03 75% 1.000000e+00 2.721600e+05 5.200000e+01 1.300000e+01 3.300000e+01 9.000000e+00 2.300000e+01 1.000000e+01 2.015000e+03 2.300000e+01 1.000000e+01 2.017000e+03 max 1.000000e+00 1.085171e+07 5.900000e+01 2.200000e+01 9.500000e+01 1.300000e+01 3.100000e+01 1.200000e+01 2.017000e+03 3.100000e+01 1.200000e+01 2.020000e+03 1234#dataframe.corr() is used to find the pairwise correlation of all columns in the dataframe. #Any na values are automatically excluded. For any non-numeric data type columns in the data frame it is ignored.corr_matrix = member_music_df.corr()_ = sns.heatmap(corr_matrix) 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354#print the top threee attributes that have the strongest correlation with \"Target\" and the corresponding correlation coefficients.corr = corr_matrix['target'].sort_values(ascending= False)for x in corr.index[1:4].to_list(): print(\"{} {}\".format(x, corr[x]))print(\"\")#print the top threee attributes that have the strongest correlation with \"song_length\" and the corresponding correlation coefficients.corr = corr_matrix['song_length'].sort_values(ascending= False)for x in corr.index[1:4].to_list(): print(\"{} {}\".format(x, corr[x]))print(\"\")#print the top threee attributes that have the strongest correlation with \"language\" and the corresponding correlation coefficients.corr = corr_matrix['language'].sort_values(ascending= False)for x in corr.index[1:4].to_list(): print(\"{} {}\".format(x, corr[x]))print(\"\")#print the top threee attributes that have the strongest correlation with \"city\" and the corresponding correlation coefficients.corr = corr_matrix['city'].sort_values(ascending= False)for x in corr.index[1:4].to_list(): print(\"{} {}\".format(x, corr[x]))print(\"\")#print the top threee attributes that have the strongest correlation with \"bd\" and the corresponding correlation coefficients.corr = corr_matrix['bd'].sort_values(ascending= False)for x in corr.index[1:4].to_list(): print(\"{} {}\".format(x, corr[x]))print(\"\")#print the top threee attributes that have the strongest correlation with \"registered_via\" and the corresponding correlation coefficients.corr = corr_matrix['registered_via'].sort_values(ascending= False)for x in corr.index[1:4].to_list(): print(\"{} {}\".format(x, corr[x]))print(\"\")corr = corr_matrix['registration_init_day'].sort_values(ascending= False)for x in corr.index[1:4].to_list(): print(\"{} {}\".format(x, corr[x]))print(\"\")corr = corr_matrix['registration_init_month'].sort_values(ascending= False)for x in corr.index[1:4].to_list(): print(\"{} {}\".format(x, corr[x]))print(\"\")corr = corr_matrix['registration_init_year'].sort_values(ascending= False)for x in corr.index[1:4].to_list(): print(\"{} {}\".format(x, corr[x]))print(\"\")corr = corr_matrix['expiration_day'].sort_values(ascending= False)for x in corr.index[1:4].to_list(): print(\"{} {}\".format(x, corr[x]))print(\"\")corr = corr_matrix['expiration_month'].sort_values(ascending= False)for x in corr.index[1:4].to_list(): print(\"{} {}\".format(x, corr[x]))print(\"\")corr = corr_matrix['expiration_year'].sort_values(ascending= False)for x in corr.index[1:4].to_list(): print(\"{} {}\".format(x, corr[x]))print(\"\") expiration_year 0.042248332355979766 city 0.01211438566189457 expiration_month 0.011817072086387569 bd 0.009861302779254176 city 0.005184912771179072 expiration_year 0.00457185870016758 registration_init_year 0.009070490482763404 registration_init_day 0.001510510575428178 bd 0.001107978394135987 expiration_year 0.15014690465127595 registered_via 0.0737556175747622 target 0.01211438566189457 registered_via 0.1753390015877422 expiration_day 0.056335854806629254 expiration_month 0.032935904360496926 bd 0.1753390015877422 expiration_year 0.08413460079453493 city 0.0737556175747622 expiration_day 0.1493505099924221 registration_init_month 0.04443692475737983 registered_via 0.02554331305533987 expiration_month 0.056911114419175665 registration_init_day 0.04443692475737983 bd 0.005399463812914416 language 0.009070490482763404 target -0.00196242388069252 song_length -0.007434856516605977 registration_init_day 0.1493505099924221 registered_via 0.05695618668075027 bd 0.056335854806629254 registered_via 0.0647318000666518 registration_init_month 0.056911114419175665 bd 0.032935904360496926 city 0.15014690465127595 registered_via 0.08413460079453493 target 0.042248332355979766 123456#graphfig, ax = plt.subplots(1,1,figsize=(10,8), sharex=False)plt.scatter(x = member_music_df['city'],y = member_music_df['bd'])ax.set_ylabel(\"bd\")ax.set_xlabel(\"city\")plt.show() 123456#graphfig, ax = plt.subplots(1,1,figsize=(10,8), sharex=False)plt.scatter(x = member_music_df['target'],y = member_music_df['expiration_year'])ax.set_ylabel(\"expiration_year\")ax.set_xlabel(\"target\")plt.show() 12345#user的musiclist里面可能重听的music print(train_df.target.value_counts()*100/train_df.target.value_counts().sum())print('unique songs ',len(train_df.song_id.unique()))#unique() = Return unique values of Series object.#len() to find unqiue sound. 1 50.351708 0 49.648292 Name: target, dtype: float64 unique songs 359966 1234567repeats=train_df[train_df.target==1]song_repeats=repeats.groupby('song_id',as_index=False).msno.count()song_repeats.columns=['song_id','count']##merge together 2 dataframe and create a new dataframesong_repeats=pd.DataFrame(song_repeats).merge(song_df,left_on='song_id',right_on='song_id')print(\"Print top 50 songs repeated\")repeats.song_id.value_counts().head(50) Print top 50 songs repeated reXuGcEWDDCnL0K3Th//3DFG4S1ACSpJMzA+CFipo1g= 10885 T86YHdD4C9JSc274b1IlMkLuNdz4BQRB50fWWE7hx9g= 10556 FynUyq0+drmIARmK1JZ/qcjNZ7DKkqTY6/0O0lTzNUI= 9808 wBTWuHbjdjxnG1lQcbqnK4FddV24rUhuyrYLd9c/hmk= 9411 PgRtmmESVNtWjoZHO5a1r21vIz9sVZmcJJpFCbRa1LI= 9004 U9kojfZSKaiWOW94PKh1Riyv/zUWxmBRmv0XInQWLGw= 8787 YN4T/yvvXtYrBVN8KTnieiQohHL3T9fnzUkbLWcgLro= 8780 M9rAajz4dYuRhZ7jLvf9RRayVA3os61X/XXHEuW4giA= 8403 43Qm2YzsP99P5wm37B1JIhezUcQ/1CDjYlQx6rBbz2U= 8112 J4qKkLIoW7aYACuTupHLAPZYmRp08en1AEux+GSUzdw= 7903 cy10N2j2sdY/X4BDUcMu2Iumfz7pV3tqE5iEaup2yGI= 7725 750RprmFfLV0bymtDH88g24pLZGVi5VpBAI300P6UOA= 7608 IKMFuL0f5Y8c63Hg9BXkeNJjE0z8yf3gMt/tOxF4QNE= 7224 +SstqMwhQPBQFTPBhLKPT642IiBDXzZFwlzsLl4cGXo= 7061 DLBDZhOoW7zd7GBV99bi92ZXYUS26lzV+jJKbHshP5c= 6901 v/3onppBGoSpGsWb8iaCIO8eX5+iacbH5a4ZUhT7N54= 6879 p/yR06j/RQ2J6yGCFL0K+1R06OeG+eXcwxRgOHDo/Tk= 6536 Xpjwi8UAE2Vv9PZ6cZnhc58MCtl3cKZEO1sdAkqJ4mo= 6399 OaEbZ6TJ1NePtNUeEgWsvFLeopkSln9WQu8PBR5B3+A= 6187 BITuBuNyXQydJcjDL2BUnCu4/IXaJg5IPOuycc/4dtY= 6160 BgqjNqzsyCpEGvxyUmktvHC8WO5+FQO/pQTaZ4broMU= 6140 3VkD5ekIf5duJm1hmYTZlXjyl0zqV8wCzuAh3uocfCg= 6012 8Ckw1wek5d6oEsNUoM4P5iag86TaEmyLwdtrckL0Re8= 6003 n+pMhj/jpCnpiUcSDl4k3i9FJODDddEXmpE48/HczTI= 5787 WL4ipO3Mx9pxd4FMs69ha6o9541+fLeOow67Qkrfnro= 5784 /70HjygVDhHsKBoV8mmsBg/WduSgs4+Zg6GfzhUQbdk= 5588 L6w2d0w84FjTvFr+BhMfgu7dZAsGiOqUGmvvxIG3gvQ= 5480 fEAIgFRWmhXmo6m3ukQeqRksZCcO/7CjkqNckRHiVQo= 5460 +Sm75wnBf/sjm/QMUAFx8N+Ae04kWCXGlgH50tTeM6c= 5412 VkDBgh89umc9m6uAEfD6LXngetyGhln4vh/ArCGO0nY= 5361 fCCmIa0Y5m+MCGbQga31MOLTIqi7ddgXvkjFPmfslGw= 5305 +LztcJcPEEwsikk6+K5udm06XJQMzR4+lzavKLUyE0k= 5298 o9HWMBZMeIPnYEpSuscGoORKE44sj3BYOdvGuIi0P68= 5233 QZBm8SOwnEjNfCpgsKBBGPMGET6y6XaQgnJiirspW7I= 5224 ClazTFnk6r0Bnuie44bocdNMM3rdlrq0bCGAsGUWcHE= 5202 wp1gSQ4LlMEF6bzvEaJl8VdHlAj/EJMTJ0ASrXeddbo= 5110 THqGcrzQyUhBn1NI/+Iptc1vKtxBIEg0uA8iaoJnO1Q= 5086 ys+EL8Sok4HC4i7sDY0+slDNGVZ8+uOQi6TQ6g8VSF4= 5012 zHqZ07gn+YvF36FWzv9+y8KiCMhYhdAUS+vSIKY3UZY= 5001 8f/T4ohROj1wa25YHMItOW2/wJhRXZM0+T5/2p86COc= 4982 G/4+VCRLpfjQJ4SAwMDcf+W8PTw0eOBRgFvg4fHUOO8= 4956 KZ5hwP74wRO6kRapVIprwodtNdVD2EVD3hkZmmyXFPk= 4888 MtFK4NN8Kv1k/xPA3wb8SQaP/jWee52FAaC1s9NFsU4= 4813 UQeOwfhcqgEcIwp3cgNiLGW1237Qjpvqzt/asQimVp0= 4778 JA6C0GEK1sSCVbHyqtruH/ARD1NKolYrw7HXy6EVNAc= 4766 8qWeDv6RTv+hYJxW94e7n6HBzHPGPEZW9FuGhj6pPhQ= 4761 35dx60z4m4+Lg+qIS0l2A8vspbthqnpTylWUu51jW+4= 4679 r4lUPUkz3tAgIWaEyrSYVCxX1yz8PnlVuQz+To0Pd+c= 4650 1PR/lVwL4VeYcZjexwBJ2NOSTfgh8JoVxWCunnbJO/8= 4592 7EnDBkQYJpipCyRd9JBsug4iKnfAunUXc14/96cNotg= 4571 Name: song_id, dtype: int64 2. Data PreprocessingNote: This section is to show how to preprocess data. We can also directly start from Step 3 for data extract, transformation and load using integrated transformation function and skip this step if necessary 2.1 Filling missing values12missing_value_cols = [c for c in member_music_df.columns if member_music_df[c].isnull().any()]missing_value_cols ['source_system_tab', 'source_screen_name', 'source_type', 'song_length', 'genre_ids', 'artist_name', 'composer', 'lyricist', 'language', 'bd', 'gender'] 1member_music_df.count() msno 7377418 song_id 7377418 source_system_tab 7352569 source_screen_name 6962614 source_type 7355879 target 7377418 song_length 7377304 genre_ids 7258963 artist_name 7377304 composer 5701712 lyricist 4198620 language 7377268 city 7377418 bd 4430216 gender 4415939 registered_via 7377418 registration_init_day 7377418 registration_init_month 7377418 registration_init_year 7377418 expiration_day 7377418 expiration_month 7377418 expiration_year 7377418 dtype: int64 1234567891011121314151617181920212223242526272829# list of columns with missing values# ['source_system_tab',# 'source_screen_name',# 'source_type',# 'song_length',# 'genre_ids',# 'artist_name',# 'composer',# 'lyricist',# 'language',# 'bd',# 'gender']def fill_missing_value_v1(x): # fill missing values with the most frequent values return x.fillna(x.value_counts().sort_values(ascending=False).index[0]) categorical_ls = ['source_system_tab', 'source_screen_name','source_type','genre_ids','artist_name','composer', 'lyricist','gender']numerical_ls = ['song_length','language','bd']# Fill missing values for index in numerical_ls: member_music_df[index].fillna(member_music_df[index].median(), inplace=True)for index in categorical_ls: member_music_df[index].fillna(\"no_data\", inplace=True) 1member_music_df.count() msno 7377418 song_id 7377418 source_system_tab 7377418 source_screen_name 7377418 source_type 7377418 target 7377418 song_length 7377418 genre_ids 7377418 artist_name 7377418 composer 7377418 lyricist 7377418 language 7377418 city 7377418 bd 7377418 gender 7377418 registered_via 7377418 registration_init_day 7377418 registration_init_month 7377418 registration_init_year 7377418 expiration_day 7377418 expiration_month 7377418 expiration_year 7377418 dtype: int64 12member_music_df[numerical_ls].head(100) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } song_length language bd 0 206471.0 52.0 27.0 1 284584.0 52.0 24.0 2 225396.0 52.0 24.0 3 255512.0 -1.0 24.0 4 187802.0 52.0 27.0 ... ... ... ... 95 333024.0 3.0 27.0 96 288391.0 3.0 46.0 97 279196.0 3.0 46.0 98 240744.0 3.0 46.0 99 221622.0 3.0 46.0 100 rows × 3 columns 12member_music_df[categorical_ls].head(100) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } source_system_tab source_screen_name source_type genre_ids artist_name composer lyricist gender 0 explore Explore online-playlist 359 Bastille Dan Smith| Mark Crew no_data no_data 1 my library Local playlist more local-playlist 1259 Various Artists no_data no_data female 2 my library Local playlist more local-playlist 1259 Nas N. Jones、W. Adams、J. Lordan、D. Ingle no_data female 3 my library Local playlist more local-playlist 1019 Soundway Kwadwo Donkoh no_data female 4 explore Explore online-playlist 1011 Brett Young Brett Young| Kelly Archer| Justin Ebach no_data no_data ... ... ... ... ... ... ... ... ... 95 my library no_data local-library 458 楊乃文 (Naiwen Yang) 黃建為 葛大為 male 96 my library Local playlist more local-library 458 陳奕迅 (Eason Chan) Jun Jie Lin no_data female 97 my library Local playlist more local-library 458 周杰倫 (Jay Chou) 周杰倫 方文山 female 98 my library Local playlist more local-library 465 范瑋琪 (Christine Fan) 非非 非非 female 99 my library Local playlist more local-library 465|1259 玖壹壹 陳皓宇 廖建至|洪瑜鴻 female 100 rows × 8 columns 2.2 Data TransformationWe can see that the columns like genre_ids, composer, lyricist have multiple values in a cell. In this case, the count of genres, composers, lyricist could be useful information as well 1member_music_df.columns Index(['msno', 'song_id', 'source_system_tab', 'source_screen_name', 'source_type', 'target', 'song_length', 'genre_ids', 'artist_name', 'composer', 'lyricist', 'language', 'city', 'bd', 'gender', 'registered_via', 'registration_init_day', 'registration_init_month', 'registration_init_year', 'expiration_day', 'expiration_month', 'expiration_year'], dtype='object') 1member_music_df.genre_ids.nunique(), member_music_df.composer.nunique(), member_music_df.lyricist.nunique() (573, 76065, 33889) 123456789def count_items(x): if x ==\"no_data\": return 0 return sum(map(x.count, ['|', '/', '\\\\', ';',','])) + 1member_music_df['genre_count']= member_music_df['genre_ids'].apply(count_items)member_music_df['composer_count']= member_music_df['composer'].apply(count_items)member_music_df['lyricist_count']= member_music_df['lyricist'].apply(count_items) 1member_music_df.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } msno song_id source_system_tab source_screen_name source_type target song_length genre_ids artist_name composer lyricist language city bd gender registered_via registration_init_day registration_init_month registration_init_year expiration_day expiration_month expiration_year genre_count composer_count lyricist_count 0 FGtllVqz18RPiwJj/edr2gV78zirAiY/9SmYvia+kCg= BBzumQNXUHKdEBOB7mAJuzok+IJA1c2Ryg/yzTF6tik= explore Explore online-playlist 1 206471.0 359 Bastille Dan Smith| Mark Crew no_data 52.0 1 27.0 no_data 7 2 1 2012 5 10 2017 1 2 0 1 Xumu+NIjS6QYVxDS4/t3SawvJ7viT9hPKXmf0RtLNx8= bhp/MpSNoqoxOIB+/l8WPqu6jldth4DIpCm3ayXnJqM= my library Local playlist more local-playlist 1 284584.0 1259 Various Artists no_data no_data 52.0 13 24.0 female 9 25 5 2011 11 9 2017 1 0 0 2 Xumu+NIjS6QYVxDS4/t3SawvJ7viT9hPKXmf0RtLNx8= JNWfrrC7zNN7BdMpsISKa4Mw+xVJYNnxXh3/Epw7QgY= my library Local playlist more local-playlist 1 225396.0 1259 Nas N. Jones、W. Adams、J. Lordan、D. Ingle no_data 52.0 13 24.0 female 9 25 5 2011 11 9 2017 1 1 0 3 Xumu+NIjS6QYVxDS4/t3SawvJ7viT9hPKXmf0RtLNx8= 2A87tzfnJTSWqD7gIZHisolhe4DMdzkbd6LzO1KHjNs= my library Local playlist more local-playlist 1 255512.0 1019 Soundway Kwadwo Donkoh no_data -1.0 13 24.0 female 9 25 5 2011 11 9 2017 1 1 0 4 FGtllVqz18RPiwJj/edr2gV78zirAiY/9SmYvia+kCg= 3qm6XTZ6MOCU11x8FIVbAGH5l5uMkT3/ZalWG1oo2Gc= explore Explore online-playlist 1 187802.0 1011 Brett Young Brett Young| Kelly Archer| Justin Ebach no_data 52.0 1 27.0 no_data 7 2 1 2012 5 10 2017 1 3 0 1member_music_df.info() 3. Data Extract, Transform and Load (ETL)We can skip Step 2 if we just want to transform data directly 3.1 Transformation Function for Data cleaning123456789101112#import necessary packages hereimport warningswarnings.filterwarnings('ignore')import numpy as np import pandas as pdimport matplotlib.pyplot as pltimport seaborn as snsimport lightgbm as lgbfrom subprocess import check_output# print(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))np.random.seed(2020) 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152def transform_data(data, song_df, members_df): # Merge song data with data set data = data.merge(song_df,on='song_id',how=\"left\", copy =False) # preprocess member data members_df[\"registration_init_time\"] = pd.to_datetime(members_df[\"registration_init_time\"], format=\"%Y%m%d\") members_df[\"expiration_date\"] = pd.to_datetime(members_df[\"expiration_date\"], format=\"%Y%m%d\") members_df[\"registration_init_day\"] = members_df[\"registration_init_time\"].dt.day members_df[\"registration_init_month\"] = members_df[\"registration_init_time\"].dt.month members_df[\"registration_init_year\"] = members_df[\"registration_init_time\"].dt.year members_df[\"expiration_day\"] = members_df[\"expiration_date\"].dt.day members_df[\"expiration_month\"] = members_df[\"expiration_date\"].dt.month members_df[\"expiration_year\"] = members_df[\"expiration_date\"].dt.year members_df = members_df.drop(columns = [\"registration_init_time\", \"expiration_date\"],axis=1) # merge member data with dataset data = data.merge(members_df,on='msno',how=\"left\", copy=False) # Remove outliers of bd age data['bd'].loc[(data['bd']&lt;=0) | (data['bd']&gt;=100)]= np.nan categorical_ls = ['source_system_tab', 'source_screen_name','source_type','genre_ids','artist_name','composer', 'lyricist','gender'] numerical_ls = ['song_length','language','bd'] # Fill missing values for index in numerical_ls: data[index].fillna(data[index].median(), inplace=True) for index in categorical_ls: data[index].fillna(\"no_data\", inplace=True) def count_items(x): if x ==\"no_data\": return 0 return sum(map(x.count, ['|', '/', '\\\\', ';',','])) + 1 data['genre_count']= data['genre_ids'].apply(count_items) data['composer_count']= data['composer'].apply(count_items) data['lyricist_count']= data['lyricist'].apply(count_items) # Convert object type to categorical type for c in data.columns: if data[c].dtype=='O': data[c] = data[c].astype(\"category\",copy=False) if 'id' in data.columns: ids = data['id'] data.drop(['id'], inplace=True,axis=1) else: ids =None return ids, data 3.2 Transform the composer, artist, lyricist to counts as new features123456root = './kaggle/working/train/data/'train_df = pd.read_csv(root+ \"train.csv\")test_df = pd.read_csv(root+ \"test.csv\")song_df = pd.read_csv(root+ \"songs.csv\")# song_extra_df = pd.read_csv(root+ \"song_extra_info.csv\")members_df = pd.read_csv(root+ \"members.csv\") 3.2.1 Transform train set1_, train_data = transform_data(train_df, song_df, members_df) 123y_train = train_data['target']train_data.drop(['target'], axis=1,inplace=True)X_train = train_data Transform the name of composer, artist, lyricist to new features like counts, number of intersection of names 12345678910111213141516def transform_names_intersection(data): #This function finds the intersection of names in composer, artist, lyricist def check_name_list(x): #convert string to name list dataframe strings = None strings = x.str.split(r\"//|/|;|、|\\| \") return strings df = data[[\"composer\",\"artist_name\", \"lyricist\"]].apply(check_name_list) data[\"composer_artist_intersect\"] =[len(set(a) &amp; set(b)) for a, b in zip(df.composer, df.artist_name)] data[\"composer_lyricist_intersect\"] =[len(set(a) &amp; set(b)) for a, b in zip(df.composer, df.lyricist)] data[\"artist_lyricist_intersect\"] =[len(set(a) &amp; set(b)) for a, b in zip(df.artist_name, df.lyricist)] return data _ = transform_names_intersection(X_train)X_train.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } msno song_id source_system_tab source_screen_name source_type song_length genre_ids artist_name composer lyricist language city bd gender registered_via registration_init_day registration_init_month registration_init_year expiration_day expiration_month expiration_year genre_count composer_count lyricist_count composer_artist_intersect composer_lyricist_intersect artist_lyricist_intersect 0 FGtllVqz18RPiwJj/edr2gV78zirAiY/9SmYvia+kCg= BBzumQNXUHKdEBOB7mAJuzok+IJA1c2Ryg/yzTF6tik= explore Explore online-playlist 206471.0 359 Bastille Dan Smith| Mark Crew no_data 52.0 1 27.0 no_data 7 2 1 2012 5 10 2017 1 2 0 0 0 0 1 Xumu+NIjS6QYVxDS4/t3SawvJ7viT9hPKXmf0RtLNx8= bhp/MpSNoqoxOIB+/l8WPqu6jldth4DIpCm3ayXnJqM= my library Local playlist more local-playlist 284584.0 1259 Various Artists no_data no_data 52.0 13 24.0 female 9 25 5 2011 11 9 2017 1 0 0 0 1 0 2 Xumu+NIjS6QYVxDS4/t3SawvJ7viT9hPKXmf0RtLNx8= JNWfrrC7zNN7BdMpsISKa4Mw+xVJYNnxXh3/Epw7QgY= my library Local playlist more local-playlist 225396.0 1259 Nas N. Jones、W. Adams、J. Lordan、D. Ingle no_data 52.0 13 24.0 female 9 25 5 2011 11 9 2017 1 1 0 0 0 0 3 Xumu+NIjS6QYVxDS4/t3SawvJ7viT9hPKXmf0RtLNx8= 2A87tzfnJTSWqD7gIZHisolhe4DMdzkbd6LzO1KHjNs= my library Local playlist more local-playlist 255512.0 1019 Soundway Kwadwo Donkoh no_data -1.0 13 24.0 female 9 25 5 2011 11 9 2017 1 1 0 0 0 0 4 FGtllVqz18RPiwJj/edr2gV78zirAiY/9SmYvia+kCg= 3qm6XTZ6MOCU11x8FIVbAGH5l5uMkT3/ZalWG1oo2Gc= explore Explore online-playlist 187802.0 1011 Brett Young Brett Young| Kelly Archer| Justin Ebach no_data 52.0 1 27.0 no_data 7 2 1 2012 5 10 2017 1 3 0 1 0 0 3.2.2 Transform Testset123ids, test_data = transform_data(test_df, song_df, members_df)_ = transform_names_intersection(test_data)test_data.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } msno song_id source_system_tab source_screen_name source_type song_length genre_ids artist_name composer lyricist language city bd gender registered_via registration_init_day registration_init_month registration_init_year expiration_day expiration_month expiration_year genre_count composer_count lyricist_count composer_artist_intersect composer_lyricist_intersect artist_lyricist_intersect 0 V8ruy7SGk7tDm3zA51DPpn6qutt+vmKMBKa21dp54uM= WmHKgKMlp1lQMecNdNvDMkvIycZYHnFwDT72I5sIssc= my library Local playlist more local-library 224130.0 458 梁文音 (Rachel Liang) Qi Zheng Zhang no_data 3.0 1 27.0 no_data 7 19 2 2016 18 9 2017 1 1 0 0 0 0 1 V8ruy7SGk7tDm3zA51DPpn6qutt+vmKMBKa21dp54uM= y/rsZ9DC7FwK5F2PK2D5mj+aOBUJAjuu3dZ14NgE0vM= my library Local playlist more local-library 320470.0 465 林俊傑 (JJ Lin) 林俊傑 孫燕姿/易家揚 3.0 1 27.0 no_data 7 19 2 2016 18 9 2017 1 1 2 0 0 0 2 /uQAlrAkaczV+nWCd2sPF2ekvXPRipV7q0l+gbLuxjw= 8eZLFOdGVdXBSqoAv5nsLigeH2BvKXzTQYtUM53I0k4= discover no_data song-based-playlist 315899.0 2022 Yu Takahashi (高橋優) Yu Takahashi Yu Takahashi 17.0 1 27.0 no_data 4 17 11 2016 24 11 2016 1 1 1 0 1 0 3 1a6oo/iXKatxQx4eS9zTVD+KlSVaAFbTIqVvwLC1Y0k= ztCf8thYsS4YN3GcIL/bvoxLm/T5mYBVKOO4C9NiVfQ= radio Radio radio 285210.0 465 U2 The Edge| Adam Clayton| Larry Mullen| Jr. no_data 52.0 3 30.0 male 9 25 7 2007 30 4 2017 1 4 0 0 0 0 4 1a6oo/iXKatxQx4eS9zTVD+KlSVaAFbTIqVvwLC1Y0k= MKVMpslKcQhMaFEgcEQhEfi5+RZhMYlU3eRDpySrH8Y= radio Radio radio 197590.0 873 Yoga Mr Sound Neuromancer no_data -1.0 3 30.0 male 9 25 7 2007 30 4 2017 1 1 0 0 0 0 1X_train.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } msno song_id source_system_tab source_screen_name source_type song_length genre_ids artist_name composer lyricist language city bd gender registered_via registration_init_day registration_init_month registration_init_year expiration_day expiration_month expiration_year genre_count composer_count lyricist_count composer_artist_intersect composer_lyricist_intersect artist_lyricist_intersect 0 FGtllVqz18RPiwJj/edr2gV78zirAiY/9SmYvia+kCg= BBzumQNXUHKdEBOB7mAJuzok+IJA1c2Ryg/yzTF6tik= explore Explore online-playlist 206471.0 359 Bastille Dan Smith| Mark Crew no_data 52.0 1 27.0 no_data 7 2 1 2012 5 10 2017 1 2 0 0 0 0 1 Xumu+NIjS6QYVxDS4/t3SawvJ7viT9hPKXmf0RtLNx8= bhp/MpSNoqoxOIB+/l8WPqu6jldth4DIpCm3ayXnJqM= my library Local playlist more local-playlist 284584.0 1259 Various Artists no_data no_data 52.0 13 24.0 female 9 25 5 2011 11 9 2017 1 0 0 0 1 0 2 Xumu+NIjS6QYVxDS4/t3SawvJ7viT9hPKXmf0RtLNx8= JNWfrrC7zNN7BdMpsISKa4Mw+xVJYNnxXh3/Epw7QgY= my library Local playlist more local-playlist 225396.0 1259 Nas N. Jones、W. Adams、J. Lordan、D. Ingle no_data 52.0 13 24.0 female 9 25 5 2011 11 9 2017 1 1 0 0 0 0 3 Xumu+NIjS6QYVxDS4/t3SawvJ7viT9hPKXmf0RtLNx8= 2A87tzfnJTSWqD7gIZHisolhe4DMdzkbd6LzO1KHjNs= my library Local playlist more local-playlist 255512.0 1019 Soundway Kwadwo Donkoh no_data -1.0 13 24.0 female 9 25 5 2011 11 9 2017 1 1 0 0 0 0 4 FGtllVqz18RPiwJj/edr2gV78zirAiY/9SmYvia+kCg= 3qm6XTZ6MOCU11x8FIVbAGH5l5uMkT3/ZalWG1oo2Gc= explore Explore online-playlist 187802.0 1011 Brett Young Brett Young| Kelly Archer| Justin Ebach no_data 52.0 1 27.0 no_data 7 2 1 2012 5 10 2017 1 3 0 1 0 0 1test_data.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } msno song_id source_system_tab source_screen_name source_type song_length genre_ids artist_name composer lyricist language city bd gender registered_via registration_init_day registration_init_month registration_init_year expiration_day expiration_month expiration_year genre_count composer_count lyricist_count composer_artist_intersect composer_lyricist_intersect artist_lyricist_intersect 0 V8ruy7SGk7tDm3zA51DPpn6qutt+vmKMBKa21dp54uM= WmHKgKMlp1lQMecNdNvDMkvIycZYHnFwDT72I5sIssc= my library Local playlist more local-library 224130.0 458 梁文音 (Rachel Liang) Qi Zheng Zhang no_data 3.0 1 27.0 no_data 7 19 2 2016 18 9 2017 1 1 0 0 0 0 1 V8ruy7SGk7tDm3zA51DPpn6qutt+vmKMBKa21dp54uM= y/rsZ9DC7FwK5F2PK2D5mj+aOBUJAjuu3dZ14NgE0vM= my library Local playlist more local-library 320470.0 465 林俊傑 (JJ Lin) 林俊傑 孫燕姿/易家揚 3.0 1 27.0 no_data 7 19 2 2016 18 9 2017 1 1 2 0 0 0 2 /uQAlrAkaczV+nWCd2sPF2ekvXPRipV7q0l+gbLuxjw= 8eZLFOdGVdXBSqoAv5nsLigeH2BvKXzTQYtUM53I0k4= discover no_data song-based-playlist 315899.0 2022 Yu Takahashi (高橋優) Yu Takahashi Yu Takahashi 17.0 1 27.0 no_data 4 17 11 2016 24 11 2016 1 1 1 0 1 0 3 1a6oo/iXKatxQx4eS9zTVD+KlSVaAFbTIqVvwLC1Y0k= ztCf8thYsS4YN3GcIL/bvoxLm/T5mYBVKOO4C9NiVfQ= radio Radio radio 285210.0 465 U2 The Edge| Adam Clayton| Larry Mullen| Jr. no_data 52.0 3 30.0 male 9 25 7 2007 30 4 2017 1 4 0 0 0 0 4 1a6oo/iXKatxQx4eS9zTVD+KlSVaAFbTIqVvwLC1Y0k= MKVMpslKcQhMaFEgcEQhEfi5+RZhMYlU3eRDpySrH8Y= radio Radio radio 197590.0 873 Yoga Mr Sound Neuromancer no_data -1.0 3 30.0 male 9 25 7 2007 30 4 2017 1 1 0 0 0 0 3.2.3 Split validation set and trainset123456789from sklearn.model_selection import StratifiedShuffleSplit, StratifiedKFold, train_test_splitss_split = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=2021)# Split training set and testing settrain_index, valid_index ,test_index = None, None, Nonefor train_i, test_i in ss_split.split(np.zeros(y_train.shape) ,y_train): train_index = train_i test_index = test_i print(train_index.shape, test_index.shape) (5901934,) (1475484,) 12345678X_validset = X_train.iloc[test_index]y_validset = y_train.iloc[test_index].valuesX_trainset = X_train.iloc[train_index]y_trainset = y_train.iloc[train_index].values#delete dataframes to save spacedel X_train, y_train 1X_trainset.info() &lt;class 'pandas.core.frame.DataFrame'&gt; Int64Index: 5901934 entries, 7066318 to 5275539 Data columns (total 27 columns): # Column Dtype --- ------ ----- 0 msno category 1 song_id category 2 source_system_tab category 3 source_screen_name category 4 source_type category 5 song_length float64 6 genre_ids category 7 artist_name category 8 composer category 9 lyricist category 10 language float64 11 city int64 12 bd float64 13 gender category 14 registered_via int64 15 registration_init_day int64 16 registration_init_month int64 17 registration_init_year int64 18 expiration_day int64 19 expiration_month int64 20 expiration_year int64 21 genre_count int64 22 composer_count int64 23 lyricist_count int64 24 composer_artist_intersect int64 25 composer_lyricist_intersect int64 26 artist_lyricist_intersect int64 dtypes: category(10), float64(3), int64(14) memory usage: 966.0 MB 4. LGBM Modeling123import lightgbm as lgbtrain_set = lgb.Dataset(X_trainset, y_trainset)valid_set = lgb.Dataset(X_validset, y_validset) 12num_leaves = 110max_depths = [10, 15, 20, 25,30] 5.Model Training and Validation on LGBM models12345678910111213141516171819params = { 'objective': 'binary', 'metric': 'binary_logloss', 'boosting': 'gbdt', 'learning_rate': 0.3 , 'verbose': 0, 'num_leaves': num_leaves, 'bagging_fraction': 0.95, 'bagging_freq': 1, 'bagging_seed': 1, 'feature_fraction': 0.9, 'feature_fraction_seed': 1, 'max_bin': 256, 'max_depth': max_depths[0], 'num_rounds': 200, 'metric' : 'auc' }%time model_f1 = lgb.train(params, train_set=train_set, valid_sets=valid_set, verbose_eval=5) [5] valid_0's auc: 0.710928 [10] valid_0's auc: 0.723954 [15] valid_0's auc: 0.731661 [20] valid_0's auc: 0.736653 [25] valid_0's auc: 0.740424 [30] valid_0's auc: 0.744678 [35] valid_0's auc: 0.749056 [40] valid_0's auc: 0.752277 [45] valid_0's auc: 0.754501 [50] valid_0's auc: 0.756448 [55] valid_0's auc: 0.758097 [60] valid_0's auc: 0.75991 [65] valid_0's auc: 0.761418 [70] valid_0's auc: 0.762683 [75] valid_0's auc: 0.764243 [80] valid_0's auc: 0.765646 [85] valid_0's auc: 0.766883 [90] valid_0's auc: 0.767921 [95] valid_0's auc: 0.769111 [100] valid_0's auc: 0.770006 [105] valid_0's auc: 0.770934 [110] valid_0's auc: 0.772012 [115] valid_0's auc: 0.772747 [120] valid_0's auc: 0.773835 [125] valid_0's auc: 0.774486 [130] valid_0's auc: 0.775258 [135] valid_0's auc: 0.775887 [140] valid_0's auc: 0.776838 [145] valid_0's auc: 0.777587 [150] valid_0's auc: 0.778113 [155] valid_0's auc: 0.778714 [160] valid_0's auc: 0.77929 [165] valid_0's auc: 0.779884 [170] valid_0's auc: 0.780354 [175] valid_0's auc: 0.781586 [180] valid_0's auc: 0.782002 [185] valid_0's auc: 0.782517 [190] valid_0's auc: 0.783075 [195] valid_0's auc: 0.783496 [200] valid_0's auc: 0.784083 CPU times: user 8min 20s, sys: 3 s, total: 8min 23s Wall time: 4min 25s 12345678910111213141516171819params = { 'objective': 'binary', 'metric': 'binary_logloss', 'boosting': 'gbdt', 'learning_rate': 0.3 , 'verbose': 0, 'num_leaves': num_leaves, 'bagging_fraction': 0.95, 'bagging_freq': 1, 'bagging_seed': 1, 'feature_fraction': 0.9, 'feature_fraction_seed': 1, 'max_bin': 256, 'max_depth': max_depths[1], 'num_rounds': 200, 'metric' : 'auc' }%time model_f2 = lgb.train(params, train_set=train_set, valid_sets=valid_set, verbose_eval=5) [5] valid_0's auc: 0.727917 [10] valid_0's auc: 0.742629 [15] valid_0's auc: 0.74811 [20] valid_0's auc: 0.754257 [25] valid_0's auc: 0.758256 [30] valid_0's auc: 0.76119 [35] valid_0's auc: 0.763674 [40] valid_0's auc: 0.76626 [45] valid_0's auc: 0.7681 [50] valid_0's auc: 0.769933 [55] valid_0's auc: 0.771692 [60] valid_0's auc: 0.773121 [65] valid_0's auc: 0.774693 [70] valid_0's auc: 0.776149 [75] valid_0's auc: 0.777157 [80] valid_0's auc: 0.778674 [85] valid_0's auc: 0.780085 [90] valid_0's auc: 0.78098 [95] valid_0's auc: 0.782016 [100] valid_0's auc: 0.783028 [105] valid_0's auc: 0.783782 [110] valid_0's auc: 0.784875 [115] valid_0's auc: 0.785417 [120] valid_0's auc: 0.786042 [125] valid_0's auc: 0.78665 [130] valid_0's auc: 0.787237 [135] valid_0's auc: 0.787897 [140] valid_0's auc: 0.788426 [145] valid_0's auc: 0.788904 [150] valid_0's auc: 0.789517 [155] valid_0's auc: 0.78991 [160] valid_0's auc: 0.790561 [165] valid_0's auc: 0.791319 [170] valid_0's auc: 0.791855 [175] valid_0's auc: 0.792519 [180] valid_0's auc: 0.792922 [185] valid_0's auc: 0.793727 [190] valid_0's auc: 0.794061 [195] valid_0's auc: 0.794584 [200] valid_0's auc: 0.794811 CPU times: user 9min 27s, sys: 1.28 s, total: 9min 28s Wall time: 4min 51s 12345678910111213141516171819params = { 'objective': 'binary', 'metric': 'binary_logloss', 'boosting': 'gbdt', 'learning_rate': 0.3 , 'verbose': 0, 'num_leaves': num_leaves, 'bagging_fraction': 0.95, 'bagging_freq': 1, 'bagging_seed': 1, 'feature_fraction': 0.9, 'feature_fraction_seed': 1, 'max_bin': 256, 'max_depth': max_depths[2], 'num_rounds': 200, 'metric' : 'auc' }%time model_f3 = lgb.train(params, train_set=train_set, valid_sets=valid_set, verbose_eval=5) [5] valid_0's auc: 0.734133 [10] valid_0's auc: 0.749742 [15] valid_0's auc: 0.75615 [20] valid_0's auc: 0.761276 [25] valid_0's auc: 0.766358 [30] valid_0's auc: 0.769127 [35] valid_0's auc: 0.771531 [40] valid_0's auc: 0.773761 [45] valid_0's auc: 0.775287 [50] valid_0's auc: 0.777329 [55] valid_0's auc: 0.779154 [60] valid_0's auc: 0.780391 [65] valid_0's auc: 0.782072 [70] valid_0's auc: 0.783786 [75] valid_0's auc: 0.784989 [80] valid_0's auc: 0.785685 [85] valid_0's auc: 0.786851 [90] valid_0's auc: 0.787643 [95] valid_0's auc: 0.788312 [100] valid_0's auc: 0.789305 [105] valid_0's auc: 0.790256 [110] valid_0's auc: 0.791037 [115] valid_0's auc: 0.79177 [120] valid_0's auc: 0.792466 [125] valid_0's auc: 0.792988 [130] valid_0's auc: 0.793478 [135] valid_0's auc: 0.793961 [140] valid_0's auc: 0.794871 [145] valid_0's auc: 0.795495 [150] valid_0's auc: 0.795952 [155] valid_0's auc: 0.796269 [160] valid_0's auc: 0.796888 [165] valid_0's auc: 0.797808 [170] valid_0's auc: 0.7982 [175] valid_0's auc: 0.798443 [180] valid_0's auc: 0.798959 [185] valid_0's auc: 0.799395 [190] valid_0's auc: 0.799687 [195] valid_0's auc: 0.800153 [200] valid_0's auc: 0.800409 CPU times: user 11min 16s, sys: 1.54 s, total: 11min 17s Wall time: 5min 47s 12345678910111213141516171819params = { 'objective': 'binary', 'metric': 'binary_logloss', 'boosting': 'gbdt', 'learning_rate': 0.3 , 'verbose': 0, 'num_leaves': num_leaves, 'bagging_fraction': 0.95, 'bagging_freq': 1, 'bagging_seed': 1, 'feature_fraction': 0.9, 'feature_fraction_seed': 1, 'max_bin': 256, 'max_depth': max_depths[3], 'num_rounds': 200, 'metric' : 'auc' }%time model_f4 = lgb.train(params, train_set=train_set, valid_sets=valid_set, verbose_eval=5) [5] valid_0's auc: 0.736351 [10] valid_0's auc: 0.754592 [15] valid_0's auc: 0.76195 [20] valid_0's auc: 0.766405 [25] valid_0's auc: 0.770538 [30] valid_0's auc: 0.772566 [35] valid_0's auc: 0.775789 [40] valid_0's auc: 0.777994 [45] valid_0's auc: 0.779658 [50] valid_0's auc: 0.781394 [55] valid_0's auc: 0.783194 [60] valid_0's auc: 0.784808 [65] valid_0's auc: 0.786109 [70] valid_0's auc: 0.787265 [75] valid_0's auc: 0.788079 [80] valid_0's auc: 0.789109 [85] valid_0's auc: 0.78986 [90] valid_0's auc: 0.790613 [95] valid_0's auc: 0.791347 [100] valid_0's auc: 0.79209 [105] valid_0's auc: 0.793348 [110] valid_0's auc: 0.79409 [115] valid_0's auc: 0.794754 [120] valid_0's auc: 0.795411 [125] valid_0's auc: 0.795866 [130] valid_0's auc: 0.796604 [135] valid_0's auc: 0.79781 [140] valid_0's auc: 0.798172 [145] valid_0's auc: 0.798723 [150] valid_0's auc: 0.799132 [155] valid_0's auc: 0.799488 [160] valid_0's auc: 0.800115 [165] valid_0's auc: 0.800509 [170] valid_0's auc: 0.800784 [175] valid_0's auc: 0.801118 [180] valid_0's auc: 0.801448 [185] valid_0's auc: 0.801882 [190] valid_0's auc: 0.8022 [195] valid_0's auc: 0.802578 [200] valid_0's auc: 0.802953 CPU times: user 12min 34s, sys: 1.44 s, total: 12min 36s Wall time: 6min 27s 12345678910111213141516171819params = { 'objective': 'binary', 'metric': 'binary_logloss', 'boosting': 'gbdt', 'learning_rate': 0.3 , 'verbose': 0, 'num_leaves': num_leaves, 'bagging_fraction': 0.95, 'bagging_freq': 1, 'bagging_seed': 1, 'feature_fraction': 0.9, 'feature_fraction_seed': 1, 'max_bin': 256, 'max_depth': max_depths[4], 'num_rounds': 200, 'metric' : 'auc' }%time model_f5 = lgb.train(params, train_set=train_set, valid_sets=valid_set, verbose_eval=5) [5] valid_0's auc: 0.739442 [10] valid_0's auc: 0.757442 [15] valid_0's auc: 0.766439 [20] valid_0's auc: 0.77132 [25] valid_0's auc: 0.774735 [30] valid_0's auc: 0.777071 [35] valid_0's auc: 0.779247 [40] valid_0's auc: 0.781616 [45] valid_0's auc: 0.782953 [50] valid_0's auc: 0.785154 [55] valid_0's auc: 0.786877 [60] valid_0's auc: 0.787993 [65] valid_0's auc: 0.788839 [70] valid_0's auc: 0.790254 [75] valid_0's auc: 0.791088 [80] valid_0's auc: 0.792455 [85] valid_0's auc: 0.79365 [90] valid_0's auc: 0.794445 [95] valid_0's auc: 0.795072 [100] valid_0's auc: 0.796276 [105] valid_0's auc: 0.797737 [110] valid_0's auc: 0.798265 [115] valid_0's auc: 0.799021 [120] valid_0's auc: 0.799964 [125] valid_0's auc: 0.800469 [130] valid_0's auc: 0.801445 [135] valid_0's auc: 0.801851 [140] valid_0's auc: 0.802299 [145] valid_0's auc: 0.802599 [150] valid_0's auc: 0.803381 [155] valid_0's auc: 0.803696 [160] valid_0's auc: 0.803926 [165] valid_0's auc: 0.80443 [170] valid_0's auc: 0.804694 [175] valid_0's auc: 0.804897 [180] valid_0's auc: 0.80524 [185] valid_0's auc: 0.805486 [190] valid_0's auc: 0.805804 [195] valid_0's auc: 0.806059 [200] valid_0's auc: 0.806525 CPU times: user 14min 7s, sys: 1.66 s, total: 14min 9s Wall time: 7min 14s 6.Model Evaluation on LGBM models12345678from sklearn.metrics import accuracy_scoredef evaluation_lgbm(model, X =X_validset , y= y_validset): out = model.predict(X) preds = out&gt;=0.5 acc = accuracy_score(preds, y) print(\"Evaluation acc:\", acc) return acc 1X_validset.shape (1475484, 27) 12345acc_1 = evaluation_lgbm(model_f1)acc_2 = evaluation_lgbm(model_f2)acc_3 = evaluation_lgbm(model_f3)acc_4 = evaluation_lgbm(model_f4)acc_5 = evaluation_lgbm(model_f5) Evaluation acc: 0.709764389176704 Evaluation acc: 0.719106408473423 Evaluation acc: 0.7236893114395005 Evaluation acc: 0.7258221708944319 Evaluation acc: 0.728842196865571 12eval_df = pd.DataFrame({\"Lgbm with max_depth\":max_depths,\"Validation Accuracy\":[acc_1,acc_2,acc_3,acc_4,acc_5]})eval_df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Lgbm with max_depth Validation Accuracy 0 10 0.709764 1 15 0.719106 2 20 0.723689 3 25 0.725822 4 30 0.728842 Create Submission Files12345678models = [model_f1,model_f2,model_f3,model_f4,model_f5]for i in range(len(models)): preds_test = models[i].predict(test_data) submission = pd.DataFrame() submission['id'] = ids submission['target'] = preds_test submission.to_csv(root + 'submission_lgbm_model_'+ str(i)+'.csv.gz', compression = 'gzip', index=False, float_format = '%.5f') print(\"Predictions from model \",i,\": \",preds_test) Predictions from model 0 : [0.47177512 0.48584262 0.19651648 ... 0.39917036 0.30263348 0.36468783] Predictions from model 1 : [0.45280296 0.55415074 0.17824637 ... 0.41500494 0.30757934 0.34520384] Predictions from model 2 : [0.39847416 0.48724786 0.15954141 ... 0.38293317 0.27657349 0.28451098] Predictions from model 3 : [0.3825275 0.39659855 0.15904321 ... 0.3515784 0.21812496 0.28995803] Predictions from model 4 : [0.3951268 0.45704878 0.14609333 ... 0.35033303 0.23065677 0.2885925 ] Scores from kaggle test set Model name private score public score LGBM Boosting Machine Model 4 0.67423 0.67256 LGBM Boosting Machine Model 3 0.67435 0.67241 LGBM Boosting Machine Model 2 0.67416 0.67208 LGBM Boosting Machine Model 1 0.67416 0.67188 LGBM Boosting Machine Model 0 0.67206 0.66940 4. Wide &amp; Depth neural network modelWide and Deep model（2 branches–&gt;merge two branches–&gt;main branch） This model converts categorical attributes into dense vectors using embedding network in neural network, which enable us to reduce the dimension of categorical data and extract main features like PCA. Then it combines dense embedded vectors with numerical data for features selection and classifcation in the main branch. The output is possibility that user may repeat listening to the music Label Encoding for categorical dataConvert categorical data into numerical labels before using embedding 12345678910111213141516171819202122232425262728from sklearn.preprocessing import LabelEncodercategorical_ls1 = ['source_system_tab', 'source_screen_name','source_type','genre_ids','gender']categorical_ls2 = ['artist_name','composer', 'lyricist']numerical_ls = ['song_length','language','bd',\"registration_init_year\", \"expiration_day\",\"expiration_month\",\"expiration_year\", \"genre_count\",\"composer_count\",\"lyricist_count\",\"composer_artist_intersect\", \"composer_lyricist_intersect\",\"artist_lyricist_intersect\"]max_values = {}# labelencoders = {}for col in categorical_ls1: print(col) lbl = LabelEncoder() df = pd.concat([X_trainset[col], X_validset[col],test_data[col]],ignore_index=True) lbl.fit(df) df = lbl.transform(list(df.values.astype('str',copy=False))) X_trainset[col] = lbl.transform(list(X_trainset[col].values.astype('str',copy=False))) X_validset[col] = lbl.transform(list(X_validset[col].values.astype('str',copy=False))) test_data[col] = lbl.transform(list(test_data[col].values.astype('str',copy=False))) max_values[col] = df.max() + 2 #set the range of embedding input larger# Compute embedding dimensionsemb_dims1 = []emb_dims2 = []for i in categorical_ls1: emb_dims1.append((max_values[i], min((max_values[i]+1)//2, 50))) source_system_tab source_screen_name source_type genre_ids gender 12# max_valuesX_trainset.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } msno song_id source_system_tab source_screen_name source_type song_length genre_ids artist_name composer lyricist language city bd gender registered_via registration_init_day registration_init_month registration_init_year expiration_day expiration_month expiration_year genre_count composer_count lyricist_count composer_artist_intersect composer_lyricist_intersect artist_lyricist_intersect 7066318 BQ7nOoOUipsqjOBANK+ilA8F7TVaOHSI8gVPWElXsuI= FaTUlIiCh/6sEOasPm1vgIk9XqavgSGgRGYuOkzTF0o= 0 3 7 203520.0 364 田馥甄 (Hebe) 倪子岡 李格弟 3.0 5 25.0 0 9 2 9 2006 11 10 2017 1 1 1 0 0 0 1471565 Ul+UpO5PxuhCn040AK8gzR1A/mE/k3KbL13gO7Uc4Ts= +SstqMwhQPBQFTPBhLKPT642IiBDXzZFwlzsLl4cGXo= 3 8 3 283846.0 371 陳勢安 (Andrew Tan) 覃嘉健 馬嵩惟 3.0 13 47.0 0 9 1 2 2006 30 9 2017 1 1 1 0 0 0 6176886 sa6oKy94c62R5Eq0YHkNzZrJSo9j5E7JGjTDHnYRKqs= K6fBQxiNhgWazjXrZUGlZIm9ltT4o+Vq19sWmZRdAhg= 2 12 2 296960.0 371 蔡依林 (Jolin Tsai) 郭子 鄔裕康 3.0 5 38.0 1 7 27 11 2011 30 12 2017 1 1 1 0 0 0 3527889 LG/BLgJxw5AvXy0pkgaHYYWeU7jKS+ms/51+7TaBY9Y= O+/KJ5a5GzbgLZrCOw/t/iDOPTrDcrz5ZnOtaK9blA8= 3 8 0 235403.0 200 ONE OK ROCK Toru/Taka Taka 17.0 1 27.0 2 7 1 6 2013 1 10 2017 1 2 1 0 1 0 6073849 KmAJtsNcrofH6qMoHvET89mQAlC1EN3r3r3rkfW2iT4= WogFv1yz1n49l4gNSbf76bWxas8nNvzHntrj4FuzC24= 3 22 7 210604.0 371 Twins no_data no_data 24.0 13 28.0 1 9 2 11 2016 7 10 2017 1 0 0 0 1 0 1X_validset.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } msno song_id source_system_tab source_screen_name source_type song_length genre_ids artist_name composer lyricist language city bd gender registered_via registration_init_day registration_init_month registration_init_year expiration_day expiration_month expiration_year genre_count composer_count lyricist_count composer_artist_intersect composer_lyricist_intersect artist_lyricist_intersect 3400479 RZDFiPWpvwi1RWF5NAPEkvmogqe+7rGys+zoLU9he2M= MvON55vzjT7QW7GSs/UVLZrE/LJpMAVFUjXwZczdw40= 0 11 7 356379.0 97 Nas Amy Winehouse| Salaam Remi| Nasir Jones p/k/a NAS no_data 52.0 1 27.0 2 7 7 12 2010 20 9 2017 1 5 0 0 0 0 2481022 C3EZ5oh7XDt5fP9OY20RPlD8MA+rBknmvmDhA1tHGMU= 5PvPCUIB7vVuCNpQRKXIOcWvh9EerujDAbrjV7G6ZE0= 3 8 3 216920.0 349 貴族精選 no_data Super Market| Microdot 31.0 5 27.0 2 3 11 12 2012 5 1 2018 1 0 2 0 0 0 5808216 O1pwjdTED6P3lKm52VBxVUtaSVc31S9PmIw+07WBNw4= va3+1L2wraJkzDbHjvdo+e+0TTJcLko0k0pqBn09nJE= 3 8 3 268225.0 548 Various Artists no_data no_data 3.0 13 82.0 0 9 29 4 2007 23 1 2018 1 0 0 0 1 0 42686 WFCCMzA4hADGBduTS6X8mXlutyiC0P33QkTG6zr5yCg= U9kojfZSKaiWOW94PKh1Riyv/zUWxmBRmv0XInQWLGw= 7 11 7 290063.0 364 周杰倫 (Jay Chou) 周杰倫 方文山 3.0 13 32.0 0 7 12 12 2010 9 9 2017 1 1 1 0 0 0 1850837 h0fTru8nYMv9bR0j6kBh8kiXDaybzWBYaSHbUIVzeBs= J1sgBEFbcXSK6eiN7CK1WNxsso0/sY6t0BMX+c+iPNw= 0 11 7 220450.0 111 Usher no_data no_data 52.0 22 28.0 0 9 2 1 2008 28 9 2017 3 0 0 0 1 0 123456789101112131415161718192021222324252627282930313233343536373839import torchfrom torch import nnimport torch.nn.functional as Ffrom torch.utils.data import Dataset, DataLoaderclass TabularDataset(Dataset): def __init__(self, x_data, y_data, cat_cols1, cat_cols2, num_cols): \"\"\" data: pandas data frame; cat_cols: list of string, the names of the categorical columns in the data, will be passed through the embedding layers; num_cols: list of string y_data: the target \"\"\" self.n = x_data.shape[0] self.y = y_data.astype(np.float32).reshape(-1, 1)#.values.reshape(-1, 1) self.cat_cols1 = cat_cols1 self.cat_cols2 = cat_cols2 self.num_cols = num_cols self.num_X = x_data[self.num_cols].astype(np.float32).values self.cat_X1 = x_data[self.cat_cols1].astype(np.int64).values self.cat_X2 = x_data[self.cat_cols2].astype(np.int64).values def print_data(self): return self.num_X, self.cat_X1, self.cat_X2, self.y def __len__(self): \"\"\" total number of samples \"\"\" return self.n def __getitem__(self, idx): \"\"\" Generates one sample of data. \"\"\" return [self.y[idx], self.num_X[idx], self.cat_X1[idx], self.cat_X2[idx]] 12345train_dataset = TabularDataset(x_data=X_trainset, y_data=y_trainset, cat_cols1=categorical_ls1, cat_cols2=[], num_cols=numerical_ls)val_dataset = TabularDataset(x_data=X_validset, y_data=y_validset, cat_cols1=categorical_ls1, cat_cols2=[], num_cols=numerical_ls) 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980class FeedForwardNN(nn.Module): def __init__(self, emb_dims1, emb_dims2, no_of_num, lin_layer_sizes, output_size, emb_dropout, lin_layer_dropouts, branch2_enable=0): \"\"\" emb_dims: List of two element tuples; no_of_num: Integer, the number of continuous features in the data; lin_layer_sizes: List of integers. The size of each linear layer; output_size: Integer, the size of the final output; emb_dropout: Float, the dropout to be used after the embedding layers. lin_layer_dropouts: List of floats, the dropouts to be used after each linear layer. \"\"\" super().__init__() self.branch2_enable = branch2_enable # embedding layers self.emb_layers1 = nn.ModuleList([nn.Embedding(x, y) for x, y in emb_dims1]) self.emb_layers2 = nn.ModuleList([nn.Embedding(x, y) for x, y in emb_dims2]) # 计算各个emb参数数量，为后续Linear layer的输入做准备 self.no_of_embs1 = sum([y for x, y in emb_dims1]) self.no_of_embs2 = sum([y for x, y in emb_dims2]) self.no_of_num = no_of_num # 分支1 self.branch1 = nn.Linear(self.no_of_embs1, lin_layer_sizes[0]) self.branch1_2 = nn.Linear(lin_layer_sizes[0], lin_layer_sizes[1]) nn.init.kaiming_normal_(self.branch1.weight.data) nn.init.kaiming_normal_(self.branch1_2.weight.data) # 分支2 if branch2_enable: self.branch2 = nn.Linear(self.no_of_embs2, lin_layer_sizes[0] * 2) self.branch2_2 = nn.Linear(lin_layer_sizes[0] * 2, lin_layer_sizes[1] * 2) nn.init.kaiming_normal_(self.branch2.weight.data) nn.init.kaiming_normal_(self.branch2_2.weight.data) # 主分支# self.main_layer1 = nn.Linear(lin_layer_sizes[1] * 3 + self.no_of_num, lin_layer_sizes[2]) self.main_layer1 = nn.Linear(77, lin_layer_sizes[2]) self.main_layer2 = nn.Linear(lin_layer_sizes[2], lin_layer_sizes[3]) # batch normal self.branch_bn_layers1 = nn.BatchNorm1d(lin_layer_sizes[0]) self.branch_bn_layers2 = nn.BatchNorm1d(lin_layer_sizes[0] * 2) self.main_bn_layer = nn.BatchNorm1d(lin_layer_sizes[2]) # Dropout Layers self.emb_dropout_layer = nn.Dropout(emb_dropout) self.dropout_layers = nn.ModuleList([nn.Dropout(size) for size in lin_layer_dropouts]) # Output layer self.output_layer = nn.Linear(lin_layer_sizes[-1], output_size) nn.init.kaiming_normal_(self.output_layer.weight.data) self.sigmoid = nn.Sigmoid() def forward(self, num_data, cat_data1, cat_data2): # embedding categorical feature and cat them together x1 = [emb_layer(torch.tensor(cat_data1[:, i])) for i, emb_layer in enumerate(self.emb_layers1)] x1 = torch.cat(x1, 1) x1 = self.emb_dropout_layer(F.relu(self.branch1(x1))) x1 = self.branch_bn_layers1(x1) x1 = self.dropout_layers[0](F.relu(self.branch1_2(x1))) if self.branch2_enable: x2 = [emb_layer(torch.tensor(cat_data2[:, i])) for i, emb_layer in enumerate(self.emb_layers2)] x2 = torch.cat(x2, 1) x2 = self.emb_dropout_layer(F.relu(self.branch2(x2))) x2 = self.branch_bn_layers2(x2) x2 = self.dropout_layers[0](F.relu(self.branch2_2(x2))) main = torch.cat([x1, x2, num_data], 1) else: main = torch.cat([x1, num_data], 1)# print(\"Main Shape: \", main.shape) main = self.dropout_layers[1](F.relu(self.main_layer1(main))) main = self.main_bn_layer(main) main = self.dropout_layers[2](F.relu(self.main_layer2(main))) out = self.output_layer(main) out = self.sigmoid(out) return out 123batchsize = 64train_dataloader = DataLoader(train_dataset, batchsize, shuffle=True, num_workers=2)val_dataloader = DataLoader(val_dataset, 64, shuffle=True, num_workers=2) 1# next(iter(train_dataloader))[3] 123456789np.random.seed(2020)device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")model = FeedForwardNN(emb_dims1=emb_dims1, emb_dims2=emb_dims2, no_of_num=len(numerical_ls), lin_layer_sizes=[128,64,32,16], output_size=1, lin_layer_dropouts=[0.1, 0.1, 0.05], emb_dropout=0.05).to(device) 1device,len(train_dataloader) (device(type='cpu'), 92218) 5.Model Training and Validation on Wide and Deep model12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273no_of_epochs = 2batch_num = 4000# criterion = torch.nn.MSELoss()criterion = torch.nn.BCELoss()optimizer = torch.optim.Adam(model.parameters(), lr=0.001)lrscheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.1, patience=2, threshold=0.9 )total_data = train_dataset.__len__()best_val_score = 0.0best_model =Noneprint_every = 500steps = 0running_loss = 0for epoch in range(no_of_epochs): model.train() batch_cnt = 0 for index, datas in enumerate(train_dataloader): if batch_cnt == batch_num: break steps += 1 batch_cnt += 1 y, num_x, cat_x1, cat_x2 = datas cat_x1 = cat_x1.to(device) cat_x2 = cat_x2.to(device) num_x = num_x.to(device) y = y.to(device) # Forward Pass optimizer.zero_grad() preds = model.forward(num_x, cat_x1, cat_x2) loss = criterion(preds, y) loss.backward() optimizer.step() running_loss += loss.item() if steps % print_every == 0: val_loss = 0 model.eval() val_acc = 0. total_len = 0. with torch.no_grad(): for val_index, val_datas in enumerate(val_dataloader): y, num_x, cat_x1, cat_x2 = val_datas cat_x1 = cat_x1.to(device) cat_x2 = cat_x2.to(device) num_x = num_x.to(device) y = y.to(device) out = model.forward(num_x, cat_x1, cat_x2) batch_loss = criterion(out, y) val_acc += ((out&gt;0.5)==y ).sum().detach().to('cpu').numpy() total_len += len(out) val_loss += batch_loss.item() val_acc /= total_len if val_acc&gt; best_val_score: best_val_score = val_acc torch.save(model,\"checkpoint.pt\") # print(\"Checkpoint saved.\") # update scheduler lrscheduler.step(val_loss) print(f\"Epoch {epoch+1}/{no_of_epochs}..\" f\"Train loss:{running_loss/print_every:.4f}..\" f\"Validation loss:{val_loss/len(val_dataloader):.4f}..\" f\"Validation Acc:{val_acc:.4f}..\" f\"Best Validation Acc:{best_val_score:.4f}..\") running_loss = 0 model.train()print(\"Training Completed\")best_model = torch.load(\"checkpoint.pt\") Epoch 1/2..Train loss:0.6945..Validation loss:0.6925..Validation Acc:0.5302..Best Validation Acc:0.5302.. Epoch 1/2..Train loss:0.6742..Validation loss:0.6663..Validation Acc:0.6229..Best Validation Acc:0.6229.. Epoch 1/2..Train loss:0.6640..Validation loss:0.6618..Validation Acc:0.6245..Best Validation Acc:0.6245.. Epoch 1/2..Train loss:0.6623..Validation loss:0.6768..Validation Acc:0.6252..Best Validation Acc:0.6252.. Epoch 1/2..Train loss:0.6628..Validation loss:0.6648..Validation Acc:0.6249..Best Validation Acc:0.6252.. Epoch 1/2..Train loss:0.6636..Validation loss:0.6656..Validation Acc:0.6253..Best Validation Acc:0.6253.. Epoch 1/2..Train loss:0.6623..Validation loss:0.6674..Validation Acc:0.6254..Best Validation Acc:0.6254.. Epoch 1/2..Train loss:0.6640..Validation loss:0.6649..Validation Acc:0.6255..Best Validation Acc:0.6255.. Epoch 2/2..Train loss:0.6597..Validation loss:0.6638..Validation Acc:0.6256..Best Validation Acc:0.6256.. Epoch 2/2..Train loss:0.6605..Validation loss:0.6584..Validation Acc:0.6259..Best Validation Acc:0.6259.. Epoch 2/2..Train loss:0.6609..Validation loss:0.6636..Validation Acc:0.6263..Best Validation Acc:0.6263.. Epoch 2/2..Train loss:0.6602..Validation loss:0.6620..Validation Acc:0.6263..Best Validation Acc:0.6263.. Epoch 2/2..Train loss:0.6647..Validation loss:0.6658..Validation Acc:0.6264..Best Validation Acc:0.6264.. Epoch 2/2..Train loss:0.6615..Validation loss:0.6644..Validation Acc:0.6262..Best Validation Acc:0.6264.. Epoch 2/2..Train loss:0.6641..Validation loss:0.6764..Validation Acc:0.6254..Best Validation Acc:0.6264.. Epoch 2/2..Train loss:0.6600..Validation loss:0.6926..Validation Acc:0.6263..Best Validation Acc:0.6264.. Training Completed 1print(f\"Best Validation Acc:{best_val_score:.4f}..\") Best Validation Acc:0.6264.. 1model = torch.load(\"checkpoint.pt\") 12test_dataset = TabularDataset(x_data=test_data, y_data=np.zeros(len(test_data)), cat_cols1=categorical_ls1, cat_cols2=[], num_cols=numerical_ls) 123456789101112131415161718192021def evaluation(test_dataloder): model.eval() total_cnt = 0. correct_cnt = 0. acc = None with torch.no_grad(): for test_index, test_datas in enumerate(test_dataloder): y, num_x, cat_x1, cat_x2 = test_datas cat_x1 = cat_x1.to(device) cat_x2 = cat_x2.to(device) num_x = num_x.to(device) y = y.to(device) out = model.forward(num_x, cat_x1, cat_x2) correct_cnt += ((out&gt;0.5)==y ).sum().detach().to('cpu').numpy() total_cnt += len(out)# out = out.squeeze().to(\"cpu\").numpy().tolist() acc = 100* correct_cnt / total_cnt print(\"Evaluation Acc: %.4f %%\"%(acc)) return acc 6.Model Evaluation on Wide and Deep model using validation set1acc = evaluation(val_dataloader) Evaluation Acc: 62.6432 % 1234567891011121314151617def predict_test(test_dataset): preds = [] model.eval() test_dataloder = DataLoader(test_dataset, 200, shuffle=False, num_workers=4) with torch.no_grad(): for test_index, test_datas in enumerate(test_dataloder): y, num_x, cat_x1, cat_x2 = test_datas cat_x1 = cat_x1.to(device) cat_x2 = cat_x2.to(device) num_x = num_x.to(device) y = y.to(device) out = model.forward(num_x, cat_x1, cat_x2) out = out.squeeze().to(\"cpu\").numpy().tolist()# print(out) preds.extend(out) return np.array(preds) Make Predictions and submission1234567preds = predict_test(test_dataset)submission = pd.DataFrame()submission['id'] = idssubmission['target'] = predssubmission.to_csv(root + 'submission_WideAndDeep_model.csv.gz', compression = 'gzip', index=False, float_format = '%.5f')print(\"Model Predictions: \",preds)# !kaggle competitions submit -c ./train/data -f submission_lgbm_model.csv.gz -m \"Message\" Model Predictions: [0.60901934 0.60901934 0.35112065 ... 0.54463851 0.48204085 0.54300648] submission_WideAndDeep_model.csv.gzWideAndDeep_model0.616280.61117 12perf_df = pd.DataFrame({\"model name\":['Wide and Deep model'],\"private_score\":[0.61628], \"public score\": [0.61117]})perf_df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } model name private_score public score 0 Wide and Deep model 0.61628 0.61117 12345model_names = [\"lgbm model 0\",\"lgbm model 1\",\"lgbm model 2\",\"lgbm model 3\",\"lgbm model 4\"]private_score = [0.67206,0.67416,0.67416,0.67435,0.67423]public_score = [0.66940,0.67188,0.67208,0.67241,0.67256]perf_df = pd.DataFrame({\"model name\":model_names,\"max_depth:\":max_depths,\"private_score\":private_score, \"public score\": public_score})perf_df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } model name max_depth: private_score public score 0 lgbm model 0 10 0.67206 0.66940 1 lgbm model 1 15 0.67416 0.67188 2 lgbm model 2 20 0.67416 0.67208 3 lgbm model 3 25 0.67435 0.67241 4 lgbm model 4 30 0.67423 0.67256 7. SummaryType answers here","link":"/2020/12/02/kkboxmusicrecommendation-notebook-v4/"}],"tags":[{"name":"DeepFM","slug":"DeepFM","link":"/tags/DeepFM/"},{"name":"Stacking","slug":"Stacking","link":"/tags/Stacking/"},{"name":"Ensemble Learning","slug":"Ensemble-Learning","link":"/tags/Ensemble-Learning/"},{"name":"Datawhale Team Learning","slug":"Datawhale-Team-Learning","link":"/tags/Datawhale-Team-Learning/"},{"name":"Convolution Neural Network","slug":"Convolution-Neural-Network","link":"/tags/Convolution-Neural-Network/"},{"name":"Backpropagation","slug":"Backpropagation","link":"/tags/Backpropagation/"},{"name":"C++","slug":"C","link":"/tags/C/"},{"name":"review","slug":"review","link":"/tags/review/"},{"name":"BuckSort","slug":"BuckSort","link":"/tags/BuckSort/"},{"name":"Sorting","slug":"Sorting","link":"/tags/Sorting/"},{"name":"Parallel Computing","slug":"Parallel-Computing","link":"/tags/Parallel-Computing/"},{"name":"Binary Tree","slug":"Binary-Tree","link":"/tags/Binary-Tree/"},{"name":"Traversal","slug":"Traversal","link":"/tags/Traversal/"},{"name":"Dropout","slug":"Dropout","link":"/tags/Dropout/"},{"name":"binary search","slug":"binary-search","link":"/tags/binary-search/"},{"name":"Amdahl&#39;s Law","slug":"Amdahl-s-Law","link":"/tags/Amdahl-s-Law/"},{"name":"Parallel Speedup","slug":"Parallel-Speedup","link":"/tags/Parallel-Speedup/"},{"name":"GATE","slug":"GATE","link":"/tags/GATE/"},{"name":"WSDM","slug":"WSDM","link":"/tags/WSDM/"},{"name":"GNN","slug":"GNN","link":"/tags/GNN/"},{"name":"Graph","slug":"Graph","link":"/tags/Graph/"},{"name":"statistic analytics tool","slug":"statistic-analytics-tool","link":"/tags/statistic-analytics-tool/"},{"name":"JavaScript","slug":"JavaScript","link":"/tags/JavaScript/"},{"name":"D3.js","slug":"D3-js","link":"/tags/D3-js/"},{"name":"commands","slug":"commands","link":"/tags/commands/"},{"name":"K-Mean Clustering","slug":"K-Mean-Clustering","link":"/tags/K-Mean-Clustering/"},{"name":"Unsupervised Learning","slug":"Unsupervised-Learning","link":"/tags/Unsupervised-Learning/"},{"name":"non-parametric learning","slug":"non-parametric-learning","link":"/tags/non-parametric-learning/"},{"name":"Regression","slug":"Regression","link":"/tags/Regression/"},{"name":"Variable Selection","slug":"Variable-Selection","link":"/tags/Variable-Selection/"},{"name":"ROC curve","slug":"ROC-curve","link":"/tags/ROC-curve/"},{"name":"Confusion Metric","slug":"Confusion-Metric","link":"/tags/Confusion-Metric/"},{"name":"Cross Validation","slug":"Cross-Validation","link":"/tags/Cross-Validation/"},{"name":"Holdout","slug":"Holdout","link":"/tags/Holdout/"},{"name":"Model Evaluation","slug":"Model-Evaluation","link":"/tags/Model-Evaluation/"},{"name":"Model Selection","slug":"Model-Selection","link":"/tags/Model-Selection/"},{"name":"Markdown","slug":"Markdown","link":"/tags/Markdown/"},{"name":"Negative Sampling","slug":"Negative-Sampling","link":"/tags/Negative-Sampling/"},{"name":"Word Embedding","slug":"Word-Embedding","link":"/tags/Word-Embedding/"},{"name":"Nature Language Processing","slug":"Nature-Language-Processing","link":"/tags/Nature-Language-Processing/"},{"name":"Word vector","slug":"Word-vector","link":"/tags/Word-vector/"},{"name":"Natural Language Representations","slug":"Natural-Language-Representations","link":"/tags/Natural-Language-Representations/"},{"name":"PySpark","slug":"PySpark","link":"/tags/PySpark/"},{"name":"Data Analysis","slug":"Data-Analysis","link":"/tags/Data-Analysis/"},{"name":"KMean Clustering","slug":"KMean-Clustering","link":"/tags/KMean-Clustering/"},{"name":"Machine Learning","slug":"Machine-Learning","link":"/tags/Machine-Learning/"},{"name":"Wide&amp;Deep Model","slug":"Wide-Deep-Model","link":"/tags/Wide-Deep-Model/"},{"name":"Collaborative Filtering","slug":"Collaborative-Filtering","link":"/tags/Collaborative-Filtering/"},{"name":"Bagging","slug":"Bagging","link":"/tags/Bagging/"},{"name":"Boosting","slug":"Boosting","link":"/tags/Boosting/"},{"name":"NeuralFM","slug":"NeuralFM","link":"/tags/NeuralFM/"},{"name":"DIN","slug":"DIN","link":"/tags/DIN/"},{"name":"Swing","slug":"Swing","link":"/tags/Swing/"},{"name":"EGES","slug":"EGES","link":"/tags/EGES/"},{"name":"Graph Embedding","slug":"Graph-Embedding","link":"/tags/Graph-Embedding/"},{"name":"FFM Model","slug":"FFM-Model","link":"/tags/FFM-Model/"},{"name":"Installation","slug":"Installation","link":"/tags/Installation/"},{"name":"Tutorial","slug":"Tutorial","link":"/tags/Tutorial/"},{"name":"Hypothesis Test","slug":"Hypothesis-Test","link":"/tags/Hypothesis-Test/"},{"name":"Regular Expression","slug":"Regular-Expression","link":"/tags/Regular-Expression/"},{"name":"BeautifulSoup","slug":"BeautifulSoup","link":"/tags/BeautifulSoup/"},{"name":"Web Scrapping","slug":"Web-Scrapping","link":"/tags/Web-Scrapping/"},{"name":"P-value","slug":"P-value","link":"/tags/P-value/"},{"name":"Independence Test","slug":"Independence-Test","link":"/tags/Independence-Test/"},{"name":"DCN","slug":"DCN","link":"/tags/DCN/"},{"name":"Data Mining","slug":"Data-Mining","link":"/tags/Data-Mining/"},{"name":"Blending","slug":"Blending","link":"/tags/Blending/"},{"name":"LGBM","slug":"LGBM","link":"/tags/LGBM/"},{"name":"Wide and Deep Model","slug":"Wide-and-Deep-Model","link":"/tags/Wide-and-Deep-Model/"},{"name":"Recommendation System","slug":"Recommendation-System","link":"/tags/Recommendation-System/"},{"name":"insertion sort","slug":"insertion-sort","link":"/tags/insertion-sort/"},{"name":"bubble sort","slug":"bubble-sort","link":"/tags/bubble-sort/"},{"name":"merge sort","slug":"merge-sort","link":"/tags/merge-sort/"},{"name":"quick sort","slug":"quick-sort","link":"/tags/quick-sort/"},{"name":"Boosting Machine","slug":"Boosting-Machine","link":"/tags/Boosting-Machine/"},{"name":"Wide and Deep","slug":"Wide-and-Deep","link":"/tags/Wide-and-Deep/"}],"categories":[{"name":"NLP","slug":"NLP","link":"/categories/NLP/"},{"name":"Machine Learning","slug":"Machine-Learning","link":"/categories/Machine-Learning/"},{"name":"Recommendation System","slug":"Recommendation-System","link":"/categories/Recommendation-System/"},{"name":"Deep Learning","slug":"Deep-Learning","link":"/categories/Deep-Learning/"},{"name":"Programming","slug":"Programming","link":"/categories/Programming/"},{"name":"Parallel Computing","slug":"Parallel-Computing","link":"/categories/Parallel-Computing/"},{"name":"Data Structure","slug":"Data-Structure","link":"/categories/Data-Structure/"},{"name":"Searching","slug":"Searching","link":"/categories/Searching/"},{"name":"Data Strucure","slug":"Searching/Data-Strucure","link":"/categories/Searching/Data-Strucure/"},{"name":"Icarus","slug":"Icarus","link":"/categories/Icarus/"},{"name":"Java","slug":"Java","link":"/categories/Java/"},{"name":"Hexo","slug":"Icarus/Hexo","link":"/categories/Icarus/Hexo/"},{"name":"Website","slug":"Icarus/Hexo/Website","link":"/categories/Icarus/Hexo/Website/"},{"name":"Linux","slug":"Linux","link":"/categories/Linux/"},{"name":"Machine Learning","slug":"NLP/Machine-Learning","link":"/categories/NLP/Machine-Learning/"},{"name":"PySpark","slug":"PySpark","link":"/categories/PySpark/"},{"name":"Accuracy Improvement","slug":"Machine-Learning/Accuracy-Improvement","link":"/categories/Machine-Learning/Accuracy-Improvement/"},{"name":"SQL","slug":"SQL","link":"/categories/SQL/"},{"name":"Statistic","slug":"Statistic","link":"/categories/Statistic/"},{"name":"Data Collection","slug":"Data-Collection","link":"/categories/Data-Collection/"},{"name":"Website","slug":"Website","link":"/categories/Website/"},{"name":"Report","slug":"Report","link":"/categories/Report/"},{"name":"ICarus","slug":"Website/ICarus","link":"/categories/Website/ICarus/"},{"name":"Sorting","slug":"Data-Structure/Sorting","link":"/categories/Data-Structure/Sorting/"},{"name":"Hexo","slug":"Website/ICarus/Hexo","link":"/categories/Website/ICarus/Hexo/"}]}