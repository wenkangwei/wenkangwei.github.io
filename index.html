<!doctype html>
<html lang="en"><head><meta charset="utf-8"><meta name="generator" content="Hexo 4.2.1"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta><title>Wenkang&#039;s Blog</title><meta description="second-year master student &amp;#x2F; Computer engineering &amp;#x2F; deep learning &amp;#x2F; machine learning &amp;#x2F; data science"><meta property="og:type" content="website"><meta property="og:title" content="Wenkang&#039;s Blog"><meta property="og:url" content="https://github.com/wenkangwei/"><meta property="og:site_name" content="Wenkang&#039;s Blog"><meta property="og:description" content="second-year master student &amp;#x2F; Computer engineering &amp;#x2F; deep learning &amp;#x2F; machine learning &amp;#x2F; data science"><meta property="og:locale" content="en_US"><meta property="og:image" content="https://github.com/img/og_image.png"><meta property="article:author" content="Wenkang Wei"><meta property="article:tag" content="Computer engineering"><meta property="article:tag" content=" machine learning"><meta property="article:tag" content=" data science"><meta property="twitter:card" content="summary"><meta property="twitter:image" content="/img/og_image.png"><script type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"https://github.com/wenkangwei/wenkangwei.github.io`"},"headline":"Wenkang's Blog","image":["https://github.com/img/og_image.png"],"author":{"@type":"Person","name":"Wenkang Wei"},"description":"second-year master student &#x2F; Computer engineering &#x2F; deep learning &#x2F; machine learning &#x2F; data science"}</script><link rel="icon" href="/images/icon.png"><link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.12.0/css/all.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@9.12.0/styles/atom-one-light.css"><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Ubuntu:wght@400;600&amp;family=Source+Code+Pro"><link rel="stylesheet" href="/css/default.css"><style>body>.footer,body>.navbar,body>.section{opacity:0}</style><!--!--><!--!--><!--!--><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/lightgallery@1.6.8/dist/css/lightgallery.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/justifiedGallery@3.7.0/dist/css/justifiedGallery.min.css"><!--!--><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/pace-js@1.0.2/pace.min.js"></script></head><body class="is-3-column"><nav class="navbar navbar-main"><div class="container"><div class="navbar-brand justify-content-center"><a class="navbar-item navbar-logo" href="/"><img src="/images/icon.png" alt="Wenkang&#039;s Blog" height="28"></a></div><div class="navbar-menu"><div class="navbar-start"><a class="navbar-item is-active" href="/">Home</a><a class="navbar-item" href="/archives">Archives</a><a class="navbar-item" href="/categories">Categories</a><a class="navbar-item" href="/tags">Tags</a><a class="navbar-item" href="/about">About</a></div><div class="navbar-end"><a class="navbar-item" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/ppoffice/hexo-theme-icarus"><i class="fab fa-github"></i></a><a class="navbar-item search" title="Search" href="javascript:;"><i class="fas fa-search"></i></a></div></div></div></nav><section class="section"><div class="container"><div class="columns"><div class="column order-2 column-main is-8-tablet is-8-desktop is-6-widescreen"><div class="card"><article class="card-content article" role="article"><div class="article-meta size-small is-uppercase level is-mobile"><div class="level-left"><time class="level-item" dateTime="2021-07-01T06:18:47.000Z" title="2021-07-01T06:18:47.000Z">2021-07-01</time><span class="level-item"><a class="link-muted" href="/categories/Deep-Learning/">Deep Learning</a></span><span class="level-item">32 minutes read (About 4825 words)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2021/07/01/GNN-5-ClusterGCN/">GNN-5-ClusterGCN</a></h1><div class="content"><a id="more"></a>

<h1 id="GCN-Task-5-Cluster-GCN"><a href="#GCN-Task-5-Cluster-GCN" class="headerlink" title="GCN-Task-5-Cluster GCN"></a>GCN-Task-5-Cluster GCN</h1><h2 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1. Introduction"></a>1. Introduction</h2><p>这篇文章的目的主要是理解 Cluster-GCN这篇文章的内容(要解决的问题，思路，等)和通过代码实现一下Cluster-GCN网络。 Cluster-GCN的文章可以查看: <a href="https://arxiv.org/pdf/1905.07953.pdf">https://arxiv.org/pdf/1905.07953.pdf</a><br>这篇blog的结构大概如下:</p>
<ul>
<li>解释Cluster-GCN的内容<ul>
<li>Cluster-GCN要解决的问题</li>
<li>基本思路</li>
<li>Cluster-GCN的特点和优缺点</li>
</ul>
</li>
<li>Coding实现<ul>
<li>数据集</li>
<li>Cluster-GCN模型</li>
<li>Training and Testing</li>
<li>Assignment from datawhale</li>
</ul>
</li>
<li>总结文章的重点</li>
<li>Reference 参考文献</li>
</ul>
<h2 id="2-Cluster-GCN"><a href="#2-Cluster-GCN" class="headerlink" title="2. Cluster-GCN"></a>2. Cluster-GCN</h2><p>Cluster GCN是由国立台湾大学Wei-Lin Chiang，Google research 的Yang Li和 Samy Bengio， Cho-Jui Hsieh 等人提出的GCN网络(看到Bengio等Google大牛的名字就知道这篇文章很值得一读)。文章的全称是Cluster-GCN: An Efficient Algorithm for Training Deep and Large Graph Convolutional Networks。 从名字就可以知道这个Cluster-GCN的目的是要优化深度学习和超大图网络的效率而提出来的(一般都会涉及时间和空间的复杂度分析)。这里先来看看这篇文章要解决的问题</p>
<h3 id="2-1-Problem-to-solve"><a href="#2-1-Problem-to-solve" class="headerlink" title="2.1 Problem to solve"></a>2.1 Problem to solve</h3><p>首先这篇文章提出和分析了在GNN学习里面的几个问题:</p>
<ul>
<li><p>Full-batch gradient descent:</p>
<p>  这里先做以下定义</p>
<ul>
<li><p>node的个数为N</p>
</li>
<li><p>Embedding dimension = F</p>
</li>
<li><p>GCN的layer层数为L.</p>
<p>那么在用Full batch GD 全梯度下降方法时所需的Space Complexity = O(NFL)并且计算梯度时如果node个数很多有上百万个，每个epoch里面梯度计算也是很慢的。因此这种方法不考虑</p>
</li>
</ul>
</li>
<li><p><strong>Mini-batch SGD</strong></p>
<p>  Mini-batch SGD算是对Full batch GD的方法通过随机采样降低要计算的梯度的存储空间以及加快了计算的速度。 但是mini-batch SGD会使得时间复杂度随着GCN的层数增加而指数级增长。paper原话是这么解释的:<br>  <code>  mini-batch SGD introduces a significant computational overhead due to the neighborhood expansion problem—to compute the loss on a single node at layer L, it requires that node’s neighbor nodes’ embeddings at layer L − 1, which again requires their neighbors’ embeddings at layer L − 2 and recursive ones in the downstream layers. This leads to time complexity exponential to the GCN depth.  </code><br>  用我自己的话来理解就是，以下图为例</p>
  <img src=https://raw.githubusercontent.com/wenkangwei/Datawhale-Team-Learning/main/GNN/Task-5-ClusterGCN/graph-1.png>

<pre><code>   如果在GCN第i层的第j个节点的embedding vector用$Z_ {i}^{j}$，而第i个节点的neighbor用$N(i)$表示， 那么在第0层输入层节点A的embedding就是$Z_ {A}^{0}$, $N(A)$ = {B,D}。那么我们就有以下的公式:
</code></pre>
<ul>
<li>在第1层 L1, $Z_ {A}^{1} = f(Z_ {B}^{0}, Z_ {D}^{0}, Z_ {A}^{0})$</li>
<li>在第2层 L2, $Z_ {A}^{2} = f(Z_ {B}^{1}, Z_ {D}^{1}, Z_ {A}^{1})$, 而其中又可以把$Z_ {A}^{1}$ 展开成L1层里面的式子，$Z_ {B}^{1},Z_ {D}^{1}$ 同理</li>
<li>第3层L3,  $Z_ {A}^{3} = f(Z_ {B}^{2}, Z_ {D}^{2}, Z_ {A}^{2})$, 计算时和第二步同理可以多次展开成用L1层的输入表示的形式，这么一来可以看到，随着GCN层数增加，neighborhood expansion problem 邻居展开问题就会使得梯度计算更加复杂</li>
<li>这个节点i在第j层梯度计算都取决于第j-1层前面一层的计算就是neighborhood expansion problem</li>
<li>如果GCN层数为L， 节点的平均degree为d，那么我们计算一个节点的梯度就需要$O(d ^L)$个节点embedding信息， 而由于要乘上权重矩阵W, 计算每个node的embedding需要O($F^2$) 的时间。 那么计算一个node的梯度为$O(d ^LF^2)$</li>
</ul>
</li>
</ul>
<ul>
<li>VR-GCN(variance reduction GCN), it uses variance reduction technique to reduce the size of neighborhood sampling nodes</li>
</ul>
<h3 id="2-2-How-Cluster-GCN-works"><a href="#2-2-How-Cluster-GCN-works" class="headerlink" title="2.2 How Cluster-GCN works"></a>2.2 How Cluster-GCN works</h3><h4 id="2-2-1-Cluster-GCN的思想"><a href="#2-2-1-Cluster-GCN的思想" class="headerlink" title="2.2.1 Cluster-GCN的思想"></a>2.2.1 Cluster-GCN的思想</h4><p>ClusterGCN的想法是我们能不能找到一把种将节点分成多个batch的方式，并将图划分成多个子图，使得表征利用率最大？我们通过将表征利用率的概念与图节点聚类的目标联系起来来回答这个问题。原文:<code>can we design a batch and the corresponding computation subgraph to maximize the embedding utilization?</code></p>
<p><strong>节点表征的利用率可以反映出计算的效率。</strong>考虑到一个batch有多个节点，时间与空间复杂度的计算就不是上面那样简单了，因为不同的节点同样距离远的邻接节点可以是重叠的，于是计算表征的次数可以小于最坏的情况$O(b d^{L})$。为了反映mini-batch SGD的计算效率，Cluster-GCN论文提出了**”表征利用率”<strong>的概念来描述计算效率。在训练过程中，如果节点$i$在$l$层的表征$z_{i}^{(l)}$被计算并在$l+1$层的表征计算中被重复使用$u$次，那么我们说$z_{i}^{(l)}$的表征利用率为$u$。</strong>对于随机抽样的mini-batch SGD，$u$非常小<strong>，因为图通常是大且稀疏的。假设$u$是一个小常数（节点间同样距离的邻接节点重叠率小），那么mini-batch SGD的训练方式对每个batch需要计算$O(b d^{L})$的表征，于是每次参数更新需要$O(b d^{L} F^{2})$的时间，</strong>每个epoch需要$O(N d^{L} F^{2})$的时间<strong>，这被称为</strong>邻域扩展问题**。</p>
<p>相反的是，<strong>全梯度下降训练具有最大的表征利用率</strong>——每个节点表征将在上一层被重复使用平均节点度次。因此，全梯度下降法在每个epoch中只需要计算$O(N L)$的表征，这意味着平均下来只需要$O(L)$的表征计算就可以获得一个节点的梯度。</p>
<h4 id="2-2-2-简单的Cluster-GCN方法"><a href="#2-2-2-简单的Cluster-GCN方法" class="headerlink" title="2.2.2 简单的Cluster-GCN方法"></a>2.2.2 简单的Cluster-GCN方法</h4><p>考虑到在每个batch中，我们计算一组节点（记为$\mathcal{B}$）从第$1$层到第$L$层的表征。由于图神经网络每一层的计算都使用相同的子图$A_{\mathcal{B}, \mathcal{B}}$（$\mathcal{B}$内部的边），所以表征利用率就是这个batch内边的数量，记为$|A_{\mathcal{B}, \mathcal{B}}|_{0}$。因此，<strong>为了最大限度地提高表征利用率，理想的划分batch的结果是，batch内的边尽可能多，batch之间的边尽可能少</strong>。基于这一点，我们将SGD图神经网络训练的效率与图聚类算法联系起来。</p>
<p><strong>现在我们正式学习Cluster-GCN方法</strong>。对于一个图$G$，我们将其节点划分为$c$个簇：$\mathcal{V}=[\mathcal{V}_ {1}, \cdots \mathcal{V}_ {c}]$，其中$\mathcal{V}<em>{t}$由第$t$个簇中的节点组成，对应的我们有$c$个子图：<br>$$<br>\bar{G}=[G</em>{1}, \cdots, G_{c}]=[{\mathcal{V}_ {1}, \mathcal{E}_ {1}}, \cdots,{\mathcal{V}_ {c}, \mathcal{E}_ {c}}]<br>\notag<br>$$<br>其中$\mathcal{E}<em>{t}$只由$\mathcal{V}</em>{t}$中的节点之间的边组成。经过节点重组，邻接矩阵被划分为大小为$c^{2}$的块矩阵，如下所示</p>
<p>$$<br>A=\bar{A}+\Delta=[\begin{array}{ccc}<br>A_{11} &amp; \cdots &amp; A_{1 c} \\<br>\vdots &amp; \ddots &amp; \vdots \\<br>A_{c 1} &amp; \cdots &amp; A_{c c}<br>\end{array}]<br>\tag{4}<br>$$</p>
<p>其中</p>
<p>$$<br>\bar{A}=[\begin{array}{ccc}<br>A_{11} &amp; \cdots &amp; 0 \\<br>\vdots &amp; \ddots &amp; \vdots \\<br>0 &amp; \cdots &amp; A_{c c}<br>\end{array}], \Delta=[\begin{array}{ccc}<br>0 &amp; \cdots &amp; A_ {1 c} \\<br>\vdots &amp; \ddots &amp; \vdots \\<br>A_{c 1} &amp; \cdots &amp; 0<br>\end{array}]<br>\tag{5}<br>$$</p>
<p>其中，对角线上的块$A_{t t}$是大小为$|\mathcal{V}<em>{t}| \times|\mathcal{V}</em>{t}|$的邻接矩阵，它由$G_{t}$内部的边构成。$\bar{A}$是图$\bar{G}$的邻接矩阵。$A_{s t}$由两个簇$\mathcal{V}_ {s}$和$\mathcal{V}_ {t}$之间的边构成。$\Delta$是由$A$的所有非对角线块组成的矩阵。同样，我们可以根据$[\mathcal{V}_ {1}, \cdots, \mathcal{V}_ {c}]$ 划分节点表征矩阵$X$和类别向量 $Y$，得到$[X_{1}, \cdots, X_{c}]$和$[Y_{1}, \cdots, Y_{c}]$，其中$X_{t}$和$Y_{t}$分别由$V_{t}$中节点的表征和类别组成。</p>
<p>接下来我们用块对角线邻接矩阵 $\bar{A}$ 去近似邻接矩阵$A$，这样做的好处是，<strong>完整的损失函数（公示(2）)可以根据batch分解成多个部分之和</strong>。以$\bar{A}^{\prime}$表示归一化后的$\bar{A}$，最后一层节点表征矩阵可以做如下的分解：</p>
<p>$$<br>\begin{aligned}<br>Z^{(L)} &amp;=\bar{A}^{\prime} \sigma(\bar{A}^{\prime} \sigma(\cdots \sigma(\bar{A}^{\prime} X W^{(0)}) W^{(1)}) \cdots) W^{(L-1)} \\<br>&amp;=[\begin{array}{c}<br>\bar{A}_ {11}^{\prime} \sigma(\bar{A}_ {11}^{\prime} \sigma(\cdots \sigma(\bar{A}_ {11}^{\prime} X_{1} W^{(0)}) W^{(1)}) \cdots) W^{(L-1)} \\<br>\vdots \<br>\bar{A}_ {c c}^{\prime} \sigma(\bar{A}_ {c c}^{\prime} \sigma(\cdots \sigma(\bar{A}_ {c c}^{\prime} X_{c} W^{(0)}) W^{(1)}) \cdots) W^{(L-1)}<br>\end{array}]<br>\end{aligned}<br>\tag{6}<br>$$</p>
<p>由于$\bar{A}$是块对角形式（$\bar{A}_ {t t}^{\prime}$是$\bar{A}^{\prime}$的对角线上的块），于是损失函数可以分解为<br>$$<br>\mathcal{L}_ {\bar{A}^{\prime}}=\sum_{t} \frac{|\mathcal{V}_ {t}|}{N} \mathcal{L}_ {\bar{A}_ {t t}^{\prime}} \text { and } \mathcal{L}_ {\bar{A}_ {t t}^{\prime}}=\frac{1}{|\mathcal{V}_ {t}|} \sum_{i \in \mathcal{V}_ {t}} \operatorname{loss}(y_{i}, z_{i}^{(L)})<br>\tag{7}<br>$$</p>
<p>基于公式(6)和公式(7)，在训练的每一步中，Cluster-GCN首先采样一个簇 $\mathcal{V}_ {t}$，然后根据$\mathcal{L}_ {\bar{A}^{\prime}}_ {tt}$ 的梯度进行参数更新。这种训练方式，只需要用到子图 $A_{t t}$, $X_{t}$, $Y_{t}$ 以及神经网络权重矩阵 ${W^{(l)}}_{l=1}^{L}$。 实际中，主要的计算开销在神经网络前向过程中的矩阵乘法运算（公式(6)的一个行）和梯度反向传播。</p>
<p>我们使用图节点聚类算法来划分图。<strong>图节点聚类算法将图节点分成多个簇，划分结果是簇内边的数量远多于簇间边的数量</strong>。如前所述，每个batch的表征利用率相当于簇内边的数量。直观地说，每个节点和它的邻接节点大部分情况下都位于同一个簇中，因此 $L$ 跳（L-hop）远的邻接节点大概率仍然在同一个簇中。由于我们用块对角线近似邻接矩阵$\bar{A}$代替邻接矩阵$A$，产生的误差与簇间的边的数量$\Delta$成正比，所以<strong>簇间的边越少越好</strong>。综上所述，使用图节点聚类算法对图节点划分多个簇的结果，正是我们希望得到的。</p>
<p>在下图，我们可以看到，<strong>Cluster-GCN方法可以避免巨大范围的邻域扩展</strong>（图右），因为Cluster-GCN方法将邻域扩展限制在簇内。</p>
<img src=https://raw.githubusercontent.com/wenkangwei/Datawhale-Team-Learning/main/GNN/Task-5-ClusterGCN/neighborhood-expansion.png>

<h4 id="2-2-3-Cluster-GCN实现过程"><a href="#2-2-3-Cluster-GCN实现过程" class="headerlink" title="2.2.3 Cluster-GCN实现过程"></a>2.2.3 Cluster-GCN实现过程</h4><img src=https://raw.githubusercontent.com/wenkangwei/Datawhale-Team-Learning/main/GNN/Task-5-ClusterGCN/alg-1.png>

<p>从上图可以看到, Cluster-GCN的实现流程基本是</p>
<ol>
<li>用METIS partition算法对图的节点进行分解成c个cluster</li>
<li>不断迭代， 每次迭代都随机选取q个cluster进行无放回采样node，links并形成subgraph</li>
<li>对subgraph进行预测和gradient计算</li>
<li>用adam进行node的更新和学习</li>
</ol>
<p>另外 Cluster-GCN方法提出了一个修改版的公式(9)，以更好地保持邻接节点信息和数值范围。首先给原始的$A$添加一个单位矩阵$I$，并进行归一化处理<br>$$<br>\tilde{A}=(D+I)^{-1}(A+I)<br>\tag{10}<br>$$<br>然后考虑，<br>$$<br>X^{(l+1)}=\sigma((\tilde{A}+\lambda \operatorname{diag}(\tilde{A})) X^{(l)} W^{(l)})<br>\tag{11}<br>$$</p>
<p>以上就是Cluster-GCN的每层layer的更新输出公式</p>
<h3 id="2-3-Properties"><a href="#2-3-Properties" class="headerlink" title="2.3 Properties"></a>2.3 Properties</h3><ul>
<li>Advantage<ul>
<li><p>先来看看时间和空间复杂度。 在时间上它只和layer层数 L, embedding feature的大小F以及邻接矩阵的非零的行数||A||和节点个数有关， 而空间上和batch的大小相关，相对于传统的GCN，它把指数次降到1次</p>
<img src=https://raw.githubusercontent.com/wenkangwei/Datawhale-Team-Learning/main/GNN/Task-5-ClusterGCN/Complexity.png>
</li>
<li><p>除了Time, Space complexity外, paper里面提及在大型的图数据里面如PPI, Reddit是最好的(这个可能有调参的因素在里面)</p>
<img src=https://raw.githubusercontent.com/wenkangwei/Datawhale-Team-Learning/main/GNN/Task-5-ClusterGCN/SOTA-result.png>
</li>
</ul>
</li>
<li>Shortage<ul>
<li>在收敛性上面, Cluster-GCN在layer数量超过3层之后Accuracy性能没有明显变大，反而layer到了6层之后，开始收敛不好性能变差,如下图所示<img src=https://raw.githubusercontent.com/wenkangwei/Datawhale-Team-Learning/main/GNN/Task-5-ClusterGCN/convergence.png>
</li>
</ul>
</li>
<li>Other properties<ul>
<li>ClusterGCN在做clustering对节点进行cluster partition时特意对比了 random partition和METIS clustering partition两种方法, 以及batch设计时是用多个cluster还是一个cluster作为一个batch。它表明了用METIS和 multiple clusters as a batch 更能使性能提升，loss降低更多。</li>
</ul>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h3 id="3-1-官方测试源码"><a href="#3-1-官方测试源码" class="headerlink" title="3.1 官方测试源码"></a>3.1 官方测试源码</h3><p>这里用了Reddit的dataset进行测试</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">from</span> torch.nn <span class="keyword">import</span> ModuleList</span><br><span class="line"><span class="keyword">from</span> tqdm <span class="keyword">import</span> tqdm</span><br><span class="line"><span class="keyword">from</span> torch_geometric.datasets <span class="keyword">import</span> Reddit</span><br><span class="line"><span class="keyword">from</span> torch_geometric.data <span class="keyword">import</span> ClusterData, ClusterLoader, NeighborSampler</span><br><span class="line"><span class="keyword">from</span> torch_geometric.nn <span class="keyword">import</span> SAGEConv</span><br><span class="line"></span><br><span class="line">dataset = Reddit(<span class="string">'./data/Reddit'</span>)</span><br><span class="line">data = dataset[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">cluster_data = ClusterData(data, num_parts=<span class="number">1500</span>, recursive=<span class="literal">False</span>,</span><br><span class="line">                           save_dir=dataset.processed_dir)</span><br><span class="line">train_loader = ClusterLoader(cluster_data, batch_size=<span class="number">20</span>, shuffle=<span class="literal">True</span>,</span><br><span class="line">                             num_workers=<span class="number">12</span>)</span><br><span class="line"></span><br><span class="line">subgraph_loader = NeighborSampler(data.edge_index, sizes=[<span class="number">-1</span>], batch_size=<span class="number">1024</span>,</span><br><span class="line">                                  shuffle=<span class="literal">False</span>, num_workers=<span class="number">12</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Net</span><span class="params">(torch.nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, in_channels, out_channels)</span>:</span></span><br><span class="line">        super(Net, self).__init__()</span><br><span class="line">        self.convs = ModuleList(</span><br><span class="line">            [SAGEConv(in_channels, <span class="number">128</span>),</span><br><span class="line">             SAGEConv(<span class="number">128</span>, out_channels)])</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x, edge_index)</span>:</span></span><br><span class="line">        <span class="keyword">for</span> i, conv <span class="keyword">in</span> enumerate(self.convs):</span><br><span class="line">            x = conv(x, edge_index)</span><br><span class="line">            <span class="keyword">if</span> i != len(self.convs) - <span class="number">1</span>:</span><br><span class="line">                x = F.relu(x)</span><br><span class="line">                x = F.dropout(x, p=<span class="number">0.5</span>, training=self.training)</span><br><span class="line">        <span class="keyword">return</span> F.log_softmax(x, dim=<span class="number">-1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">inference</span><span class="params">(self, x_all)</span>:</span></span><br><span class="line">        pbar = tqdm(total=x_all.size(<span class="number">0</span>) * len(self.convs))</span><br><span class="line">        pbar.set_description(<span class="string">'Evaluating'</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Compute representations of nodes layer by layer, using *all*</span></span><br><span class="line">        <span class="comment"># available edges. This leads to faster computation in contrast to</span></span><br><span class="line">        <span class="comment"># immediately computing the final representations of each batch.</span></span><br><span class="line">        <span class="keyword">for</span> i, conv <span class="keyword">in</span> enumerate(self.convs):</span><br><span class="line">            xs = []</span><br><span class="line">            <span class="keyword">for</span> batch_size, n_id, adj <span class="keyword">in</span> subgraph_loader:</span><br><span class="line">                edge_index, _, size = adj.to(device)</span><br><span class="line">                x = x_all[n_id].to(device)</span><br><span class="line">                x_target = x[:size[<span class="number">1</span>]]</span><br><span class="line">                x = conv((x, x_target), edge_index)</span><br><span class="line">                <span class="keyword">if</span> i != len(self.convs) - <span class="number">1</span>:</span><br><span class="line">                    x = F.relu(x)</span><br><span class="line">                xs.append(x.cpu())</span><br><span class="line"></span><br><span class="line">                pbar.update(batch_size)</span><br><span class="line"></span><br><span class="line">            x_all = torch.cat(xs, dim=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">        pbar.close()</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> x_all</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">device = torch.device(<span class="string">'cuda'</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">'cpu'</span>)</span><br><span class="line">model = Net(dataset.num_features, dataset.num_classes).to(device)</span><br><span class="line">optimizer = torch.optim.Adam(model.parameters(), lr=<span class="number">0.005</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train</span><span class="params">()</span>:</span></span><br><span class="line">    model.train()</span><br><span class="line"></span><br><span class="line">    total_loss = total_nodes = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> batch <span class="keyword">in</span> train_loader:</span><br><span class="line">        batch = batch.to(device)</span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line">        out = model(batch.x, batch.edge_index)</span><br><span class="line">        loss = F.nll_loss(out[batch.train_mask], batch.y[batch.train_mask])</span><br><span class="line">        loss.backward()</span><br><span class="line">        optimizer.step()</span><br><span class="line"></span><br><span class="line">        nodes = batch.train_mask.sum().item()</span><br><span class="line">        total_loss += loss.item() * nodes</span><br><span class="line">        total_nodes += nodes</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> total_loss / total_nodes</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="meta">@torch.no_grad()</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">test</span><span class="params">()</span>:</span>  <span class="comment"># Inference should be performed on the full graph.</span></span><br><span class="line">    model.eval()</span><br><span class="line"></span><br><span class="line">    out = model.inference(data.x)</span><br><span class="line">    y_pred = out.argmax(dim=<span class="number">-1</span>)</span><br><span class="line"></span><br><span class="line">    accs = []</span><br><span class="line">    <span class="keyword">for</span> mask <span class="keyword">in</span> [data.train_mask, data.val_mask, data.test_mask]:</span><br><span class="line">        correct = y_pred[mask].eq(data.y[mask]).sum().item()</span><br><span class="line">        accs.append(correct / mask.sum().item())</span><br><span class="line">    <span class="keyword">return</span> accs</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> range(<span class="number">1</span>, <span class="number">31</span>):</span><br><span class="line">    loss = train()</span><br><span class="line">    <span class="keyword">if</span> epoch % <span class="number">5</span> == <span class="number">0</span>:</span><br><span class="line">        train_acc, val_acc, test_acc = test()</span><br><span class="line">        print(<span class="string">f'Epoch: <span class="subst">&#123;epoch:<span class="number">02</span>d&#125;</span>, Loss: <span class="subst">&#123;loss:<span class="number">.4</span>f&#125;</span>, Train: <span class="subst">&#123;train_acc:<span class="number">.4</span>f&#125;</span>, '</span></span><br><span class="line">              <span class="string">f'Val: <span class="subst">&#123;val_acc:<span class="number">.4</span>f&#125;</span>, test: <span class="subst">&#123;test_acc:<span class="number">.4</span>f&#125;</span>'</span>)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        print(<span class="string">f'Epoch: <span class="subst">&#123;epoch:<span class="number">02</span>d&#125;</span>, Loss: <span class="subst">&#123;loss:<span class="number">.4</span>f&#125;</span>'</span>)</span><br></pre></td></tr></table></figure>

<pre><code>Downloading https://data.dgl.ai/dataset/reddit.zip
Extracting data/Reddit/raw/reddit.zip
Processing...
Done!
Computing METIS partitioning...
Done!


/home/wenkanw/.conda/envs/mlenv/lib/python3.8/site-packages/torch/utils/data/dataloader.py:474: UserWarning: This DataLoader will create 12 worker processes in total. Our suggested max number of worker in current system is 8, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(


Epoch: 01, Loss: 1.0684
Epoch: 02, Loss: 0.4532
Epoch: 03, Loss: 0.3874
Epoch: 04, Loss: 0.3552


Evaluating: 100%|██████████| 465930/465930 [00:42&lt;00:00, 10886.52it/s]

Epoch: 05, Loss: 0.3361, Train: 0.9568, Val: 0.9523, test: 0.9509





Epoch: 06, Loss: 0.3220
Epoch: 07, Loss: 0.3259
Epoch: 08, Loss: 0.3068
Epoch: 09, Loss: 0.2899


Evaluating: 100%|██████████| 465930/465930 [00:43&lt;00:00, 10825.21it/s]

Epoch: 10, Loss: 0.2844, Train: 0.9639, Val: 0.9517, test: 0.9523





Epoch: 11, Loss: 0.2850
Epoch: 12, Loss: 0.2700
Epoch: 13, Loss: 0.2705
Epoch: 14, Loss: 0.2696


Evaluating: 100%|██████████| 465930/465930 [00:43&lt;00:00, 10803.24it/s]

Epoch: 15, Loss: 0.2793, Train: 0.9637, Val: 0.9524, test: 0.9506





Epoch: 16, Loss: 0.2699
Epoch: 17, Loss: 0.2556
Epoch: 18, Loss: 0.2656
Epoch: 19, Loss: 0.2642


Evaluating: 100%|██████████| 465930/465930 [00:43&lt;00:00, 10797.29it/s]


Epoch: 20, Loss: 0.2491, Train: 0.9686, Val: 0.9551, test: 0.9537
Epoch: 21, Loss: 0.2450
Epoch: 22, Loss: 0.2449
Epoch: 23, Loss: 0.2456
Epoch: 24, Loss: 0.2491


Evaluating: 100%|██████████| 465930/465930 [00:43&lt;00:00, 10737.69it/s]


Epoch: 25, Loss: 0.2518, Train: 0.9572, Val: 0.9433, test: 0.9389
Epoch: 26, Loss: 0.2430
Epoch: 27, Loss: 0.2342
Epoch: 28, Loss: 0.2297
Epoch: 29, Loss: 0.2270


Evaluating: 100%|██████████| 465930/465930 [00:43&lt;00:00, 10824.00it/s]


Epoch: 30, Loss: 0.2319, Train: 0.9716, Val: 0.9514, test: 0.9517
</code></pre>
<h3 id="3-2-自己调整的代码"><a href="#3-2-自己调整的代码" class="headerlink" title="3.2 自己调整的代码"></a>3.2 自己调整的代码</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line">os.environ[<span class="string">"WITH_METIS"</span>] =<span class="string">"1"</span></span><br><span class="line">print(os.getenv(<span class="string">'WITH_METIS'</span>))</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> tqdm <span class="keyword">import</span> tqdm</span><br><span class="line"><span class="keyword">from</span> torch.nn <span class="keyword">import</span> ModuleList, functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">from</span> torch_geometric.nn <span class="keyword">import</span> SAGEConv</span><br><span class="line"><span class="keyword">from</span> torch_geometric.datasets <span class="keyword">import</span> Reddit</span><br><span class="line"><span class="keyword">from</span> torch_geometric.data <span class="keyword">import</span> ClusterData, ClusterLoader, NeighborSampler</span><br><span class="line"></span><br><span class="line">dataset = Reddit(<span class="string">'./data/Reddit'</span>)</span><br><span class="line">data = dataset[<span class="number">0</span>]</span><br><span class="line">print(dataset.num_classes)</span><br><span class="line">print(data.num_nodes)</span><br><span class="line">print(data.num_edges)</span><br><span class="line">print(data.num_features)</span><br></pre></td></tr></table></figure>

<pre><code>1
41
232965
114615892
602
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ClusterGCNNet</span><span class="params">(torch.nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, in_channels, out_channels,hidden_dim= <span class="number">128</span>,num_layers=<span class="number">4</span>)</span>:</span></span><br><span class="line">        super(ClusterGCNNet, self).__init__()</span><br><span class="line">        <span class="comment"># GraphSAGE layer</span></span><br><span class="line">        <span class="comment"># 这里参考了 原paper里面的4-layer的设定 + 128 hidden units</span></span><br><span class="line">        layer_ls=[SAGEConv(in_channels, hidden_dim)]</span><br><span class="line">        <span class="keyword">if</span> num_layers &lt;=<span class="number">2</span>:</span><br><span class="line">            layer_ls += [SAGEConv(hidden_dim, hidden_dim) <span class="keyword">for</span> i <span class="keyword">in</span> range(num_layers<span class="number">-2</span>)]</span><br><span class="line">        layer_ls.append(SAGEConv(hidden_dim, out_channels))</span><br><span class="line">        self.convs = ModuleList(layer_ls)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x, edge_index)</span>:</span></span><br><span class="line">        <span class="keyword">for</span> i, conv <span class="keyword">in</span> enumerate(self.convs):</span><br><span class="line">            x = conv(x, edge_index)</span><br><span class="line">            <span class="keyword">if</span> i != len(self.convs) - <span class="number">1</span>:</span><br><span class="line">                x = F.relu(x)</span><br><span class="line">                x = F.dropout(x, p=<span class="number">0.5</span>, training=self.training)</span><br><span class="line">        <span class="keyword">return</span> F.log_softmax(x, dim=<span class="number">-1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">inference</span><span class="params">(self, x_all, subgraph_loader,device)</span>:</span></span><br><span class="line">        pbar = tqdm(total=x_all.size(<span class="number">0</span>) * len(self.convs))</span><br><span class="line">        pbar.set_description(<span class="string">'Evaluating'</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Compute representations of nodes layer by layer, using *all*</span></span><br><span class="line">        <span class="comment"># available edges. This leads to faster computation in contrast to</span></span><br><span class="line">        <span class="comment"># immediately computing the final representations of each batch.</span></span><br><span class="line">        </span><br><span class="line">        </span><br><span class="line">        <span class="keyword">for</span> i, conv <span class="keyword">in</span> enumerate(self.convs):</span><br><span class="line">            xs = []</span><br><span class="line">            <span class="keyword">for</span> batch_size, n_id, adj <span class="keyword">in</span> subgraph_loader:</span><br><span class="line">                edge_index, _, size = adj.to(device)</span><br><span class="line">                x = x_all[n_id].to(device)</span><br><span class="line">                x_target = x[:size[<span class="number">1</span>]]</span><br><span class="line">                x = conv((x, x_target), edge_index)</span><br><span class="line">                <span class="keyword">if</span> i != len(self.convs) - <span class="number">1</span>:</span><br><span class="line">                    x = F.relu(x)</span><br><span class="line">                xs.append(x.cpu())</span><br><span class="line"></span><br><span class="line">                pbar.update(batch_size)</span><br><span class="line"></span><br><span class="line">            x_all = torch.cat(xs, dim=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">        pbar.close()</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> x_all</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train</span><span class="params">(model,optimizer)</span>:</span></span><br><span class="line">    model.train()</span><br><span class="line"></span><br><span class="line">    total_loss = total_nodes = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> batch <span class="keyword">in</span> train_loader:</span><br><span class="line">        batch = batch.to(device)</span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line">        out = model(batch.x, batch.edge_index)</span><br><span class="line">        loss = F.nll_loss(out[batch.train_mask], batch.y[batch.train_mask])</span><br><span class="line">        loss.backward()</span><br><span class="line">        optimizer.step()</span><br><span class="line"></span><br><span class="line">        nodes = batch.train_mask.sum().item()</span><br><span class="line">        total_loss += loss.item() * nodes</span><br><span class="line">        total_nodes += nodes</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> total_loss / total_nodes</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="meta">@torch.no_grad()</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">test</span><span class="params">(data, model)</span>:</span>  <span class="comment"># Inference should be performed on the full graph.</span></span><br><span class="line">    model.eval()</span><br><span class="line"></span><br><span class="line">    out = model.inference(data.x,subgraph_loader,device)</span><br><span class="line">    y_pred = out.argmax(dim=<span class="number">-1</span>)</span><br><span class="line"></span><br><span class="line">    accs = []</span><br><span class="line">    <span class="keyword">for</span> mask <span class="keyword">in</span> [data.train_mask, data.val_mask, data.test_mask]:</span><br><span class="line">        correct = y_pred[mask].eq(data.y[mask]).sum().item()</span><br><span class="line">        accs.append(correct / mask.sum().item())</span><br><span class="line">    <span class="keyword">return</span> accs</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">import</span> datetime</span><br><span class="line"><span class="comment"># def test_num_parts(data, num_partitions = [500, 1000, 1500,2000],num_layers=2):</span></span><br><span class="line">device = torch.device(<span class="string">'cuda'</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">'cpu'</span>)</span><br><span class="line"><span class="keyword">for</span> cnt <span class="keyword">in</span> range(<span class="number">1</span>):</span><br><span class="line">    num_partitions = [<span class="number">1000</span>, <span class="number">1500</span>,<span class="number">2000</span>]</span><br><span class="line">    num_layers=<span class="number">2</span></span><br><span class="line">    torch.manual_seed(<span class="number">2019</span>)</span><br><span class="line">    result = &#123;<span class="string">"num_cluster"</span>:[],<span class="string">"partition_t"</span>:[],<span class="string">"train_t"</span>:[],<span class="string">"train_acc"</span>:[] ,<span class="string">"val_acc"</span>:[],<span class="string">"test_acc"</span>:[]&#125;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> num_part <span class="keyword">in</span> num_partitions:</span><br><span class="line">        </span><br><span class="line">        </span><br><span class="line">        print(<span class="string">f"Testing number of cluster:<span class="subst">&#123;num_part&#125;</span>"</span>)</span><br><span class="line">        model = ClusterGCNNet(dataset.num_features, dataset.num_classes,num_layers=num_layers).to(device)</span><br><span class="line">        optimizer = torch.optim.Adam(model.parameters(), lr=<span class="number">0.005</span>)</span><br><span class="line">        <span class="comment"># 如果recursive = True， 它会自动用多层的二分法进行partition而不是多层k-way 分法，这样会比multilevel k-way 要慢</span></span><br><span class="line">        <span class="comment"># 可以参考paper: http://glaros.dtc.umn.edu/gkhome/node/81</span></span><br><span class="line">        </span><br><span class="line">        start_t = time.time()</span><br><span class="line">        cluster_gnn_data = ClusterData(data, num_parts =num_part, recursive=<span class="literal">False</span>, save_dir = dataset.processed_dir )</span><br><span class="line">        end_t = time.time()</span><br><span class="line">        partition_t =end_t - start_t</span><br><span class="line">        print(<span class="string">f"Partition Time: <span class="subst">&#123;partition_t&#125;</span> s"</span>)</span><br><span class="line"></span><br><span class="line">        </span><br><span class="line">        train_loader = ClusterLoader(cluster_gnn_data, batch_size= <span class="number">20</span>, shuffle= <span class="literal">True</span>, num_workers= <span class="number">16</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 采样子图的edges， 这里subgraph_loader 用于 testing里面 取样子图来测试model的infernece</span></span><br><span class="line">        subgraph_loader = NeighborSampler(data.edge_index, sizes = [<span class="number">-1</span>], shuffle=<span class="literal">False</span>,num_workers=<span class="number">16</span>)</span><br><span class="line"></span><br><span class="line">        start_t = time.time()</span><br><span class="line">        <span class="keyword">for</span> epoch <span class="keyword">in</span> range(<span class="number">1</span>, <span class="number">31</span>):</span><br><span class="line">            loss = train(model,optimizer)</span><br><span class="line">            <span class="keyword">if</span> epoch % <span class="number">5</span> == <span class="number">0</span>:</span><br><span class="line">                train_acc, val_acc, test_acc = test(data, model)</span><br><span class="line">                print(<span class="string">f'Epoch: <span class="subst">&#123;epoch:<span class="number">02</span>d&#125;</span>, Loss: <span class="subst">&#123;loss:<span class="number">.4</span>f&#125;</span>, Train: <span class="subst">&#123;train_acc:<span class="number">.4</span>f&#125;</span>, '</span></span><br><span class="line">                      <span class="string">f'Val: <span class="subst">&#123;val_acc:<span class="number">.4</span>f&#125;</span>, test: <span class="subst">&#123;test_acc:<span class="number">.4</span>f&#125;</span>'</span>)</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                print(<span class="string">f'Epoch: <span class="subst">&#123;epoch:<span class="number">02</span>d&#125;</span>, Loss: <span class="subst">&#123;loss:<span class="number">.4</span>f&#125;</span>'</span>)</span><br><span class="line">        end_t = time.time()</span><br><span class="line">        train_t =end_t - start_t</span><br><span class="line">        train_t = datetime.timedelta(seconds=train_t)</span><br><span class="line">        print(<span class="string">f"Training Time: <span class="subst">&#123;train_t&#125;</span> s"</span>)</span><br><span class="line">        </span><br><span class="line">        result[<span class="string">"num_cluster"</span>].append(num_part)</span><br><span class="line">        result[<span class="string">'partition_t'</span>].append(partition_t)</span><br><span class="line">        result[<span class="string">'train_t'</span>].append(train_t)</span><br><span class="line">        result[<span class="string">"train_acc"</span>].append(train_acc) </span><br><span class="line">        result[<span class="string">"val_acc"</span>].append(val_acc)</span><br><span class="line">        result[<span class="string">"test_acc"</span>].append(test_acc)</span><br><span class="line">        </span><br><span class="line">        torch.cuda.empty_cache()</span><br><span class="line">        <span class="keyword">del</span> model</span><br><span class="line">        <span class="comment">#return result, pd.DataFrame(result)</span></span><br><span class="line">df_res= pd.DataFrame(result)</span><br><span class="line">df_res</span><br></pre></td></tr></table></figure>


<h2 id="4-Conclusion-and-Take-away"><a href="#4-Conclusion-and-Take-away" class="headerlink" title="4. Conclusion and Take-away"></a>4. Conclusion and Take-away</h2><ul>
<li><p>ClusterGCN的成果</p>
<ul>
<li>对不同的batch，graph partition的方法进行研究</li>
<li>通过batch的设计和Clustering partition(METIS) 对GCN算法的Time, Space Complexity 在大型图数据里面有很大的提升(解决了neighborhood expansion problem问题)</li>
<li>相对于VR-GCN， 训练时间随着DNN 层数变多而增加的幅度不大。 Cluster-GCN的训练时间随着层数增多几乎是线性的。</li>
<li>能够用于训练大型embedding的特征</li>
</ul>
</li>
<li><p>Note</p>
<ul>
<li>Vanilla 在CS里面的含义<br> vanilla is the term used to refer when computer software and sometimes also other computing-related systems like computer hardware or algorithms are not customized from their original form</li>
</ul>
</li>
</ul>
<h2 id="5-Reference"><a href="#5-Reference" class="headerlink" title="5. Reference"></a>5. Reference</h2><p>[1] Datawhale: <a href="https://github.com/datawhalechina/team-learning-nlp/blob/master/GNN/Markdown%E7%89%88%E6%9C%AC/7-%E8%B6%85%E5%A4%A7%E5%9B%BE%E4%B8%8A%E7%9A%84%E8%8A%82%E7%82%B9%E8%A1%A8%E5%BE%81%E5%AD%A6%E4%B9%A0.md">https://github.com/datawhalechina/team-learning-nlp/blob/master/GNN/Markdown%E7%89%88%E6%9C%AC/7-%E8%B6%85%E5%A4%A7%E5%9B%BE%E4%B8%8A%E7%9A%84%E8%8A%82%E7%82%B9%E8%A1%A8%E5%BE%81%E5%AD%A6%E4%B9%A0.md</a><br>[2] Cluster-GCN 原文: <a href="https://arxiv.org/pdf/1905.07953.pdf">https://arxiv.org/pdf/1905.07953.pdf</a></p>
<p>[3] torch_geometric source code 参考: <a href="https://github.com/rusty1s/pytorch_geometric/blob/master/examples/cluster_gcn_reddit.py">https://github.com/rusty1s/pytorch_geometric/blob/master/examples/cluster_gcn_reddit.py</a></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>

</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta size-small is-uppercase level is-mobile"><div class="level-left"><time class="level-item" dateTime="2021-06-27T04:26:51.000Z" title="2021-06-27T04:26:51.000Z">2021-06-27</time><span class="level-item"><a class="link-muted" href="/categories/Deep-Learning/">Deep Learning</a></span><span class="level-item">19 minutes read (About 2906 words)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2021/06/27/GNN-4-EdgePrediction/">GNN-4-EdgePrediction</a></h1><div class="content"><img src= https://miro.medium.com/max/723/0*4sbvBTWVmXnEC53v.png></div><a class="article-more button is-small size-small" href="/2021/06/27/GNN-4-EdgePrediction/#more">Read More</a></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta size-small is-uppercase level is-mobile"><div class="level-left"><time class="level-item" dateTime="2021-06-22T17:08:04.000Z" title="2021-06-22T17:08:04.000Z">2021-06-22</time><span class="level-item"><a class="link-muted" href="/categories/Deep-Learning/">Deep Learning</a></span><span class="level-item">35 minutes read (About 5175 words)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2021/06/22/GNN-3-NodeEmbedding/">GNN-3-NodeEmbedding</a></h1><div class="content"><img src=https://raw.githubusercontent.com/wenkangwei/Datawhale-Team-Learning/main/GNN/Task-3-NodeEmbedding/GNN-Task-3-NodeEmbedding/GraphSAGE-process.png ></div><a class="article-more button is-small size-small" href="/2021/06/22/GNN-3-NodeEmbedding/#more">Read More</a></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta size-small is-uppercase level is-mobile"><div class="level-left"><time class="level-item" dateTime="2021-06-18T23:00:02.000Z" title="2021-06-18T23:00:02.000Z">2021-06-18</time><span class="level-item"><a class="link-muted" href="/categories/Deep-Learning/">Deep Learning</a></span><span class="level-item">an hour read (About 10012 words)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2021/06/18/GNN-2-MessagePassing/">GNN-2-MessagePassing</a></h1><div class="content"><img src=https://panonit.com/sites/default/files/Message-passing-in-oop.png></div><a class="article-more button is-small size-small" href="/2021/06/18/GNN-2-MessagePassing/#more">Read More</a></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta size-small is-uppercase level is-mobile"><div class="level-left"><time class="level-item" dateTime="2021-06-16T03:16:25.000Z" title="2021-06-16T03:16:25.000Z">2021-06-15</time><span class="level-item"><a class="link-muted" href="/categories/Deep-Learning/">Deep Learning</a></span><span class="level-item">28 minutes read (About 4125 words)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2021/06/15/GNN-1-Basic/">GNN-1-Basic</a></h1><div class="content"><img src=https://blog.tylerbuchea.com/content/images/2018/08/Graph-database-2.png></div><a class="article-more button is-small size-small" href="/2021/06/15/GNN-1-Basic/#more">Read More</a></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta size-small is-uppercase level is-mobile"><div class="level-left"><time class="level-item" dateTime="2021-05-20T21:29:11.000Z" title="2021-05-20T21:29:11.000Z">2021-05-20</time><span class="level-item"><a class="link-muted" href="/categories/Recommendation-System/">Recommendation System</a></span><span class="level-item">22 minutes read (About 3252 words)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2021/05/20/Recommendation-System-6-DCN/">Recommendation System-6-DCN</a></h1><div class="content"><img src="http://ryluo.oss-cn-chengdu.aliyuncs.com/图片cross.png" style="zoom:67%;" /></div><a class="article-more button is-small size-small" href="/2021/05/20/Recommendation-System-6-DCN/#more">Read More</a></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta size-small is-uppercase level is-mobile"><div class="level-left"><time class="level-item" dateTime="2021-05-18T07:03:39.000Z" title="2021-05-18T07:03:39.000Z">2021-05-18</time><span class="level-item"><a class="link-muted" href="/categories/Machine-Learning/">Machine Learning</a></span><span class="level-item">23 minutes read (About 3389 words)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2021/05/18/ensemble-learning-project-HappinessPrediction/">ensemble-learning-project-HappinessPrediction</a></h1><div class="content"><img src=https://vitalrecord.tamhsc.edu/wp-content/uploads/2019/07/Happiness.jpg></div><a class="article-more button is-small size-small" href="/2021/05/18/ensemble-learning-project-HappinessPrediction/#more">Read More</a></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta size-small is-uppercase level is-mobile"><div class="level-left"><time class="level-item" dateTime="2021-05-13T05:48:29.000Z" title="2021-05-13T05:48:29.000Z">2021-05-13</time><span class="level-item"><a class="link-muted" href="/categories/Machine-Learning/">Machine Learning</a></span><span class="level-item">3 minutes read (About 490 words)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2021/05/13/ensemble-learning-Stacking/">ensemble-learning-Stacking</a></h1><div class="content"><img src=https://pic1.zhimg.com/80/v2-490b92e364070e07a4bd39d514000748_720w.jpg></div><a class="article-more button is-small size-small" href="/2021/05/13/ensemble-learning-Stacking/#more">Read More</a></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta size-small is-uppercase level is-mobile"><div class="level-left"><time class="level-item" dateTime="2021-05-11T17:53:43.000Z" title="2021-05-11T17:53:43.000Z">2021-05-11</time><span class="level-item"><a class="link-muted" href="/categories/Machine-Learning/">Machine Learning</a></span><span class="level-item">an hour read (About 7396 words)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2021/05/11/ensemble-learning-blending/">Ensemble Learning - Blending</a></h1><div class="content"><img src = http://rasbt.github.io/mlxtend/user_guide/regressor/StackingRegressor_files/stackingregression_overview.png></div><a class="article-more button is-small size-small" href="/2021/05/11/ensemble-learning-blending/#more">Read More</a></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta size-small is-uppercase level is-mobile"><div class="level-left"><time class="level-item" dateTime="2021-03-27T17:59:45.000Z" title="2021-03-27T17:59:45.000Z">2021-03-27</time><span class="level-item"><a class="link-muted" href="/categories/Recommendation-System/">Recommendation System</a></span><span class="level-item">26 minutes read (About 3913 words)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2021/03/27/Recommendation-System-5-DIN/">Recommendation-System-5-DIN</a></h1><div class="content"><img src="http://ryluo.oss-cn-chengdu.aliyuncs.com/图片4.png" style="zoom: 67%;" /></div><a class="article-more button is-small size-small" href="/2021/03/27/Recommendation-System-5-DIN/#more">Read More</a></article></div><nav class="pagination is-centered mt-4" role="navigation" aria-label="pagination"><div class="pagination-previous is-invisible is-hidden-mobile"><a href="/page/0/">Previous</a></div><div class="pagination-next"><a href="/page/2/">Next</a></div><ul class="pagination-list is-hidden-mobile"><li><a class="pagination-link is-current" href="/">1</a></li><li><a class="pagination-link" href="/page/2/">2</a></li><li><a class="pagination-link" href="/page/3/">3</a></li><li><a class="pagination-link" href="/page/4/">4</a></li></ul></nav></div><div class="column column-left is-4-tablet is-4-desktop is-3-widescreen  order-1"><div class="card widget"><div class="card-content"><div class="menu"><h3 class="menu-label">Links</h3><ul class="menu-list"><li><a class="level is-mobile is-mobile" href="https://github.com/wenkangwei" target="_blank" rel="noopener"><span class="level-left"><span class="level-item">Github</span></span><span class="level-right"><span class="level-item tag">github.com</span></span></a></li></ul></div></div></div><div class="card widget"><div class="card-content"><div class="menu"><h3 class="menu-label">Categories</h3><ul class="menu-list"><li><a class="level is-mobile is-marginless" href="/categories/Data-Collection/"><span class="level-start"><span class="level-item">Data Collection</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile is-marginless" href="/categories/Data-Structure/"><span class="level-start"><span class="level-item">Data Structure</span></span><span class="level-end"><span class="level-item tag">2</span></span></a><ul class="mr-0"><li><a class="level is-mobile is-marginless" href="/categories/Data-Structure/Sorting/"><span class="level-start"><span class="level-item">Sorting</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></li><li><a class="level is-mobile is-marginless" href="/categories/Deep-Learning/"><span class="level-start"><span class="level-item">Deep Learning</span></span><span class="level-end"><span class="level-item tag">7</span></span></a></li><li><a class="level is-mobile is-marginless" href="/categories/Linux/"><span class="level-start"><span class="level-item">Linux</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile is-marginless" href="/categories/Machine-Learning/"><span class="level-start"><span class="level-item">Machine Learning</span></span><span class="level-end"><span class="level-item tag">7</span></span></a><ul class="mr-0"><li><a class="level is-mobile is-marginless" href="/categories/Machine-Learning/Accuracy-Improvement/"><span class="level-start"><span class="level-item">Accuracy Improvement</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></li><li><a class="level is-mobile is-marginless" href="/categories/NLP/"><span class="level-start"><span class="level-item">NLP</span></span><span class="level-end"><span class="level-item tag">2</span></span></a><ul class="mr-0"><li><a class="level is-mobile is-marginless" href="/categories/NLP/Machine-Learning/"><span class="level-start"><span class="level-item">Machine Learning</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li></ul></li><li><a class="level is-mobile is-marginless" href="/categories/Parallel-Computing/"><span class="level-start"><span class="level-item">Parallel Computing</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile is-marginless" href="/categories/Programming/"><span class="level-start"><span class="level-item">Programming</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile is-marginless" href="/categories/PySpark/"><span class="level-start"><span class="level-item">PySpark</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile is-marginless" href="/categories/Recommendation-System/"><span class="level-start"><span class="level-item">Recommendation System</span></span><span class="level-end"><span class="level-item tag">7</span></span></a></li><li><a class="level is-mobile is-marginless" href="/categories/Report/"><span class="level-start"><span class="level-item">Report</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile is-marginless" href="/categories/Searching/"><span class="level-start"><span class="level-item">Searching</span></span><span class="level-end"><span class="level-item tag">1</span></span></a><ul class="mr-0"><li><a class="level is-mobile is-marginless" href="/categories/Searching/Data-Strucure/"><span class="level-start"><span class="level-item">Data Strucure</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></li><li><a class="level is-mobile is-marginless" href="/categories/Statistic/"><span class="level-start"><span class="level-item">Statistic</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile is-marginless" href="/categories/Web/"><span class="level-start"><span class="level-item">Web</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></div></div></div><div class="card widget"><div class="card-content"><div class="menu"><h3 class="menu-label">Tags</h3><div class="field is-grouped is-grouped-multiline"><div class="control"><a class="tags has-addons" href="/tags/Amdahl-s-Law/"><span class="tag">Amdahl&#039;s Law</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Backpropagation/"><span class="tag">Backpropagation</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Bagging/"><span class="tag">Bagging</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/BeautifulSoup/"><span class="tag">BeautifulSoup</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Binary-Tree/"><span class="tag">Binary Tree</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Blending/"><span class="tag">Blending</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Boosting/"><span class="tag">Boosting</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Boosting-Machine/"><span class="tag">Boosting Machine</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/BuckSort/"><span class="tag">BuckSort</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/C/"><span class="tag">C++</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Collaborative-Filtering/"><span class="tag">Collaborative Filtering</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Confusion-Metric/"><span class="tag">Confusion Metric</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Convolution-Neural-Network/"><span class="tag">Convolution Neural Network</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Cross-Validation/"><span class="tag">Cross Validation</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/D3-js/"><span class="tag">D3.js</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/DCN/"><span class="tag">DCN</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/DIN/"><span class="tag">DIN</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Data-Analysis/"><span class="tag">Data Analysis</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Data-Mining/"><span class="tag">Data Mining</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Datawhale-Team-Learning/"><span class="tag">Datawhale Team Learning</span><span class="tag is-grey-lightest">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/DeepFM/"><span class="tag">DeepFM</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Dropout/"><span class="tag">Dropout</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Ensemble-Learning/"><span class="tag">Ensemble Learning</span><span class="tag is-grey-lightest">5</span></a></div><div class="control"><a class="tags has-addons" href="/tags/GNN/"><span class="tag">GNN</span><span class="tag is-grey-lightest">5</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Graph/"><span class="tag">Graph</span><span class="tag is-grey-lightest">5</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Hexo/"><span class="tag">Hexo</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Holdout/"><span class="tag">Holdout</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Hypothesis-Test/"><span class="tag">Hypothesis Test</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Icarus/"><span class="tag">Icarus</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Independence-Test/"><span class="tag">Independence Test</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/JavaScript/"><span class="tag">JavaScript</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/K-Mean-Clustering/"><span class="tag">K-Mean Clustering</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/KMean-Clustering/"><span class="tag">KMean Clustering</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/LGBM/"><span class="tag">LGBM</span><span class="tag is-grey-lightest">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Machine-Learning/"><span class="tag">Machine Learning</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Markdown/"><span class="tag">Markdown</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Model-Evaluation/"><span class="tag">Model Evaluation</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Model-Selection/"><span class="tag">Model Selection</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Natural-Language-Representations/"><span class="tag">Natural Language Representations</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Nature-Language-Processing/"><span class="tag">Nature Language Processing</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Negative-Sampling/"><span class="tag">Negative Sampling</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/NeuralFM/"><span class="tag">NeuralFM</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/P-value/"><span class="tag">P-value</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Parallel-Computing/"><span class="tag">Parallel Computing</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Parallel-Speedup/"><span class="tag">Parallel Speedup</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/PySpark/"><span class="tag">PySpark</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/ROC-curve/"><span class="tag">ROC curve</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Recommendation-System/"><span class="tag">Recommendation System</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Regression/"><span class="tag">Regression</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Regular-Expression/"><span class="tag">Regular Expression</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Sorting/"><span class="tag">Sorting</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Stacking/"><span class="tag">Stacking</span><span class="tag is-grey-lightest">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Traversal/"><span class="tag">Traversal</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Unsupervised-Learning/"><span class="tag">Unsupervised Learning</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Variable-Selection/"><span class="tag">Variable Selection</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Web-Scrapping/"><span class="tag">Web Scrapping</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Wide-and-Deep/"><span class="tag">Wide and Deep</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Wide-and-Deep-Model/"><span class="tag">Wide and Deep Model</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Wide-Deep-Model/"><span class="tag">Wide&amp;Deep Model</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Word-Embedding/"><span class="tag">Word Embedding</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Word-vector/"><span class="tag">Word vector</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/binary-search/"><span class="tag">binary search</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/bubble-sort/"><span class="tag">bubble sort</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/commands/"><span class="tag">commands</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/insertion-sort/"><span class="tag">insertion sort</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/merge-sort/"><span class="tag">merge sort</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/non-parametric-learning/"><span class="tag">non-parametric learning</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/quick-sort/"><span class="tag">quick sort</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/review/"><span class="tag">review</span><span class="tag is-grey-lightest">1</span></a></div></div></div></div></div><div class="card widget"><div class="card-content"><h3 class="menu-label">Recent</h3><article class="media"><div class="media-content size-small"><p><time dateTime="2021-07-01T06:18:47.000Z">2021-07-01</time></p><p class="title is-6"><a class="link-muted" href="/2021/07/01/GNN-5-ClusterGCN/">GNN-5-ClusterGCN</a></p><p class="is-uppercase"><a class="link-muted" href="/categories/Deep-Learning/">Deep Learning</a></p></div></article><article class="media"><div class="media-content size-small"><p><time dateTime="2021-06-27T04:26:51.000Z">2021-06-27</time></p><p class="title is-6"><a class="link-muted" href="/2021/06/27/GNN-4-EdgePrediction/">GNN-4-EdgePrediction</a></p><p class="is-uppercase"><a class="link-muted" href="/categories/Deep-Learning/">Deep Learning</a></p></div></article><article class="media"><div class="media-content size-small"><p><time dateTime="2021-06-22T17:08:04.000Z">2021-06-22</time></p><p class="title is-6"><a class="link-muted" href="/2021/06/22/GNN-3-NodeEmbedding/">GNN-3-NodeEmbedding</a></p><p class="is-uppercase"><a class="link-muted" href="/categories/Deep-Learning/">Deep Learning</a></p></div></article><article class="media"><div class="media-content size-small"><p><time dateTime="2021-06-18T23:00:02.000Z">2021-06-18</time></p><p class="title is-6"><a class="link-muted" href="/2021/06/18/GNN-2-MessagePassing/">GNN-2-MessagePassing</a></p><p class="is-uppercase"><a class="link-muted" href="/categories/Deep-Learning/">Deep Learning</a></p></div></article><article class="media"><div class="media-content size-small"><p><time dateTime="2021-06-16T03:16:25.000Z">2021-06-15</time></p><p class="title is-6"><a class="link-muted" href="/2021/06/15/GNN-1-Basic/">GNN-1-Basic</a></p><p class="is-uppercase"><a class="link-muted" href="/categories/Deep-Learning/">Deep Learning</a></p></div></article></div></div><div class="card widget"><div class="card-content"><div class="menu"><h3 class="menu-label">Archives</h3><ul class="menu-list"><li><a class="level is-mobile is-marginless" href="/archives/2021/07/"><span class="level-start"><span class="level-item">July 2021</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile is-marginless" href="/archives/2021/06/"><span class="level-start"><span class="level-item">June 2021</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li><li><a class="level is-mobile is-marginless" href="/archives/2021/05/"><span class="level-start"><span class="level-item">May 2021</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li><li><a class="level is-mobile is-marginless" href="/archives/2021/03/"><span class="level-start"><span class="level-item">March 2021</span></span><span class="level-end"><span class="level-item tag">5</span></span></a></li><li><a class="level is-mobile is-marginless" href="/archives/2021/02/"><span class="level-start"><span class="level-item">February 2021</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile is-marginless" href="/archives/2020/12/"><span class="level-start"><span class="level-item">December 2020</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile is-marginless" href="/archives/2020/11/"><span class="level-start"><span class="level-item">November 2020</span></span><span class="level-end"><span class="level-item tag">5</span></span></a></li><li><a class="level is-mobile is-marginless" href="/archives/2020/10/"><span class="level-start"><span class="level-item">October 2020</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile is-marginless" href="/archives/2020/09/"><span class="level-start"><span class="level-item">September 2020</span></span><span class="level-end"><span class="level-item tag">5</span></span></a></li><li><a class="level is-mobile is-marginless" href="/archives/2020/08/"><span class="level-start"><span class="level-item">August 2020</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile is-marginless" href="/archives/2020/07/"><span class="level-start"><span class="level-item">July 2020</span></span><span class="level-end"><span class="level-item tag">7</span></span></a></li><li><a class="level is-mobile is-marginless" href="/archives/2020/06/"><span class="level-start"><span class="level-item">June 2020</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li></ul></div></div></div><div class="card widget"><div class="card-content"><div class="menu"><h3 class="menu-label">Subscribe to Updates</h3><form action="https://feedburner.google.com/fb/a/mailverify" method="post" target="popupwindow" onsubmit="window.open(&#039;https://feedburner.google.com/fb/a/mailverify?uri=&#039;,&#039;popupwindow&#039;,&#039;scrollbars=yes,width=550,height=520&#039;);return true"><input type="hidden" value="" name="uri"><input type="hidden" name="loc" value="en_US"><div class="field has-addons"><div class="control has-icons-left is-expanded"><input class="input" name="email" type="email" placeholder="Email"><span class="icon is-small is-left"><i class="fas fa-envelope"></i></span></div><div class="control"><input class="button is-primary" type="submit" value="Subscribe"></div></div></form></div></div></div><div class="column-right-shadow is-hidden-widescreen"></div></div><div class="column column-right is-4-tablet is-4-desktop is-3-widescreen is-hidden-touch is-hidden-desktop-only order-3"><div class="card widget"><div class="card-content"><nav class="level"><div class="level-item has-text-centered flex-shrink-1"><div><figure class="image is-128x128 mx-auto mb-2"><img class="avatar is-rounded" src="/images/avatar.jpg" alt="Wenkang Wei"></figure><p class="title is-size-4 is-block line-height-inherit">Wenkang Wei</p><p class="is-size-6 is-block">computer engineering| machine learning</p><p class="is-size-6 is-flex justify-content-center"><i class="fas fa-map-marker-alt mr-1"></i><span>Clemson,SC</span></p></div></div></nav><nav class="level is-mobile"><div class="level-item has-text-centered is-marginless"><div><p class="heading">Posts</p><a href="/archives"><p class="title">40</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">Categories</p><a href="/categories"><p class="title">18</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">Tags</p><a href="/tags"><p class="title">69</p></a></div></div></nav><div class="level"><a class="level-item button is-primary is-rounded" href="https://github.com/wenkangwei" target="_blank" rel="noopener">Follow</a></div><div class="level is-mobile"><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Github" href="https://github.com/wenkangwei"><i class="fab fa-github"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="LinkedIn" href="https://www.linkedin.com/in/wenkang-wei-588811167"><i class="fab fa-linkedin"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Facebook" href="https://facebook.com"><i class="fab fa-facebook"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Twitter" href="https://twitter.com"><i class="fab fa-twitter"></i></a></div></div></div><!--!--></div></div></div></section><footer class="footer"><div class="container"><div class="level"><div class="level-start"><a class="footer-logo is-block mb-2" href="/"><img src="/images/icon.png" alt="Wenkang&#039;s Blog" height="28"></a><p class="size-small"><span>&copy; 2021 Wenkang Wei</span>  Powered by <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a> &amp; <a href="https://github.com/ppoffice/hexo-theme-icarus" target="_blank" rel="noopener">Icarus</a></p></div><div class="level-end"><div class="field has-addons"><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Creative Commons" href="https://creativecommons.org/"><i class="fab fa-creative-commons"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Attribution 4.0 International" href="https://creativecommons.org/licenses/by/4.0/"><i class="fab fa-creative-commons-by"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/ppoffice/hexo-theme-icarus"><i class="fab fa-github"></i></a></p></div></div></div></div></footer><script src="https://cdn.jsdelivr.net/npm/jquery@3.3.1/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/moment@2.22.2/min/moment-with-locales.min.js"></script><script>moment.locale("en");</script><script>var IcarusThemeSettings = {
            site: {
                url: 'https://github.com/wenkangwei/wenkangwei.github.io`',
                external_link: {"enable":true,"exclude":[]}
            },
            article: {
                highlight: {
                    clipboard: true,
                    fold: 'unfolded'
                }
            }
        };</script><script src="https://cdn.jsdelivr.net/npm/clipboard@2.0.4/dist/clipboard.min.js" defer></script><script src="/js/animation.js"></script><a id="back-to-top" title="Back to Top" href="javascript:;"><i class="fas fa-chevron-up"></i></a><script src="/js/back_to_top.js" defer></script><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/lightgallery@1.6.8/dist/js/lightgallery.min.js" defer></script><script src="https://cdn.jsdelivr.net/npm/justifiedGallery@3.7.0/dist/js/jquery.justifiedGallery.min.js" defer></script><script>window.addEventListener("load", () => {
            if (typeof $.fn.lightGallery === 'function') {
                $('.article').lightGallery({ selector: '.gallery-item' });
            }
            if (typeof $.fn.justifiedGallery === 'function') {
                if ($('.justified-gallery > p > .gallery-item').length) {
                    $('.justified-gallery > p > .gallery-item').unwrap();
                }
                $('.justified-gallery').justifiedGallery();
            }
        });</script><!--!--><!--!--><script type="text/x-mathjax-config">MathJax.Hub.Config({
            'HTML-CSS': {
                matchFontHeight: false
            },
            SVG: {
                matchFontHeight: false
            },
            CommonHTML: {
                matchFontHeight: false
            },
            tex2jax: {
                inlineMath: [
                    ['$','$'],
                    ['\\(','\\)']
                ]
            }
        });</script><script src="https://cdn.jsdelivr.net/npm/mathjax@2.7.5/unpacked/MathJax.js?config=TeX-MML-AM_CHTML" defer></script><!--!--><script src="/js/main.js" defer></script><div class="searchbox"><div class="searchbox-container"><div class="searchbox-header"><div class="searchbox-input-container"><input class="searchbox-input" type="text" placeholder="Type something..."></div><a class="searchbox-close" href="javascript:;">×</a></div><div class="searchbox-body"></div></div></div><script src="/js/insight.js" defer></script><script>document.addEventListener('DOMContentLoaded', function () {
            loadInsight({"contentUrl":"/content.json"}, {"hint":"Type something...","untitled":"(Untitled)","posts":"Posts","pages":"Pages","categories":"Categories","tags":"Tags"});
        });</script></body></html>